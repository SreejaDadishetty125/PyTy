[
  {
    "project": "dpressel/mead-baseline",
    "commit": "6555b646013e79470d53964d15850224063485b8",
    "filename": "api-examples/pretrain-paired.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/dpressel-mead-baseline/api-examples/pretrain-paired.py",
    "file_hunks_size": 17,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "api-examples/pretrain-paired.py:576:17 Unbound name [10]: Name `nn` is used but not defined in the current scope.",
    "message": " Name `nn` is used but not defined in the current scope.",
    "rule_id": "Unbound name [10]",
    "warning_line_no": 576,
    "warning_line": "class PairedLoss(nn.Module):",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nclass Average(object):\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\n\nclass PairedLoss(nn.Module):\n    \"\"\"Provide a Triplet Loss using the reversed batch for negatives\"\"\"\n    def __init__(self, model):\n        super().__init__()\n        self.score = nn.CosineSimilarity(dim=1, eps=1e-6)\n        self.model = model\n\n    def forward(self, inputs, targets):\n        # reverse the batch and use as a negative example\n        neg = targets.flip(0)\n        query = self.model.encode_query(inputs)\n        response = self.model.encode_response(targets)\n        neg_response = self.model.encode_response(neg)\n        pos_score = self.score(query, response)\n        neg_score = self.score(query, neg_response)\n        score = neg_score - pos_score\n        score = score.masked_fill(score < 0.0, 0.0).sum(0)\n        return score\n\n\nclass AllLoss(nn.Module):\n    def __init__(self, model):\n        r\"\"\"Loss from here https://arxiv.org/pdf/1705.00652.pdf see section 4\n\n        We want to minimize the negative log prob of y given x\n\n        -log P(y|x)\n\n        P(y|x) P(x) = P(x, y)                             Chain Rule of Probability\n        P(y|x) = P(x, y) / P(x)                           Algebra\n        P(y|x) = P(x, y) / \\sum_\\hat(y) P(x, y = \\hat(y)) Marginalize over all possible ys to get the probability of x\n        P_approx(y|x) = P(x, y) / \\sum_i^k P(x, y_k)      Approximate the Marginalization by just using the ys in the batch\n\n        S(x, y) is the score (cosine similarity between x and y in this case) from our neural network\n        P(x, y) = e^S(x, y)\n\n        P(y|x) = e^S(x, y) / \\sum_i^k e^S(x, y_k)\n        log P(y|x) = log( e^S(x, y) / \\sum_i^k e^S(x, y_k))\n        log P(y|x) = S(x, y) - log \\sum_i^k e^S(x, y_k)\n        -log P(y|x) = -(S(x, y) - log \\sum_i^k e^S(x, y_k))\n        \"\"\"\n        super().__init__()\n        self.score = nn.CosineSimilarity(dim=-1, eps=1e-6)\n        self.model = model\n\n    def forward(self, inputs, targets):\n        # These will get broadcast to [B, B, H]\n        query = self.model.encode_query(inputs).unsqueeze(1)  # [B, 1, H]\n        response = self.model.encode_response(targets).unsqueeze(0)  # [1, B, H]\n        # all_scores is now a batch x batch matrix where index (i, j) is the score between\n        # the i^th x vector and the j^th y vector\n        all_score = self.score(query, response) # [B, B]\n        # The diagonal has the scores of correct pair, (i, i)\n        pos_score = torch.diag(all_score)\n        # vec_log_sum_exp will calculate the batched log_sum_exp in a numerically stable way\n        # the result is a [B, 1] vector which we squeeze to make it [B] to match the diag\n        # Because we are minimizing the negative log we turned the division into a subtraction here\n        loss = pos_score - vec_log_sum_exp(all_score, -1).squeeze()\n        # Batch loss\n        loss = torch.mean(loss)\n        # minimize the negative lol\n        return -loss\n\n\ndef test_all_loss():\n    from unittest.mock import MagicMock\n    def test():\n        B = np.random.randint(2, 64)\n        H = np.random.randint(128, 256)\n        model = MagicMock()\n        rs = torch.rand(B, H)\n        qs = torch.rand(B, H)\n        model.encode_query.return_value = qs\n        model.encode_response.return_value = rs\n        all_loss = AllLoss(model)\n        loss = all_loss(None, None)\n\n        def cosine_sim(x, y):\n            return np.dot(x, y) / (np.sqrt(np.sum(np.square(x))) * np.sqrt(np.sum(np.square(y))))\n\n        def log_all_loss(q, correct, rs):\n            pos = cosine_sim(q, correct)\n            scores = []\n            for r in rs:\n                scores.append(cosine_sim(q, r))\n            scores = np.stack(scores)\n            norm = np.max(scores) + np.log(np.sum(np.exp(scores - np.max(scores))))\n            return norm - pos\n\n        def all_loss(q, correct, rs):\n            pos = np.exp(cosine_sim(q, correct))\n            norm = 0\n            for r in rs:\n                norm += np.exp(cosine_sim(q, r))\n            return pos / norm\n\n        def batched_log_all_loss(qs, rs):\n            batch_loss = []\n            for q, r in zip(qs, rs):\n                batch_loss.append(log_all_loss(q, r, rs))\n            return np.mean(np.stack(batch_loss))\n\n        def batched_all_loss(qs, rs):\n            batch_loss = []\n            for q, r in zip(qs, rs):\n                batch_loss.append(np.log(all_loss(q, r, rs)))\n            return -(np.mean(np.stack(batch_loss)))\n\n        log_all = batched_log_all_loss(qs.numpy(), rs.numpy())\n        all_ = batched_all_loss(qs.numpy(), rs.numpy())\n        np.testing.assert_allclose(loss, log_all, rtol=1e-6)\n        np.testing.assert_allclose(loss, all_, rtol=1e-6)\n\n    for _ in range(100):\n        test()\n\n\nclass PairedModel(nn.Module):\n\n    def __init__(self, embeddings, d_model, d_ff, dropout, num_heads, num_layers, stacking_layers=None, d_out=512, d_k=64):\n        super().__init__()\n        if stacking_layers is None:\n            stacking_layers = [d_model] * 3\n\n        stacking_layers = listify(stacking_layers)\n        transformer = TransformerEncoderStack(num_heads=num_heads, d_model=d_model,\n                                              pdrop=dropout, layers=num_layers, activation='gelu', d_ff=d_ff, d_k=d_k)\n        self.attention_layer = MultiHeadedAttention(2, d_model, dropout, scale=True, d_k = d_k)\n        self.transformer_layers = transformer\n        self.embedding_layers = embeddings\n        self.ff1 = DenseStack(d_model, stacking_layers + [d_out], activation='gelu')\n        self.ff2 = DenseStack(d_model, stacking_layers + [d_out], activation='gelu')\n\n    def encode_query(self, query):\n        query_mask = (query != Offsets.PAD)\n        query_length = query_mask.sum(-1)\n        query_mask = query_mask.unsqueeze(1).unsqueeze(1)\n        embedded = self.embedding_layers(query)\n        encoded_query = self.transformer_layers((embedded, query_mask))\n        encoded_query = self.attention_layer((encoded_query, encoded_query, encoded_query, query_mask))\n        encoded_query = encoded_query.sum(1) / query_length.float().unsqueeze(1)\n        encoded_query = self.ff1(encoded_query)\n        return encoded_query\n\n    def encode_response(self, response):\n        response_mask = (response != Offsets.PAD)\n        response_length = response_mask.sum(-1)\n        response_mask = response_mask.unsqueeze(1).unsqueeze(1)\n        embedded = self.embedding_layers(response)\n        encoded_response = self.transformer_layers((embedded, response_mask))\n        encoded_response = self.attention_layer((encoded_response, encoded_response, encoded_response, response_mask))\n        encoded_response = encoded_response.sum(1) / response_length.float().unsqueeze(1)\n        encoded_response = self.ff2(encoded_response)\n\n        return encoded_response\n\n    def forward(self, query, response):\n        encoded_query = self.encode_query(query)\n        encoded_response = self.encode_response(response)\n        return encoded_query, encoded_response\n\n    def create_loss(self, loss_type='all'):\n        if loss_type == 'all':\n            return AllLoss(self)\n        return PairedLoss(self)\n\ndef create_model(embeddings, d_model, d_ff, dropout, num_heads, num_layers):\n\n    model = PairedModel(embeddings, d_model, d_ff, dropout, num_heads, num_layers)\n    logger.info(model)\n",
        "source_code_len": 7727,
        "target_code": "\ndef create_model(embeddings, d_model, d_ff, dropout, num_heads, num_layers, model_type, rpr_k, d_k):\n    if len(rpr_k) == 0 or rpr_k[0] < 1:\n        rpr_k = None\n    if model_type == \"encoder-decoder\":\n        logger.info(\"Creating tied encoder decoder model\")\n        hps = {\"dsz\": d_model,\n               \"hsz\": d_model,\n               \"d_ff\": d_ff,\n               \"dropout\": dropout,\n               \"num_heads\": num_heads,\n               \"layers\": num_layers,\n               \"encoder_type\": \"transformer\",\n               \"decoder_type\": \"transformer\",\n               \"src_lengths_key\": \"x_lengths\",\n               \"d_k\": d_k,\n               \"rpr_k\": rpr_k}\n        model = TiedEmbeddingsSeq2SeqModel(embeddings, **hps)\n    else:\n        model = PairedModel(embeddings, d_model, d_ff, dropout, num_heads, num_layers, rpr_k=rpr_k, d_k=d_k)\n\n    logger.info(model)\n",
        "target_code_len": 866,
        "diff_format": "@@ -555,191 +41,22 @@\n \n-class Average(object):\n-    def __init__(self, name, fmt=':f'):\n-        self.name = name\n-        self.fmt = fmt\n-        self.val = 0\n-        self.avg = 0\n-        self.sum = 0\n-        self.count = 0\n-\n-    def update(self, val, n=1):\n-        self.val = val\n-        self.sum += val * n\n-        self.count += n\n-        self.avg = self.sum / self.count\n-\n-    def __str__(self):\n-        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n-        return fmtstr.format(**self.__dict__)\n-\n-\n-class PairedLoss(nn.Module):\n-    \"\"\"Provide a Triplet Loss using the reversed batch for negatives\"\"\"\n-    def __init__(self, model):\n-        super().__init__()\n-        self.score = nn.CosineSimilarity(dim=1, eps=1e-6)\n-        self.model = model\n-\n-    def forward(self, inputs, targets):\n-        # reverse the batch and use as a negative example\n-        neg = targets.flip(0)\n-        query = self.model.encode_query(inputs)\n-        response = self.model.encode_response(targets)\n-        neg_response = self.model.encode_response(neg)\n-        pos_score = self.score(query, response)\n-        neg_score = self.score(query, neg_response)\n-        score = neg_score - pos_score\n-        score = score.masked_fill(score < 0.0, 0.0).sum(0)\n-        return score\n-\n-\n-class AllLoss(nn.Module):\n-    def __init__(self, model):\n-        r\"\"\"Loss from here https://arxiv.org/pdf/1705.00652.pdf see section 4\n-\n-        We want to minimize the negative log prob of y given x\n-\n-        -log P(y|x)\n-\n-        P(y|x) P(x) = P(x, y)                             Chain Rule of Probability\n-        P(y|x) = P(x, y) / P(x)                           Algebra\n-        P(y|x) = P(x, y) / \\sum_\\hat(y) P(x, y = \\hat(y)) Marginalize over all possible ys to get the probability of x\n-        P_approx(y|x) = P(x, y) / \\sum_i^k P(x, y_k)      Approximate the Marginalization by just using the ys in the batch\n-\n-        S(x, y) is the score (cosine similarity between x and y in this case) from our neural network\n-        P(x, y) = e^S(x, y)\n-\n-        P(y|x) = e^S(x, y) / \\sum_i^k e^S(x, y_k)\n-        log P(y|x) = log( e^S(x, y) / \\sum_i^k e^S(x, y_k))\n-        log P(y|x) = S(x, y) - log \\sum_i^k e^S(x, y_k)\n-        -log P(y|x) = -(S(x, y) - log \\sum_i^k e^S(x, y_k))\n-        \"\"\"\n-        super().__init__()\n-        self.score = nn.CosineSimilarity(dim=-1, eps=1e-6)\n-        self.model = model\n-\n-    def forward(self, inputs, targets):\n-        # These will get broadcast to [B, B, H]\n-        query = self.model.encode_query(inputs).unsqueeze(1)  # [B, 1, H]\n-        response = self.model.encode_response(targets).unsqueeze(0)  # [1, B, H]\n-        # all_scores is now a batch x batch matrix where index (i, j) is the score between\n-        # the i^th x vector and the j^th y vector\n-        all_score = self.score(query, response) # [B, B]\n-        # The diagonal has the scores of correct pair, (i, i)\n-        pos_score = torch.diag(all_score)\n-        # vec_log_sum_exp will calculate the batched log_sum_exp in a numerically stable way\n-        # the result is a [B, 1] vector which we squeeze to make it [B] to match the diag\n-        # Because we are minimizing the negative log we turned the division into a subtraction here\n-        loss = pos_score - vec_log_sum_exp(all_score, -1).squeeze()\n-        # Batch loss\n-        loss = torch.mean(loss)\n-        # minimize the negative lol\n-        return -loss\n-\n-\n-def test_all_loss():\n-    from unittest.mock import MagicMock\n-    def test():\n-        B = np.random.randint(2, 64)\n-        H = np.random.randint(128, 256)\n-        model = MagicMock()\n-        rs = torch.rand(B, H)\n-        qs = torch.rand(B, H)\n-        model.encode_query.return_value = qs\n-        model.encode_response.return_value = rs\n-        all_loss = AllLoss(model)\n-        loss = all_loss(None, None)\n-\n-        def cosine_sim(x, y):\n-            return np.dot(x, y) / (np.sqrt(np.sum(np.square(x))) * np.sqrt(np.sum(np.square(y))))\n-\n-        def log_all_loss(q, correct, rs):\n-            pos = cosine_sim(q, correct)\n-            scores = []\n-            for r in rs:\n-                scores.append(cosine_sim(q, r))\n-            scores = np.stack(scores)\n-            norm = np.max(scores) + np.log(np.sum(np.exp(scores - np.max(scores))))\n-            return norm - pos\n-\n-        def all_loss(q, correct, rs):\n-            pos = np.exp(cosine_sim(q, correct))\n-            norm = 0\n-            for r in rs:\n-                norm += np.exp(cosine_sim(q, r))\n-            return pos / norm\n-\n-        def batched_log_all_loss(qs, rs):\n-            batch_loss = []\n-            for q, r in zip(qs, rs):\n-                batch_loss.append(log_all_loss(q, r, rs))\n-            return np.mean(np.stack(batch_loss))\n-\n-        def batched_all_loss(qs, rs):\n-            batch_loss = []\n-            for q, r in zip(qs, rs):\n-                batch_loss.append(np.log(all_loss(q, r, rs)))\n-            return -(np.mean(np.stack(batch_loss)))\n-\n-        log_all = batched_log_all_loss(qs.numpy(), rs.numpy())\n-        all_ = batched_all_loss(qs.numpy(), rs.numpy())\n-        np.testing.assert_allclose(loss, log_all, rtol=1e-6)\n-        np.testing.assert_allclose(loss, all_, rtol=1e-6)\n-\n-    for _ in range(100):\n-        test()\n-\n-\n-class PairedModel(nn.Module):\n-\n-    def __init__(self, embeddings, d_model, d_ff, dropout, num_heads, num_layers, stacking_layers=None, d_out=512, d_k=64):\n-        super().__init__()\n-        if stacking_layers is None:\n-            stacking_layers = [d_model] * 3\n-\n-        stacking_layers = listify(stacking_layers)\n-        transformer = TransformerEncoderStack(num_heads=num_heads, d_model=d_model,\n-                                              pdrop=dropout, layers=num_layers, activation='gelu', d_ff=d_ff, d_k=d_k)\n-        self.attention_layer = MultiHeadedAttention(2, d_model, dropout, scale=True, d_k = d_k)\n-        self.transformer_layers = transformer\n-        self.embedding_layers = embeddings\n-        self.ff1 = DenseStack(d_model, stacking_layers + [d_out], activation='gelu')\n-        self.ff2 = DenseStack(d_model, stacking_layers + [d_out], activation='gelu')\n-\n-    def encode_query(self, query):\n-        query_mask = (query != Offsets.PAD)\n-        query_length = query_mask.sum(-1)\n-        query_mask = query_mask.unsqueeze(1).unsqueeze(1)\n-        embedded = self.embedding_layers(query)\n-        encoded_query = self.transformer_layers((embedded, query_mask))\n-        encoded_query = self.attention_layer((encoded_query, encoded_query, encoded_query, query_mask))\n-        encoded_query = encoded_query.sum(1) / query_length.float().unsqueeze(1)\n-        encoded_query = self.ff1(encoded_query)\n-        return encoded_query\n-\n-    def encode_response(self, response):\n-        response_mask = (response != Offsets.PAD)\n-        response_length = response_mask.sum(-1)\n-        response_mask = response_mask.unsqueeze(1).unsqueeze(1)\n-        embedded = self.embedding_layers(response)\n-        encoded_response = self.transformer_layers((embedded, response_mask))\n-        encoded_response = self.attention_layer((encoded_response, encoded_response, encoded_response, response_mask))\n-        encoded_response = encoded_response.sum(1) / response_length.float().unsqueeze(1)\n-        encoded_response = self.ff2(encoded_response)\n-\n-        return encoded_response\n-\n-    def forward(self, query, response):\n-        encoded_query = self.encode_query(query)\n-        encoded_response = self.encode_response(response)\n-        return encoded_query, encoded_response\n-\n-    def create_loss(self, loss_type='all'):\n-        if loss_type == 'all':\n-            return AllLoss(self)\n-        return PairedLoss(self)\n-\n-def create_model(embeddings, d_model, d_ff, dropout, num_heads, num_layers):\n-\n-    model = PairedModel(embeddings, d_model, d_ff, dropout, num_heads, num_layers)\n+def create_model(embeddings, d_model, d_ff, dropout, num_heads, num_layers, model_type, rpr_k, d_k):\n+    if len(rpr_k) == 0 or rpr_k[0] < 1:\n+        rpr_k = None\n+    if model_type == \"encoder-decoder\":\n+        logger.info(\"Creating tied encoder decoder model\")\n+        hps = {\"dsz\": d_model,\n+               \"hsz\": d_model,\n+               \"d_ff\": d_ff,\n+               \"dropout\": dropout,\n+               \"num_heads\": num_heads,\n+               \"layers\": num_layers,\n+               \"encoder_type\": \"transformer\",\n+               \"decoder_type\": \"transformer\",\n+               \"src_lengths_key\": \"x_lengths\",\n+               \"d_k\": d_k,\n+               \"rpr_k\": rpr_k}\n+        model = TiedEmbeddingsSeq2SeqModel(embeddings, **hps)\n+    else:\n+        model = PairedModel(embeddings, d_model, d_ff, dropout, num_heads, num_layers, rpr_k=rpr_k, d_k=d_k)\n+\n     logger.info(model)\n",
        "source_code_with_indent": "\n<DED>class Average(object):\n    <IND>def __init__(self, name, fmt=':f'):\n        <IND>self.name = name\n        self.fmt = fmt\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    <DED>def update(self, val, n=1):\n        <IND>self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    <DED>def __str__(self):\n        <IND>fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\n\n<DED><DED>class PairedLoss(nn.Module):\n    <IND>\"\"\"Provide a Triplet Loss using the reversed batch for negatives\"\"\"\n    def __init__(self, model):\n        <IND>super().__init__()\n        self.score = nn.CosineSimilarity(dim=1, eps=1e-6)\n        self.model = model\n\n    <DED>def forward(self, inputs, targets):\n        # reverse the batch and use as a negative example\n        <IND>neg = targets.flip(0)\n        query = self.model.encode_query(inputs)\n        response = self.model.encode_response(targets)\n        neg_response = self.model.encode_response(neg)\n        pos_score = self.score(query, response)\n        neg_score = self.score(query, neg_response)\n        score = neg_score - pos_score\n        score = score.masked_fill(score < 0.0, 0.0).sum(0)\n        return score\n\n\n<DED><DED>class AllLoss(nn.Module):\n    <IND>def __init__(self, model):\n        <IND>r\"\"\"Loss from here https://arxiv.org/pdf/1705.00652.pdf see section 4\n\n        We want to minimize the negative log prob of y given x\n\n        -log P(y|x)\n\n        P(y|x) P(x) = P(x, y)                             Chain Rule of Probability\n        P(y|x) = P(x, y) / P(x)                           Algebra\n        P(y|x) = P(x, y) / \\sum_\\hat(y) P(x, y = \\hat(y)) Marginalize over all possible ys to get the probability of x\n        P_approx(y|x) = P(x, y) / \\sum_i^k P(x, y_k)      Approximate the Marginalization by just using the ys in the batch\n\n        S(x, y) is the score (cosine similarity between x and y in this case) from our neural network\n        P(x, y) = e^S(x, y)\n\n        P(y|x) = e^S(x, y) / \\sum_i^k e^S(x, y_k)\n        log P(y|x) = log( e^S(x, y) / \\sum_i^k e^S(x, y_k))\n        log P(y|x) = S(x, y) - log \\sum_i^k e^S(x, y_k)\n        -log P(y|x) = -(S(x, y) - log \\sum_i^k e^S(x, y_k))\n        \"\"\"\n        super().__init__()\n        self.score = nn.CosineSimilarity(dim=-1, eps=1e-6)\n        self.model = model\n\n    <DED>def forward(self, inputs, targets):\n        # These will get broadcast to [B, B, H]\n        <IND>query = self.model.encode_query(inputs).unsqueeze(1)  # [B, 1, H]\n        response = self.model.encode_response(targets).unsqueeze(0)  # [1, B, H]\n        # all_scores is now a batch x batch matrix where index (i, j) is the score between\n        # the i^th x vector and the j^th y vector\n        all_score = self.score(query, response) # [B, B]\n        # The diagonal has the scores of correct pair, (i, i)\n        pos_score = torch.diag(all_score)\n        # vec_log_sum_exp will calculate the batched log_sum_exp in a numerically stable way\n        # the result is a [B, 1] vector which we squeeze to make it [B] to match the diag\n        # Because we are minimizing the negative log we turned the division into a subtraction here\n        loss = pos_score - vec_log_sum_exp(all_score, -1).squeeze()\n        # Batch loss\n        loss = torch.mean(loss)\n        # minimize the negative lol\n        return -loss\n\n\n<DED><DED>def test_all_loss():\n    <IND>from unittest.mock import MagicMock\n    def test():\n        <IND>B = np.random.randint(2, 64)\n        H = np.random.randint(128, 256)\n        model = MagicMock()\n        rs = torch.rand(B, H)\n        qs = torch.rand(B, H)\n        model.encode_query.return_value = qs\n        model.encode_response.return_value = rs\n        all_loss = AllLoss(model)\n        loss = all_loss(None, None)\n\n        def cosine_sim(x, y):\n            <IND>return np.dot(x, y) / (np.sqrt(np.sum(np.square(x))) * np.sqrt(np.sum(np.square(y))))\n\n        <DED>def log_all_loss(q, correct, rs):\n            <IND>pos = cosine_sim(q, correct)\n            scores = []\n            for r in rs:\n                <IND>scores.append(cosine_sim(q, r))\n            <DED>scores = np.stack(scores)\n            norm = np.max(scores) + np.log(np.sum(np.exp(scores - np.max(scores))))\n            return norm - pos\n\n        <DED>def all_loss(q, correct, rs):\n            <IND>pos = np.exp(cosine_sim(q, correct))\n            norm = 0\n            for r in rs:\n                <IND>norm += np.exp(cosine_sim(q, r))\n            <DED>return pos / norm\n\n        <DED>def batched_log_all_loss(qs, rs):\n            <IND>batch_loss = []\n            for q, r in zip(qs, rs):\n                <IND>batch_loss.append(log_all_loss(q, r, rs))\n            <DED>return np.mean(np.stack(batch_loss))\n\n        <DED>def batched_all_loss(qs, rs):\n            <IND>batch_loss = []\n            for q, r in zip(qs, rs):\n                <IND>batch_loss.append(np.log(all_loss(q, r, rs)))\n            <DED>return -(np.mean(np.stack(batch_loss)))\n\n        <DED>log_all = batched_log_all_loss(qs.numpy(), rs.numpy())\n        all_ = batched_all_loss(qs.numpy(), rs.numpy())\n        np.testing.assert_allclose(loss, log_all, rtol=1e-6)\n        np.testing.assert_allclose(loss, all_, rtol=1e-6)\n\n    <DED>for _ in range(100):\n        <IND>test()\n\n\n<DED><DED>class PairedModel(nn.Module):\n\n    <IND>def __init__(self, embeddings, d_model, d_ff, dropout, num_heads, num_layers, stacking_layers=None, d_out=512, d_k=64):\n        <IND>super().__init__()\n        if stacking_layers is None:\n            <IND>stacking_layers = [d_model] * 3\n\n        <DED>stacking_layers = listify(stacking_layers)\n        transformer = TransformerEncoderStack(num_heads=num_heads, d_model=d_model,\n                                              pdrop=dropout, layers=num_layers, activation='gelu', d_ff=d_ff, d_k=d_k)\n        self.attention_layer = MultiHeadedAttention(2, d_model, dropout, scale=True, d_k = d_k)\n        self.transformer_layers = transformer\n        self.embedding_layers = embeddings\n        self.ff1 = DenseStack(d_model, stacking_layers + [d_out], activation='gelu')\n        self.ff2 = DenseStack(d_model, stacking_layers + [d_out], activation='gelu')\n\n    <DED>def encode_query(self, query):\n        <IND>query_mask = (query != Offsets.PAD)\n        query_length = query_mask.sum(-1)\n        query_mask = query_mask.unsqueeze(1).unsqueeze(1)\n        embedded = self.embedding_layers(query)\n        encoded_query = self.transformer_layers((embedded, query_mask))\n        encoded_query = self.attention_layer((encoded_query, encoded_query, encoded_query, query_mask))\n        encoded_query = encoded_query.sum(1) / query_length.float().unsqueeze(1)\n        encoded_query = self.ff1(encoded_query)\n        return encoded_query\n\n    <DED>def encode_response(self, response):\n        <IND>response_mask = (response != Offsets.PAD)\n        response_length = response_mask.sum(-1)\n        response_mask = response_mask.unsqueeze(1).unsqueeze(1)\n        embedded = self.embedding_layers(response)\n        encoded_response = self.transformer_layers((embedded, response_mask))\n        encoded_response = self.attention_layer((encoded_response, encoded_response, encoded_response, response_mask))\n        encoded_response = encoded_response.sum(1) / response_length.float().unsqueeze(1)\n        encoded_response = self.ff2(encoded_response)\n\n        return encoded_response\n\n    <DED>def forward(self, query, response):\n        <IND>encoded_query = self.encode_query(query)\n        encoded_response = self.encode_response(response)\n        return encoded_query, encoded_response\n\n    <DED>def create_loss(self, loss_type='all'):\n        <IND>if loss_type == 'all':\n            <IND>return AllLoss(self)\n        <DED>return PairedLoss(self)\n\n<DED><DED>def create_model(embeddings, d_model, d_ff, dropout, num_heads, num_layers):\n\n    <IND>model = PairedModel(embeddings, d_model, d_ff, dropout, num_heads, num_layers)\n    logger.info(model)\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n<DED>def create_model(embeddings, d_model, d_ff, dropout, num_heads, num_layers, model_type, rpr_k, d_k):\n    <IND>if len(rpr_k) == 0 or rpr_k[0] < 1:\n        <IND>rpr_k = None\n    <DED>if model_type == \"encoder-decoder\":\n        <IND>logger.info(\"Creating tied encoder decoder model\")\n        hps = {\"dsz\": d_model,\n               \"hsz\": d_model,\n               \"d_ff\": d_ff,\n               \"dropout\": dropout,\n               \"num_heads\": num_heads,\n               \"layers\": num_layers,\n               \"encoder_type\": \"transformer\",\n               \"decoder_type\": \"transformer\",\n               \"src_lengths_key\": \"x_lengths\",\n               \"d_k\": d_k,\n               \"rpr_k\": rpr_k}\n        model = TiedEmbeddingsSeq2SeqModel(embeddings, **hps)\n    <DED>else:\n        <IND>model = PairedModel(embeddings, d_model, d_ff, dropout, num_heads, num_layers, rpr_k=rpr_k, d_k=d_k)\n\n    <DED>logger.info(model)\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "dpressel/mead-baseline",
    "commit": "6555b646013e79470d53964d15850224063485b8",
    "filename": "api-examples/pretrain-seq2seq.py",
    "min_patch_found": false,
    "full_warning_msg": "api-examples/pretrain-seq2seq.py:523:27 Unbound name [10]: Name `torch` is used but not defined in the current scope.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/dpressel-mead-baseline/api-examples/pretrain-seq2seq.py'",
    "dd_fail": true
  }
]