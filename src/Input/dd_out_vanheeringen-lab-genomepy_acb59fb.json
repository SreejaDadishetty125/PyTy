[
  {
    "project": "vanheeringen-lab/genomepy",
    "commit": "acb59fbc2678caa2f7a261e53c6656ff9e6e5925",
    "filename": "genomepy/provider.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/vanheeringen-lab-genomepy/genomepy/provider.py",
    "file_hunks_size": 8,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "genomepy/provider.py:354:8 Incompatible variable type [9]: genomes_dir is declared to have type `str` but is used as type `None`.",
    "message": " genomes_dir is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 354,
    "warning_line": "        genomes_dir: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        return mapping\n\n    def download_genome(\n        self,\n        name: str,\n        genomes_dir: str = None,\n        localname: str = None,\n        mask: Optional[str] = \"soft\",\n        **kwargs,\n    ):\n        \"\"\"\n        Download a (gzipped) genome file to a specific directory\n\n        Parameters\n        ----------\n        name : str\n            Genome / species name\n\n        genomes_dir : str , optional\n            Directory to install genome\n\n        localname : str , optional\n            Custom name for your genome\n\n        mask: str , optional\n            Masking, soft, hard or none (all other strings)\n        \"\"\"\n        name = safe(name)\n        self.check_name(name)\n\n        link = self.get_genome_download_link(name, mask=mask, **kwargs)\n\n        localname = get_localname(name, localname)\n        genomes_dir = get_genomes_dir(genomes_dir, check_exist=False)\n        out_dir = os.path.join(genomes_dir, localname)\n        mkdir_p(out_dir)\n\n        logger.info(f\"Downloading genome from {self.name}. Target URL: {link}...\")\n\n        # download to tmp dir. Move genome on completion.\n        # tmp dir is in genome_dir to prevent moving the genome between disks\n        tmp_dir = mkdtemp(dir=out_dir)\n        fname = os.path.join(tmp_dir, f\"{localname}.fa\")\n\n        download_file(link, fname)\n        logger.info(\"Genome download successful, starting post processing...\")\n\n        # unzip genome\n        if link.endswith(\".tar.gz\"):\n            tar_to_bigfile(fname, fname)\n        elif link.endswith(\".gz\"):\n            os.rename(fname, fname + \".gz\")\n            gunzip_and_name(fname + \".gz\")\n\n        # process genome (e.g. masking)\n        if hasattr(self, \"_post_process_download\"):\n            self._post_process_download(\n                name=name, localname=localname, out_dir=tmp_dir, mask=mask\n            )\n\n        # transfer the genome from the tmpdir to the genome_dir\n        src = fname\n        dst = os.path.join(genomes_dir, localname, os.path.basename(fname))\n        shutil.move(src, dst)\n        rm_rf(tmp_dir)\n\n        asm_report = os.path.join(out_dir, \"assembly_report.txt\")\n        asm_acc = self.assembly_accession(self.genomes.get(name))\n        if asm_acc != \"na\":\n            self.download_assembly_report(asm_acc, asm_report)\n\n        logger.info(\"name: {}\".format(name))\n        logger.info(\"local name: {}\".format(localname))\n        logger.info(\"fasta: {}\".format(dst))\n\n        # Create readme with information\n        readme = os.path.join(genomes_dir, localname, \"README.txt\")\n        metadata = {\n            \"name\": localname,\n            \"provider\": self.name,\n            \"original name\": name,\n            \"original filename\": os.path.split(link)[-1],\n            \"assembly_accession\": asm_acc,\n            \"tax_id\": self.genome_taxid(self.genomes.get(name)),\n            \"mask\": mask,\n            \"genome url\": link,\n            \"annotation url\": \"na\",\n            \"date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        }\n        write_readme(readme, metadata)\n\n    def get_annotation_download_link(self, name, **kwargs):\n        raise NotImplementedError()\n\n    @staticmethod\n    def download_and_generate_annotation(genomes_dir, annot_url, localname):\n        \"\"\"download annotation file, convert to intermediate file and generate output files\"\"\"\n\n        # create output directory if missing\n        out_dir = os.path.join(genomes_dir, localname)\n        mkdir_p(out_dir)\n\n        # download to tmp dir. Move genome on completion.\n        # tmp dir is in genome_dir to prevent moving the genome between disks\n        tmp_dir = mkdtemp(dir=out_dir)\n        ext, gz = get_file_info(annot_url)\n        annot_file = os.path.join(tmp_dir, localname + \".annotation\" + ext)\n        download_file(annot_url, annot_file)\n\n        # unzip input file (if needed)\n        if gz:\n            cmd = \"mv {0} {1} && gunzip -f {1}\"\n            sp.check_call(cmd.format(annot_file, annot_file + \".gz\"), shell=True)\n\n        # generate intermediate file (GenePred)\n        pred_file = annot_file.replace(ext, \".gp\")\n        if \"bed\" in ext:\n            cmd = \"bedToGenePred {0} {1}\"\n        elif \"gff\" in ext:\n            cmd = \"gff3ToGenePred -geneNameAttr=gene {0} {1}\"\n        elif \"gtf\" in ext:\n            cmd = \"gtfToGenePred -ignoreGroupsWithoutExons {0} {1}\"\n        elif \"txt\" in ext:\n            # UCSC annotations only\n            with open(annot_file) as f:\n                cols = f.readline().split(\"\\t\")\n\n            # extract the genePred format columns\n            start_col = 1\n            for i, col in enumerate(cols):\n                if col in [\"+\", \"-\"]:\n                    start_col = i - 1\n                    break\n            end_col = start_col + 10\n            cmd = (\n                f\"\"\"cat {{0}} | cut -f {start_col}-{end_col} | \"\"\"\n                # knownGene.txt.gz has spotty fields, this replaces non-integer fields with zeroes\n                + \"\"\"awk 'BEGIN {{FS=OFS=\"\\t\"}} !($11 ~ /^[0-9]+$/) {{$11=\"0\"}}1' > {1}\"\"\"\n            )\n        else:\n            raise TypeError(f\"file type extension {ext} not recognized!\")\n\n        sp.check_call(cmd.format(annot_file, pred_file), shell=True)\n\n        # generate gzipped gtf file (if required)\n        gtf_file = annot_file.replace(ext, \".gtf\")\n        if \"gtf\" not in ext:\n            cmd = \"genePredToGtf -source=genomepy file {0} {1}\"\n            sp.check_call(cmd.format(pred_file, gtf_file), shell=True)\n\n        # generate gzipped bed file (if required)\n        bed_file = annot_file.replace(ext, \".bed\")\n        if \"bed\" not in ext:\n            cmd = \"genePredToBed {0} {1}\"\n            sp.check_call(cmd.format(pred_file, bed_file), shell=True)\n\n        # transfer the files from the tmpdir to the genome_dir\n        for f in [gtf_file, bed_file]:\n            src = f\n            dst = os.path.join(out_dir, os.path.basename(f))\n            shutil.move(src, dst)\n        rm_rf(tmp_dir)\n\n    def attempt_and_report(self, name, localname, link, genomes_dir):\n        if not link:\n            logger.error(\n                f\"Could not download gene annotation for {name} from {self.name}.\"\n            )\n            return\n\n        try:\n            logger.info(\n                f\"Downloading annotation from {self.name}. Target URL: {link}...\"\n            )\n            self.download_and_generate_annotation(genomes_dir, link, localname)\n            logger.info(\"Annotation download successful\")\n        except Exception:\n            raise GenomeDownloadError(\n                f\"\\nCould not download annotation for {name} from {self.name}\\n\"\n                \"If you think the annotation should be there, please file a bug report at:\\n\"\n                \"https://github.com/vanheeringen-lab/genomepy/issues\\n\"\n            )\n\n        # Add annotation URL to readme\n        readme = os.path.join(genomes_dir, localname, \"README.txt\")\n        update_readme(readme, updated_metadata={\"annotation url\": link})\n\n    def download_annotation(self, name, genomes_dir=None, localname=None, **kwargs):\n        \"\"\"\n        Download annotation file to to a specific directory\n\n        Parameters\n        ----------\n        name : str\n            Genome / species name\n\n        genomes_dir : str , optional\n            Directory to install annotation\n\n        localname : str , optional\n            Custom name for your genome\n        \"\"\"\n        self.check_name(name)\n\n        link = self.get_annotation_download_link(name, **kwargs)\n\n        localname = get_localname(name, localname)\n        genomes_dir = get_genomes_dir(genomes_dir, check_exist=False)\n        self.attempt_and_report(name, localname, link, genomes_dir)\n\n    def _search_text(self, term: str) -> Iterator[str]:\n        \"\"\"check if search term is found in the provider's genome name or description field(s)\"\"\"\n        for name, metadata in self.genomes.items():\n            if term in lower(name) or any(\n                [term in lower(metadata[f]) for f in self.description_fields]\n            ):\n                yield name\n\n    def _search_accession(self, term: str) -> Iterator[str]:\n        \"\"\"check if search term is found in the provider's accession field(s)\"\"\"\n        # cut off prefix (GCA_/GCF_) and suffix (version numbers, e.g. '.3')\n        term = term[4:].split(\".\")[0]\n        for name, metadata in self.genomes.items():\n            if any([term in str(metadata[f]) for f in self.accession_fields]):\n                yield name\n\n    def _search_taxonomy(self, term: str) -> Iterator[str]:\n        \"\"\"check if search term matches to any of the provider's taxonomy field(s)\"\"\"\n        for name, metadata in self.genomes.items():\n            if any([term == lower(metadata[f]) for f in self.taxid_fields]):\n                yield name\n\n    def search(self, term: Union[str, int]):\n        \"\"\"\n        Search for term in genome names, descriptions and taxonomy ID.\n\n        The search is case-insensitive.\n\n        Parameters\n        ----------\n        term : str, int\n            Search term, case-insensitive.\n            Can be (part of) an assembly name (e.g. hg38),\n            scientific name (Danio rerio) or assembly\n            accession (GCA_000146045/GCF_...),\n            or an exact taxonomy id (7227).\n\n        Yields\n        ------\n        tuples with name and metadata\n        \"\"\"\n        term = lower(term)\n\n        search_function = self._search_text\n        if term.startswith((\"gca_\", \"gcf_\")):\n            search_function = self._search_accession\n        if term.isdigit():\n            search_function = self._search_taxonomy\n\n        for genome in search_function(term):\n            yield self._genome_info_tuple(genome)\n\n\nregister_provider = ProviderBase.register_provider\n\n\n@register_provider(\"Ensembl\")\nclass EnsemblProvider(ProviderBase):\n    \"\"\"\n    Ensembl genome provider.\n\n    Will search both ensembl.org as well as ensemblgenomes.org.\n    The bacteria division is not yet supported.\n    \"\"\"\n\n    rest_url = \"https://rest.ensembl.org/\"\n    provider_specific_install_options = {\n        \"toplevel\": {\n            \"long\": \"toplevel\",\n            \"help\": \"always download toplevel-genome\",\n            \"flag_value\": True,\n        },\n        \"version\": {\n            \"long\": \"version\",\n            \"help\": \"select release version\",\n            \"type\": int,\n            \"default\": None,\n        },\n    }\n\n    def __init__(self):\n        self.name = \"Ensembl\"\n        self.provider_status(self.rest_url + \"info/ping?\", max_tries=2)\n        # Populate on init, so that methods can be cached\n        self.genomes = self._get_genomes(self.rest_url)\n        self.accession_fields = [\"assembly_accession\"]\n        self.taxid_fields = [\"taxonomy_id\"]\n        self.description_fields = [\n            \"name\",\n            \"scientific_name\",\n            \"url_name\",\n            \"display_name\",\n        ]\n\n    @staticmethod\n    def _request_json(rest_url, ext):\n        \"\"\"Make a REST request and return as json.\"\"\"\n        if rest_url.endswith(\"/\") and ext.startswith(\"/\"):\n            ext = ext[1:]\n\n        r = requests.get(rest_url + ext, headers={\"Content-Type\": \"application/json\"})\n\n        if not r.ok:\n            r.raise_for_status()\n\n        return r.json()\n\n    @cache(ignore=[\"self\"])\n    def _get_genomes(self, rest_url):\n        logger.info(\"Downloading assembly summaries from Ensembl\")\n\n        genomes = {}\n        divisions = retry(self._request_json, 3, rest_url, \"info/divisions?\")\n        for division in divisions:\n            if division == \"EnsemblBacteria\":\n                continue\n            division_genomes = retry(\n                self._request_json, 3, rest_url, f\"info/genomes/division/{division}?\"\n            )\n            for genome in division_genomes:\n                genomes[safe(genome[\"assembly_name\"])] = genome\n        return genomes\n\n    def _genome_info_tuple(self, name):\n        \"\"\"tuple with assembly metadata\"\"\"\n        genome = self.genomes[name]\n        accession = self.assembly_accession(genome)\n        taxid = self.genome_taxid(genome)\n        taxid = str(taxid) if taxid != 0 else \"na\"\n\n        return (\n            name,\n            accession,\n            genome.get(\"scientific_name\", \"na\"),\n            taxid,\n            genome.get(\"genebuild\", \"na\"),\n        )\n\n    @goldfish_cache(ignore=[\"self\", \"rest_url\"])\n    def get_version(self, rest_url, vertebrates=False):\n        \"\"\"Retrieve current version from Ensembl FTP.\"\"\"\n        ext = \"/info/data/?\" if vertebrates else \"/info/eg_version?\"\n        ret = retry(self._request_json, 3, rest_url, ext)\n        releases = ret[\"releases\"] if vertebrates else [ret[\"version\"]]\n        return str(max(releases))\n\n    def get_genome_download_link(self, name, mask=\"soft\", **kwargs):\n        \"\"\"\n        Return Ensembl http or ftp link to the genome sequence\n\n        Parameters\n        ----------\n        name : str\n            Genome name. Current implementation will fail if exact\n            name is not found.\n\n        mask : str , optional\n            Masking level. Options: soft, hard or none. Default is soft.\n\n        Returns\n        ------\n        str with the http/ftp download link.\n        \"\"\"\n        genome = self.genomes[safe(name)]\n\n        # parse the division\n        division = genome[\"division\"].lower().replace(\"ensembl\", \"\")\n        if division == \"bacteria\":\n            raise NotImplementedError(\"bacteria from ensembl not yet supported\")\n\n        ftp_site = \"ftp://ftp.ensemblgenomes.org/pub\"\n        if division == \"vertebrates\":\n            ftp_site = \"ftp://ftp.ensembl.org/pub\"\n\n        # Ensembl release version\n        version = kwargs.get(\"version\")\n        if version is None:\n            version = self.get_version(self.rest_url, division == \"vertebrates\")\n\n        # division dependent url format\n        ftp_dir = \"{}/release-{}/fasta/{}/dna\".format(\n            division, version, genome[\"url_name\"].lower()\n        )\n        if division == \"vertebrates\":\n            ftp_dir = \"release-{}/fasta/{}/dna\".format(\n                version, genome[\"url_name\"].lower()\n            )\n        url = f\"{ftp_site}/{ftp_dir}\"\n\n        # masking and assembly level\n        def get_url(level=\"toplevel\"):\n            masks = {\"soft\": \"dna_sm.{}\", \"hard\": \"dna_rm.{}\", \"none\": \"dna.{}\"}\n            pattern = masks[mask].format(level)\n\n            asm_url = \"{}/{}.{}.{}.fa.gz\".format(\n                url,\n                genome[\"url_name\"].capitalize(),\n                re.sub(r\"\\.p\\d+$\", \"\", safe(genome[\"assembly_name\"])),\n                pattern,\n            )\n            return asm_url\n\n        # try to get the (much smaller) primary assembly,\n        # unless specified otherwise\n        link = get_url(\"primary_assembly\")\n        if kwargs.get(\"toplevel\") or not check_url(link, 2):\n            link = get_url()\n\n        if check_url(link, 2):\n            return link\n\n        raise GenomeDownloadError(\n            f\"Could not download genome {name} from {self.name}.\\n\"\n            \"URL is broken. Select another genome or provider.\\n\"\n            f\"Broken URL: {link}\"\n        )\n\n    def get_annotation_download_link(self, name, **kwargs):\n        \"\"\"\n        Parse and test the link to the Ensembl annotation file.\n\n        Parameters\n        ----------\n        name : str\n            Genome name\n        kwargs: dict , optional:\n            Provider specific options.\n\n            version : int , optional\n                Ensembl version. By default the latest version is used.\n        \"\"\"\n        genome = self.genomes[safe(name)]\n        division = genome[\"division\"].lower().replace(\"ensembl\", \"\")\n\n        ftp_site = \"ftp://ftp.ensemblgenomes.org/pub\"\n        if division == \"vertebrates\":\n            ftp_site = \"ftp://ftp.ensembl.org/pub\"\n\n        # Ensembl release version\n        version = kwargs.get(\"version\")\n        if version is None:\n            version = self.get_version(self.rest_url, division == \"vertebrates\")\n\n        if division != \"vertebrates\":\n            ftp_site += f\"/{division}\"\n\n        # Get the GTF URL\n        base_url = ftp_site + \"/release-{}/gtf/{}/{}.{}.{}.gtf.gz\"\n        safe_name = re.sub(r\"\\.p\\d+$\", \"\", name)\n        link = base_url.format(\n            version,\n            genome[\"url_name\"].lower(),\n            genome[\"url_name\"].capitalize(),\n            safe_name,\n            version,\n        )\n\n        if check_url(link, 2):\n            return link\n\n\n@register_provider(\"UCSC\")\nclass UcscProvider(ProviderBase):\n    \"\"\"\n    UCSC genome provider.\n\n    The UCSC API REST server is used to search and list genomes.\n    \"\"\"\n\n    base_url = \"http://hgdownload.soe.ucsc.edu/goldenPath\"\n    ucsc_url = base_url + \"/{0}/bigZips/chromFa.tar.gz\"\n    ucsc_url_masked = base_url + \"/{0}/bigZips/chromFaMasked.tar.gz\"\n    alt_ucsc_url = base_url + \"/{0}/bigZips/{0}.fa.gz\"\n    alt_ucsc_url_masked = base_url + \"/{0}/bigZips/{0}.fa.masked.gz\"\n    rest_url = \"http://api.genome.ucsc.edu/list/ucscGenomes\"\n    provider_specific_install_options = {\n        \"ucsc_annotation_type\": {\n            \"long\": \"annotation\",\n            \"help\": \"specify annotation to download: UCSC, Ensembl, NCBI_refseq or UCSC_refseq\",\n            \"default\": None,\n        },\n    }\n\n    def __init__(self):\n        self.name = \"UCSC\"\n        self.provider_status(self.base_url)\n        # Populate on init, so that methods can be cached\n        self.genomes = self._get_genomes(self.rest_url)\n        self.accession_fields = []\n        self.taxid_fields = [\"taxId\"]\n        self.description_fields = [\"description\", \"scientificName\"]\n\n    @staticmethod\n    @cache\n    def _get_genomes(rest_url):\n        logger.info(\"Downloading assembly summaries from UCSC\")\n\n        r = requests.get(rest_url, headers={\"Content-Type\": \"application/json\"})\n        if not r.ok:\n            r.raise_for_status()\n        ucsc_json = r.json()\n        genomes = ucsc_json[\"ucscGenomes\"]\n        return genomes\n\n    def _search_accession(self, term: str) -> Iterator[str]:\n        \"\"\"\n        UCSC does not store assembly accessions.\n        This function searches NCBI (most genomes + stable accession IDs),\n        then uses the NCBI accession search results for a UCSC text search.\n\n        Parameters\n        ----------\n        term : str\n            Assembly accession, GCA_/GCF_....\n\n        Yields\n        ------\n        genome names\n        \"\"\"\n        # NCBI provides a consistent assembly accession. This can be used to\n        # retrieve the species, and then search for that.\n        p = ProviderBase.create(\"NCBI\")\n        ncbi_genomes = list(p._search_accession(term))  # noqa\n\n        # remove superstrings (keep GRCh38, not GRCh38.p1 to GRCh38.p13)\n        unique_ncbi_genomes = []\n        for i in ncbi_genomes:\n            if sum([j in i for j in ncbi_genomes]) == 1:\n                unique_ncbi_genomes.append(i)\n\n        # add NCBI organism names to search terms\n        organism_names = [\n            p.genomes[name][\"organism_name\"] for name in unique_ncbi_genomes\n        ]\n        terms = list(set(unique_ncbi_genomes + organism_names))\n\n        # search with NCBI results in the given provider\n        for name, metadata in self.genomes.items():\n            for term in terms:\n                term = lower(term)\n                if term in lower(name) or any(\n                    [term in lower(metadata[f]) for f in self.description_fields]\n                ):\n                    yield name\n                    break  # max one hit per genome\n\n    @staticmethod\n    @cache\n    def assembly_accession(genome):\n        \"\"\"Return the assembly accession (GCA_/GCF_....) for a genome.\n\n        UCSC does not serve the assembly accession through the REST API.\n        Therefore, the readme.html is scanned for an assembly accession. If it is\n        not found, the linked NCBI assembly page will be checked.\n\n        Parameters\n        ----------\n        genome : dict\n            provider metadata dict of a genome.\n\n        Returns\n        ------\n        str\n            Assembly accession.\n        \"\"\"\n        try:\n            ucsc_url = \"https://hgdownload.soe.ucsc.edu/\" + genome[\"htmlPath\"]\n            text = read_url(ucsc_url)\n        except UnicodeDecodeError:\n            return \"na\"\n\n        # example accessions: GCA_000004335.1 (ailMel1)\n        # regex: GC[AF]_ = GCA_ or GCF_, \\d = digit, \\. = period\n        accession_regex = re.compile(r\"GC[AF]_\\d{9}\\.\\d+\")\n        match = accession_regex.search(text)\n        if match:\n            return match.group(0)\n\n        # Search for an assembly link at NCBI\n        match = re.search(r\"https?://www.ncbi.nlm.nih.gov/assembly/\\d+\", text)\n        if match:\n            ncbi_url = match.group(0)\n            text = read_url(ncbi_url)\n\n            # retrieve valid assembly accessions.\n            # contains additional info, such as '(latest)' or '(suppressed)'. Unused for now.\n            valid_accessions = re.findall(r\"assembly accession:.*?GC[AF]_.*?<\", text)\n            text = \" \".join(valid_accessions)\n            match = accession_regex.search(text)\n            if match:\n                return match.group(0)\n\n        return \"na\"\n\n    def _genome_info_tuple(self, name):\n        \"\"\"tuple with assembly metadata\"\"\"\n        genome = self.genomes[name]\n        accession = self.assembly_accession(genome)\n        taxid = self.genome_taxid(genome)\n        taxid = str(taxid) if taxid != 0 else \"na\"\n\n        return (\n            name,\n            accession,\n            genome.get(\"scientificName\", \"na\"),\n            taxid,\n            genome.get(\"description\", \"na\"),\n        )\n\n    def get_genome_download_link(self, name, mask=\"soft\", **kwargs):\n        \"\"\"\n        Return UCSC http link to genome sequence\n\n        Parameters\n        ----------\n        name : str\n            Genome name. Current implementation will fail if exact\n            name is not found.\n\n        mask : str , optional\n            Masking level. Options: soft, hard or none. Default is soft.\n\n        Returns\n        ------\n        str with the http/ftp download link.\n        \"\"\"\n        # soft masked genomes. can be unmasked in _post _process_download\n        urls = [self.ucsc_url, self.alt_ucsc_url]\n        if mask == \"hard\":\n            urls = [self.ucsc_url_masked, self.alt_ucsc_url_masked]\n\n        for genome_url in urls:\n            link = genome_url.format(name)\n\n            if check_url(link, 2):\n                return link\n\n        raise GenomeDownloadError(\n            f\"Could not download genome {name} from {self.name}.\\n\"\n            \"URLs are broken. Select another genome or provider.\\n\"\n            f\"Broken URLs: {', '.join([url.format(name) for url in urls])}\"\n        )\n\n    @staticmethod\n    def _post_process_download(name, localname, out_dir, mask=\"soft\"):  # noqa\n        \"\"\"\n        Unmask a softmasked genome if required\n\n        Parameters\n        ----------\n        name : str\n            unused for the UCSC function\n\n        localname : str\n            Custom name for your genome\n\n        out_dir : str\n            Output directory\n\n        mask : str , optional\n            masking level: soft/hard/none, default=soft\n        \"\"\"\n        if mask != \"none\":\n            return\n\n        logger.info(\"UCSC genomes are softmasked by default. Unmasking...\")\n\n        fa = os.path.join(out_dir, f\"{localname}.fa\")\n        old_fa = os.path.join(out_dir, f\"old_{localname}.fa\")\n        os.rename(fa, old_fa)\n        with open(old_fa) as old, open(fa, \"w\") as new:\n            for line in old:\n                if line.startswith(\">\"):\n                    new.write(line)\n                else:\n                    new.write(line.upper())\n\n    def get_annotation_download_link(self, name, **kwargs):\n        \"\"\"\n        Parse and test the link to the UCSC annotation file.\n\n        Will check UCSC, Ensembl, NCBI RefSeq and UCSC RefSeq annotation, respectively.\n        More info on the annotation file on: https://genome.ucsc.edu/FAQ/FAQgenes.html#whatdo\n\n        Parameters\n        ----------\n        name : str\n            Genome name\n        \"\"\"\n        gtf_url = f\"http://hgdownload.soe.ucsc.edu/goldenPath/{name}/bigZips/genes/\"\n        txt_url = f\"http://hgdownload.cse.ucsc.edu/goldenPath/{name}/database/\"\n        annot_files = {\n            \"ucsc\": \"knownGene\",\n            \"ensembl\": \"ensGene\",\n            \"ncbi_refseq\": \"ncbiRefSeq\",\n            \"ucsc_refseq\": \"refGene\",\n        }\n\n        # download gtf format if possible, txt format if not\n        gtfs_exists = check_url(gtf_url, 2)\n        base_url = gtf_url + name + \".\" if gtfs_exists else txt_url\n        base_ext = \".gtf.gz\" if gtfs_exists else \".txt.gz\"\n\n        # download specified annotation type if requested\n        file = kwargs.get(\"ucsc_annotation_type\")\n        if file:\n            link = base_url + annot_files[file.lower()] + base_ext\n            if check_url(link, 2):\n                return link\n            logger.warning(f\"Specified annotation type ({file}) not found for {name}.\")\n\n        else:\n            # download first available annotation type found\n            for file in annot_files.values():\n                link = base_url + file + base_ext\n                if check_url(link, 2):\n                    return link\n\n\n@register_provider(\"NCBI\")\nclass NcbiProvider(ProviderBase):\n    \"\"\"\n    NCBI genome provider.\n\n    Uses the assembly reports page to search and list genomes.\n    \"\"\"\n\n    assembly_url = \"https://ftp.ncbi.nlm.nih.gov/genomes/ASSEMBLY_REPORTS/\"\n    provider_specific_install_options = {}\n\n    def __init__(self):\n        self.name = \"NCBI\"\n        self.provider_status(self.assembly_url)\n        # Populate on init, so that methods can be cached\n        self.genomes = self._get_genomes(self.assembly_url)\n        self.accession_fields = [\"assembly_accession\", \"gbrs_paired_asm\"]\n        self.taxid_fields = [\"species_taxid\", \"taxid\"]\n        self.description_fields = [\n            \"submitter\",\n            \"organism_name\",\n            \"assembly_accession\",\n            \"gbrs_paired_asm\",\n            \"paired_asm_comp\",\n        ]\n\n    @staticmethod\n    @cache\n    def _get_genomes(assembly_url):\n        \"\"\"Parse genomes from assembly summary txt files.\"\"\"\n        logger.info(\n            \"Downloading assembly summaries from NCBI, this will take a while...\"\n        )\n\n        def load_summary(url):\n            \"\"\"\n            lazy loading of the url so we can parse while downloading\n            \"\"\"\n            for row in urlopen(url):\n                yield row\n\n        genomes = {}\n        # order is important as asm_name can repeat (overwriting the older name)\n        names = [\n            \"assembly_summary_genbank_historical.txt\",\n            \"assembly_summary_refseq_historical.txt\",\n            \"assembly_summary_genbank.txt\",\n            \"assembly_summary_refseq.txt\",\n        ]\n        for fname in names:\n            lines = load_summary(f\"{assembly_url}/{fname}\")\n            _ = next(lines)  # line 0 = comment\n            header = (\n                next(lines).decode(\"utf-8\").strip(\"# \").strip(\"\\n\").split(\"\\t\")\n            )  # line 1 = header\n            for line in tqdm(lines, desc=fname[17:-4], unit_scale=1, unit=\" genomes\"):\n                line = line.decode(\"utf-8\").strip(\"\\n\").split(\"\\t\")\n                if line[19] != \"na\":  # ftp_path must exist\n                    name = safe(line[15])  # overwrites older asm_names\n                    genomes[name] = dict(zip(header, line))\n        return genomes\n\n    def _genome_info_tuple(self, name):\n        \"\"\"tuple with assembly metadata\"\"\"\n        genome = self.genomes[name]\n        accession = self.assembly_accession(genome)\n        taxid = self.genome_taxid(genome)\n        taxid = str(taxid) if taxid != 0 else \"na\"\n\n        return (\n            name,\n            accession,\n            genome.get(\"organism_name\", \"na\"),\n            taxid,\n            genome.get(\"submitter\", \"na\"),\n        )\n\n    def get_genome_download_link(self, name, mask=\"soft\", **kwargs):\n        \"\"\"\n        Return NCBI ftp link to top-level genome sequence\n\n        Parameters\n        ----------\n        name : str\n            Genome name. Current implementation will fail if exact\n            name is not found.\n\n        mask : str , optional\n            Masking level. Options: soft, hard or none. Default is soft.\n\n        Returns\n        ------\n        str with the http/ftp download link.\n        \"\"\"\n        # only soft masked genomes available. can be (un)masked in _post_process_download\n        link = self._ftp_or_html_link(name, file_suffix=\"_genomic.fna.gz\")\n\n        if link:\n            return link\n\n        raise GenomeDownloadError(\n            f\"Could not download genome {name} from {self.name}.\\n\"\n            \"URL is broken. Select another genome or provider.\\n\"\n            f\"Broken URL: {link}\"\n        )\n\n    def _post_process_download(self, name, localname, out_dir, mask=\"soft\"):\n        \"\"\"\n        Replace accessions with sequence names in fasta file.\n\n        Applies masking.\n\n        Parameters\n        ----------\n        name : str\n            NCBI genome name\n\n        localname : str\n            Custom name for your genome\n\n        out_dir : str\n            Output directory\n\n        mask : str , optional\n            masking level: soft/hard/none, default=soft\n        \"\"\"\n        # Create mapping of accessions to names\n        url = self._ftp_or_html_link(\n            name, file_suffix=\"_assembly_report.txt\", skip_check=True\n        )\n\n        tr = {}\n        with urlopen(url) as response:\n            for line in response.read().decode(\"utf-8\").splitlines():\n                if line.startswith(\"#\"):\n                    continue\n                vals = line.strip().split(\"\\t\")\n                tr[vals[6]] = vals[0]\n\n        # mask sequence if required\n        if mask == \"soft\":\n\n            def mask_cmd(txt):\n                return txt\n\n        elif mask == \"hard\":\n            logger.info(\"NCBI genomes are softmasked by default. Hard masking...\")\n\n            def mask_cmd(txt):\n                return re.sub(\"[actg]\", \"N\", txt)\n\n        else:\n            logger.info(\"NCBI genomes are softmasked by default. Unmasking...\")\n\n            def mask_cmd(txt):\n                return txt.upper()\n\n        # apply mapping and masking\n        fa = os.path.join(out_dir, f\"{localname}.fa\")\n        old_fa = os.path.join(out_dir, f\"old_{localname}.fa\")\n        os.rename(fa, old_fa)\n        with open(old_fa) as old, open(fa, \"w\") as new:\n            for line in old:\n                if line.startswith(\">\"):\n                    desc = line.strip()[1:]\n                    name = desc.split(\" \")[0]\n                    new.write(\">{} {}\\n\".format(tr.get(name, name), desc))\n                else:\n                    new.write(mask_cmd(line))\n\n    def get_annotation_download_link(self, name, **kwargs):\n        \"\"\"\n        Parse and test the link to the NCBI annotation file.\n\n        Parameters\n        ----------\n        name : str\n            Genome name\n        \"\"\"\n        return self._ftp_or_html_link(name, file_suffix=\"_genomic.gff.gz\")\n\n    def _ftp_or_html_link(self, name, file_suffix, skip_check=False):\n        \"\"\"\n        NCBI's files are accessible over FTP and HTTPS\n        Try HTTPS first and return the first functioning link\n        \"\"\"\n        genome = self.genomes[safe(name)]\n        ftp_link = genome[\"ftp_path\"]\n        html_link = ftp_link.replace(\"ftp://\", \"https://\")\n        for link in [html_link, ftp_link]:\n            link += \"/\" + link.split(\"/\")[-1] + file_suffix\n\n            if skip_check or check_url(link, max_tries=2, timeout=10):\n                return link\n\n\n@register_provider(\"URL\")\nclass UrlProvider(ProviderBase):\n    \"\"\"\n    URL genome provider.\n\n    Simply download a genome directly through an url.\n    \"\"\"\n\n    provider_specific_install_options = {\n        \"to_annotation\": {\n            \"long\": \"to-annotation\",\n            \"help\": \"link to the annotation file, required if this is not in the same directory as the fasta file\",\n            \"default\": None,\n        },\n    }\n\n    def __init__(self):\n        self.name = \"URL\"\n        self.genomes = {}\n\n    def genome_taxid(self, genome):\n        return \"na\"\n\n    def assembly_accession(self, genome):\n        return \"na\"\n\n    def search(self, term):\n        \"\"\"return an empty generator,\n        same as if no genomes were found at the other providers\"\"\"\n        yield from ()\n\n    def _genome_info_tuple(self, name):\n        return tuple()\n\n    def check_name(self, name):\n        \"\"\"check if genome name can be found for provider\"\"\"\n        return\n\n    def get_genome_download_link(self, url, mask=None, **kwargs):\n        return url\n\n    def get_annotation_download_link(self, name, **kwargs):\n        \"\"\"\n        check if the linked annotation file is of a supported file type (gtf/gff3/bed)\n        \"\"\"\n        link = kwargs.get(\"to_annotation\")\n        if link:\n            ext = get_file_info(link)[0]\n            if ext not in [\".gtf\", \".gff\", \".gff3\", \".bed\"]:\n                raise TypeError(\n                    \"Only (gzipped) gtf, gff and bed files are supported.\\n\"\n                )\n\n            return link\n\n    @staticmethod\n    def search_url_for_annotations(url, name):\n        \"\"\"Attempts to find gtf or gff3 files in the same location as the genome url\"\"\"\n        urldir = os.path.dirname(url)\n        logger.info(\n            \"You have requested the gene annotation to be downloaded. \"\n            \"Genomepy will check the remote directory: \"\n            f\"{urldir} for annotation files...\"\n        )\n\n        def fuzzy_annotation_search(search_name, search_list):\n            \"\"\"Returns all files containing both name and an annotation extension\"\"\"\n            hits = []\n            for ext in [\"gtf\", \"gff\"]:\n                # .*? = non greedy filler. 3? = optional 3 (for gff3). (\\.gz)? = optional .gz\n                expr = f\"{search_name}.*?\\.{ext}3?(\\.gz)?\"  # noqa: W605\n                for line in search_list:\n                    hit = re.search(expr, line, flags=re.IGNORECASE)\n                    if hit:\n                        hits.append(hit[0])\n            return hits\n\n        # try to find a GTF or GFF3 file\n        dirty_list = [str(line) for line in urlopen(urldir).readlines()]\n        fnames = fuzzy_annotation_search(name, dirty_list)\n        if not fnames:\n            raise FileNotFoundError(\n                \"Could not parse the remote directory. \"\n                \"Please supply a URL using --url-to-annotation.\\n\"\n            )\n\n        links = [urldir + \"/\" + fname for fname in fnames]\n        return links\n\n    def download_annotation(self, url, genomes_dir=None, localname=None, **kwargs):\n        \"\"\"\n        Attempts to download a gtf or gff3 file from the same location as the genome url\n\n        Parameters\n        ----------\n        url : str\n            url of where to download genome from\n\n        genomes_dir : str\n            Directory to install annotation\n\n        localname : str , optional\n            Custom name for your genome\n\n        kwargs: dict , optional:\n            Provider specific options.\n\n            to_annotation : str , optional\n                url to annotation file (only required if this not located in the same directory as the fasta)\n        \"\"\"\n        name = get_localname(url)\n        localname = get_localname(name, localname)\n        genomes_dir = get_genomes_dir(genomes_dir, check_exist=False)\n\n        if kwargs.get(\"to_annotation\"):\n            links = [self.get_annotation_download_link(None, **kwargs)]\n        else:\n            # can return multiple possible hits\n            links = self.search_url_for_annotations(url, name)\n\n        for link in links:\n            try:\n                self.attempt_and_report(name, localname, link, genomes_dir)\n                break\n            except GenomeDownloadError as e:\n                if not link == links[-1]:\n                    logger.info(\n                        \"One of the potential annotations was incompatible with genomepy. \"\n                        \"Attempting another...\"\n                    )\n                    continue\n                return e\n",
        "source_code_len": 36065,
        "target_code": "        return mapping\n",
        "target_code_len": 23,
        "diff_format": "@@ -349,1057 +239,1 @@\n         return mapping\n-\n-    def download_genome(\n-        self,\n-        name: str,\n-        genomes_dir: str = None,\n-        localname: str = None,\n-        mask: Optional[str] = \"soft\",\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Download a (gzipped) genome file to a specific directory\n-\n-        Parameters\n-        ----------\n-        name : str\n-            Genome / species name\n-\n-        genomes_dir : str , optional\n-            Directory to install genome\n-\n-        localname : str , optional\n-            Custom name for your genome\n-\n-        mask: str , optional\n-            Masking, soft, hard or none (all other strings)\n-        \"\"\"\n-        name = safe(name)\n-        self.check_name(name)\n-\n-        link = self.get_genome_download_link(name, mask=mask, **kwargs)\n-\n-        localname = get_localname(name, localname)\n-        genomes_dir = get_genomes_dir(genomes_dir, check_exist=False)\n-        out_dir = os.path.join(genomes_dir, localname)\n-        mkdir_p(out_dir)\n-\n-        logger.info(f\"Downloading genome from {self.name}. Target URL: {link}...\")\n-\n-        # download to tmp dir. Move genome on completion.\n-        # tmp dir is in genome_dir to prevent moving the genome between disks\n-        tmp_dir = mkdtemp(dir=out_dir)\n-        fname = os.path.join(tmp_dir, f\"{localname}.fa\")\n-\n-        download_file(link, fname)\n-        logger.info(\"Genome download successful, starting post processing...\")\n-\n-        # unzip genome\n-        if link.endswith(\".tar.gz\"):\n-            tar_to_bigfile(fname, fname)\n-        elif link.endswith(\".gz\"):\n-            os.rename(fname, fname + \".gz\")\n-            gunzip_and_name(fname + \".gz\")\n-\n-        # process genome (e.g. masking)\n-        if hasattr(self, \"_post_process_download\"):\n-            self._post_process_download(\n-                name=name, localname=localname, out_dir=tmp_dir, mask=mask\n-            )\n-\n-        # transfer the genome from the tmpdir to the genome_dir\n-        src = fname\n-        dst = os.path.join(genomes_dir, localname, os.path.basename(fname))\n-        shutil.move(src, dst)\n-        rm_rf(tmp_dir)\n-\n-        asm_report = os.path.join(out_dir, \"assembly_report.txt\")\n-        asm_acc = self.assembly_accession(self.genomes.get(name))\n-        if asm_acc != \"na\":\n-            self.download_assembly_report(asm_acc, asm_report)\n-\n-        logger.info(\"name: {}\".format(name))\n-        logger.info(\"local name: {}\".format(localname))\n-        logger.info(\"fasta: {}\".format(dst))\n-\n-        # Create readme with information\n-        readme = os.path.join(genomes_dir, localname, \"README.txt\")\n-        metadata = {\n-            \"name\": localname,\n-            \"provider\": self.name,\n-            \"original name\": name,\n-            \"original filename\": os.path.split(link)[-1],\n-            \"assembly_accession\": asm_acc,\n-            \"tax_id\": self.genome_taxid(self.genomes.get(name)),\n-            \"mask\": mask,\n-            \"genome url\": link,\n-            \"annotation url\": \"na\",\n-            \"date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n-        }\n-        write_readme(readme, metadata)\n-\n-    def get_annotation_download_link(self, name, **kwargs):\n-        raise NotImplementedError()\n-\n-    @staticmethod\n-    def download_and_generate_annotation(genomes_dir, annot_url, localname):\n-        \"\"\"download annotation file, convert to intermediate file and generate output files\"\"\"\n-\n-        # create output directory if missing\n-        out_dir = os.path.join(genomes_dir, localname)\n-        mkdir_p(out_dir)\n-\n-        # download to tmp dir. Move genome on completion.\n-        # tmp dir is in genome_dir to prevent moving the genome between disks\n-        tmp_dir = mkdtemp(dir=out_dir)\n-        ext, gz = get_file_info(annot_url)\n-        annot_file = os.path.join(tmp_dir, localname + \".annotation\" + ext)\n-        download_file(annot_url, annot_file)\n-\n-        # unzip input file (if needed)\n-        if gz:\n-            cmd = \"mv {0} {1} && gunzip -f {1}\"\n-            sp.check_call(cmd.format(annot_file, annot_file + \".gz\"), shell=True)\n-\n-        # generate intermediate file (GenePred)\n-        pred_file = annot_file.replace(ext, \".gp\")\n-        if \"bed\" in ext:\n-            cmd = \"bedToGenePred {0} {1}\"\n-        elif \"gff\" in ext:\n-            cmd = \"gff3ToGenePred -geneNameAttr=gene {0} {1}\"\n-        elif \"gtf\" in ext:\n-            cmd = \"gtfToGenePred -ignoreGroupsWithoutExons {0} {1}\"\n-        elif \"txt\" in ext:\n-            # UCSC annotations only\n-            with open(annot_file) as f:\n-                cols = f.readline().split(\"\\t\")\n-\n-            # extract the genePred format columns\n-            start_col = 1\n-            for i, col in enumerate(cols):\n-                if col in [\"+\", \"-\"]:\n-                    start_col = i - 1\n-                    break\n-            end_col = start_col + 10\n-            cmd = (\n-                f\"\"\"cat {{0}} | cut -f {start_col}-{end_col} | \"\"\"\n-                # knownGene.txt.gz has spotty fields, this replaces non-integer fields with zeroes\n-                + \"\"\"awk 'BEGIN {{FS=OFS=\"\\t\"}} !($11 ~ /^[0-9]+$/) {{$11=\"0\"}}1' > {1}\"\"\"\n-            )\n-        else:\n-            raise TypeError(f\"file type extension {ext} not recognized!\")\n-\n-        sp.check_call(cmd.format(annot_file, pred_file), shell=True)\n-\n-        # generate gzipped gtf file (if required)\n-        gtf_file = annot_file.replace(ext, \".gtf\")\n-        if \"gtf\" not in ext:\n-            cmd = \"genePredToGtf -source=genomepy file {0} {1}\"\n-            sp.check_call(cmd.format(pred_file, gtf_file), shell=True)\n-\n-        # generate gzipped bed file (if required)\n-        bed_file = annot_file.replace(ext, \".bed\")\n-        if \"bed\" not in ext:\n-            cmd = \"genePredToBed {0} {1}\"\n-            sp.check_call(cmd.format(pred_file, bed_file), shell=True)\n-\n-        # transfer the files from the tmpdir to the genome_dir\n-        for f in [gtf_file, bed_file]:\n-            src = f\n-            dst = os.path.join(out_dir, os.path.basename(f))\n-            shutil.move(src, dst)\n-        rm_rf(tmp_dir)\n-\n-    def attempt_and_report(self, name, localname, link, genomes_dir):\n-        if not link:\n-            logger.error(\n-                f\"Could not download gene annotation for {name} from {self.name}.\"\n-            )\n-            return\n-\n-        try:\n-            logger.info(\n-                f\"Downloading annotation from {self.name}. Target URL: {link}...\"\n-            )\n-            self.download_and_generate_annotation(genomes_dir, link, localname)\n-            logger.info(\"Annotation download successful\")\n-        except Exception:\n-            raise GenomeDownloadError(\n-                f\"\\nCould not download annotation for {name} from {self.name}\\n\"\n-                \"If you think the annotation should be there, please file a bug report at:\\n\"\n-                \"https://github.com/vanheeringen-lab/genomepy/issues\\n\"\n-            )\n-\n-        # Add annotation URL to readme\n-        readme = os.path.join(genomes_dir, localname, \"README.txt\")\n-        update_readme(readme, updated_metadata={\"annotation url\": link})\n-\n-    def download_annotation(self, name, genomes_dir=None, localname=None, **kwargs):\n-        \"\"\"\n-        Download annotation file to to a specific directory\n-\n-        Parameters\n-        ----------\n-        name : str\n-            Genome / species name\n-\n-        genomes_dir : str , optional\n-            Directory to install annotation\n-\n-        localname : str , optional\n-            Custom name for your genome\n-        \"\"\"\n-        self.check_name(name)\n-\n-        link = self.get_annotation_download_link(name, **kwargs)\n-\n-        localname = get_localname(name, localname)\n-        genomes_dir = get_genomes_dir(genomes_dir, check_exist=False)\n-        self.attempt_and_report(name, localname, link, genomes_dir)\n-\n-    def _search_text(self, term: str) -> Iterator[str]:\n-        \"\"\"check if search term is found in the provider's genome name or description field(s)\"\"\"\n-        for name, metadata in self.genomes.items():\n-            if term in lower(name) or any(\n-                [term in lower(metadata[f]) for f in self.description_fields]\n-            ):\n-                yield name\n-\n-    def _search_accession(self, term: str) -> Iterator[str]:\n-        \"\"\"check if search term is found in the provider's accession field(s)\"\"\"\n-        # cut off prefix (GCA_/GCF_) and suffix (version numbers, e.g. '.3')\n-        term = term[4:].split(\".\")[0]\n-        for name, metadata in self.genomes.items():\n-            if any([term in str(metadata[f]) for f in self.accession_fields]):\n-                yield name\n-\n-    def _search_taxonomy(self, term: str) -> Iterator[str]:\n-        \"\"\"check if search term matches to any of the provider's taxonomy field(s)\"\"\"\n-        for name, metadata in self.genomes.items():\n-            if any([term == lower(metadata[f]) for f in self.taxid_fields]):\n-                yield name\n-\n-    def search(self, term: Union[str, int]):\n-        \"\"\"\n-        Search for term in genome names, descriptions and taxonomy ID.\n-\n-        The search is case-insensitive.\n-\n-        Parameters\n-        ----------\n-        term : str, int\n-            Search term, case-insensitive.\n-            Can be (part of) an assembly name (e.g. hg38),\n-            scientific name (Danio rerio) or assembly\n-            accession (GCA_000146045/GCF_...),\n-            or an exact taxonomy id (7227).\n-\n-        Yields\n-        ------\n-        tuples with name and metadata\n-        \"\"\"\n-        term = lower(term)\n-\n-        search_function = self._search_text\n-        if term.startswith((\"gca_\", \"gcf_\")):\n-            search_function = self._search_accession\n-        if term.isdigit():\n-            search_function = self._search_taxonomy\n-\n-        for genome in search_function(term):\n-            yield self._genome_info_tuple(genome)\n-\n-\n-register_provider = ProviderBase.register_provider\n-\n-\n-@register_provider(\"Ensembl\")\n-class EnsemblProvider(ProviderBase):\n-    \"\"\"\n-    Ensembl genome provider.\n-\n-    Will search both ensembl.org as well as ensemblgenomes.org.\n-    The bacteria division is not yet supported.\n-    \"\"\"\n-\n-    rest_url = \"https://rest.ensembl.org/\"\n-    provider_specific_install_options = {\n-        \"toplevel\": {\n-            \"long\": \"toplevel\",\n-            \"help\": \"always download toplevel-genome\",\n-            \"flag_value\": True,\n-        },\n-        \"version\": {\n-            \"long\": \"version\",\n-            \"help\": \"select release version\",\n-            \"type\": int,\n-            \"default\": None,\n-        },\n-    }\n-\n-    def __init__(self):\n-        self.name = \"Ensembl\"\n-        self.provider_status(self.rest_url + \"info/ping?\", max_tries=2)\n-        # Populate on init, so that methods can be cached\n-        self.genomes = self._get_genomes(self.rest_url)\n-        self.accession_fields = [\"assembly_accession\"]\n-        self.taxid_fields = [\"taxonomy_id\"]\n-        self.description_fields = [\n-            \"name\",\n-            \"scientific_name\",\n-            \"url_name\",\n-            \"display_name\",\n-        ]\n-\n-    @staticmethod\n-    def _request_json(rest_url, ext):\n-        \"\"\"Make a REST request and return as json.\"\"\"\n-        if rest_url.endswith(\"/\") and ext.startswith(\"/\"):\n-            ext = ext[1:]\n-\n-        r = requests.get(rest_url + ext, headers={\"Content-Type\": \"application/json\"})\n-\n-        if not r.ok:\n-            r.raise_for_status()\n-\n-        return r.json()\n-\n-    @cache(ignore=[\"self\"])\n-    def _get_genomes(self, rest_url):\n-        logger.info(\"Downloading assembly summaries from Ensembl\")\n-\n-        genomes = {}\n-        divisions = retry(self._request_json, 3, rest_url, \"info/divisions?\")\n-        for division in divisions:\n-            if division == \"EnsemblBacteria\":\n-                continue\n-            division_genomes = retry(\n-                self._request_json, 3, rest_url, f\"info/genomes/division/{division}?\"\n-            )\n-            for genome in division_genomes:\n-                genomes[safe(genome[\"assembly_name\"])] = genome\n-        return genomes\n-\n-    def _genome_info_tuple(self, name):\n-        \"\"\"tuple with assembly metadata\"\"\"\n-        genome = self.genomes[name]\n-        accession = self.assembly_accession(genome)\n-        taxid = self.genome_taxid(genome)\n-        taxid = str(taxid) if taxid != 0 else \"na\"\n-\n-        return (\n-            name,\n-            accession,\n-            genome.get(\"scientific_name\", \"na\"),\n-            taxid,\n-            genome.get(\"genebuild\", \"na\"),\n-        )\n-\n-    @goldfish_cache(ignore=[\"self\", \"rest_url\"])\n-    def get_version(self, rest_url, vertebrates=False):\n-        \"\"\"Retrieve current version from Ensembl FTP.\"\"\"\n-        ext = \"/info/data/?\" if vertebrates else \"/info/eg_version?\"\n-        ret = retry(self._request_json, 3, rest_url, ext)\n-        releases = ret[\"releases\"] if vertebrates else [ret[\"version\"]]\n-        return str(max(releases))\n-\n-    def get_genome_download_link(self, name, mask=\"soft\", **kwargs):\n-        \"\"\"\n-        Return Ensembl http or ftp link to the genome sequence\n-\n-        Parameters\n-        ----------\n-        name : str\n-            Genome name. Current implementation will fail if exact\n-            name is not found.\n-\n-        mask : str , optional\n-            Masking level. Options: soft, hard or none. Default is soft.\n-\n-        Returns\n-        ------\n-        str with the http/ftp download link.\n-        \"\"\"\n-        genome = self.genomes[safe(name)]\n-\n-        # parse the division\n-        division = genome[\"division\"].lower().replace(\"ensembl\", \"\")\n-        if division == \"bacteria\":\n-            raise NotImplementedError(\"bacteria from ensembl not yet supported\")\n-\n-        ftp_site = \"ftp://ftp.ensemblgenomes.org/pub\"\n-        if division == \"vertebrates\":\n-            ftp_site = \"ftp://ftp.ensembl.org/pub\"\n-\n-        # Ensembl release version\n-        version = kwargs.get(\"version\")\n-        if version is None:\n-            version = self.get_version(self.rest_url, division == \"vertebrates\")\n-\n-        # division dependent url format\n-        ftp_dir = \"{}/release-{}/fasta/{}/dna\".format(\n-            division, version, genome[\"url_name\"].lower()\n-        )\n-        if division == \"vertebrates\":\n-            ftp_dir = \"release-{}/fasta/{}/dna\".format(\n-                version, genome[\"url_name\"].lower()\n-            )\n-        url = f\"{ftp_site}/{ftp_dir}\"\n-\n-        # masking and assembly level\n-        def get_url(level=\"toplevel\"):\n-            masks = {\"soft\": \"dna_sm.{}\", \"hard\": \"dna_rm.{}\", \"none\": \"dna.{}\"}\n-            pattern = masks[mask].format(level)\n-\n-            asm_url = \"{}/{}.{}.{}.fa.gz\".format(\n-                url,\n-                genome[\"url_name\"].capitalize(),\n-                re.sub(r\"\\.p\\d+$\", \"\", safe(genome[\"assembly_name\"])),\n-                pattern,\n-            )\n-            return asm_url\n-\n-        # try to get the (much smaller) primary assembly,\n-        # unless specified otherwise\n-        link = get_url(\"primary_assembly\")\n-        if kwargs.get(\"toplevel\") or not check_url(link, 2):\n-            link = get_url()\n-\n-        if check_url(link, 2):\n-            return link\n-\n-        raise GenomeDownloadError(\n-            f\"Could not download genome {name} from {self.name}.\\n\"\n-            \"URL is broken. Select another genome or provider.\\n\"\n-            f\"Broken URL: {link}\"\n-        )\n-\n-    def get_annotation_download_link(self, name, **kwargs):\n-        \"\"\"\n-        Parse and test the link to the Ensembl annotation file.\n-\n-        Parameters\n-        ----------\n-        name : str\n-            Genome name\n-        kwargs: dict , optional:\n-            Provider specific options.\n-\n-            version : int , optional\n-                Ensembl version. By default the latest version is used.\n-        \"\"\"\n-        genome = self.genomes[safe(name)]\n-        division = genome[\"division\"].lower().replace(\"ensembl\", \"\")\n-\n-        ftp_site = \"ftp://ftp.ensemblgenomes.org/pub\"\n-        if division == \"vertebrates\":\n-            ftp_site = \"ftp://ftp.ensembl.org/pub\"\n-\n-        # Ensembl release version\n-        version = kwargs.get(\"version\")\n-        if version is None:\n-            version = self.get_version(self.rest_url, division == \"vertebrates\")\n-\n-        if division != \"vertebrates\":\n-            ftp_site += f\"/{division}\"\n-\n-        # Get the GTF URL\n-        base_url = ftp_site + \"/release-{}/gtf/{}/{}.{}.{}.gtf.gz\"\n-        safe_name = re.sub(r\"\\.p\\d+$\", \"\", name)\n-        link = base_url.format(\n-            version,\n-            genome[\"url_name\"].lower(),\n-            genome[\"url_name\"].capitalize(),\n-            safe_name,\n-            version,\n-        )\n-\n-        if check_url(link, 2):\n-            return link\n-\n-\n-@register_provider(\"UCSC\")\n-class UcscProvider(ProviderBase):\n-    \"\"\"\n-    UCSC genome provider.\n-\n-    The UCSC API REST server is used to search and list genomes.\n-    \"\"\"\n-\n-    base_url = \"http://hgdownload.soe.ucsc.edu/goldenPath\"\n-    ucsc_url = base_url + \"/{0}/bigZips/chromFa.tar.gz\"\n-    ucsc_url_masked = base_url + \"/{0}/bigZips/chromFaMasked.tar.gz\"\n-    alt_ucsc_url = base_url + \"/{0}/bigZips/{0}.fa.gz\"\n-    alt_ucsc_url_masked = base_url + \"/{0}/bigZips/{0}.fa.masked.gz\"\n-    rest_url = \"http://api.genome.ucsc.edu/list/ucscGenomes\"\n-    provider_specific_install_options = {\n-        \"ucsc_annotation_type\": {\n-            \"long\": \"annotation\",\n-            \"help\": \"specify annotation to download: UCSC, Ensembl, NCBI_refseq or UCSC_refseq\",\n-            \"default\": None,\n-        },\n-    }\n-\n-    def __init__(self):\n-        self.name = \"UCSC\"\n-        self.provider_status(self.base_url)\n-        # Populate on init, so that methods can be cached\n-        self.genomes = self._get_genomes(self.rest_url)\n-        self.accession_fields = []\n-        self.taxid_fields = [\"taxId\"]\n-        self.description_fields = [\"description\", \"scientificName\"]\n-\n-    @staticmethod\n-    @cache\n-    def _get_genomes(rest_url):\n-        logger.info(\"Downloading assembly summaries from UCSC\")\n-\n-        r = requests.get(rest_url, headers={\"Content-Type\": \"application/json\"})\n-        if not r.ok:\n-            r.raise_for_status()\n-        ucsc_json = r.json()\n-        genomes = ucsc_json[\"ucscGenomes\"]\n-        return genomes\n-\n-    def _search_accession(self, term: str) -> Iterator[str]:\n-        \"\"\"\n-        UCSC does not store assembly accessions.\n-        This function searches NCBI (most genomes + stable accession IDs),\n-        then uses the NCBI accession search results for a UCSC text search.\n-\n-        Parameters\n-        ----------\n-        term : str\n-            Assembly accession, GCA_/GCF_....\n-\n-        Yields\n-        ------\n-        genome names\n-        \"\"\"\n-        # NCBI provides a consistent assembly accession. This can be used to\n-        # retrieve the species, and then search for that.\n-        p = ProviderBase.create(\"NCBI\")\n-        ncbi_genomes = list(p._search_accession(term))  # noqa\n-\n-        # remove superstrings (keep GRCh38, not GRCh38.p1 to GRCh38.p13)\n-        unique_ncbi_genomes = []\n-        for i in ncbi_genomes:\n-            if sum([j in i for j in ncbi_genomes]) == 1:\n-                unique_ncbi_genomes.append(i)\n-\n-        # add NCBI organism names to search terms\n-        organism_names = [\n-            p.genomes[name][\"organism_name\"] for name in unique_ncbi_genomes\n-        ]\n-        terms = list(set(unique_ncbi_genomes + organism_names))\n-\n-        # search with NCBI results in the given provider\n-        for name, metadata in self.genomes.items():\n-            for term in terms:\n-                term = lower(term)\n-                if term in lower(name) or any(\n-                    [term in lower(metadata[f]) for f in self.description_fields]\n-                ):\n-                    yield name\n-                    break  # max one hit per genome\n-\n-    @staticmethod\n-    @cache\n-    def assembly_accession(genome):\n-        \"\"\"Return the assembly accession (GCA_/GCF_....) for a genome.\n-\n-        UCSC does not serve the assembly accession through the REST API.\n-        Therefore, the readme.html is scanned for an assembly accession. If it is\n-        not found, the linked NCBI assembly page will be checked.\n-\n-        Parameters\n-        ----------\n-        genome : dict\n-            provider metadata dict of a genome.\n-\n-        Returns\n-        ------\n-        str\n-            Assembly accession.\n-        \"\"\"\n-        try:\n-            ucsc_url = \"https://hgdownload.soe.ucsc.edu/\" + genome[\"htmlPath\"]\n-            text = read_url(ucsc_url)\n-        except UnicodeDecodeError:\n-            return \"na\"\n-\n-        # example accessions: GCA_000004335.1 (ailMel1)\n-        # regex: GC[AF]_ = GCA_ or GCF_, \\d = digit, \\. = period\n-        accession_regex = re.compile(r\"GC[AF]_\\d{9}\\.\\d+\")\n-        match = accession_regex.search(text)\n-        if match:\n-            return match.group(0)\n-\n-        # Search for an assembly link at NCBI\n-        match = re.search(r\"https?://www.ncbi.nlm.nih.gov/assembly/\\d+\", text)\n-        if match:\n-            ncbi_url = match.group(0)\n-            text = read_url(ncbi_url)\n-\n-            # retrieve valid assembly accessions.\n-            # contains additional info, such as '(latest)' or '(suppressed)'. Unused for now.\n-            valid_accessions = re.findall(r\"assembly accession:.*?GC[AF]_.*?<\", text)\n-            text = \" \".join(valid_accessions)\n-            match = accession_regex.search(text)\n-            if match:\n-                return match.group(0)\n-\n-        return \"na\"\n-\n-    def _genome_info_tuple(self, name):\n-        \"\"\"tuple with assembly metadata\"\"\"\n-        genome = self.genomes[name]\n-        accession = self.assembly_accession(genome)\n-        taxid = self.genome_taxid(genome)\n-        taxid = str(taxid) if taxid != 0 else \"na\"\n-\n-        return (\n-            name,\n-            accession,\n-            genome.get(\"scientificName\", \"na\"),\n-            taxid,\n-            genome.get(\"description\", \"na\"),\n-        )\n-\n-    def get_genome_download_link(self, name, mask=\"soft\", **kwargs):\n-        \"\"\"\n-        Return UCSC http link to genome sequence\n-\n-        Parameters\n-        ----------\n-        name : str\n-            Genome name. Current implementation will fail if exact\n-            name is not found.\n-\n-        mask : str , optional\n-            Masking level. Options: soft, hard or none. Default is soft.\n-\n-        Returns\n-        ------\n-        str with the http/ftp download link.\n-        \"\"\"\n-        # soft masked genomes. can be unmasked in _post _process_download\n-        urls = [self.ucsc_url, self.alt_ucsc_url]\n-        if mask == \"hard\":\n-            urls = [self.ucsc_url_masked, self.alt_ucsc_url_masked]\n-\n-        for genome_url in urls:\n-            link = genome_url.format(name)\n-\n-            if check_url(link, 2):\n-                return link\n-\n-        raise GenomeDownloadError(\n-            f\"Could not download genome {name} from {self.name}.\\n\"\n-            \"URLs are broken. Select another genome or provider.\\n\"\n-            f\"Broken URLs: {', '.join([url.format(name) for url in urls])}\"\n-        )\n-\n-    @staticmethod\n-    def _post_process_download(name, localname, out_dir, mask=\"soft\"):  # noqa\n-        \"\"\"\n-        Unmask a softmasked genome if required\n-\n-        Parameters\n-        ----------\n-        name : str\n-            unused for the UCSC function\n-\n-        localname : str\n-            Custom name for your genome\n-\n-        out_dir : str\n-            Output directory\n-\n-        mask : str , optional\n-            masking level: soft/hard/none, default=soft\n-        \"\"\"\n-        if mask != \"none\":\n-            return\n-\n-        logger.info(\"UCSC genomes are softmasked by default. Unmasking...\")\n-\n-        fa = os.path.join(out_dir, f\"{localname}.fa\")\n-        old_fa = os.path.join(out_dir, f\"old_{localname}.fa\")\n-        os.rename(fa, old_fa)\n-        with open(old_fa) as old, open(fa, \"w\") as new:\n-            for line in old:\n-                if line.startswith(\">\"):\n-                    new.write(line)\n-                else:\n-                    new.write(line.upper())\n-\n-    def get_annotation_download_link(self, name, **kwargs):\n-        \"\"\"\n-        Parse and test the link to the UCSC annotation file.\n-\n-        Will check UCSC, Ensembl, NCBI RefSeq and UCSC RefSeq annotation, respectively.\n-        More info on the annotation file on: https://genome.ucsc.edu/FAQ/FAQgenes.html#whatdo\n-\n-        Parameters\n-        ----------\n-        name : str\n-            Genome name\n-        \"\"\"\n-        gtf_url = f\"http://hgdownload.soe.ucsc.edu/goldenPath/{name}/bigZips/genes/\"\n-        txt_url = f\"http://hgdownload.cse.ucsc.edu/goldenPath/{name}/database/\"\n-        annot_files = {\n-            \"ucsc\": \"knownGene\",\n-            \"ensembl\": \"ensGene\",\n-            \"ncbi_refseq\": \"ncbiRefSeq\",\n-            \"ucsc_refseq\": \"refGene\",\n-        }\n-\n-        # download gtf format if possible, txt format if not\n-        gtfs_exists = check_url(gtf_url, 2)\n-        base_url = gtf_url + name + \".\" if gtfs_exists else txt_url\n-        base_ext = \".gtf.gz\" if gtfs_exists else \".txt.gz\"\n-\n-        # download specified annotation type if requested\n-        file = kwargs.get(\"ucsc_annotation_type\")\n-        if file:\n-            link = base_url + annot_files[file.lower()] + base_ext\n-            if check_url(link, 2):\n-                return link\n-            logger.warning(f\"Specified annotation type ({file}) not found for {name}.\")\n-\n-        else:\n-            # download first available annotation type found\n-            for file in annot_files.values():\n-                link = base_url + file + base_ext\n-                if check_url(link, 2):\n-                    return link\n-\n-\n-@register_provider(\"NCBI\")\n-class NcbiProvider(ProviderBase):\n-    \"\"\"\n-    NCBI genome provider.\n-\n-    Uses the assembly reports page to search and list genomes.\n-    \"\"\"\n-\n-    assembly_url = \"https://ftp.ncbi.nlm.nih.gov/genomes/ASSEMBLY_REPORTS/\"\n-    provider_specific_install_options = {}\n-\n-    def __init__(self):\n-        self.name = \"NCBI\"\n-        self.provider_status(self.assembly_url)\n-        # Populate on init, so that methods can be cached\n-        self.genomes = self._get_genomes(self.assembly_url)\n-        self.accession_fields = [\"assembly_accession\", \"gbrs_paired_asm\"]\n-        self.taxid_fields = [\"species_taxid\", \"taxid\"]\n-        self.description_fields = [\n-            \"submitter\",\n-            \"organism_name\",\n-            \"assembly_accession\",\n-            \"gbrs_paired_asm\",\n-            \"paired_asm_comp\",\n-        ]\n-\n-    @staticmethod\n-    @cache\n-    def _get_genomes(assembly_url):\n-        \"\"\"Parse genomes from assembly summary txt files.\"\"\"\n-        logger.info(\n-            \"Downloading assembly summaries from NCBI, this will take a while...\"\n-        )\n-\n-        def load_summary(url):\n-            \"\"\"\n-            lazy loading of the url so we can parse while downloading\n-            \"\"\"\n-            for row in urlopen(url):\n-                yield row\n-\n-        genomes = {}\n-        # order is important as asm_name can repeat (overwriting the older name)\n-        names = [\n-            \"assembly_summary_genbank_historical.txt\",\n-            \"assembly_summary_refseq_historical.txt\",\n-            \"assembly_summary_genbank.txt\",\n-            \"assembly_summary_refseq.txt\",\n-        ]\n-        for fname in names:\n-            lines = load_summary(f\"{assembly_url}/{fname}\")\n-            _ = next(lines)  # line 0 = comment\n-            header = (\n-                next(lines).decode(\"utf-8\").strip(\"# \").strip(\"\\n\").split(\"\\t\")\n-            )  # line 1 = header\n-            for line in tqdm(lines, desc=fname[17:-4], unit_scale=1, unit=\" genomes\"):\n-                line = line.decode(\"utf-8\").strip(\"\\n\").split(\"\\t\")\n-                if line[19] != \"na\":  # ftp_path must exist\n-                    name = safe(line[15])  # overwrites older asm_names\n-                    genomes[name] = dict(zip(header, line))\n-        return genomes\n-\n-    def _genome_info_tuple(self, name):\n-        \"\"\"tuple with assembly metadata\"\"\"\n-        genome = self.genomes[name]\n-        accession = self.assembly_accession(genome)\n-        taxid = self.genome_taxid(genome)\n-        taxid = str(taxid) if taxid != 0 else \"na\"\n-\n-        return (\n-            name,\n-            accession,\n-            genome.get(\"organism_name\", \"na\"),\n-            taxid,\n-            genome.get(\"submitter\", \"na\"),\n-        )\n-\n-    def get_genome_download_link(self, name, mask=\"soft\", **kwargs):\n-        \"\"\"\n-        Return NCBI ftp link to top-level genome sequence\n-\n-        Parameters\n-        ----------\n-        name : str\n-            Genome name. Current implementation will fail if exact\n-            name is not found.\n-\n-        mask : str , optional\n-            Masking level. Options: soft, hard or none. Default is soft.\n-\n-        Returns\n-        ------\n-        str with the http/ftp download link.\n-        \"\"\"\n-        # only soft masked genomes available. can be (un)masked in _post_process_download\n-        link = self._ftp_or_html_link(name, file_suffix=\"_genomic.fna.gz\")\n-\n-        if link:\n-            return link\n-\n-        raise GenomeDownloadError(\n-            f\"Could not download genome {name} from {self.name}.\\n\"\n-            \"URL is broken. Select another genome or provider.\\n\"\n-            f\"Broken URL: {link}\"\n-        )\n-\n-    def _post_process_download(self, name, localname, out_dir, mask=\"soft\"):\n-        \"\"\"\n-        Replace accessions with sequence names in fasta file.\n-\n-        Applies masking.\n-\n-        Parameters\n-        ----------\n-        name : str\n-            NCBI genome name\n-\n-        localname : str\n-            Custom name for your genome\n-\n-        out_dir : str\n-            Output directory\n-\n-        mask : str , optional\n-            masking level: soft/hard/none, default=soft\n-        \"\"\"\n-        # Create mapping of accessions to names\n-        url = self._ftp_or_html_link(\n-            name, file_suffix=\"_assembly_report.txt\", skip_check=True\n-        )\n-\n-        tr = {}\n-        with urlopen(url) as response:\n-            for line in response.read().decode(\"utf-8\").splitlines():\n-                if line.startswith(\"#\"):\n-                    continue\n-                vals = line.strip().split(\"\\t\")\n-                tr[vals[6]] = vals[0]\n-\n-        # mask sequence if required\n-        if mask == \"soft\":\n-\n-            def mask_cmd(txt):\n-                return txt\n-\n-        elif mask == \"hard\":\n-            logger.info(\"NCBI genomes are softmasked by default. Hard masking...\")\n-\n-            def mask_cmd(txt):\n-                return re.sub(\"[actg]\", \"N\", txt)\n-\n-        else:\n-            logger.info(\"NCBI genomes are softmasked by default. Unmasking...\")\n-\n-            def mask_cmd(txt):\n-                return txt.upper()\n-\n-        # apply mapping and masking\n-        fa = os.path.join(out_dir, f\"{localname}.fa\")\n-        old_fa = os.path.join(out_dir, f\"old_{localname}.fa\")\n-        os.rename(fa, old_fa)\n-        with open(old_fa) as old, open(fa, \"w\") as new:\n-            for line in old:\n-                if line.startswith(\">\"):\n-                    desc = line.strip()[1:]\n-                    name = desc.split(\" \")[0]\n-                    new.write(\">{} {}\\n\".format(tr.get(name, name), desc))\n-                else:\n-                    new.write(mask_cmd(line))\n-\n-    def get_annotation_download_link(self, name, **kwargs):\n-        \"\"\"\n-        Parse and test the link to the NCBI annotation file.\n-\n-        Parameters\n-        ----------\n-        name : str\n-            Genome name\n-        \"\"\"\n-        return self._ftp_or_html_link(name, file_suffix=\"_genomic.gff.gz\")\n-\n-    def _ftp_or_html_link(self, name, file_suffix, skip_check=False):\n-        \"\"\"\n-        NCBI's files are accessible over FTP and HTTPS\n-        Try HTTPS first and return the first functioning link\n-        \"\"\"\n-        genome = self.genomes[safe(name)]\n-        ftp_link = genome[\"ftp_path\"]\n-        html_link = ftp_link.replace(\"ftp://\", \"https://\")\n-        for link in [html_link, ftp_link]:\n-            link += \"/\" + link.split(\"/\")[-1] + file_suffix\n-\n-            if skip_check or check_url(link, max_tries=2, timeout=10):\n-                return link\n-\n-\n-@register_provider(\"URL\")\n-class UrlProvider(ProviderBase):\n-    \"\"\"\n-    URL genome provider.\n-\n-    Simply download a genome directly through an url.\n-    \"\"\"\n-\n-    provider_specific_install_options = {\n-        \"to_annotation\": {\n-            \"long\": \"to-annotation\",\n-            \"help\": \"link to the annotation file, required if this is not in the same directory as the fasta file\",\n-            \"default\": None,\n-        },\n-    }\n-\n-    def __init__(self):\n-        self.name = \"URL\"\n-        self.genomes = {}\n-\n-    def genome_taxid(self, genome):\n-        return \"na\"\n-\n-    def assembly_accession(self, genome):\n-        return \"na\"\n-\n-    def search(self, term):\n-        \"\"\"return an empty generator,\n-        same as if no genomes were found at the other providers\"\"\"\n-        yield from ()\n-\n-    def _genome_info_tuple(self, name):\n-        return tuple()\n-\n-    def check_name(self, name):\n-        \"\"\"check if genome name can be found for provider\"\"\"\n-        return\n-\n-    def get_genome_download_link(self, url, mask=None, **kwargs):\n-        return url\n-\n-    def get_annotation_download_link(self, name, **kwargs):\n-        \"\"\"\n-        check if the linked annotation file is of a supported file type (gtf/gff3/bed)\n-        \"\"\"\n-        link = kwargs.get(\"to_annotation\")\n-        if link:\n-            ext = get_file_info(link)[0]\n-            if ext not in [\".gtf\", \".gff\", \".gff3\", \".bed\"]:\n-                raise TypeError(\n-                    \"Only (gzipped) gtf, gff and bed files are supported.\\n\"\n-                )\n-\n-            return link\n-\n-    @staticmethod\n-    def search_url_for_annotations(url, name):\n-        \"\"\"Attempts to find gtf or gff3 files in the same location as the genome url\"\"\"\n-        urldir = os.path.dirname(url)\n-        logger.info(\n-            \"You have requested the gene annotation to be downloaded. \"\n-            \"Genomepy will check the remote directory: \"\n-            f\"{urldir} for annotation files...\"\n-        )\n-\n-        def fuzzy_annotation_search(search_name, search_list):\n-            \"\"\"Returns all files containing both name and an annotation extension\"\"\"\n-            hits = []\n-            for ext in [\"gtf\", \"gff\"]:\n-                # .*? = non greedy filler. 3? = optional 3 (for gff3). (\\.gz)? = optional .gz\n-                expr = f\"{search_name}.*?\\.{ext}3?(\\.gz)?\"  # noqa: W605\n-                for line in search_list:\n-                    hit = re.search(expr, line, flags=re.IGNORECASE)\n-                    if hit:\n-                        hits.append(hit[0])\n-            return hits\n-\n-        # try to find a GTF or GFF3 file\n-        dirty_list = [str(line) for line in urlopen(urldir).readlines()]\n-        fnames = fuzzy_annotation_search(name, dirty_list)\n-        if not fnames:\n-            raise FileNotFoundError(\n-                \"Could not parse the remote directory. \"\n-                \"Please supply a URL using --url-to-annotation.\\n\"\n-            )\n-\n-        links = [urldir + \"/\" + fname for fname in fnames]\n-        return links\n-\n-    def download_annotation(self, url, genomes_dir=None, localname=None, **kwargs):\n-        \"\"\"\n-        Attempts to download a gtf or gff3 file from the same location as the genome url\n-\n-        Parameters\n-        ----------\n-        url : str\n-            url of where to download genome from\n-\n-        genomes_dir : str\n-            Directory to install annotation\n-\n-        localname : str , optional\n-            Custom name for your genome\n-\n-        kwargs: dict , optional:\n-            Provider specific options.\n-\n-            to_annotation : str , optional\n-                url to annotation file (only required if this not located in the same directory as the fasta)\n-        \"\"\"\n-        name = get_localname(url)\n-        localname = get_localname(name, localname)\n-        genomes_dir = get_genomes_dir(genomes_dir, check_exist=False)\n-\n-        if kwargs.get(\"to_annotation\"):\n-            links = [self.get_annotation_download_link(None, **kwargs)]\n-        else:\n-            # can return multiple possible hits\n-            links = self.search_url_for_annotations(url, name)\n-\n-        for link in links:\n-            try:\n-                self.attempt_and_report(name, localname, link, genomes_dir)\n-                break\n-            except GenomeDownloadError as e:\n-                if not link == links[-1]:\n-                    logger.info(\n-                        \"One of the potential annotations was incompatible with genomepy. \"\n-                        \"Attempting another...\"\n-                    )\n-                    continue\n-                return e\n",
        "source_code_with_indent": "        return mapping\n\n    <DED>def download_genome(\n        self,\n        name: str,\n        genomes_dir: str = None,\n        localname: str = None,\n        mask: Optional[str] = \"soft\",\n        **kwargs,\n    ):\n        <IND>\"\"\"\n        Download a (gzipped) genome file to a specific directory\n\n        Parameters\n        ----------\n        name : str\n            Genome / species name\n\n        genomes_dir : str , optional\n            Directory to install genome\n\n        localname : str , optional\n            Custom name for your genome\n\n        mask: str , optional\n            Masking, soft, hard or none (all other strings)\n        \"\"\"\n        name = safe(name)\n        self.check_name(name)\n\n        link = self.get_genome_download_link(name, mask=mask, **kwargs)\n\n        localname = get_localname(name, localname)\n        genomes_dir = get_genomes_dir(genomes_dir, check_exist=False)\n        out_dir = os.path.join(genomes_dir, localname)\n        mkdir_p(out_dir)\n\n        logger.info(f\"Downloading genome from {self.name}. Target URL: {link}...\")\n\n        # download to tmp dir. Move genome on completion.\n        # tmp dir is in genome_dir to prevent moving the genome between disks\n        tmp_dir = mkdtemp(dir=out_dir)\n        fname = os.path.join(tmp_dir, f\"{localname}.fa\")\n\n        download_file(link, fname)\n        logger.info(\"Genome download successful, starting post processing...\")\n\n        # unzip genome\n        if link.endswith(\".tar.gz\"):\n            <IND>tar_to_bigfile(fname, fname)\n        <DED>elif link.endswith(\".gz\"):\n            <IND>os.rename(fname, fname + \".gz\")\n            gunzip_and_name(fname + \".gz\")\n\n        # process genome (e.g. masking)\n        <DED>if hasattr(self, \"_post_process_download\"):\n            <IND>self._post_process_download(\n                name=name, localname=localname, out_dir=tmp_dir, mask=mask\n            )\n\n        # transfer the genome from the tmpdir to the genome_dir\n        <DED>src = fname\n        dst = os.path.join(genomes_dir, localname, os.path.basename(fname))\n        shutil.move(src, dst)\n        rm_rf(tmp_dir)\n\n        asm_report = os.path.join(out_dir, \"assembly_report.txt\")\n        asm_acc = self.assembly_accession(self.genomes.get(name))\n        if asm_acc != \"na\":\n            <IND>self.download_assembly_report(asm_acc, asm_report)\n\n        <DED>logger.info(\"name: {}\".format(name))\n        logger.info(\"local name: {}\".format(localname))\n        logger.info(\"fasta: {}\".format(dst))\n\n        # Create readme with information\n        readme = os.path.join(genomes_dir, localname, \"README.txt\")\n        metadata = {\n            \"name\": localname,\n            \"provider\": self.name,\n            \"original name\": name,\n            \"original filename\": os.path.split(link)[-1],\n            \"assembly_accession\": asm_acc,\n            \"tax_id\": self.genome_taxid(self.genomes.get(name)),\n            \"mask\": mask,\n            \"genome url\": link,\n            \"annotation url\": \"na\",\n            \"date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        }\n        write_readme(readme, metadata)\n\n    <DED>def get_annotation_download_link(self, name, **kwargs):\n        <IND>raise NotImplementedError()\n\n    <DED>@staticmethod\n    def download_and_generate_annotation(genomes_dir, annot_url, localname):\n        <IND>\"\"\"download annotation file, convert to intermediate file and generate output files\"\"\"\n\n        # create output directory if missing\n        out_dir = os.path.join(genomes_dir, localname)\n        mkdir_p(out_dir)\n\n        # download to tmp dir. Move genome on completion.\n        # tmp dir is in genome_dir to prevent moving the genome between disks\n        tmp_dir = mkdtemp(dir=out_dir)\n        ext, gz = get_file_info(annot_url)\n        annot_file = os.path.join(tmp_dir, localname + \".annotation\" + ext)\n        download_file(annot_url, annot_file)\n\n        # unzip input file (if needed)\n        if gz:\n            <IND>cmd = \"mv {0} {1} && gunzip -f {1}\"\n            sp.check_call(cmd.format(annot_file, annot_file + \".gz\"), shell=True)\n\n        # generate intermediate file (GenePred)\n        <DED>pred_file = annot_file.replace(ext, \".gp\")\n        if \"bed\" in ext:\n            <IND>cmd = \"bedToGenePred {0} {1}\"\n        <DED>elif \"gff\" in ext:\n            <IND>cmd = \"gff3ToGenePred -geneNameAttr=gene {0} {1}\"\n        <DED>elif \"gtf\" in ext:\n            <IND>cmd = \"gtfToGenePred -ignoreGroupsWithoutExons {0} {1}\"\n        <DED>elif \"txt\" in ext:\n            # UCSC annotations only\n            <IND>with open(annot_file) as f:\n                <IND>cols = f.readline().split(\"\\t\")\n\n            # extract the genePred format columns\n            <DED>start_col = 1\n            for i, col in enumerate(cols):\n                <IND>if col in [\"+\", \"-\"]:\n                    <IND>start_col = i - 1\n                    break\n            <DED><DED>end_col = start_col + 10\n            cmd = (\n                f\"\"\"cat {{0}} | cut -f {start_col}-{end_col} | \"\"\"\n                # knownGene.txt.gz has spotty fields, this replaces non-integer fields with zeroes\n                + \"\"\"awk 'BEGIN {{FS=OFS=\"\\t\"}} !($11 ~ /^[0-9]+$/) {{$11=\"0\"}}1' > {1}\"\"\"\n            )\n        <DED>else:\n            <IND>raise TypeError(f\"file type extension {ext} not recognized!\")\n\n        <DED>sp.check_call(cmd.format(annot_file, pred_file), shell=True)\n\n        # generate gzipped gtf file (if required)\n        gtf_file = annot_file.replace(ext, \".gtf\")\n        if \"gtf\" not in ext:\n            <IND>cmd = \"genePredToGtf -source=genomepy file {0} {1}\"\n            sp.check_call(cmd.format(pred_file, gtf_file), shell=True)\n\n        # generate gzipped bed file (if required)\n        <DED>bed_file = annot_file.replace(ext, \".bed\")\n        if \"bed\" not in ext:\n            <IND>cmd = \"genePredToBed {0} {1}\"\n            sp.check_call(cmd.format(pred_file, bed_file), shell=True)\n\n        # transfer the files from the tmpdir to the genome_dir\n        <DED>for f in [gtf_file, bed_file]:\n            <IND>src = f\n            dst = os.path.join(out_dir, os.path.basename(f))\n            shutil.move(src, dst)\n        <DED>rm_rf(tmp_dir)\n\n    <DED>def attempt_and_report(self, name, localname, link, genomes_dir):\n        <IND>if not link:\n            <IND>logger.error(\n                f\"Could not download gene annotation for {name} from {self.name}.\"\n            )\n            return\n\n        <DED>try:\n            <IND>logger.info(\n                f\"Downloading annotation from {self.name}. Target URL: {link}...\"\n            )\n            self.download_and_generate_annotation(genomes_dir, link, localname)\n            logger.info(\"Annotation download successful\")\n        <DED>except Exception:\n            <IND>raise GenomeDownloadError(\n                f\"\\nCould not download annotation for {name} from {self.name}\\n\"\n                \"If you think the annotation should be there, please file a bug report at:\\n\"\n                \"https://github.com/vanheeringen-lab/genomepy/issues\\n\"\n            )\n\n        # Add annotation URL to readme\n        <DED>readme = os.path.join(genomes_dir, localname, \"README.txt\")\n        update_readme(readme, updated_metadata={\"annotation url\": link})\n\n    <DED>def download_annotation(self, name, genomes_dir=None, localname=None, **kwargs):\n        <IND>\"\"\"\n        Download annotation file to to a specific directory\n\n        Parameters\n        ----------\n        name : str\n            Genome / species name\n\n        genomes_dir : str , optional\n            Directory to install annotation\n\n        localname : str , optional\n            Custom name for your genome\n        \"\"\"\n        self.check_name(name)\n\n        link = self.get_annotation_download_link(name, **kwargs)\n\n        localname = get_localname(name, localname)\n        genomes_dir = get_genomes_dir(genomes_dir, check_exist=False)\n        self.attempt_and_report(name, localname, link, genomes_dir)\n\n    <DED>def _search_text(self, term: str) -> Iterator[str]:\n        <IND>\"\"\"check if search term is found in the provider's genome name or description field(s)\"\"\"\n        for name, metadata in self.genomes.items():\n            <IND>if term in lower(name) or any(\n                [term in lower(metadata[f]) for f in self.description_fields]\n            ):\n                <IND>yield name\n\n    <DED><DED><DED>def _search_accession(self, term: str) -> Iterator[str]:\n        <IND>\"\"\"check if search term is found in the provider's accession field(s)\"\"\"\n        # cut off prefix (GCA_/GCF_) and suffix (version numbers, e.g. '.3')\n        term = term[4:].split(\".\")[0]\n        for name, metadata in self.genomes.items():\n            <IND>if any([term in str(metadata[f]) for f in self.accession_fields]):\n                <IND>yield name\n\n    <DED><DED><DED>def _search_taxonomy(self, term: str) -> Iterator[str]:\n        <IND>\"\"\"check if search term matches to any of the provider's taxonomy field(s)\"\"\"\n        for name, metadata in self.genomes.items():\n            <IND>if any([term == lower(metadata[f]) for f in self.taxid_fields]):\n                <IND>yield name\n\n    <DED><DED><DED>def search(self, term: Union[str, int]):\n        <IND>\"\"\"\n        Search for term in genome names, descriptions and taxonomy ID.\n\n        The search is case-insensitive.\n\n        Parameters\n        ----------\n        term : str, int\n            Search term, case-insensitive.\n            Can be (part of) an assembly name (e.g. hg38),\n            scientific name (Danio rerio) or assembly\n            accession (GCA_000146045/GCF_...),\n            or an exact taxonomy id (7227).\n\n        Yields\n        ------\n        tuples with name and metadata\n        \"\"\"\n        term = lower(term)\n\n        search_function = self._search_text\n        if term.startswith((\"gca_\", \"gcf_\")):\n            <IND>search_function = self._search_accession\n        <DED>if term.isdigit():\n            <IND>search_function = self._search_taxonomy\n\n        <DED>for genome in search_function(term):\n            <IND>yield self._genome_info_tuple(genome)\n\n\n<DED><DED><DED>register_provider = ProviderBase.register_provider\n\n\n@register_provider(\"Ensembl\")\nclass EnsemblProvider(ProviderBase):\n    <IND>\"\"\"\n    Ensembl genome provider.\n\n    Will search both ensembl.org as well as ensemblgenomes.org.\n    The bacteria division is not yet supported.\n    \"\"\"\n\n    rest_url = \"https://rest.ensembl.org/\"\n    provider_specific_install_options = {\n        \"toplevel\": {\n            \"long\": \"toplevel\",\n            \"help\": \"always download toplevel-genome\",\n            \"flag_value\": True,\n        },\n        \"version\": {\n            \"long\": \"version\",\n            \"help\": \"select release version\",\n            \"type\": int,\n            \"default\": None,\n        },\n    }\n\n    def __init__(self):\n        <IND>self.name = \"Ensembl\"\n        self.provider_status(self.rest_url + \"info/ping?\", max_tries=2)\n        # Populate on init, so that methods can be cached\n        self.genomes = self._get_genomes(self.rest_url)\n        self.accession_fields = [\"assembly_accession\"]\n        self.taxid_fields = [\"taxonomy_id\"]\n        self.description_fields = [\n            \"name\",\n            \"scientific_name\",\n            \"url_name\",\n            \"display_name\",\n        ]\n\n    <DED>@staticmethod\n    def _request_json(rest_url, ext):\n        <IND>\"\"\"Make a REST request and return as json.\"\"\"\n        if rest_url.endswith(\"/\") and ext.startswith(\"/\"):\n            <IND>ext = ext[1:]\n\n        <DED>r = requests.get(rest_url + ext, headers={\"Content-Type\": \"application/json\"})\n\n        if not r.ok:\n            <IND>r.raise_for_status()\n\n        <DED>return r.json()\n\n    <DED>@cache(ignore=[\"self\"])\n    def _get_genomes(self, rest_url):\n        <IND>logger.info(\"Downloading assembly summaries from Ensembl\")\n\n        genomes = {}\n        divisions = retry(self._request_json, 3, rest_url, \"info/divisions?\")\n        for division in divisions:\n            <IND>if division == \"EnsemblBacteria\":\n                <IND>continue\n            <DED>division_genomes = retry(\n                self._request_json, 3, rest_url, f\"info/genomes/division/{division}?\"\n            )\n            for genome in division_genomes:\n                <IND>genomes[safe(genome[\"assembly_name\"])] = genome\n        <DED><DED>return genomes\n\n    <DED>def _genome_info_tuple(self, name):\n        <IND>\"\"\"tuple with assembly metadata\"\"\"\n        genome = self.genomes[name]\n        accession = self.assembly_accession(genome)\n        taxid = self.genome_taxid(genome)\n        taxid = str(taxid) if taxid != 0 else \"na\"\n\n        return (\n            name,\n            accession,\n            genome.get(\"scientific_name\", \"na\"),\n            taxid,\n            genome.get(\"genebuild\", \"na\"),\n        )\n\n    <DED>@goldfish_cache(ignore=[\"self\", \"rest_url\"])\n    def get_version(self, rest_url, vertebrates=False):\n        <IND>\"\"\"Retrieve current version from Ensembl FTP.\"\"\"\n        ext = \"/info/data/?\" if vertebrates else \"/info/eg_version?\"\n        ret = retry(self._request_json, 3, rest_url, ext)\n        releases = ret[\"releases\"] if vertebrates else [ret[\"version\"]]\n        return str(max(releases))\n\n    <DED>def get_genome_download_link(self, name, mask=\"soft\", **kwargs):\n        <IND>\"\"\"\n        Return Ensembl http or ftp link to the genome sequence\n\n        Parameters\n        ----------\n        name : str\n            Genome name. Current implementation will fail if exact\n            name is not found.\n\n        mask : str , optional\n            Masking level. Options: soft, hard or none. Default is soft.\n\n        Returns\n        ------\n        str with the http/ftp download link.\n        \"\"\"\n        genome = self.genomes[safe(name)]\n\n        # parse the division\n        division = genome[\"division\"].lower().replace(\"ensembl\", \"\")\n        if division == \"bacteria\":\n            <IND>raise NotImplementedError(\"bacteria from ensembl not yet supported\")\n\n        <DED>ftp_site = \"ftp://ftp.ensemblgenomes.org/pub\"\n        if division == \"vertebrates\":\n            <IND>ftp_site = \"ftp://ftp.ensembl.org/pub\"\n\n        # Ensembl release version\n        <DED>version = kwargs.get(\"version\")\n        if version is None:\n            <IND>version = self.get_version(self.rest_url, division == \"vertebrates\")\n\n        # division dependent url format\n        <DED>ftp_dir = \"{}/release-{}/fasta/{}/dna\".format(\n            division, version, genome[\"url_name\"].lower()\n        )\n        if division == \"vertebrates\":\n            <IND>ftp_dir = \"release-{}/fasta/{}/dna\".format(\n                version, genome[\"url_name\"].lower()\n            )\n        <DED>url = f\"{ftp_site}/{ftp_dir}\"\n\n        # masking and assembly level\n        def get_url(level=\"toplevel\"):\n            <IND>masks = {\"soft\": \"dna_sm.{}\", \"hard\": \"dna_rm.{}\", \"none\": \"dna.{}\"}\n            pattern = masks[mask].format(level)\n\n            asm_url = \"{}/{}.{}.{}.fa.gz\".format(\n                url,\n                genome[\"url_name\"].capitalize(),\n                re.sub(r\"\\.p\\d+$\", \"\", safe(genome[\"assembly_name\"])),\n                pattern,\n            )\n            return asm_url\n\n        # try to get the (much smaller) primary assembly,\n        # unless specified otherwise\n        <DED>link = get_url(\"primary_assembly\")\n        if kwargs.get(\"toplevel\") or not check_url(link, 2):\n            <IND>link = get_url()\n\n        <DED>if check_url(link, 2):\n            <IND>return link\n\n        <DED>raise GenomeDownloadError(\n            f\"Could not download genome {name} from {self.name}.\\n\"\n            \"URL is broken. Select another genome or provider.\\n\"\n            f\"Broken URL: {link}\"\n        )\n\n    <DED>def get_annotation_download_link(self, name, **kwargs):\n        <IND>\"\"\"\n        Parse and test the link to the Ensembl annotation file.\n\n        Parameters\n        ----------\n        name : str\n            Genome name\n        kwargs: dict , optional:\n            Provider specific options.\n\n            version : int , optional\n                Ensembl version. By default the latest version is used.\n        \"\"\"\n        genome = self.genomes[safe(name)]\n        division = genome[\"division\"].lower().replace(\"ensembl\", \"\")\n\n        ftp_site = \"ftp://ftp.ensemblgenomes.org/pub\"\n        if division == \"vertebrates\":\n            <IND>ftp_site = \"ftp://ftp.ensembl.org/pub\"\n\n        # Ensembl release version\n        <DED>version = kwargs.get(\"version\")\n        if version is None:\n            <IND>version = self.get_version(self.rest_url, division == \"vertebrates\")\n\n        <DED>if division != \"vertebrates\":\n            <IND>ftp_site += f\"/{division}\"\n\n        # Get the GTF URL\n        <DED>base_url = ftp_site + \"/release-{}/gtf/{}/{}.{}.{}.gtf.gz\"\n        safe_name = re.sub(r\"\\.p\\d+$\", \"\", name)\n        link = base_url.format(\n            version,\n            genome[\"url_name\"].lower(),\n            genome[\"url_name\"].capitalize(),\n            safe_name,\n            version,\n        )\n\n        if check_url(link, 2):\n            <IND>return link\n\n\n<DED><DED><DED>@register_provider(\"UCSC\")\nclass UcscProvider(ProviderBase):\n    <IND>\"\"\"\n    UCSC genome provider.\n\n    The UCSC API REST server is used to search and list genomes.\n    \"\"\"\n\n    base_url = \"http://hgdownload.soe.ucsc.edu/goldenPath\"\n    ucsc_url = base_url + \"/{0}/bigZips/chromFa.tar.gz\"\n    ucsc_url_masked = base_url + \"/{0}/bigZips/chromFaMasked.tar.gz\"\n    alt_ucsc_url = base_url + \"/{0}/bigZips/{0}.fa.gz\"\n    alt_ucsc_url_masked = base_url + \"/{0}/bigZips/{0}.fa.masked.gz\"\n    rest_url = \"http://api.genome.ucsc.edu/list/ucscGenomes\"\n    provider_specific_install_options = {\n        \"ucsc_annotation_type\": {\n            \"long\": \"annotation\",\n            \"help\": \"specify annotation to download: UCSC, Ensembl, NCBI_refseq or UCSC_refseq\",\n            \"default\": None,\n        },\n    }\n\n    def __init__(self):\n        <IND>self.name = \"UCSC\"\n        self.provider_status(self.base_url)\n        # Populate on init, so that methods can be cached\n        self.genomes = self._get_genomes(self.rest_url)\n        self.accession_fields = []\n        self.taxid_fields = [\"taxId\"]\n        self.description_fields = [\"description\", \"scientificName\"]\n\n    <DED>@staticmethod\n    @cache\n    def _get_genomes(rest_url):\n        <IND>logger.info(\"Downloading assembly summaries from UCSC\")\n\n        r = requests.get(rest_url, headers={\"Content-Type\": \"application/json\"})\n        if not r.ok:\n            <IND>r.raise_for_status()\n        <DED>ucsc_json = r.json()\n        genomes = ucsc_json[\"ucscGenomes\"]\n        return genomes\n\n    <DED>def _search_accession(self, term: str) -> Iterator[str]:\n        <IND>\"\"\"\n        UCSC does not store assembly accessions.\n        This function searches NCBI (most genomes + stable accession IDs),\n        then uses the NCBI accession search results for a UCSC text search.\n\n        Parameters\n        ----------\n        term : str\n            Assembly accession, GCA_/GCF_....\n\n        Yields\n        ------\n        genome names\n        \"\"\"\n        # NCBI provides a consistent assembly accession. This can be used to\n        # retrieve the species, and then search for that.\n        p = ProviderBase.create(\"NCBI\")\n        ncbi_genomes = list(p._search_accession(term))  # noqa\n\n        # remove superstrings (keep GRCh38, not GRCh38.p1 to GRCh38.p13)\n        unique_ncbi_genomes = []\n        for i in ncbi_genomes:\n            <IND>if sum([j in i for j in ncbi_genomes]) == 1:\n                <IND>unique_ncbi_genomes.append(i)\n\n        # add NCBI organism names to search terms\n        <DED><DED>organism_names = [\n            p.genomes[name][\"organism_name\"] for name in unique_ncbi_genomes\n        ]\n        terms = list(set(unique_ncbi_genomes + organism_names))\n\n        # search with NCBI results in the given provider\n        for name, metadata in self.genomes.items():\n            <IND>for term in terms:\n                <IND>term = lower(term)\n                if term in lower(name) or any(\n                    [term in lower(metadata[f]) for f in self.description_fields]\n                ):\n                    <IND>yield name\n                    break  # max one hit per genome\n\n    <DED><DED><DED><DED>@staticmethod\n    @cache\n    def assembly_accession(genome):\n        <IND>\"\"\"Return the assembly accession (GCA_/GCF_....) for a genome.\n\n        UCSC does not serve the assembly accession through the REST API.\n        Therefore, the readme.html is scanned for an assembly accession. If it is\n        not found, the linked NCBI assembly page will be checked.\n\n        Parameters\n        ----------\n        genome : dict\n            provider metadata dict of a genome.\n\n        Returns\n        ------\n        str\n            Assembly accession.\n        \"\"\"\n        try:\n            <IND>ucsc_url = \"https://hgdownload.soe.ucsc.edu/\" + genome[\"htmlPath\"]\n            text = read_url(ucsc_url)\n        <DED>except UnicodeDecodeError:\n            <IND>return \"na\"\n\n        # example accessions: GCA_000004335.1 (ailMel1)\n        # regex: GC[AF]_ = GCA_ or GCF_, \\d = digit, \\. = period\n        <DED>accession_regex = re.compile(r\"GC[AF]_\\d{9}\\.\\d+\")\n        match = accession_regex.search(text)\n        if match:\n            <IND>return match.group(0)\n\n        # Search for an assembly link at NCBI\n        <DED>match = re.search(r\"https?://www.ncbi.nlm.nih.gov/assembly/\\d+\", text)\n        if match:\n            <IND>ncbi_url = match.group(0)\n            text = read_url(ncbi_url)\n\n            # retrieve valid assembly accessions.\n            # contains additional info, such as '(latest)' or '(suppressed)'. Unused for now.\n            valid_accessions = re.findall(r\"assembly accession:.*?GC[AF]_.*?<\", text)\n            text = \" \".join(valid_accessions)\n            match = accession_regex.search(text)\n            if match:\n                <IND>return match.group(0)\n\n        <DED><DED>return \"na\"\n\n    <DED>def _genome_info_tuple(self, name):\n        <IND>\"\"\"tuple with assembly metadata\"\"\"\n        genome = self.genomes[name]\n        accession = self.assembly_accession(genome)\n        taxid = self.genome_taxid(genome)\n        taxid = str(taxid) if taxid != 0 else \"na\"\n\n        return (\n            name,\n            accession,\n            genome.get(\"scientificName\", \"na\"),\n            taxid,\n            genome.get(\"description\", \"na\"),\n        )\n\n    <DED>def get_genome_download_link(self, name, mask=\"soft\", **kwargs):\n        <IND>\"\"\"\n        Return UCSC http link to genome sequence\n\n        Parameters\n        ----------\n        name : str\n            Genome name. Current implementation will fail if exact\n            name is not found.\n\n        mask : str , optional\n            Masking level. Options: soft, hard or none. Default is soft.\n\n        Returns\n        ------\n        str with the http/ftp download link.\n        \"\"\"\n        # soft masked genomes. can be unmasked in _post _process_download\n        urls = [self.ucsc_url, self.alt_ucsc_url]\n        if mask == \"hard\":\n            <IND>urls = [self.ucsc_url_masked, self.alt_ucsc_url_masked]\n\n        <DED>for genome_url in urls:\n            <IND>link = genome_url.format(name)\n\n            if check_url(link, 2):\n                <IND>return link\n\n        <DED><DED>raise GenomeDownloadError(\n            f\"Could not download genome {name} from {self.name}.\\n\"\n            \"URLs are broken. Select another genome or provider.\\n\"\n            f\"Broken URLs: {', '.join([url.format(name) for url in urls])}\"\n        )\n\n    <DED>@staticmethod\n    def _post_process_download(name, localname, out_dir, mask=\"soft\"):  # noqa\n        <IND>\"\"\"\n        Unmask a softmasked genome if required\n\n        Parameters\n        ----------\n        name : str\n            unused for the UCSC function\n\n        localname : str\n            Custom name for your genome\n\n        out_dir : str\n            Output directory\n\n        mask : str , optional\n            masking level: soft/hard/none, default=soft\n        \"\"\"\n        if mask != \"none\":\n            <IND>return\n\n        <DED>logger.info(\"UCSC genomes are softmasked by default. Unmasking...\")\n\n        fa = os.path.join(out_dir, f\"{localname}.fa\")\n        old_fa = os.path.join(out_dir, f\"old_{localname}.fa\")\n        os.rename(fa, old_fa)\n        with open(old_fa) as old, open(fa, \"w\") as new:\n            <IND>for line in old:\n                <IND>if line.startswith(\">\"):\n                    <IND>new.write(line)\n                <DED>else:\n                    <IND>new.write(line.upper())\n\n    <DED><DED><DED><DED>def get_annotation_download_link(self, name, **kwargs):\n        <IND>\"\"\"\n        Parse and test the link to the UCSC annotation file.\n\n        Will check UCSC, Ensembl, NCBI RefSeq and UCSC RefSeq annotation, respectively.\n        More info on the annotation file on: https://genome.ucsc.edu/FAQ/FAQgenes.html#whatdo\n\n        Parameters\n        ----------\n        name : str\n            Genome name\n        \"\"\"\n        gtf_url = f\"http://hgdownload.soe.ucsc.edu/goldenPath/{name}/bigZips/genes/\"\n        txt_url = f\"http://hgdownload.cse.ucsc.edu/goldenPath/{name}/database/\"\n        annot_files = {\n            \"ucsc\": \"knownGene\",\n            \"ensembl\": \"ensGene\",\n            \"ncbi_refseq\": \"ncbiRefSeq\",\n            \"ucsc_refseq\": \"refGene\",\n        }\n\n        # download gtf format if possible, txt format if not\n        gtfs_exists = check_url(gtf_url, 2)\n        base_url = gtf_url + name + \".\" if gtfs_exists else txt_url\n        base_ext = \".gtf.gz\" if gtfs_exists else \".txt.gz\"\n\n        # download specified annotation type if requested\n        file = kwargs.get(\"ucsc_annotation_type\")\n        if file:\n            <IND>link = base_url + annot_files[file.lower()] + base_ext\n            if check_url(link, 2):\n                <IND>return link\n            <DED>logger.warning(f\"Specified annotation type ({file}) not found for {name}.\")\n\n        <DED>else:\n            # download first available annotation type found\n            <IND>for file in annot_files.values():\n                <IND>link = base_url + file + base_ext\n                if check_url(link, 2):\n                    <IND>return link\n\n\n<DED><DED><DED><DED><DED>@register_provider(\"NCBI\")\nclass NcbiProvider(ProviderBase):\n    <IND>\"\"\"\n    NCBI genome provider.\n\n    Uses the assembly reports page to search and list genomes.\n    \"\"\"\n\n    assembly_url = \"https://ftp.ncbi.nlm.nih.gov/genomes/ASSEMBLY_REPORTS/\"\n    provider_specific_install_options = {}\n\n    def __init__(self):\n        <IND>self.name = \"NCBI\"\n        self.provider_status(self.assembly_url)\n        # Populate on init, so that methods can be cached\n        self.genomes = self._get_genomes(self.assembly_url)\n        self.accession_fields = [\"assembly_accession\", \"gbrs_paired_asm\"]\n        self.taxid_fields = [\"species_taxid\", \"taxid\"]\n        self.description_fields = [\n            \"submitter\",\n            \"organism_name\",\n            \"assembly_accession\",\n            \"gbrs_paired_asm\",\n            \"paired_asm_comp\",\n        ]\n\n    <DED>@staticmethod\n    @cache\n    def _get_genomes(assembly_url):\n        <IND>\"\"\"Parse genomes from assembly summary txt files.\"\"\"\n        logger.info(\n            \"Downloading assembly summaries from NCBI, this will take a while...\"\n        )\n\n        def load_summary(url):\n            <IND>\"\"\"\n            lazy loading of the url so we can parse while downloading\n            \"\"\"\n            for row in urlopen(url):\n                <IND>yield row\n\n        <DED><DED>genomes = {}\n        # order is important as asm_name can repeat (overwriting the older name)\n        names = [\n            \"assembly_summary_genbank_historical.txt\",\n            \"assembly_summary_refseq_historical.txt\",\n            \"assembly_summary_genbank.txt\",\n            \"assembly_summary_refseq.txt\",\n        ]\n        for fname in names:\n            <IND>lines = load_summary(f\"{assembly_url}/{fname}\")\n            _ = next(lines)  # line 0 = comment\n            header = (\n                next(lines).decode(\"utf-8\").strip(\"# \").strip(\"\\n\").split(\"\\t\")\n            )  # line 1 = header\n            for line in tqdm(lines, desc=fname[17:-4], unit_scale=1, unit=\" genomes\"):\n                <IND>line = line.decode(\"utf-8\").strip(\"\\n\").split(\"\\t\")\n                if line[19] != \"na\":  # ftp_path must exist\n                    <IND>name = safe(line[15])  # overwrites older asm_names\n                    genomes[name] = dict(zip(header, line))\n        <DED><DED><DED>return genomes\n\n    <DED>def _genome_info_tuple(self, name):\n        <IND>\"\"\"tuple with assembly metadata\"\"\"\n        genome = self.genomes[name]\n        accession = self.assembly_accession(genome)\n        taxid = self.genome_taxid(genome)\n        taxid = str(taxid) if taxid != 0 else \"na\"\n\n        return (\n            name,\n            accession,\n            genome.get(\"organism_name\", \"na\"),\n            taxid,\n            genome.get(\"submitter\", \"na\"),\n        )\n\n    <DED>def get_genome_download_link(self, name, mask=\"soft\", **kwargs):\n        <IND>\"\"\"\n        Return NCBI ftp link to top-level genome sequence\n\n        Parameters\n        ----------\n        name : str\n            Genome name. Current implementation will fail if exact\n            name is not found.\n\n        mask : str , optional\n            Masking level. Options: soft, hard or none. Default is soft.\n\n        Returns\n        ------\n        str with the http/ftp download link.\n        \"\"\"\n        # only soft masked genomes available. can be (un)masked in _post_process_download\n        link = self._ftp_or_html_link(name, file_suffix=\"_genomic.fna.gz\")\n\n        if link:\n            <IND>return link\n\n        <DED>raise GenomeDownloadError(\n            f\"Could not download genome {name} from {self.name}.\\n\"\n            \"URL is broken. Select another genome or provider.\\n\"\n            f\"Broken URL: {link}\"\n        )\n\n    <DED>def _post_process_download(self, name, localname, out_dir, mask=\"soft\"):\n        <IND>\"\"\"\n        Replace accessions with sequence names in fasta file.\n\n        Applies masking.\n\n        Parameters\n        ----------\n        name : str\n            NCBI genome name\n\n        localname : str\n            Custom name for your genome\n\n        out_dir : str\n            Output directory\n\n        mask : str , optional\n            masking level: soft/hard/none, default=soft\n        \"\"\"\n        # Create mapping of accessions to names\n        url = self._ftp_or_html_link(\n            name, file_suffix=\"_assembly_report.txt\", skip_check=True\n        )\n\n        tr = {}\n        with urlopen(url) as response:\n            <IND>for line in response.read().decode(\"utf-8\").splitlines():\n                <IND>if line.startswith(\"#\"):\n                    <IND>continue\n                <DED>vals = line.strip().split(\"\\t\")\n                tr[vals[6]] = vals[0]\n\n        # mask sequence if required\n        <DED><DED>if mask == \"soft\":\n\n            <IND>def mask_cmd(txt):\n                <IND>return txt\n\n        <DED><DED>elif mask == \"hard\":\n            <IND>logger.info(\"NCBI genomes are softmasked by default. Hard masking...\")\n\n            def mask_cmd(txt):\n                <IND>return re.sub(\"[actg]\", \"N\", txt)\n\n        <DED><DED>else:\n            <IND>logger.info(\"NCBI genomes are softmasked by default. Unmasking...\")\n\n            def mask_cmd(txt):\n                <IND>return txt.upper()\n\n        # apply mapping and masking\n        <DED><DED>fa = os.path.join(out_dir, f\"{localname}.fa\")\n        old_fa = os.path.join(out_dir, f\"old_{localname}.fa\")\n        os.rename(fa, old_fa)\n        with open(old_fa) as old, open(fa, \"w\") as new:\n            <IND>for line in old:\n                <IND>if line.startswith(\">\"):\n                    <IND>desc = line.strip()[1:]\n                    name = desc.split(\" \")[0]\n                    new.write(\">{} {}\\n\".format(tr.get(name, name), desc))\n                <DED>else:\n                    <IND>new.write(mask_cmd(line))\n\n    <DED><DED><DED><DED>def get_annotation_download_link(self, name, **kwargs):\n        <IND>\"\"\"\n        Parse and test the link to the NCBI annotation file.\n\n        Parameters\n        ----------\n        name : str\n            Genome name\n        \"\"\"\n        return self._ftp_or_html_link(name, file_suffix=\"_genomic.gff.gz\")\n\n    <DED>def _ftp_or_html_link(self, name, file_suffix, skip_check=False):\n        <IND>\"\"\"\n        NCBI's files are accessible over FTP and HTTPS\n        Try HTTPS first and return the first functioning link\n        \"\"\"\n        genome = self.genomes[safe(name)]\n        ftp_link = genome[\"ftp_path\"]\n        html_link = ftp_link.replace(\"ftp://\", \"https://\")\n        for link in [html_link, ftp_link]:\n            <IND>link += \"/\" + link.split(\"/\")[-1] + file_suffix\n\n            if skip_check or check_url(link, max_tries=2, timeout=10):\n                <IND>return link\n\n\n<DED><DED><DED><DED>@register_provider(\"URL\")\nclass UrlProvider(ProviderBase):\n    <IND>\"\"\"\n    URL genome provider.\n\n    Simply download a genome directly through an url.\n    \"\"\"\n\n    provider_specific_install_options = {\n        \"to_annotation\": {\n            \"long\": \"to-annotation\",\n            \"help\": \"link to the annotation file, required if this is not in the same directory as the fasta file\",\n            \"default\": None,\n        },\n    }\n\n    def __init__(self):\n        <IND>self.name = \"URL\"\n        self.genomes = {}\n\n    <DED>def genome_taxid(self, genome):\n        <IND>return \"na\"\n\n    <DED>def assembly_accession(self, genome):\n        <IND>return \"na\"\n\n    <DED>def search(self, term):\n        <IND>\"\"\"return an empty generator,\n        same as if no genomes were found at the other providers\"\"\"\n        yield from ()\n\n    <DED>def _genome_info_tuple(self, name):\n        <IND>return tuple()\n\n    <DED>def check_name(self, name):\n        <IND>\"\"\"check if genome name can be found for provider\"\"\"\n        return\n\n    <DED>def get_genome_download_link(self, url, mask=None, **kwargs):\n        <IND>return url\n\n    <DED>def get_annotation_download_link(self, name, **kwargs):\n        <IND>\"\"\"\n        check if the linked annotation file is of a supported file type (gtf/gff3/bed)\n        \"\"\"\n        link = kwargs.get(\"to_annotation\")\n        if link:\n            <IND>ext = get_file_info(link)[0]\n            if ext not in [\".gtf\", \".gff\", \".gff3\", \".bed\"]:\n                <IND>raise TypeError(\n                    \"Only (gzipped) gtf, gff and bed files are supported.\\n\"\n                )\n\n            <DED>return link\n\n    <DED><DED>@staticmethod\n    def search_url_for_annotations(url, name):\n        <IND>\"\"\"Attempts to find gtf or gff3 files in the same location as the genome url\"\"\"\n        urldir = os.path.dirname(url)\n        logger.info(\n            \"You have requested the gene annotation to be downloaded. \"\n            \"Genomepy will check the remote directory: \"\n            f\"{urldir} for annotation files...\"\n        )\n\n        def fuzzy_annotation_search(search_name, search_list):\n            <IND>\"\"\"Returns all files containing both name and an annotation extension\"\"\"\n            hits = []\n            for ext in [\"gtf\", \"gff\"]:\n                # .*? = non greedy filler. 3? = optional 3 (for gff3). (\\.gz)? = optional .gz\n                <IND>expr = f\"{search_name}.*?\\.{ext}3?(\\.gz)?\"  # noqa: W605\n                for line in search_list:\n                    <IND>hit = re.search(expr, line, flags=re.IGNORECASE)\n                    if hit:\n                        <IND>hits.append(hit[0])\n            <DED><DED><DED>return hits\n\n        # try to find a GTF or GFF3 file\n        <DED>dirty_list = [str(line) for line in urlopen(urldir).readlines()]\n        fnames = fuzzy_annotation_search(name, dirty_list)\n        if not fnames:\n            <IND>raise FileNotFoundError(\n                \"Could not parse the remote directory. \"\n                \"Please supply a URL using --url-to-annotation.\\n\"\n            )\n\n        <DED>links = [urldir + \"/\" + fname for fname in fnames]\n        return links\n\n    <DED>def download_annotation(self, url, genomes_dir=None, localname=None, **kwargs):\n        <IND>\"\"\"\n        Attempts to download a gtf or gff3 file from the same location as the genome url\n\n        Parameters\n        ----------\n        url : str\n            url of where to download genome from\n\n        genomes_dir : str\n            Directory to install annotation\n\n        localname : str , optional\n            Custom name for your genome\n\n        kwargs: dict , optional:\n            Provider specific options.\n\n            to_annotation : str , optional\n                url to annotation file (only required if this not located in the same directory as the fasta)\n        \"\"\"\n        name = get_localname(url)\n        localname = get_localname(name, localname)\n        genomes_dir = get_genomes_dir(genomes_dir, check_exist=False)\n\n        if kwargs.get(\"to_annotation\"):\n            <IND>links = [self.get_annotation_download_link(None, **kwargs)]\n        <DED>else:\n            # can return multiple possible hits\n            <IND>links = self.search_url_for_annotations(url, name)\n\n        <DED>for link in links:\n            <IND>try:\n                <IND>self.attempt_and_report(name, localname, link, genomes_dir)\n                break\n            <DED>except GenomeDownloadError as e:\n                <IND>if not link == links[-1]:\n                    <IND>logger.info(\n                        \"One of the potential annotations was incompatible with genomepy. \"\n                        \"Attempting another...\"\n                    )\n                    continue\n                <DED>return e\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        return mapping\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "vanheeringen-lab/genomepy",
    "commit": "acb59fbc2678caa2f7a261e53c6656ff9e6e5925",
    "filename": "genomepy/provider.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/vanheeringen-lab-genomepy/genomepy/provider.py",
    "file_hunks_size": 8,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "genomepy/provider.py:355:8 Incompatible variable type [9]: localname is declared to have type `str` but is used as type `None`.",
    "message": " localname is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 355,
    "warning_line": "        localname: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        return mapping\n\n    def download_genome(\n        self,\n        name: str,\n        genomes_dir: str = None,\n        localname: str = None,\n        mask: Optional[str] = \"soft\",\n        **kwargs,\n    ):\n        \"\"\"\n        Download a (gzipped) genome file to a specific directory\n\n        Parameters\n        ----------\n        name : str\n            Genome / species name\n\n        genomes_dir : str , optional\n            Directory to install genome\n\n        localname : str , optional\n            Custom name for your genome\n\n        mask: str , optional\n            Masking, soft, hard or none (all other strings)\n        \"\"\"\n        name = safe(name)\n        self.check_name(name)\n\n        link = self.get_genome_download_link(name, mask=mask, **kwargs)\n\n        localname = get_localname(name, localname)\n        genomes_dir = get_genomes_dir(genomes_dir, check_exist=False)\n        out_dir = os.path.join(genomes_dir, localname)\n        mkdir_p(out_dir)\n\n        logger.info(f\"Downloading genome from {self.name}. Target URL: {link}...\")\n\n        # download to tmp dir. Move genome on completion.\n        # tmp dir is in genome_dir to prevent moving the genome between disks\n        tmp_dir = mkdtemp(dir=out_dir)\n        fname = os.path.join(tmp_dir, f\"{localname}.fa\")\n\n        download_file(link, fname)\n        logger.info(\"Genome download successful, starting post processing...\")\n\n        # unzip genome\n        if link.endswith(\".tar.gz\"):\n            tar_to_bigfile(fname, fname)\n        elif link.endswith(\".gz\"):\n            os.rename(fname, fname + \".gz\")\n            gunzip_and_name(fname + \".gz\")\n\n        # process genome (e.g. masking)\n        if hasattr(self, \"_post_process_download\"):\n            self._post_process_download(\n                name=name, localname=localname, out_dir=tmp_dir, mask=mask\n            )\n\n        # transfer the genome from the tmpdir to the genome_dir\n        src = fname\n        dst = os.path.join(genomes_dir, localname, os.path.basename(fname))\n        shutil.move(src, dst)\n        rm_rf(tmp_dir)\n\n        asm_report = os.path.join(out_dir, \"assembly_report.txt\")\n        asm_acc = self.assembly_accession(self.genomes.get(name))\n        if asm_acc != \"na\":\n            self.download_assembly_report(asm_acc, asm_report)\n\n        logger.info(\"name: {}\".format(name))\n        logger.info(\"local name: {}\".format(localname))\n        logger.info(\"fasta: {}\".format(dst))\n\n        # Create readme with information\n        readme = os.path.join(genomes_dir, localname, \"README.txt\")\n        metadata = {\n            \"name\": localname,\n            \"provider\": self.name,\n            \"original name\": name,\n            \"original filename\": os.path.split(link)[-1],\n            \"assembly_accession\": asm_acc,\n            \"tax_id\": self.genome_taxid(self.genomes.get(name)),\n            \"mask\": mask,\n            \"genome url\": link,\n            \"annotation url\": \"na\",\n            \"date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        }\n        write_readme(readme, metadata)\n\n    def get_annotation_download_link(self, name, **kwargs):\n        raise NotImplementedError()\n\n    @staticmethod\n    def download_and_generate_annotation(genomes_dir, annot_url, localname):\n        \"\"\"download annotation file, convert to intermediate file and generate output files\"\"\"\n\n        # create output directory if missing\n        out_dir = os.path.join(genomes_dir, localname)\n        mkdir_p(out_dir)\n\n        # download to tmp dir. Move genome on completion.\n        # tmp dir is in genome_dir to prevent moving the genome between disks\n        tmp_dir = mkdtemp(dir=out_dir)\n        ext, gz = get_file_info(annot_url)\n        annot_file = os.path.join(tmp_dir, localname + \".annotation\" + ext)\n        download_file(annot_url, annot_file)\n\n        # unzip input file (if needed)\n        if gz:\n            cmd = \"mv {0} {1} && gunzip -f {1}\"\n            sp.check_call(cmd.format(annot_file, annot_file + \".gz\"), shell=True)\n\n        # generate intermediate file (GenePred)\n        pred_file = annot_file.replace(ext, \".gp\")\n        if \"bed\" in ext:\n            cmd = \"bedToGenePred {0} {1}\"\n        elif \"gff\" in ext:\n            cmd = \"gff3ToGenePred -geneNameAttr=gene {0} {1}\"\n        elif \"gtf\" in ext:\n            cmd = \"gtfToGenePred -ignoreGroupsWithoutExons {0} {1}\"\n        elif \"txt\" in ext:\n            # UCSC annotations only\n            with open(annot_file) as f:\n                cols = f.readline().split(\"\\t\")\n\n            # extract the genePred format columns\n            start_col = 1\n            for i, col in enumerate(cols):\n                if col in [\"+\", \"-\"]:\n                    start_col = i - 1\n                    break\n            end_col = start_col + 10\n            cmd = (\n                f\"\"\"cat {{0}} | cut -f {start_col}-{end_col} | \"\"\"\n                # knownGene.txt.gz has spotty fields, this replaces non-integer fields with zeroes\n                + \"\"\"awk 'BEGIN {{FS=OFS=\"\\t\"}} !($11 ~ /^[0-9]+$/) {{$11=\"0\"}}1' > {1}\"\"\"\n            )\n        else:\n            raise TypeError(f\"file type extension {ext} not recognized!\")\n\n        sp.check_call(cmd.format(annot_file, pred_file), shell=True)\n\n        # generate gzipped gtf file (if required)\n        gtf_file = annot_file.replace(ext, \".gtf\")\n        if \"gtf\" not in ext:\n            cmd = \"genePredToGtf -source=genomepy file {0} {1}\"\n            sp.check_call(cmd.format(pred_file, gtf_file), shell=True)\n\n        # generate gzipped bed file (if required)\n        bed_file = annot_file.replace(ext, \".bed\")\n        if \"bed\" not in ext:\n            cmd = \"genePredToBed {0} {1}\"\n            sp.check_call(cmd.format(pred_file, bed_file), shell=True)\n\n        # transfer the files from the tmpdir to the genome_dir\n        for f in [gtf_file, bed_file]:\n            src = f\n            dst = os.path.join(out_dir, os.path.basename(f))\n            shutil.move(src, dst)\n        rm_rf(tmp_dir)\n\n    def attempt_and_report(self, name, localname, link, genomes_dir):\n        if not link:\n            logger.error(\n                f\"Could not download gene annotation for {name} from {self.name}.\"\n            )\n            return\n\n        try:\n            logger.info(\n                f\"Downloading annotation from {self.name}. Target URL: {link}...\"\n            )\n            self.download_and_generate_annotation(genomes_dir, link, localname)\n            logger.info(\"Annotation download successful\")\n        except Exception:\n            raise GenomeDownloadError(\n                f\"\\nCould not download annotation for {name} from {self.name}\\n\"\n                \"If you think the annotation should be there, please file a bug report at:\\n\"\n                \"https://github.com/vanheeringen-lab/genomepy/issues\\n\"\n            )\n\n        # Add annotation URL to readme\n        readme = os.path.join(genomes_dir, localname, \"README.txt\")\n        update_readme(readme, updated_metadata={\"annotation url\": link})\n\n    def download_annotation(self, name, genomes_dir=None, localname=None, **kwargs):\n        \"\"\"\n        Download annotation file to to a specific directory\n\n        Parameters\n        ----------\n        name : str\n            Genome / species name\n\n        genomes_dir : str , optional\n            Directory to install annotation\n\n        localname : str , optional\n            Custom name for your genome\n        \"\"\"\n        self.check_name(name)\n\n        link = self.get_annotation_download_link(name, **kwargs)\n\n        localname = get_localname(name, localname)\n        genomes_dir = get_genomes_dir(genomes_dir, check_exist=False)\n        self.attempt_and_report(name, localname, link, genomes_dir)\n\n    def _search_text(self, term: str) -> Iterator[str]:\n        \"\"\"check if search term is found in the provider's genome name or description field(s)\"\"\"\n        for name, metadata in self.genomes.items():\n            if term in lower(name) or any(\n                [term in lower(metadata[f]) for f in self.description_fields]\n            ):\n                yield name\n\n    def _search_accession(self, term: str) -> Iterator[str]:\n        \"\"\"check if search term is found in the provider's accession field(s)\"\"\"\n        # cut off prefix (GCA_/GCF_) and suffix (version numbers, e.g. '.3')\n        term = term[4:].split(\".\")[0]\n        for name, metadata in self.genomes.items():\n            if any([term in str(metadata[f]) for f in self.accession_fields]):\n                yield name\n\n    def _search_taxonomy(self, term: str) -> Iterator[str]:\n        \"\"\"check if search term matches to any of the provider's taxonomy field(s)\"\"\"\n        for name, metadata in self.genomes.items():\n            if any([term == lower(metadata[f]) for f in self.taxid_fields]):\n                yield name\n\n    def search(self, term: Union[str, int]):\n        \"\"\"\n        Search for term in genome names, descriptions and taxonomy ID.\n\n        The search is case-insensitive.\n\n        Parameters\n        ----------\n        term : str, int\n            Search term, case-insensitive.\n            Can be (part of) an assembly name (e.g. hg38),\n            scientific name (Danio rerio) or assembly\n            accession (GCA_000146045/GCF_...),\n            or an exact taxonomy id (7227).\n\n        Yields\n        ------\n        tuples with name and metadata\n        \"\"\"\n        term = lower(term)\n\n        search_function = self._search_text\n        if term.startswith((\"gca_\", \"gcf_\")):\n            search_function = self._search_accession\n        if term.isdigit():\n            search_function = self._search_taxonomy\n\n        for genome in search_function(term):\n            yield self._genome_info_tuple(genome)\n\n\nregister_provider = ProviderBase.register_provider\n\n\n@register_provider(\"Ensembl\")\nclass EnsemblProvider(ProviderBase):\n    \"\"\"\n    Ensembl genome provider.\n\n    Will search both ensembl.org as well as ensemblgenomes.org.\n    The bacteria division is not yet supported.\n    \"\"\"\n\n    rest_url = \"https://rest.ensembl.org/\"\n    provider_specific_install_options = {\n        \"toplevel\": {\n            \"long\": \"toplevel\",\n            \"help\": \"always download toplevel-genome\",\n            \"flag_value\": True,\n        },\n        \"version\": {\n            \"long\": \"version\",\n            \"help\": \"select release version\",\n            \"type\": int,\n            \"default\": None,\n        },\n    }\n\n    def __init__(self):\n        self.name = \"Ensembl\"\n        self.provider_status(self.rest_url + \"info/ping?\", max_tries=2)\n        # Populate on init, so that methods can be cached\n        self.genomes = self._get_genomes(self.rest_url)\n        self.accession_fields = [\"assembly_accession\"]\n        self.taxid_fields = [\"taxonomy_id\"]\n        self.description_fields = [\n            \"name\",\n            \"scientific_name\",\n            \"url_name\",\n            \"display_name\",\n        ]\n\n    @staticmethod\n    def _request_json(rest_url, ext):\n        \"\"\"Make a REST request and return as json.\"\"\"\n        if rest_url.endswith(\"/\") and ext.startswith(\"/\"):\n            ext = ext[1:]\n\n        r = requests.get(rest_url + ext, headers={\"Content-Type\": \"application/json\"})\n\n        if not r.ok:\n            r.raise_for_status()\n\n        return r.json()\n\n    @cache(ignore=[\"self\"])\n    def _get_genomes(self, rest_url):\n        logger.info(\"Downloading assembly summaries from Ensembl\")\n\n        genomes = {}\n        divisions = retry(self._request_json, 3, rest_url, \"info/divisions?\")\n        for division in divisions:\n            if division == \"EnsemblBacteria\":\n                continue\n            division_genomes = retry(\n                self._request_json, 3, rest_url, f\"info/genomes/division/{division}?\"\n            )\n            for genome in division_genomes:\n                genomes[safe(genome[\"assembly_name\"])] = genome\n        return genomes\n\n    def _genome_info_tuple(self, name):\n        \"\"\"tuple with assembly metadata\"\"\"\n        genome = self.genomes[name]\n        accession = self.assembly_accession(genome)\n        taxid = self.genome_taxid(genome)\n        taxid = str(taxid) if taxid != 0 else \"na\"\n\n        return (\n            name,\n            accession,\n            genome.get(\"scientific_name\", \"na\"),\n            taxid,\n            genome.get(\"genebuild\", \"na\"),\n        )\n\n    @goldfish_cache(ignore=[\"self\", \"rest_url\"])\n    def get_version(self, rest_url, vertebrates=False):\n        \"\"\"Retrieve current version from Ensembl FTP.\"\"\"\n        ext = \"/info/data/?\" if vertebrates else \"/info/eg_version?\"\n        ret = retry(self._request_json, 3, rest_url, ext)\n        releases = ret[\"releases\"] if vertebrates else [ret[\"version\"]]\n        return str(max(releases))\n\n    def get_genome_download_link(self, name, mask=\"soft\", **kwargs):\n        \"\"\"\n        Return Ensembl http or ftp link to the genome sequence\n\n        Parameters\n        ----------\n        name : str\n            Genome name. Current implementation will fail if exact\n            name is not found.\n\n        mask : str , optional\n            Masking level. Options: soft, hard or none. Default is soft.\n\n        Returns\n        ------\n        str with the http/ftp download link.\n        \"\"\"\n        genome = self.genomes[safe(name)]\n\n        # parse the division\n        division = genome[\"division\"].lower().replace(\"ensembl\", \"\")\n        if division == \"bacteria\":\n            raise NotImplementedError(\"bacteria from ensembl not yet supported\")\n\n        ftp_site = \"ftp://ftp.ensemblgenomes.org/pub\"\n        if division == \"vertebrates\":\n            ftp_site = \"ftp://ftp.ensembl.org/pub\"\n\n        # Ensembl release version\n        version = kwargs.get(\"version\")\n        if version is None:\n            version = self.get_version(self.rest_url, division == \"vertebrates\")\n\n        # division dependent url format\n        ftp_dir = \"{}/release-{}/fasta/{}/dna\".format(\n            division, version, genome[\"url_name\"].lower()\n        )\n        if division == \"vertebrates\":\n            ftp_dir = \"release-{}/fasta/{}/dna\".format(\n                version, genome[\"url_name\"].lower()\n            )\n        url = f\"{ftp_site}/{ftp_dir}\"\n\n        # masking and assembly level\n        def get_url(level=\"toplevel\"):\n            masks = {\"soft\": \"dna_sm.{}\", \"hard\": \"dna_rm.{}\", \"none\": \"dna.{}\"}\n            pattern = masks[mask].format(level)\n\n            asm_url = \"{}/{}.{}.{}.fa.gz\".format(\n                url,\n                genome[\"url_name\"].capitalize(),\n                re.sub(r\"\\.p\\d+$\", \"\", safe(genome[\"assembly_name\"])),\n                pattern,\n            )\n            return asm_url\n\n        # try to get the (much smaller) primary assembly,\n        # unless specified otherwise\n        link = get_url(\"primary_assembly\")\n        if kwargs.get(\"toplevel\") or not check_url(link, 2):\n            link = get_url()\n\n        if check_url(link, 2):\n            return link\n\n        raise GenomeDownloadError(\n            f\"Could not download genome {name} from {self.name}.\\n\"\n            \"URL is broken. Select another genome or provider.\\n\"\n            f\"Broken URL: {link}\"\n        )\n\n    def get_annotation_download_link(self, name, **kwargs):\n        \"\"\"\n        Parse and test the link to the Ensembl annotation file.\n\n        Parameters\n        ----------\n        name : str\n            Genome name\n        kwargs: dict , optional:\n            Provider specific options.\n\n            version : int , optional\n                Ensembl version. By default the latest version is used.\n        \"\"\"\n        genome = self.genomes[safe(name)]\n        division = genome[\"division\"].lower().replace(\"ensembl\", \"\")\n\n        ftp_site = \"ftp://ftp.ensemblgenomes.org/pub\"\n        if division == \"vertebrates\":\n            ftp_site = \"ftp://ftp.ensembl.org/pub\"\n\n        # Ensembl release version\n        version = kwargs.get(\"version\")\n        if version is None:\n            version = self.get_version(self.rest_url, division == \"vertebrates\")\n\n        if division != \"vertebrates\":\n            ftp_site += f\"/{division}\"\n\n        # Get the GTF URL\n        base_url = ftp_site + \"/release-{}/gtf/{}/{}.{}.{}.gtf.gz\"\n        safe_name = re.sub(r\"\\.p\\d+$\", \"\", name)\n        link = base_url.format(\n            version,\n            genome[\"url_name\"].lower(),\n            genome[\"url_name\"].capitalize(),\n            safe_name,\n            version,\n        )\n\n        if check_url(link, 2):\n            return link\n\n\n@register_provider(\"UCSC\")\nclass UcscProvider(ProviderBase):\n    \"\"\"\n    UCSC genome provider.\n\n    The UCSC API REST server is used to search and list genomes.\n    \"\"\"\n\n    base_url = \"http://hgdownload.soe.ucsc.edu/goldenPath\"\n    ucsc_url = base_url + \"/{0}/bigZips/chromFa.tar.gz\"\n    ucsc_url_masked = base_url + \"/{0}/bigZips/chromFaMasked.tar.gz\"\n    alt_ucsc_url = base_url + \"/{0}/bigZips/{0}.fa.gz\"\n    alt_ucsc_url_masked = base_url + \"/{0}/bigZips/{0}.fa.masked.gz\"\n    rest_url = \"http://api.genome.ucsc.edu/list/ucscGenomes\"\n    provider_specific_install_options = {\n        \"ucsc_annotation_type\": {\n            \"long\": \"annotation\",\n            \"help\": \"specify annotation to download: UCSC, Ensembl, NCBI_refseq or UCSC_refseq\",\n            \"default\": None,\n        },\n    }\n\n    def __init__(self):\n        self.name = \"UCSC\"\n        self.provider_status(self.base_url)\n        # Populate on init, so that methods can be cached\n        self.genomes = self._get_genomes(self.rest_url)\n        self.accession_fields = []\n        self.taxid_fields = [\"taxId\"]\n        self.description_fields = [\"description\", \"scientificName\"]\n\n    @staticmethod\n    @cache\n    def _get_genomes(rest_url):\n        logger.info(\"Downloading assembly summaries from UCSC\")\n\n        r = requests.get(rest_url, headers={\"Content-Type\": \"application/json\"})\n        if not r.ok:\n            r.raise_for_status()\n        ucsc_json = r.json()\n        genomes = ucsc_json[\"ucscGenomes\"]\n        return genomes\n\n    def _search_accession(self, term: str) -> Iterator[str]:\n        \"\"\"\n        UCSC does not store assembly accessions.\n        This function searches NCBI (most genomes + stable accession IDs),\n        then uses the NCBI accession search results for a UCSC text search.\n\n        Parameters\n        ----------\n        term : str\n            Assembly accession, GCA_/GCF_....\n\n        Yields\n        ------\n        genome names\n        \"\"\"\n        # NCBI provides a consistent assembly accession. This can be used to\n        # retrieve the species, and then search for that.\n        p = ProviderBase.create(\"NCBI\")\n        ncbi_genomes = list(p._search_accession(term))  # noqa\n\n        # remove superstrings (keep GRCh38, not GRCh38.p1 to GRCh38.p13)\n        unique_ncbi_genomes = []\n        for i in ncbi_genomes:\n            if sum([j in i for j in ncbi_genomes]) == 1:\n                unique_ncbi_genomes.append(i)\n\n        # add NCBI organism names to search terms\n        organism_names = [\n            p.genomes[name][\"organism_name\"] for name in unique_ncbi_genomes\n        ]\n        terms = list(set(unique_ncbi_genomes + organism_names))\n\n        # search with NCBI results in the given provider\n        for name, metadata in self.genomes.items():\n            for term in terms:\n                term = lower(term)\n                if term in lower(name) or any(\n                    [term in lower(metadata[f]) for f in self.description_fields]\n                ):\n                    yield name\n                    break  # max one hit per genome\n\n    @staticmethod\n    @cache\n    def assembly_accession(genome):\n        \"\"\"Return the assembly accession (GCA_/GCF_....) for a genome.\n\n        UCSC does not serve the assembly accession through the REST API.\n        Therefore, the readme.html is scanned for an assembly accession. If it is\n        not found, the linked NCBI assembly page will be checked.\n\n        Parameters\n        ----------\n        genome : dict\n            provider metadata dict of a genome.\n\n        Returns\n        ------\n        str\n            Assembly accession.\n        \"\"\"\n        try:\n            ucsc_url = \"https://hgdownload.soe.ucsc.edu/\" + genome[\"htmlPath\"]\n            text = read_url(ucsc_url)\n        except UnicodeDecodeError:\n            return \"na\"\n\n        # example accessions: GCA_000004335.1 (ailMel1)\n        # regex: GC[AF]_ = GCA_ or GCF_, \\d = digit, \\. = period\n        accession_regex = re.compile(r\"GC[AF]_\\d{9}\\.\\d+\")\n        match = accession_regex.search(text)\n        if match:\n            return match.group(0)\n\n        # Search for an assembly link at NCBI\n        match = re.search(r\"https?://www.ncbi.nlm.nih.gov/assembly/\\d+\", text)\n        if match:\n            ncbi_url = match.group(0)\n            text = read_url(ncbi_url)\n\n            # retrieve valid assembly accessions.\n            # contains additional info, such as '(latest)' or '(suppressed)'. Unused for now.\n            valid_accessions = re.findall(r\"assembly accession:.*?GC[AF]_.*?<\", text)\n            text = \" \".join(valid_accessions)\n            match = accession_regex.search(text)\n            if match:\n                return match.group(0)\n\n        return \"na\"\n\n    def _genome_info_tuple(self, name):\n        \"\"\"tuple with assembly metadata\"\"\"\n        genome = self.genomes[name]\n        accession = self.assembly_accession(genome)\n        taxid = self.genome_taxid(genome)\n        taxid = str(taxid) if taxid != 0 else \"na\"\n\n        return (\n            name,\n            accession,\n            genome.get(\"scientificName\", \"na\"),\n            taxid,\n            genome.get(\"description\", \"na\"),\n        )\n\n    def get_genome_download_link(self, name, mask=\"soft\", **kwargs):\n        \"\"\"\n        Return UCSC http link to genome sequence\n\n        Parameters\n        ----------\n        name : str\n            Genome name. Current implementation will fail if exact\n            name is not found.\n\n        mask : str , optional\n            Masking level. Options: soft, hard or none. Default is soft.\n\n        Returns\n        ------\n        str with the http/ftp download link.\n        \"\"\"\n        # soft masked genomes. can be unmasked in _post _process_download\n        urls = [self.ucsc_url, self.alt_ucsc_url]\n        if mask == \"hard\":\n            urls = [self.ucsc_url_masked, self.alt_ucsc_url_masked]\n\n        for genome_url in urls:\n            link = genome_url.format(name)\n\n            if check_url(link, 2):\n                return link\n\n        raise GenomeDownloadError(\n            f\"Could not download genome {name} from {self.name}.\\n\"\n            \"URLs are broken. Select another genome or provider.\\n\"\n            f\"Broken URLs: {', '.join([url.format(name) for url in urls])}\"\n        )\n\n    @staticmethod\n    def _post_process_download(name, localname, out_dir, mask=\"soft\"):  # noqa\n        \"\"\"\n        Unmask a softmasked genome if required\n\n        Parameters\n        ----------\n        name : str\n            unused for the UCSC function\n\n        localname : str\n            Custom name for your genome\n\n        out_dir : str\n            Output directory\n\n        mask : str , optional\n            masking level: soft/hard/none, default=soft\n        \"\"\"\n        if mask != \"none\":\n            return\n\n        logger.info(\"UCSC genomes are softmasked by default. Unmasking...\")\n\n        fa = os.path.join(out_dir, f\"{localname}.fa\")\n        old_fa = os.path.join(out_dir, f\"old_{localname}.fa\")\n        os.rename(fa, old_fa)\n        with open(old_fa) as old, open(fa, \"w\") as new:\n            for line in old:\n                if line.startswith(\">\"):\n                    new.write(line)\n                else:\n                    new.write(line.upper())\n\n    def get_annotation_download_link(self, name, **kwargs):\n        \"\"\"\n        Parse and test the link to the UCSC annotation file.\n\n        Will check UCSC, Ensembl, NCBI RefSeq and UCSC RefSeq annotation, respectively.\n        More info on the annotation file on: https://genome.ucsc.edu/FAQ/FAQgenes.html#whatdo\n\n        Parameters\n        ----------\n        name : str\n            Genome name\n        \"\"\"\n        gtf_url = f\"http://hgdownload.soe.ucsc.edu/goldenPath/{name}/bigZips/genes/\"\n        txt_url = f\"http://hgdownload.cse.ucsc.edu/goldenPath/{name}/database/\"\n        annot_files = {\n            \"ucsc\": \"knownGene\",\n            \"ensembl\": \"ensGene\",\n            \"ncbi_refseq\": \"ncbiRefSeq\",\n            \"ucsc_refseq\": \"refGene\",\n        }\n\n        # download gtf format if possible, txt format if not\n        gtfs_exists = check_url(gtf_url, 2)\n        base_url = gtf_url + name + \".\" if gtfs_exists else txt_url\n        base_ext = \".gtf.gz\" if gtfs_exists else \".txt.gz\"\n\n        # download specified annotation type if requested\n        file = kwargs.get(\"ucsc_annotation_type\")\n        if file:\n            link = base_url + annot_files[file.lower()] + base_ext\n            if check_url(link, 2):\n                return link\n            logger.warning(f\"Specified annotation type ({file}) not found for {name}.\")\n\n        else:\n            # download first available annotation type found\n            for file in annot_files.values():\n                link = base_url + file + base_ext\n                if check_url(link, 2):\n                    return link\n\n\n@register_provider(\"NCBI\")\nclass NcbiProvider(ProviderBase):\n    \"\"\"\n    NCBI genome provider.\n\n    Uses the assembly reports page to search and list genomes.\n    \"\"\"\n\n    assembly_url = \"https://ftp.ncbi.nlm.nih.gov/genomes/ASSEMBLY_REPORTS/\"\n    provider_specific_install_options = {}\n\n    def __init__(self):\n        self.name = \"NCBI\"\n        self.provider_status(self.assembly_url)\n        # Populate on init, so that methods can be cached\n        self.genomes = self._get_genomes(self.assembly_url)\n        self.accession_fields = [\"assembly_accession\", \"gbrs_paired_asm\"]\n        self.taxid_fields = [\"species_taxid\", \"taxid\"]\n        self.description_fields = [\n            \"submitter\",\n            \"organism_name\",\n            \"assembly_accession\",\n            \"gbrs_paired_asm\",\n            \"paired_asm_comp\",\n        ]\n\n    @staticmethod\n    @cache\n    def _get_genomes(assembly_url):\n        \"\"\"Parse genomes from assembly summary txt files.\"\"\"\n        logger.info(\n            \"Downloading assembly summaries from NCBI, this will take a while...\"\n        )\n\n        def load_summary(url):\n            \"\"\"\n            lazy loading of the url so we can parse while downloading\n            \"\"\"\n            for row in urlopen(url):\n                yield row\n\n        genomes = {}\n        # order is important as asm_name can repeat (overwriting the older name)\n        names = [\n            \"assembly_summary_genbank_historical.txt\",\n            \"assembly_summary_refseq_historical.txt\",\n            \"assembly_summary_genbank.txt\",\n            \"assembly_summary_refseq.txt\",\n        ]\n        for fname in names:\n            lines = load_summary(f\"{assembly_url}/{fname}\")\n            _ = next(lines)  # line 0 = comment\n            header = (\n                next(lines).decode(\"utf-8\").strip(\"# \").strip(\"\\n\").split(\"\\t\")\n            )  # line 1 = header\n            for line in tqdm(lines, desc=fname[17:-4], unit_scale=1, unit=\" genomes\"):\n                line = line.decode(\"utf-8\").strip(\"\\n\").split(\"\\t\")\n                if line[19] != \"na\":  # ftp_path must exist\n                    name = safe(line[15])  # overwrites older asm_names\n                    genomes[name] = dict(zip(header, line))\n        return genomes\n\n    def _genome_info_tuple(self, name):\n        \"\"\"tuple with assembly metadata\"\"\"\n        genome = self.genomes[name]\n        accession = self.assembly_accession(genome)\n        taxid = self.genome_taxid(genome)\n        taxid = str(taxid) if taxid != 0 else \"na\"\n\n        return (\n            name,\n            accession,\n            genome.get(\"organism_name\", \"na\"),\n            taxid,\n            genome.get(\"submitter\", \"na\"),\n        )\n\n    def get_genome_download_link(self, name, mask=\"soft\", **kwargs):\n        \"\"\"\n        Return NCBI ftp link to top-level genome sequence\n\n        Parameters\n        ----------\n        name : str\n            Genome name. Current implementation will fail if exact\n            name is not found.\n\n        mask : str , optional\n            Masking level. Options: soft, hard or none. Default is soft.\n\n        Returns\n        ------\n        str with the http/ftp download link.\n        \"\"\"\n        # only soft masked genomes available. can be (un)masked in _post_process_download\n        link = self._ftp_or_html_link(name, file_suffix=\"_genomic.fna.gz\")\n\n        if link:\n            return link\n\n        raise GenomeDownloadError(\n            f\"Could not download genome {name} from {self.name}.\\n\"\n            \"URL is broken. Select another genome or provider.\\n\"\n            f\"Broken URL: {link}\"\n        )\n\n    def _post_process_download(self, name, localname, out_dir, mask=\"soft\"):\n        \"\"\"\n        Replace accessions with sequence names in fasta file.\n\n        Applies masking.\n\n        Parameters\n        ----------\n        name : str\n            NCBI genome name\n\n        localname : str\n            Custom name for your genome\n\n        out_dir : str\n            Output directory\n\n        mask : str , optional\n            masking level: soft/hard/none, default=soft\n        \"\"\"\n        # Create mapping of accessions to names\n        url = self._ftp_or_html_link(\n            name, file_suffix=\"_assembly_report.txt\", skip_check=True\n        )\n\n        tr = {}\n        with urlopen(url) as response:\n            for line in response.read().decode(\"utf-8\").splitlines():\n                if line.startswith(\"#\"):\n                    continue\n                vals = line.strip().split(\"\\t\")\n                tr[vals[6]] = vals[0]\n\n        # mask sequence if required\n        if mask == \"soft\":\n\n            def mask_cmd(txt):\n                return txt\n\n        elif mask == \"hard\":\n            logger.info(\"NCBI genomes are softmasked by default. Hard masking...\")\n\n            def mask_cmd(txt):\n                return re.sub(\"[actg]\", \"N\", txt)\n\n        else:\n            logger.info(\"NCBI genomes are softmasked by default. Unmasking...\")\n\n            def mask_cmd(txt):\n                return txt.upper()\n\n        # apply mapping and masking\n        fa = os.path.join(out_dir, f\"{localname}.fa\")\n        old_fa = os.path.join(out_dir, f\"old_{localname}.fa\")\n        os.rename(fa, old_fa)\n        with open(old_fa) as old, open(fa, \"w\") as new:\n            for line in old:\n                if line.startswith(\">\"):\n                    desc = line.strip()[1:]\n                    name = desc.split(\" \")[0]\n                    new.write(\">{} {}\\n\".format(tr.get(name, name), desc))\n                else:\n                    new.write(mask_cmd(line))\n\n    def get_annotation_download_link(self, name, **kwargs):\n        \"\"\"\n        Parse and test the link to the NCBI annotation file.\n\n        Parameters\n        ----------\n        name : str\n            Genome name\n        \"\"\"\n        return self._ftp_or_html_link(name, file_suffix=\"_genomic.gff.gz\")\n\n    def _ftp_or_html_link(self, name, file_suffix, skip_check=False):\n        \"\"\"\n        NCBI's files are accessible over FTP and HTTPS\n        Try HTTPS first and return the first functioning link\n        \"\"\"\n        genome = self.genomes[safe(name)]\n        ftp_link = genome[\"ftp_path\"]\n        html_link = ftp_link.replace(\"ftp://\", \"https://\")\n        for link in [html_link, ftp_link]:\n            link += \"/\" + link.split(\"/\")[-1] + file_suffix\n\n            if skip_check or check_url(link, max_tries=2, timeout=10):\n                return link\n\n\n@register_provider(\"URL\")\nclass UrlProvider(ProviderBase):\n    \"\"\"\n    URL genome provider.\n\n    Simply download a genome directly through an url.\n    \"\"\"\n\n    provider_specific_install_options = {\n        \"to_annotation\": {\n            \"long\": \"to-annotation\",\n            \"help\": \"link to the annotation file, required if this is not in the same directory as the fasta file\",\n            \"default\": None,\n        },\n    }\n\n    def __init__(self):\n        self.name = \"URL\"\n        self.genomes = {}\n\n    def genome_taxid(self, genome):\n        return \"na\"\n\n    def assembly_accession(self, genome):\n        return \"na\"\n\n    def search(self, term):\n        \"\"\"return an empty generator,\n        same as if no genomes were found at the other providers\"\"\"\n        yield from ()\n\n    def _genome_info_tuple(self, name):\n        return tuple()\n\n    def check_name(self, name):\n        \"\"\"check if genome name can be found for provider\"\"\"\n        return\n\n    def get_genome_download_link(self, url, mask=None, **kwargs):\n        return url\n\n    def get_annotation_download_link(self, name, **kwargs):\n        \"\"\"\n        check if the linked annotation file is of a supported file type (gtf/gff3/bed)\n        \"\"\"\n        link = kwargs.get(\"to_annotation\")\n        if link:\n            ext = get_file_info(link)[0]\n            if ext not in [\".gtf\", \".gff\", \".gff3\", \".bed\"]:\n                raise TypeError(\n                    \"Only (gzipped) gtf, gff and bed files are supported.\\n\"\n                )\n\n            return link\n\n    @staticmethod\n    def search_url_for_annotations(url, name):\n        \"\"\"Attempts to find gtf or gff3 files in the same location as the genome url\"\"\"\n        urldir = os.path.dirname(url)\n        logger.info(\n            \"You have requested the gene annotation to be downloaded. \"\n            \"Genomepy will check the remote directory: \"\n            f\"{urldir} for annotation files...\"\n        )\n\n        def fuzzy_annotation_search(search_name, search_list):\n            \"\"\"Returns all files containing both name and an annotation extension\"\"\"\n            hits = []\n            for ext in [\"gtf\", \"gff\"]:\n                # .*? = non greedy filler. 3? = optional 3 (for gff3). (\\.gz)? = optional .gz\n                expr = f\"{search_name}.*?\\.{ext}3?(\\.gz)?\"  # noqa: W605\n                for line in search_list:\n                    hit = re.search(expr, line, flags=re.IGNORECASE)\n                    if hit:\n                        hits.append(hit[0])\n            return hits\n\n        # try to find a GTF or GFF3 file\n        dirty_list = [str(line) for line in urlopen(urldir).readlines()]\n        fnames = fuzzy_annotation_search(name, dirty_list)\n        if not fnames:\n            raise FileNotFoundError(\n                \"Could not parse the remote directory. \"\n                \"Please supply a URL using --url-to-annotation.\\n\"\n            )\n\n        links = [urldir + \"/\" + fname for fname in fnames]\n        return links\n\n    def download_annotation(self, url, genomes_dir=None, localname=None, **kwargs):\n        \"\"\"\n        Attempts to download a gtf or gff3 file from the same location as the genome url\n\n        Parameters\n        ----------\n        url : str\n            url of where to download genome from\n\n        genomes_dir : str\n            Directory to install annotation\n\n        localname : str , optional\n            Custom name for your genome\n\n        kwargs: dict , optional:\n            Provider specific options.\n\n            to_annotation : str , optional\n                url to annotation file (only required if this not located in the same directory as the fasta)\n        \"\"\"\n        name = get_localname(url)\n        localname = get_localname(name, localname)\n        genomes_dir = get_genomes_dir(genomes_dir, check_exist=False)\n\n        if kwargs.get(\"to_annotation\"):\n            links = [self.get_annotation_download_link(None, **kwargs)]\n        else:\n            # can return multiple possible hits\n            links = self.search_url_for_annotations(url, name)\n\n        for link in links:\n            try:\n                self.attempt_and_report(name, localname, link, genomes_dir)\n                break\n            except GenomeDownloadError as e:\n                if not link == links[-1]:\n                    logger.info(\n                        \"One of the potential annotations was incompatible with genomepy. \"\n                        \"Attempting another...\"\n                    )\n                    continue\n                return e\n",
        "source_code_len": 36065,
        "target_code": "        return mapping\n",
        "target_code_len": 23,
        "diff_format": "@@ -349,1057 +239,1 @@\n         return mapping\n-\n-    def download_genome(\n-        self,\n-        name: str,\n-        genomes_dir: str = None,\n-        localname: str = None,\n-        mask: Optional[str] = \"soft\",\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Download a (gzipped) genome file to a specific directory\n-\n-        Parameters\n-        ----------\n-        name : str\n-            Genome / species name\n-\n-        genomes_dir : str , optional\n-            Directory to install genome\n-\n-        localname : str , optional\n-            Custom name for your genome\n-\n-        mask: str , optional\n-            Masking, soft, hard or none (all other strings)\n-        \"\"\"\n-        name = safe(name)\n-        self.check_name(name)\n-\n-        link = self.get_genome_download_link(name, mask=mask, **kwargs)\n-\n-        localname = get_localname(name, localname)\n-        genomes_dir = get_genomes_dir(genomes_dir, check_exist=False)\n-        out_dir = os.path.join(genomes_dir, localname)\n-        mkdir_p(out_dir)\n-\n-        logger.info(f\"Downloading genome from {self.name}. Target URL: {link}...\")\n-\n-        # download to tmp dir. Move genome on completion.\n-        # tmp dir is in genome_dir to prevent moving the genome between disks\n-        tmp_dir = mkdtemp(dir=out_dir)\n-        fname = os.path.join(tmp_dir, f\"{localname}.fa\")\n-\n-        download_file(link, fname)\n-        logger.info(\"Genome download successful, starting post processing...\")\n-\n-        # unzip genome\n-        if link.endswith(\".tar.gz\"):\n-            tar_to_bigfile(fname, fname)\n-        elif link.endswith(\".gz\"):\n-            os.rename(fname, fname + \".gz\")\n-            gunzip_and_name(fname + \".gz\")\n-\n-        # process genome (e.g. masking)\n-        if hasattr(self, \"_post_process_download\"):\n-            self._post_process_download(\n-                name=name, localname=localname, out_dir=tmp_dir, mask=mask\n-            )\n-\n-        # transfer the genome from the tmpdir to the genome_dir\n-        src = fname\n-        dst = os.path.join(genomes_dir, localname, os.path.basename(fname))\n-        shutil.move(src, dst)\n-        rm_rf(tmp_dir)\n-\n-        asm_report = os.path.join(out_dir, \"assembly_report.txt\")\n-        asm_acc = self.assembly_accession(self.genomes.get(name))\n-        if asm_acc != \"na\":\n-            self.download_assembly_report(asm_acc, asm_report)\n-\n-        logger.info(\"name: {}\".format(name))\n-        logger.info(\"local name: {}\".format(localname))\n-        logger.info(\"fasta: {}\".format(dst))\n-\n-        # Create readme with information\n-        readme = os.path.join(genomes_dir, localname, \"README.txt\")\n-        metadata = {\n-            \"name\": localname,\n-            \"provider\": self.name,\n-            \"original name\": name,\n-            \"original filename\": os.path.split(link)[-1],\n-            \"assembly_accession\": asm_acc,\n-            \"tax_id\": self.genome_taxid(self.genomes.get(name)),\n-            \"mask\": mask,\n-            \"genome url\": link,\n-            \"annotation url\": \"na\",\n-            \"date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n-        }\n-        write_readme(readme, metadata)\n-\n-    def get_annotation_download_link(self, name, **kwargs):\n-        raise NotImplementedError()\n-\n-    @staticmethod\n-    def download_and_generate_annotation(genomes_dir, annot_url, localname):\n-        \"\"\"download annotation file, convert to intermediate file and generate output files\"\"\"\n-\n-        # create output directory if missing\n-        out_dir = os.path.join(genomes_dir, localname)\n-        mkdir_p(out_dir)\n-\n-        # download to tmp dir. Move genome on completion.\n-        # tmp dir is in genome_dir to prevent moving the genome between disks\n-        tmp_dir = mkdtemp(dir=out_dir)\n-        ext, gz = get_file_info(annot_url)\n-        annot_file = os.path.join(tmp_dir, localname + \".annotation\" + ext)\n-        download_file(annot_url, annot_file)\n-\n-        # unzip input file (if needed)\n-        if gz:\n-            cmd = \"mv {0} {1} && gunzip -f {1}\"\n-            sp.check_call(cmd.format(annot_file, annot_file + \".gz\"), shell=True)\n-\n-        # generate intermediate file (GenePred)\n-        pred_file = annot_file.replace(ext, \".gp\")\n-        if \"bed\" in ext:\n-            cmd = \"bedToGenePred {0} {1}\"\n-        elif \"gff\" in ext:\n-            cmd = \"gff3ToGenePred -geneNameAttr=gene {0} {1}\"\n-        elif \"gtf\" in ext:\n-            cmd = \"gtfToGenePred -ignoreGroupsWithoutExons {0} {1}\"\n-        elif \"txt\" in ext:\n-            # UCSC annotations only\n-            with open(annot_file) as f:\n-                cols = f.readline().split(\"\\t\")\n-\n-            # extract the genePred format columns\n-            start_col = 1\n-            for i, col in enumerate(cols):\n-                if col in [\"+\", \"-\"]:\n-                    start_col = i - 1\n-                    break\n-            end_col = start_col + 10\n-            cmd = (\n-                f\"\"\"cat {{0}} | cut -f {start_col}-{end_col} | \"\"\"\n-                # knownGene.txt.gz has spotty fields, this replaces non-integer fields with zeroes\n-                + \"\"\"awk 'BEGIN {{FS=OFS=\"\\t\"}} !($11 ~ /^[0-9]+$/) {{$11=\"0\"}}1' > {1}\"\"\"\n-            )\n-        else:\n-            raise TypeError(f\"file type extension {ext} not recognized!\")\n-\n-        sp.check_call(cmd.format(annot_file, pred_file), shell=True)\n-\n-        # generate gzipped gtf file (if required)\n-        gtf_file = annot_file.replace(ext, \".gtf\")\n-        if \"gtf\" not in ext:\n-            cmd = \"genePredToGtf -source=genomepy file {0} {1}\"\n-            sp.check_call(cmd.format(pred_file, gtf_file), shell=True)\n-\n-        # generate gzipped bed file (if required)\n-        bed_file = annot_file.replace(ext, \".bed\")\n-        if \"bed\" not in ext:\n-            cmd = \"genePredToBed {0} {1}\"\n-            sp.check_call(cmd.format(pred_file, bed_file), shell=True)\n-\n-        # transfer the files from the tmpdir to the genome_dir\n-        for f in [gtf_file, bed_file]:\n-            src = f\n-            dst = os.path.join(out_dir, os.path.basename(f))\n-            shutil.move(src, dst)\n-        rm_rf(tmp_dir)\n-\n-    def attempt_and_report(self, name, localname, link, genomes_dir):\n-        if not link:\n-            logger.error(\n-                f\"Could not download gene annotation for {name} from {self.name}.\"\n-            )\n-            return\n-\n-        try:\n-            logger.info(\n-                f\"Downloading annotation from {self.name}. Target URL: {link}...\"\n-            )\n-            self.download_and_generate_annotation(genomes_dir, link, localname)\n-            logger.info(\"Annotation download successful\")\n-        except Exception:\n-            raise GenomeDownloadError(\n-                f\"\\nCould not download annotation for {name} from {self.name}\\n\"\n-                \"If you think the annotation should be there, please file a bug report at:\\n\"\n-                \"https://github.com/vanheeringen-lab/genomepy/issues\\n\"\n-            )\n-\n-        # Add annotation URL to readme\n-        readme = os.path.join(genomes_dir, localname, \"README.txt\")\n-        update_readme(readme, updated_metadata={\"annotation url\": link})\n-\n-    def download_annotation(self, name, genomes_dir=None, localname=None, **kwargs):\n-        \"\"\"\n-        Download annotation file to to a specific directory\n-\n-        Parameters\n-        ----------\n-        name : str\n-            Genome / species name\n-\n-        genomes_dir : str , optional\n-            Directory to install annotation\n-\n-        localname : str , optional\n-            Custom name for your genome\n-        \"\"\"\n-        self.check_name(name)\n-\n-        link = self.get_annotation_download_link(name, **kwargs)\n-\n-        localname = get_localname(name, localname)\n-        genomes_dir = get_genomes_dir(genomes_dir, check_exist=False)\n-        self.attempt_and_report(name, localname, link, genomes_dir)\n-\n-    def _search_text(self, term: str) -> Iterator[str]:\n-        \"\"\"check if search term is found in the provider's genome name or description field(s)\"\"\"\n-        for name, metadata in self.genomes.items():\n-            if term in lower(name) or any(\n-                [term in lower(metadata[f]) for f in self.description_fields]\n-            ):\n-                yield name\n-\n-    def _search_accession(self, term: str) -> Iterator[str]:\n-        \"\"\"check if search term is found in the provider's accession field(s)\"\"\"\n-        # cut off prefix (GCA_/GCF_) and suffix (version numbers, e.g. '.3')\n-        term = term[4:].split(\".\")[0]\n-        for name, metadata in self.genomes.items():\n-            if any([term in str(metadata[f]) for f in self.accession_fields]):\n-                yield name\n-\n-    def _search_taxonomy(self, term: str) -> Iterator[str]:\n-        \"\"\"check if search term matches to any of the provider's taxonomy field(s)\"\"\"\n-        for name, metadata in self.genomes.items():\n-            if any([term == lower(metadata[f]) for f in self.taxid_fields]):\n-                yield name\n-\n-    def search(self, term: Union[str, int]):\n-        \"\"\"\n-        Search for term in genome names, descriptions and taxonomy ID.\n-\n-        The search is case-insensitive.\n-\n-        Parameters\n-        ----------\n-        term : str, int\n-            Search term, case-insensitive.\n-            Can be (part of) an assembly name (e.g. hg38),\n-            scientific name (Danio rerio) or assembly\n-            accession (GCA_000146045/GCF_...),\n-            or an exact taxonomy id (7227).\n-\n-        Yields\n-        ------\n-        tuples with name and metadata\n-        \"\"\"\n-        term = lower(term)\n-\n-        search_function = self._search_text\n-        if term.startswith((\"gca_\", \"gcf_\")):\n-            search_function = self._search_accession\n-        if term.isdigit():\n-            search_function = self._search_taxonomy\n-\n-        for genome in search_function(term):\n-            yield self._genome_info_tuple(genome)\n-\n-\n-register_provider = ProviderBase.register_provider\n-\n-\n-@register_provider(\"Ensembl\")\n-class EnsemblProvider(ProviderBase):\n-    \"\"\"\n-    Ensembl genome provider.\n-\n-    Will search both ensembl.org as well as ensemblgenomes.org.\n-    The bacteria division is not yet supported.\n-    \"\"\"\n-\n-    rest_url = \"https://rest.ensembl.org/\"\n-    provider_specific_install_options = {\n-        \"toplevel\": {\n-            \"long\": \"toplevel\",\n-            \"help\": \"always download toplevel-genome\",\n-            \"flag_value\": True,\n-        },\n-        \"version\": {\n-            \"long\": \"version\",\n-            \"help\": \"select release version\",\n-            \"type\": int,\n-            \"default\": None,\n-        },\n-    }\n-\n-    def __init__(self):\n-        self.name = \"Ensembl\"\n-        self.provider_status(self.rest_url + \"info/ping?\", max_tries=2)\n-        # Populate on init, so that methods can be cached\n-        self.genomes = self._get_genomes(self.rest_url)\n-        self.accession_fields = [\"assembly_accession\"]\n-        self.taxid_fields = [\"taxonomy_id\"]\n-        self.description_fields = [\n-            \"name\",\n-            \"scientific_name\",\n-            \"url_name\",\n-            \"display_name\",\n-        ]\n-\n-    @staticmethod\n-    def _request_json(rest_url, ext):\n-        \"\"\"Make a REST request and return as json.\"\"\"\n-        if rest_url.endswith(\"/\") and ext.startswith(\"/\"):\n-            ext = ext[1:]\n-\n-        r = requests.get(rest_url + ext, headers={\"Content-Type\": \"application/json\"})\n-\n-        if not r.ok:\n-            r.raise_for_status()\n-\n-        return r.json()\n-\n-    @cache(ignore=[\"self\"])\n-    def _get_genomes(self, rest_url):\n-        logger.info(\"Downloading assembly summaries from Ensembl\")\n-\n-        genomes = {}\n-        divisions = retry(self._request_json, 3, rest_url, \"info/divisions?\")\n-        for division in divisions:\n-            if division == \"EnsemblBacteria\":\n-                continue\n-            division_genomes = retry(\n-                self._request_json, 3, rest_url, f\"info/genomes/division/{division}?\"\n-            )\n-            for genome in division_genomes:\n-                genomes[safe(genome[\"assembly_name\"])] = genome\n-        return genomes\n-\n-    def _genome_info_tuple(self, name):\n-        \"\"\"tuple with assembly metadata\"\"\"\n-        genome = self.genomes[name]\n-        accession = self.assembly_accession(genome)\n-        taxid = self.genome_taxid(genome)\n-        taxid = str(taxid) if taxid != 0 else \"na\"\n-\n-        return (\n-            name,\n-            accession,\n-            genome.get(\"scientific_name\", \"na\"),\n-            taxid,\n-            genome.get(\"genebuild\", \"na\"),\n-        )\n-\n-    @goldfish_cache(ignore=[\"self\", \"rest_url\"])\n-    def get_version(self, rest_url, vertebrates=False):\n-        \"\"\"Retrieve current version from Ensembl FTP.\"\"\"\n-        ext = \"/info/data/?\" if vertebrates else \"/info/eg_version?\"\n-        ret = retry(self._request_json, 3, rest_url, ext)\n-        releases = ret[\"releases\"] if vertebrates else [ret[\"version\"]]\n-        return str(max(releases))\n-\n-    def get_genome_download_link(self, name, mask=\"soft\", **kwargs):\n-        \"\"\"\n-        Return Ensembl http or ftp link to the genome sequence\n-\n-        Parameters\n-        ----------\n-        name : str\n-            Genome name. Current implementation will fail if exact\n-            name is not found.\n-\n-        mask : str , optional\n-            Masking level. Options: soft, hard or none. Default is soft.\n-\n-        Returns\n-        ------\n-        str with the http/ftp download link.\n-        \"\"\"\n-        genome = self.genomes[safe(name)]\n-\n-        # parse the division\n-        division = genome[\"division\"].lower().replace(\"ensembl\", \"\")\n-        if division == \"bacteria\":\n-            raise NotImplementedError(\"bacteria from ensembl not yet supported\")\n-\n-        ftp_site = \"ftp://ftp.ensemblgenomes.org/pub\"\n-        if division == \"vertebrates\":\n-            ftp_site = \"ftp://ftp.ensembl.org/pub\"\n-\n-        # Ensembl release version\n-        version = kwargs.get(\"version\")\n-        if version is None:\n-            version = self.get_version(self.rest_url, division == \"vertebrates\")\n-\n-        # division dependent url format\n-        ftp_dir = \"{}/release-{}/fasta/{}/dna\".format(\n-            division, version, genome[\"url_name\"].lower()\n-        )\n-        if division == \"vertebrates\":\n-            ftp_dir = \"release-{}/fasta/{}/dna\".format(\n-                version, genome[\"url_name\"].lower()\n-            )\n-        url = f\"{ftp_site}/{ftp_dir}\"\n-\n-        # masking and assembly level\n-        def get_url(level=\"toplevel\"):\n-            masks = {\"soft\": \"dna_sm.{}\", \"hard\": \"dna_rm.{}\", \"none\": \"dna.{}\"}\n-            pattern = masks[mask].format(level)\n-\n-            asm_url = \"{}/{}.{}.{}.fa.gz\".format(\n-                url,\n-                genome[\"url_name\"].capitalize(),\n-                re.sub(r\"\\.p\\d+$\", \"\", safe(genome[\"assembly_name\"])),\n-                pattern,\n-            )\n-            return asm_url\n-\n-        # try to get the (much smaller) primary assembly,\n-        # unless specified otherwise\n-        link = get_url(\"primary_assembly\")\n-        if kwargs.get(\"toplevel\") or not check_url(link, 2):\n-            link = get_url()\n-\n-        if check_url(link, 2):\n-            return link\n-\n-        raise GenomeDownloadError(\n-            f\"Could not download genome {name} from {self.name}.\\n\"\n-            \"URL is broken. Select another genome or provider.\\n\"\n-            f\"Broken URL: {link}\"\n-        )\n-\n-    def get_annotation_download_link(self, name, **kwargs):\n-        \"\"\"\n-        Parse and test the link to the Ensembl annotation file.\n-\n-        Parameters\n-        ----------\n-        name : str\n-            Genome name\n-        kwargs: dict , optional:\n-            Provider specific options.\n-\n-            version : int , optional\n-                Ensembl version. By default the latest version is used.\n-        \"\"\"\n-        genome = self.genomes[safe(name)]\n-        division = genome[\"division\"].lower().replace(\"ensembl\", \"\")\n-\n-        ftp_site = \"ftp://ftp.ensemblgenomes.org/pub\"\n-        if division == \"vertebrates\":\n-            ftp_site = \"ftp://ftp.ensembl.org/pub\"\n-\n-        # Ensembl release version\n-        version = kwargs.get(\"version\")\n-        if version is None:\n-            version = self.get_version(self.rest_url, division == \"vertebrates\")\n-\n-        if division != \"vertebrates\":\n-            ftp_site += f\"/{division}\"\n-\n-        # Get the GTF URL\n-        base_url = ftp_site + \"/release-{}/gtf/{}/{}.{}.{}.gtf.gz\"\n-        safe_name = re.sub(r\"\\.p\\d+$\", \"\", name)\n-        link = base_url.format(\n-            version,\n-            genome[\"url_name\"].lower(),\n-            genome[\"url_name\"].capitalize(),\n-            safe_name,\n-            version,\n-        )\n-\n-        if check_url(link, 2):\n-            return link\n-\n-\n-@register_provider(\"UCSC\")\n-class UcscProvider(ProviderBase):\n-    \"\"\"\n-    UCSC genome provider.\n-\n-    The UCSC API REST server is used to search and list genomes.\n-    \"\"\"\n-\n-    base_url = \"http://hgdownload.soe.ucsc.edu/goldenPath\"\n-    ucsc_url = base_url + \"/{0}/bigZips/chromFa.tar.gz\"\n-    ucsc_url_masked = base_url + \"/{0}/bigZips/chromFaMasked.tar.gz\"\n-    alt_ucsc_url = base_url + \"/{0}/bigZips/{0}.fa.gz\"\n-    alt_ucsc_url_masked = base_url + \"/{0}/bigZips/{0}.fa.masked.gz\"\n-    rest_url = \"http://api.genome.ucsc.edu/list/ucscGenomes\"\n-    provider_specific_install_options = {\n-        \"ucsc_annotation_type\": {\n-            \"long\": \"annotation\",\n-            \"help\": \"specify annotation to download: UCSC, Ensembl, NCBI_refseq or UCSC_refseq\",\n-            \"default\": None,\n-        },\n-    }\n-\n-    def __init__(self):\n-        self.name = \"UCSC\"\n-        self.provider_status(self.base_url)\n-        # Populate on init, so that methods can be cached\n-        self.genomes = self._get_genomes(self.rest_url)\n-        self.accession_fields = []\n-        self.taxid_fields = [\"taxId\"]\n-        self.description_fields = [\"description\", \"scientificName\"]\n-\n-    @staticmethod\n-    @cache\n-    def _get_genomes(rest_url):\n-        logger.info(\"Downloading assembly summaries from UCSC\")\n-\n-        r = requests.get(rest_url, headers={\"Content-Type\": \"application/json\"})\n-        if not r.ok:\n-            r.raise_for_status()\n-        ucsc_json = r.json()\n-        genomes = ucsc_json[\"ucscGenomes\"]\n-        return genomes\n-\n-    def _search_accession(self, term: str) -> Iterator[str]:\n-        \"\"\"\n-        UCSC does not store assembly accessions.\n-        This function searches NCBI (most genomes + stable accession IDs),\n-        then uses the NCBI accession search results for a UCSC text search.\n-\n-        Parameters\n-        ----------\n-        term : str\n-            Assembly accession, GCA_/GCF_....\n-\n-        Yields\n-        ------\n-        genome names\n-        \"\"\"\n-        # NCBI provides a consistent assembly accession. This can be used to\n-        # retrieve the species, and then search for that.\n-        p = ProviderBase.create(\"NCBI\")\n-        ncbi_genomes = list(p._search_accession(term))  # noqa\n-\n-        # remove superstrings (keep GRCh38, not GRCh38.p1 to GRCh38.p13)\n-        unique_ncbi_genomes = []\n-        for i in ncbi_genomes:\n-            if sum([j in i for j in ncbi_genomes]) == 1:\n-                unique_ncbi_genomes.append(i)\n-\n-        # add NCBI organism names to search terms\n-        organism_names = [\n-            p.genomes[name][\"organism_name\"] for name in unique_ncbi_genomes\n-        ]\n-        terms = list(set(unique_ncbi_genomes + organism_names))\n-\n-        # search with NCBI results in the given provider\n-        for name, metadata in self.genomes.items():\n-            for term in terms:\n-                term = lower(term)\n-                if term in lower(name) or any(\n-                    [term in lower(metadata[f]) for f in self.description_fields]\n-                ):\n-                    yield name\n-                    break  # max one hit per genome\n-\n-    @staticmethod\n-    @cache\n-    def assembly_accession(genome):\n-        \"\"\"Return the assembly accession (GCA_/GCF_....) for a genome.\n-\n-        UCSC does not serve the assembly accession through the REST API.\n-        Therefore, the readme.html is scanned for an assembly accession. If it is\n-        not found, the linked NCBI assembly page will be checked.\n-\n-        Parameters\n-        ----------\n-        genome : dict\n-            provider metadata dict of a genome.\n-\n-        Returns\n-        ------\n-        str\n-            Assembly accession.\n-        \"\"\"\n-        try:\n-            ucsc_url = \"https://hgdownload.soe.ucsc.edu/\" + genome[\"htmlPath\"]\n-            text = read_url(ucsc_url)\n-        except UnicodeDecodeError:\n-            return \"na\"\n-\n-        # example accessions: GCA_000004335.1 (ailMel1)\n-        # regex: GC[AF]_ = GCA_ or GCF_, \\d = digit, \\. = period\n-        accession_regex = re.compile(r\"GC[AF]_\\d{9}\\.\\d+\")\n-        match = accession_regex.search(text)\n-        if match:\n-            return match.group(0)\n-\n-        # Search for an assembly link at NCBI\n-        match = re.search(r\"https?://www.ncbi.nlm.nih.gov/assembly/\\d+\", text)\n-        if match:\n-            ncbi_url = match.group(0)\n-            text = read_url(ncbi_url)\n-\n-            # retrieve valid assembly accessions.\n-            # contains additional info, such as '(latest)' or '(suppressed)'. Unused for now.\n-            valid_accessions = re.findall(r\"assembly accession:.*?GC[AF]_.*?<\", text)\n-            text = \" \".join(valid_accessions)\n-            match = accession_regex.search(text)\n-            if match:\n-                return match.group(0)\n-\n-        return \"na\"\n-\n-    def _genome_info_tuple(self, name):\n-        \"\"\"tuple with assembly metadata\"\"\"\n-        genome = self.genomes[name]\n-        accession = self.assembly_accession(genome)\n-        taxid = self.genome_taxid(genome)\n-        taxid = str(taxid) if taxid != 0 else \"na\"\n-\n-        return (\n-            name,\n-            accession,\n-            genome.get(\"scientificName\", \"na\"),\n-            taxid,\n-            genome.get(\"description\", \"na\"),\n-        )\n-\n-    def get_genome_download_link(self, name, mask=\"soft\", **kwargs):\n-        \"\"\"\n-        Return UCSC http link to genome sequence\n-\n-        Parameters\n-        ----------\n-        name : str\n-            Genome name. Current implementation will fail if exact\n-            name is not found.\n-\n-        mask : str , optional\n-            Masking level. Options: soft, hard or none. Default is soft.\n-\n-        Returns\n-        ------\n-        str with the http/ftp download link.\n-        \"\"\"\n-        # soft masked genomes. can be unmasked in _post _process_download\n-        urls = [self.ucsc_url, self.alt_ucsc_url]\n-        if mask == \"hard\":\n-            urls = [self.ucsc_url_masked, self.alt_ucsc_url_masked]\n-\n-        for genome_url in urls:\n-            link = genome_url.format(name)\n-\n-            if check_url(link, 2):\n-                return link\n-\n-        raise GenomeDownloadError(\n-            f\"Could not download genome {name} from {self.name}.\\n\"\n-            \"URLs are broken. Select another genome or provider.\\n\"\n-            f\"Broken URLs: {', '.join([url.format(name) for url in urls])}\"\n-        )\n-\n-    @staticmethod\n-    def _post_process_download(name, localname, out_dir, mask=\"soft\"):  # noqa\n-        \"\"\"\n-        Unmask a softmasked genome if required\n-\n-        Parameters\n-        ----------\n-        name : str\n-            unused for the UCSC function\n-\n-        localname : str\n-            Custom name for your genome\n-\n-        out_dir : str\n-            Output directory\n-\n-        mask : str , optional\n-            masking level: soft/hard/none, default=soft\n-        \"\"\"\n-        if mask != \"none\":\n-            return\n-\n-        logger.info(\"UCSC genomes are softmasked by default. Unmasking...\")\n-\n-        fa = os.path.join(out_dir, f\"{localname}.fa\")\n-        old_fa = os.path.join(out_dir, f\"old_{localname}.fa\")\n-        os.rename(fa, old_fa)\n-        with open(old_fa) as old, open(fa, \"w\") as new:\n-            for line in old:\n-                if line.startswith(\">\"):\n-                    new.write(line)\n-                else:\n-                    new.write(line.upper())\n-\n-    def get_annotation_download_link(self, name, **kwargs):\n-        \"\"\"\n-        Parse and test the link to the UCSC annotation file.\n-\n-        Will check UCSC, Ensembl, NCBI RefSeq and UCSC RefSeq annotation, respectively.\n-        More info on the annotation file on: https://genome.ucsc.edu/FAQ/FAQgenes.html#whatdo\n-\n-        Parameters\n-        ----------\n-        name : str\n-            Genome name\n-        \"\"\"\n-        gtf_url = f\"http://hgdownload.soe.ucsc.edu/goldenPath/{name}/bigZips/genes/\"\n-        txt_url = f\"http://hgdownload.cse.ucsc.edu/goldenPath/{name}/database/\"\n-        annot_files = {\n-            \"ucsc\": \"knownGene\",\n-            \"ensembl\": \"ensGene\",\n-            \"ncbi_refseq\": \"ncbiRefSeq\",\n-            \"ucsc_refseq\": \"refGene\",\n-        }\n-\n-        # download gtf format if possible, txt format if not\n-        gtfs_exists = check_url(gtf_url, 2)\n-        base_url = gtf_url + name + \".\" if gtfs_exists else txt_url\n-        base_ext = \".gtf.gz\" if gtfs_exists else \".txt.gz\"\n-\n-        # download specified annotation type if requested\n-        file = kwargs.get(\"ucsc_annotation_type\")\n-        if file:\n-            link = base_url + annot_files[file.lower()] + base_ext\n-            if check_url(link, 2):\n-                return link\n-            logger.warning(f\"Specified annotation type ({file}) not found for {name}.\")\n-\n-        else:\n-            # download first available annotation type found\n-            for file in annot_files.values():\n-                link = base_url + file + base_ext\n-                if check_url(link, 2):\n-                    return link\n-\n-\n-@register_provider(\"NCBI\")\n-class NcbiProvider(ProviderBase):\n-    \"\"\"\n-    NCBI genome provider.\n-\n-    Uses the assembly reports page to search and list genomes.\n-    \"\"\"\n-\n-    assembly_url = \"https://ftp.ncbi.nlm.nih.gov/genomes/ASSEMBLY_REPORTS/\"\n-    provider_specific_install_options = {}\n-\n-    def __init__(self):\n-        self.name = \"NCBI\"\n-        self.provider_status(self.assembly_url)\n-        # Populate on init, so that methods can be cached\n-        self.genomes = self._get_genomes(self.assembly_url)\n-        self.accession_fields = [\"assembly_accession\", \"gbrs_paired_asm\"]\n-        self.taxid_fields = [\"species_taxid\", \"taxid\"]\n-        self.description_fields = [\n-            \"submitter\",\n-            \"organism_name\",\n-            \"assembly_accession\",\n-            \"gbrs_paired_asm\",\n-            \"paired_asm_comp\",\n-        ]\n-\n-    @staticmethod\n-    @cache\n-    def _get_genomes(assembly_url):\n-        \"\"\"Parse genomes from assembly summary txt files.\"\"\"\n-        logger.info(\n-            \"Downloading assembly summaries from NCBI, this will take a while...\"\n-        )\n-\n-        def load_summary(url):\n-            \"\"\"\n-            lazy loading of the url so we can parse while downloading\n-            \"\"\"\n-            for row in urlopen(url):\n-                yield row\n-\n-        genomes = {}\n-        # order is important as asm_name can repeat (overwriting the older name)\n-        names = [\n-            \"assembly_summary_genbank_historical.txt\",\n-            \"assembly_summary_refseq_historical.txt\",\n-            \"assembly_summary_genbank.txt\",\n-            \"assembly_summary_refseq.txt\",\n-        ]\n-        for fname in names:\n-            lines = load_summary(f\"{assembly_url}/{fname}\")\n-            _ = next(lines)  # line 0 = comment\n-            header = (\n-                next(lines).decode(\"utf-8\").strip(\"# \").strip(\"\\n\").split(\"\\t\")\n-            )  # line 1 = header\n-            for line in tqdm(lines, desc=fname[17:-4], unit_scale=1, unit=\" genomes\"):\n-                line = line.decode(\"utf-8\").strip(\"\\n\").split(\"\\t\")\n-                if line[19] != \"na\":  # ftp_path must exist\n-                    name = safe(line[15])  # overwrites older asm_names\n-                    genomes[name] = dict(zip(header, line))\n-        return genomes\n-\n-    def _genome_info_tuple(self, name):\n-        \"\"\"tuple with assembly metadata\"\"\"\n-        genome = self.genomes[name]\n-        accession = self.assembly_accession(genome)\n-        taxid = self.genome_taxid(genome)\n-        taxid = str(taxid) if taxid != 0 else \"na\"\n-\n-        return (\n-            name,\n-            accession,\n-            genome.get(\"organism_name\", \"na\"),\n-            taxid,\n-            genome.get(\"submitter\", \"na\"),\n-        )\n-\n-    def get_genome_download_link(self, name, mask=\"soft\", **kwargs):\n-        \"\"\"\n-        Return NCBI ftp link to top-level genome sequence\n-\n-        Parameters\n-        ----------\n-        name : str\n-            Genome name. Current implementation will fail if exact\n-            name is not found.\n-\n-        mask : str , optional\n-            Masking level. Options: soft, hard or none. Default is soft.\n-\n-        Returns\n-        ------\n-        str with the http/ftp download link.\n-        \"\"\"\n-        # only soft masked genomes available. can be (un)masked in _post_process_download\n-        link = self._ftp_or_html_link(name, file_suffix=\"_genomic.fna.gz\")\n-\n-        if link:\n-            return link\n-\n-        raise GenomeDownloadError(\n-            f\"Could not download genome {name} from {self.name}.\\n\"\n-            \"URL is broken. Select another genome or provider.\\n\"\n-            f\"Broken URL: {link}\"\n-        )\n-\n-    def _post_process_download(self, name, localname, out_dir, mask=\"soft\"):\n-        \"\"\"\n-        Replace accessions with sequence names in fasta file.\n-\n-        Applies masking.\n-\n-        Parameters\n-        ----------\n-        name : str\n-            NCBI genome name\n-\n-        localname : str\n-            Custom name for your genome\n-\n-        out_dir : str\n-            Output directory\n-\n-        mask : str , optional\n-            masking level: soft/hard/none, default=soft\n-        \"\"\"\n-        # Create mapping of accessions to names\n-        url = self._ftp_or_html_link(\n-            name, file_suffix=\"_assembly_report.txt\", skip_check=True\n-        )\n-\n-        tr = {}\n-        with urlopen(url) as response:\n-            for line in response.read().decode(\"utf-8\").splitlines():\n-                if line.startswith(\"#\"):\n-                    continue\n-                vals = line.strip().split(\"\\t\")\n-                tr[vals[6]] = vals[0]\n-\n-        # mask sequence if required\n-        if mask == \"soft\":\n-\n-            def mask_cmd(txt):\n-                return txt\n-\n-        elif mask == \"hard\":\n-            logger.info(\"NCBI genomes are softmasked by default. Hard masking...\")\n-\n-            def mask_cmd(txt):\n-                return re.sub(\"[actg]\", \"N\", txt)\n-\n-        else:\n-            logger.info(\"NCBI genomes are softmasked by default. Unmasking...\")\n-\n-            def mask_cmd(txt):\n-                return txt.upper()\n-\n-        # apply mapping and masking\n-        fa = os.path.join(out_dir, f\"{localname}.fa\")\n-        old_fa = os.path.join(out_dir, f\"old_{localname}.fa\")\n-        os.rename(fa, old_fa)\n-        with open(old_fa) as old, open(fa, \"w\") as new:\n-            for line in old:\n-                if line.startswith(\">\"):\n-                    desc = line.strip()[1:]\n-                    name = desc.split(\" \")[0]\n-                    new.write(\">{} {}\\n\".format(tr.get(name, name), desc))\n-                else:\n-                    new.write(mask_cmd(line))\n-\n-    def get_annotation_download_link(self, name, **kwargs):\n-        \"\"\"\n-        Parse and test the link to the NCBI annotation file.\n-\n-        Parameters\n-        ----------\n-        name : str\n-            Genome name\n-        \"\"\"\n-        return self._ftp_or_html_link(name, file_suffix=\"_genomic.gff.gz\")\n-\n-    def _ftp_or_html_link(self, name, file_suffix, skip_check=False):\n-        \"\"\"\n-        NCBI's files are accessible over FTP and HTTPS\n-        Try HTTPS first and return the first functioning link\n-        \"\"\"\n-        genome = self.genomes[safe(name)]\n-        ftp_link = genome[\"ftp_path\"]\n-        html_link = ftp_link.replace(\"ftp://\", \"https://\")\n-        for link in [html_link, ftp_link]:\n-            link += \"/\" + link.split(\"/\")[-1] + file_suffix\n-\n-            if skip_check or check_url(link, max_tries=2, timeout=10):\n-                return link\n-\n-\n-@register_provider(\"URL\")\n-class UrlProvider(ProviderBase):\n-    \"\"\"\n-    URL genome provider.\n-\n-    Simply download a genome directly through an url.\n-    \"\"\"\n-\n-    provider_specific_install_options = {\n-        \"to_annotation\": {\n-            \"long\": \"to-annotation\",\n-            \"help\": \"link to the annotation file, required if this is not in the same directory as the fasta file\",\n-            \"default\": None,\n-        },\n-    }\n-\n-    def __init__(self):\n-        self.name = \"URL\"\n-        self.genomes = {}\n-\n-    def genome_taxid(self, genome):\n-        return \"na\"\n-\n-    def assembly_accession(self, genome):\n-        return \"na\"\n-\n-    def search(self, term):\n-        \"\"\"return an empty generator,\n-        same as if no genomes were found at the other providers\"\"\"\n-        yield from ()\n-\n-    def _genome_info_tuple(self, name):\n-        return tuple()\n-\n-    def check_name(self, name):\n-        \"\"\"check if genome name can be found for provider\"\"\"\n-        return\n-\n-    def get_genome_download_link(self, url, mask=None, **kwargs):\n-        return url\n-\n-    def get_annotation_download_link(self, name, **kwargs):\n-        \"\"\"\n-        check if the linked annotation file is of a supported file type (gtf/gff3/bed)\n-        \"\"\"\n-        link = kwargs.get(\"to_annotation\")\n-        if link:\n-            ext = get_file_info(link)[0]\n-            if ext not in [\".gtf\", \".gff\", \".gff3\", \".bed\"]:\n-                raise TypeError(\n-                    \"Only (gzipped) gtf, gff and bed files are supported.\\n\"\n-                )\n-\n-            return link\n-\n-    @staticmethod\n-    def search_url_for_annotations(url, name):\n-        \"\"\"Attempts to find gtf or gff3 files in the same location as the genome url\"\"\"\n-        urldir = os.path.dirname(url)\n-        logger.info(\n-            \"You have requested the gene annotation to be downloaded. \"\n-            \"Genomepy will check the remote directory: \"\n-            f\"{urldir} for annotation files...\"\n-        )\n-\n-        def fuzzy_annotation_search(search_name, search_list):\n-            \"\"\"Returns all files containing both name and an annotation extension\"\"\"\n-            hits = []\n-            for ext in [\"gtf\", \"gff\"]:\n-                # .*? = non greedy filler. 3? = optional 3 (for gff3). (\\.gz)? = optional .gz\n-                expr = f\"{search_name}.*?\\.{ext}3?(\\.gz)?\"  # noqa: W605\n-                for line in search_list:\n-                    hit = re.search(expr, line, flags=re.IGNORECASE)\n-                    if hit:\n-                        hits.append(hit[0])\n-            return hits\n-\n-        # try to find a GTF or GFF3 file\n-        dirty_list = [str(line) for line in urlopen(urldir).readlines()]\n-        fnames = fuzzy_annotation_search(name, dirty_list)\n-        if not fnames:\n-            raise FileNotFoundError(\n-                \"Could not parse the remote directory. \"\n-                \"Please supply a URL using --url-to-annotation.\\n\"\n-            )\n-\n-        links = [urldir + \"/\" + fname for fname in fnames]\n-        return links\n-\n-    def download_annotation(self, url, genomes_dir=None, localname=None, **kwargs):\n-        \"\"\"\n-        Attempts to download a gtf or gff3 file from the same location as the genome url\n-\n-        Parameters\n-        ----------\n-        url : str\n-            url of where to download genome from\n-\n-        genomes_dir : str\n-            Directory to install annotation\n-\n-        localname : str , optional\n-            Custom name for your genome\n-\n-        kwargs: dict , optional:\n-            Provider specific options.\n-\n-            to_annotation : str , optional\n-                url to annotation file (only required if this not located in the same directory as the fasta)\n-        \"\"\"\n-        name = get_localname(url)\n-        localname = get_localname(name, localname)\n-        genomes_dir = get_genomes_dir(genomes_dir, check_exist=False)\n-\n-        if kwargs.get(\"to_annotation\"):\n-            links = [self.get_annotation_download_link(None, **kwargs)]\n-        else:\n-            # can return multiple possible hits\n-            links = self.search_url_for_annotations(url, name)\n-\n-        for link in links:\n-            try:\n-                self.attempt_and_report(name, localname, link, genomes_dir)\n-                break\n-            except GenomeDownloadError as e:\n-                if not link == links[-1]:\n-                    logger.info(\n-                        \"One of the potential annotations was incompatible with genomepy. \"\n-                        \"Attempting another...\"\n-                    )\n-                    continue\n-                return e\n",
        "source_code_with_indent": "        return mapping\n\n    <DED>def download_genome(\n        self,\n        name: str,\n        genomes_dir: str = None,\n        localname: str = None,\n        mask: Optional[str] = \"soft\",\n        **kwargs,\n    ):\n        <IND>\"\"\"\n        Download a (gzipped) genome file to a specific directory\n\n        Parameters\n        ----------\n        name : str\n            Genome / species name\n\n        genomes_dir : str , optional\n            Directory to install genome\n\n        localname : str , optional\n            Custom name for your genome\n\n        mask: str , optional\n            Masking, soft, hard or none (all other strings)\n        \"\"\"\n        name = safe(name)\n        self.check_name(name)\n\n        link = self.get_genome_download_link(name, mask=mask, **kwargs)\n\n        localname = get_localname(name, localname)\n        genomes_dir = get_genomes_dir(genomes_dir, check_exist=False)\n        out_dir = os.path.join(genomes_dir, localname)\n        mkdir_p(out_dir)\n\n        logger.info(f\"Downloading genome from {self.name}. Target URL: {link}...\")\n\n        # download to tmp dir. Move genome on completion.\n        # tmp dir is in genome_dir to prevent moving the genome between disks\n        tmp_dir = mkdtemp(dir=out_dir)\n        fname = os.path.join(tmp_dir, f\"{localname}.fa\")\n\n        download_file(link, fname)\n        logger.info(\"Genome download successful, starting post processing...\")\n\n        # unzip genome\n        if link.endswith(\".tar.gz\"):\n            <IND>tar_to_bigfile(fname, fname)\n        <DED>elif link.endswith(\".gz\"):\n            <IND>os.rename(fname, fname + \".gz\")\n            gunzip_and_name(fname + \".gz\")\n\n        # process genome (e.g. masking)\n        <DED>if hasattr(self, \"_post_process_download\"):\n            <IND>self._post_process_download(\n                name=name, localname=localname, out_dir=tmp_dir, mask=mask\n            )\n\n        # transfer the genome from the tmpdir to the genome_dir\n        <DED>src = fname\n        dst = os.path.join(genomes_dir, localname, os.path.basename(fname))\n        shutil.move(src, dst)\n        rm_rf(tmp_dir)\n\n        asm_report = os.path.join(out_dir, \"assembly_report.txt\")\n        asm_acc = self.assembly_accession(self.genomes.get(name))\n        if asm_acc != \"na\":\n            <IND>self.download_assembly_report(asm_acc, asm_report)\n\n        <DED>logger.info(\"name: {}\".format(name))\n        logger.info(\"local name: {}\".format(localname))\n        logger.info(\"fasta: {}\".format(dst))\n\n        # Create readme with information\n        readme = os.path.join(genomes_dir, localname, \"README.txt\")\n        metadata = {\n            \"name\": localname,\n            \"provider\": self.name,\n            \"original name\": name,\n            \"original filename\": os.path.split(link)[-1],\n            \"assembly_accession\": asm_acc,\n            \"tax_id\": self.genome_taxid(self.genomes.get(name)),\n            \"mask\": mask,\n            \"genome url\": link,\n            \"annotation url\": \"na\",\n            \"date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        }\n        write_readme(readme, metadata)\n\n    <DED>def get_annotation_download_link(self, name, **kwargs):\n        <IND>raise NotImplementedError()\n\n    <DED>@staticmethod\n    def download_and_generate_annotation(genomes_dir, annot_url, localname):\n        <IND>\"\"\"download annotation file, convert to intermediate file and generate output files\"\"\"\n\n        # create output directory if missing\n        out_dir = os.path.join(genomes_dir, localname)\n        mkdir_p(out_dir)\n\n        # download to tmp dir. Move genome on completion.\n        # tmp dir is in genome_dir to prevent moving the genome between disks\n        tmp_dir = mkdtemp(dir=out_dir)\n        ext, gz = get_file_info(annot_url)\n        annot_file = os.path.join(tmp_dir, localname + \".annotation\" + ext)\n        download_file(annot_url, annot_file)\n\n        # unzip input file (if needed)\n        if gz:\n            <IND>cmd = \"mv {0} {1} && gunzip -f {1}\"\n            sp.check_call(cmd.format(annot_file, annot_file + \".gz\"), shell=True)\n\n        # generate intermediate file (GenePred)\n        <DED>pred_file = annot_file.replace(ext, \".gp\")\n        if \"bed\" in ext:\n            <IND>cmd = \"bedToGenePred {0} {1}\"\n        <DED>elif \"gff\" in ext:\n            <IND>cmd = \"gff3ToGenePred -geneNameAttr=gene {0} {1}\"\n        <DED>elif \"gtf\" in ext:\n            <IND>cmd = \"gtfToGenePred -ignoreGroupsWithoutExons {0} {1}\"\n        <DED>elif \"txt\" in ext:\n            # UCSC annotations only\n            <IND>with open(annot_file) as f:\n                <IND>cols = f.readline().split(\"\\t\")\n\n            # extract the genePred format columns\n            <DED>start_col = 1\n            for i, col in enumerate(cols):\n                <IND>if col in [\"+\", \"-\"]:\n                    <IND>start_col = i - 1\n                    break\n            <DED><DED>end_col = start_col + 10\n            cmd = (\n                f\"\"\"cat {{0}} | cut -f {start_col}-{end_col} | \"\"\"\n                # knownGene.txt.gz has spotty fields, this replaces non-integer fields with zeroes\n                + \"\"\"awk 'BEGIN {{FS=OFS=\"\\t\"}} !($11 ~ /^[0-9]+$/) {{$11=\"0\"}}1' > {1}\"\"\"\n            )\n        <DED>else:\n            <IND>raise TypeError(f\"file type extension {ext} not recognized!\")\n\n        <DED>sp.check_call(cmd.format(annot_file, pred_file), shell=True)\n\n        # generate gzipped gtf file (if required)\n        gtf_file = annot_file.replace(ext, \".gtf\")\n        if \"gtf\" not in ext:\n            <IND>cmd = \"genePredToGtf -source=genomepy file {0} {1}\"\n            sp.check_call(cmd.format(pred_file, gtf_file), shell=True)\n\n        # generate gzipped bed file (if required)\n        <DED>bed_file = annot_file.replace(ext, \".bed\")\n        if \"bed\" not in ext:\n            <IND>cmd = \"genePredToBed {0} {1}\"\n            sp.check_call(cmd.format(pred_file, bed_file), shell=True)\n\n        # transfer the files from the tmpdir to the genome_dir\n        <DED>for f in [gtf_file, bed_file]:\n            <IND>src = f\n            dst = os.path.join(out_dir, os.path.basename(f))\n            shutil.move(src, dst)\n        <DED>rm_rf(tmp_dir)\n\n    <DED>def attempt_and_report(self, name, localname, link, genomes_dir):\n        <IND>if not link:\n            <IND>logger.error(\n                f\"Could not download gene annotation for {name} from {self.name}.\"\n            )\n            return\n\n        <DED>try:\n            <IND>logger.info(\n                f\"Downloading annotation from {self.name}. Target URL: {link}...\"\n            )\n            self.download_and_generate_annotation(genomes_dir, link, localname)\n            logger.info(\"Annotation download successful\")\n        <DED>except Exception:\n            <IND>raise GenomeDownloadError(\n                f\"\\nCould not download annotation for {name} from {self.name}\\n\"\n                \"If you think the annotation should be there, please file a bug report at:\\n\"\n                \"https://github.com/vanheeringen-lab/genomepy/issues\\n\"\n            )\n\n        # Add annotation URL to readme\n        <DED>readme = os.path.join(genomes_dir, localname, \"README.txt\")\n        update_readme(readme, updated_metadata={\"annotation url\": link})\n\n    <DED>def download_annotation(self, name, genomes_dir=None, localname=None, **kwargs):\n        <IND>\"\"\"\n        Download annotation file to to a specific directory\n\n        Parameters\n        ----------\n        name : str\n            Genome / species name\n\n        genomes_dir : str , optional\n            Directory to install annotation\n\n        localname : str , optional\n            Custom name for your genome\n        \"\"\"\n        self.check_name(name)\n\n        link = self.get_annotation_download_link(name, **kwargs)\n\n        localname = get_localname(name, localname)\n        genomes_dir = get_genomes_dir(genomes_dir, check_exist=False)\n        self.attempt_and_report(name, localname, link, genomes_dir)\n\n    <DED>def _search_text(self, term: str) -> Iterator[str]:\n        <IND>\"\"\"check if search term is found in the provider's genome name or description field(s)\"\"\"\n        for name, metadata in self.genomes.items():\n            <IND>if term in lower(name) or any(\n                [term in lower(metadata[f]) for f in self.description_fields]\n            ):\n                <IND>yield name\n\n    <DED><DED><DED>def _search_accession(self, term: str) -> Iterator[str]:\n        <IND>\"\"\"check if search term is found in the provider's accession field(s)\"\"\"\n        # cut off prefix (GCA_/GCF_) and suffix (version numbers, e.g. '.3')\n        term = term[4:].split(\".\")[0]\n        for name, metadata in self.genomes.items():\n            <IND>if any([term in str(metadata[f]) for f in self.accession_fields]):\n                <IND>yield name\n\n    <DED><DED><DED>def _search_taxonomy(self, term: str) -> Iterator[str]:\n        <IND>\"\"\"check if search term matches to any of the provider's taxonomy field(s)\"\"\"\n        for name, metadata in self.genomes.items():\n            <IND>if any([term == lower(metadata[f]) for f in self.taxid_fields]):\n                <IND>yield name\n\n    <DED><DED><DED>def search(self, term: Union[str, int]):\n        <IND>\"\"\"\n        Search for term in genome names, descriptions and taxonomy ID.\n\n        The search is case-insensitive.\n\n        Parameters\n        ----------\n        term : str, int\n            Search term, case-insensitive.\n            Can be (part of) an assembly name (e.g. hg38),\n            scientific name (Danio rerio) or assembly\n            accession (GCA_000146045/GCF_...),\n            or an exact taxonomy id (7227).\n\n        Yields\n        ------\n        tuples with name and metadata\n        \"\"\"\n        term = lower(term)\n\n        search_function = self._search_text\n        if term.startswith((\"gca_\", \"gcf_\")):\n            <IND>search_function = self._search_accession\n        <DED>if term.isdigit():\n            <IND>search_function = self._search_taxonomy\n\n        <DED>for genome in search_function(term):\n            <IND>yield self._genome_info_tuple(genome)\n\n\n<DED><DED><DED>register_provider = ProviderBase.register_provider\n\n\n@register_provider(\"Ensembl\")\nclass EnsemblProvider(ProviderBase):\n    <IND>\"\"\"\n    Ensembl genome provider.\n\n    Will search both ensembl.org as well as ensemblgenomes.org.\n    The bacteria division is not yet supported.\n    \"\"\"\n\n    rest_url = \"https://rest.ensembl.org/\"\n    provider_specific_install_options = {\n        \"toplevel\": {\n            \"long\": \"toplevel\",\n            \"help\": \"always download toplevel-genome\",\n            \"flag_value\": True,\n        },\n        \"version\": {\n            \"long\": \"version\",\n            \"help\": \"select release version\",\n            \"type\": int,\n            \"default\": None,\n        },\n    }\n\n    def __init__(self):\n        <IND>self.name = \"Ensembl\"\n        self.provider_status(self.rest_url + \"info/ping?\", max_tries=2)\n        # Populate on init, so that methods can be cached\n        self.genomes = self._get_genomes(self.rest_url)\n        self.accession_fields = [\"assembly_accession\"]\n        self.taxid_fields = [\"taxonomy_id\"]\n        self.description_fields = [\n            \"name\",\n            \"scientific_name\",\n            \"url_name\",\n            \"display_name\",\n        ]\n\n    <DED>@staticmethod\n    def _request_json(rest_url, ext):\n        <IND>\"\"\"Make a REST request and return as json.\"\"\"\n        if rest_url.endswith(\"/\") and ext.startswith(\"/\"):\n            <IND>ext = ext[1:]\n\n        <DED>r = requests.get(rest_url + ext, headers={\"Content-Type\": \"application/json\"})\n\n        if not r.ok:\n            <IND>r.raise_for_status()\n\n        <DED>return r.json()\n\n    <DED>@cache(ignore=[\"self\"])\n    def _get_genomes(self, rest_url):\n        <IND>logger.info(\"Downloading assembly summaries from Ensembl\")\n\n        genomes = {}\n        divisions = retry(self._request_json, 3, rest_url, \"info/divisions?\")\n        for division in divisions:\n            <IND>if division == \"EnsemblBacteria\":\n                <IND>continue\n            <DED>division_genomes = retry(\n                self._request_json, 3, rest_url, f\"info/genomes/division/{division}?\"\n            )\n            for genome in division_genomes:\n                <IND>genomes[safe(genome[\"assembly_name\"])] = genome\n        <DED><DED>return genomes\n\n    <DED>def _genome_info_tuple(self, name):\n        <IND>\"\"\"tuple with assembly metadata\"\"\"\n        genome = self.genomes[name]\n        accession = self.assembly_accession(genome)\n        taxid = self.genome_taxid(genome)\n        taxid = str(taxid) if taxid != 0 else \"na\"\n\n        return (\n            name,\n            accession,\n            genome.get(\"scientific_name\", \"na\"),\n            taxid,\n            genome.get(\"genebuild\", \"na\"),\n        )\n\n    <DED>@goldfish_cache(ignore=[\"self\", \"rest_url\"])\n    def get_version(self, rest_url, vertebrates=False):\n        <IND>\"\"\"Retrieve current version from Ensembl FTP.\"\"\"\n        ext = \"/info/data/?\" if vertebrates else \"/info/eg_version?\"\n        ret = retry(self._request_json, 3, rest_url, ext)\n        releases = ret[\"releases\"] if vertebrates else [ret[\"version\"]]\n        return str(max(releases))\n\n    <DED>def get_genome_download_link(self, name, mask=\"soft\", **kwargs):\n        <IND>\"\"\"\n        Return Ensembl http or ftp link to the genome sequence\n\n        Parameters\n        ----------\n        name : str\n            Genome name. Current implementation will fail if exact\n            name is not found.\n\n        mask : str , optional\n            Masking level. Options: soft, hard or none. Default is soft.\n\n        Returns\n        ------\n        str with the http/ftp download link.\n        \"\"\"\n        genome = self.genomes[safe(name)]\n\n        # parse the division\n        division = genome[\"division\"].lower().replace(\"ensembl\", \"\")\n        if division == \"bacteria\":\n            <IND>raise NotImplementedError(\"bacteria from ensembl not yet supported\")\n\n        <DED>ftp_site = \"ftp://ftp.ensemblgenomes.org/pub\"\n        if division == \"vertebrates\":\n            <IND>ftp_site = \"ftp://ftp.ensembl.org/pub\"\n\n        # Ensembl release version\n        <DED>version = kwargs.get(\"version\")\n        if version is None:\n            <IND>version = self.get_version(self.rest_url, division == \"vertebrates\")\n\n        # division dependent url format\n        <DED>ftp_dir = \"{}/release-{}/fasta/{}/dna\".format(\n            division, version, genome[\"url_name\"].lower()\n        )\n        if division == \"vertebrates\":\n            <IND>ftp_dir = \"release-{}/fasta/{}/dna\".format(\n                version, genome[\"url_name\"].lower()\n            )\n        <DED>url = f\"{ftp_site}/{ftp_dir}\"\n\n        # masking and assembly level\n        def get_url(level=\"toplevel\"):\n            <IND>masks = {\"soft\": \"dna_sm.{}\", \"hard\": \"dna_rm.{}\", \"none\": \"dna.{}\"}\n            pattern = masks[mask].format(level)\n\n            asm_url = \"{}/{}.{}.{}.fa.gz\".format(\n                url,\n                genome[\"url_name\"].capitalize(),\n                re.sub(r\"\\.p\\d+$\", \"\", safe(genome[\"assembly_name\"])),\n                pattern,\n            )\n            return asm_url\n\n        # try to get the (much smaller) primary assembly,\n        # unless specified otherwise\n        <DED>link = get_url(\"primary_assembly\")\n        if kwargs.get(\"toplevel\") or not check_url(link, 2):\n            <IND>link = get_url()\n\n        <DED>if check_url(link, 2):\n            <IND>return link\n\n        <DED>raise GenomeDownloadError(\n            f\"Could not download genome {name} from {self.name}.\\n\"\n            \"URL is broken. Select another genome or provider.\\n\"\n            f\"Broken URL: {link}\"\n        )\n\n    <DED>def get_annotation_download_link(self, name, **kwargs):\n        <IND>\"\"\"\n        Parse and test the link to the Ensembl annotation file.\n\n        Parameters\n        ----------\n        name : str\n            Genome name\n        kwargs: dict , optional:\n            Provider specific options.\n\n            version : int , optional\n                Ensembl version. By default the latest version is used.\n        \"\"\"\n        genome = self.genomes[safe(name)]\n        division = genome[\"division\"].lower().replace(\"ensembl\", \"\")\n\n        ftp_site = \"ftp://ftp.ensemblgenomes.org/pub\"\n        if division == \"vertebrates\":\n            <IND>ftp_site = \"ftp://ftp.ensembl.org/pub\"\n\n        # Ensembl release version\n        <DED>version = kwargs.get(\"version\")\n        if version is None:\n            <IND>version = self.get_version(self.rest_url, division == \"vertebrates\")\n\n        <DED>if division != \"vertebrates\":\n            <IND>ftp_site += f\"/{division}\"\n\n        # Get the GTF URL\n        <DED>base_url = ftp_site + \"/release-{}/gtf/{}/{}.{}.{}.gtf.gz\"\n        safe_name = re.sub(r\"\\.p\\d+$\", \"\", name)\n        link = base_url.format(\n            version,\n            genome[\"url_name\"].lower(),\n            genome[\"url_name\"].capitalize(),\n            safe_name,\n            version,\n        )\n\n        if check_url(link, 2):\n            <IND>return link\n\n\n<DED><DED><DED>@register_provider(\"UCSC\")\nclass UcscProvider(ProviderBase):\n    <IND>\"\"\"\n    UCSC genome provider.\n\n    The UCSC API REST server is used to search and list genomes.\n    \"\"\"\n\n    base_url = \"http://hgdownload.soe.ucsc.edu/goldenPath\"\n    ucsc_url = base_url + \"/{0}/bigZips/chromFa.tar.gz\"\n    ucsc_url_masked = base_url + \"/{0}/bigZips/chromFaMasked.tar.gz\"\n    alt_ucsc_url = base_url + \"/{0}/bigZips/{0}.fa.gz\"\n    alt_ucsc_url_masked = base_url + \"/{0}/bigZips/{0}.fa.masked.gz\"\n    rest_url = \"http://api.genome.ucsc.edu/list/ucscGenomes\"\n    provider_specific_install_options = {\n        \"ucsc_annotation_type\": {\n            \"long\": \"annotation\",\n            \"help\": \"specify annotation to download: UCSC, Ensembl, NCBI_refseq or UCSC_refseq\",\n            \"default\": None,\n        },\n    }\n\n    def __init__(self):\n        <IND>self.name = \"UCSC\"\n        self.provider_status(self.base_url)\n        # Populate on init, so that methods can be cached\n        self.genomes = self._get_genomes(self.rest_url)\n        self.accession_fields = []\n        self.taxid_fields = [\"taxId\"]\n        self.description_fields = [\"description\", \"scientificName\"]\n\n    <DED>@staticmethod\n    @cache\n    def _get_genomes(rest_url):\n        <IND>logger.info(\"Downloading assembly summaries from UCSC\")\n\n        r = requests.get(rest_url, headers={\"Content-Type\": \"application/json\"})\n        if not r.ok:\n            <IND>r.raise_for_status()\n        <DED>ucsc_json = r.json()\n        genomes = ucsc_json[\"ucscGenomes\"]\n        return genomes\n\n    <DED>def _search_accession(self, term: str) -> Iterator[str]:\n        <IND>\"\"\"\n        UCSC does not store assembly accessions.\n        This function searches NCBI (most genomes + stable accession IDs),\n        then uses the NCBI accession search results for a UCSC text search.\n\n        Parameters\n        ----------\n        term : str\n            Assembly accession, GCA_/GCF_....\n\n        Yields\n        ------\n        genome names\n        \"\"\"\n        # NCBI provides a consistent assembly accession. This can be used to\n        # retrieve the species, and then search for that.\n        p = ProviderBase.create(\"NCBI\")\n        ncbi_genomes = list(p._search_accession(term))  # noqa\n\n        # remove superstrings (keep GRCh38, not GRCh38.p1 to GRCh38.p13)\n        unique_ncbi_genomes = []\n        for i in ncbi_genomes:\n            <IND>if sum([j in i for j in ncbi_genomes]) == 1:\n                <IND>unique_ncbi_genomes.append(i)\n\n        # add NCBI organism names to search terms\n        <DED><DED>organism_names = [\n            p.genomes[name][\"organism_name\"] for name in unique_ncbi_genomes\n        ]\n        terms = list(set(unique_ncbi_genomes + organism_names))\n\n        # search with NCBI results in the given provider\n        for name, metadata in self.genomes.items():\n            <IND>for term in terms:\n                <IND>term = lower(term)\n                if term in lower(name) or any(\n                    [term in lower(metadata[f]) for f in self.description_fields]\n                ):\n                    <IND>yield name\n                    break  # max one hit per genome\n\n    <DED><DED><DED><DED>@staticmethod\n    @cache\n    def assembly_accession(genome):\n        <IND>\"\"\"Return the assembly accession (GCA_/GCF_....) for a genome.\n\n        UCSC does not serve the assembly accession through the REST API.\n        Therefore, the readme.html is scanned for an assembly accession. If it is\n        not found, the linked NCBI assembly page will be checked.\n\n        Parameters\n        ----------\n        genome : dict\n            provider metadata dict of a genome.\n\n        Returns\n        ------\n        str\n            Assembly accession.\n        \"\"\"\n        try:\n            <IND>ucsc_url = \"https://hgdownload.soe.ucsc.edu/\" + genome[\"htmlPath\"]\n            text = read_url(ucsc_url)\n        <DED>except UnicodeDecodeError:\n            <IND>return \"na\"\n\n        # example accessions: GCA_000004335.1 (ailMel1)\n        # regex: GC[AF]_ = GCA_ or GCF_, \\d = digit, \\. = period\n        <DED>accession_regex = re.compile(r\"GC[AF]_\\d{9}\\.\\d+\")\n        match = accession_regex.search(text)\n        if match:\n            <IND>return match.group(0)\n\n        # Search for an assembly link at NCBI\n        <DED>match = re.search(r\"https?://www.ncbi.nlm.nih.gov/assembly/\\d+\", text)\n        if match:\n            <IND>ncbi_url = match.group(0)\n            text = read_url(ncbi_url)\n\n            # retrieve valid assembly accessions.\n            # contains additional info, such as '(latest)' or '(suppressed)'. Unused for now.\n            valid_accessions = re.findall(r\"assembly accession:.*?GC[AF]_.*?<\", text)\n            text = \" \".join(valid_accessions)\n            match = accession_regex.search(text)\n            if match:\n                <IND>return match.group(0)\n\n        <DED><DED>return \"na\"\n\n    <DED>def _genome_info_tuple(self, name):\n        <IND>\"\"\"tuple with assembly metadata\"\"\"\n        genome = self.genomes[name]\n        accession = self.assembly_accession(genome)\n        taxid = self.genome_taxid(genome)\n        taxid = str(taxid) if taxid != 0 else \"na\"\n\n        return (\n            name,\n            accession,\n            genome.get(\"scientificName\", \"na\"),\n            taxid,\n            genome.get(\"description\", \"na\"),\n        )\n\n    <DED>def get_genome_download_link(self, name, mask=\"soft\", **kwargs):\n        <IND>\"\"\"\n        Return UCSC http link to genome sequence\n\n        Parameters\n        ----------\n        name : str\n            Genome name. Current implementation will fail if exact\n            name is not found.\n\n        mask : str , optional\n            Masking level. Options: soft, hard or none. Default is soft.\n\n        Returns\n        ------\n        str with the http/ftp download link.\n        \"\"\"\n        # soft masked genomes. can be unmasked in _post _process_download\n        urls = [self.ucsc_url, self.alt_ucsc_url]\n        if mask == \"hard\":\n            <IND>urls = [self.ucsc_url_masked, self.alt_ucsc_url_masked]\n\n        <DED>for genome_url in urls:\n            <IND>link = genome_url.format(name)\n\n            if check_url(link, 2):\n                <IND>return link\n\n        <DED><DED>raise GenomeDownloadError(\n            f\"Could not download genome {name} from {self.name}.\\n\"\n            \"URLs are broken. Select another genome or provider.\\n\"\n            f\"Broken URLs: {', '.join([url.format(name) for url in urls])}\"\n        )\n\n    <DED>@staticmethod\n    def _post_process_download(name, localname, out_dir, mask=\"soft\"):  # noqa\n        <IND>\"\"\"\n        Unmask a softmasked genome if required\n\n        Parameters\n        ----------\n        name : str\n            unused for the UCSC function\n\n        localname : str\n            Custom name for your genome\n\n        out_dir : str\n            Output directory\n\n        mask : str , optional\n            masking level: soft/hard/none, default=soft\n        \"\"\"\n        if mask != \"none\":\n            <IND>return\n\n        <DED>logger.info(\"UCSC genomes are softmasked by default. Unmasking...\")\n\n        fa = os.path.join(out_dir, f\"{localname}.fa\")\n        old_fa = os.path.join(out_dir, f\"old_{localname}.fa\")\n        os.rename(fa, old_fa)\n        with open(old_fa) as old, open(fa, \"w\") as new:\n            <IND>for line in old:\n                <IND>if line.startswith(\">\"):\n                    <IND>new.write(line)\n                <DED>else:\n                    <IND>new.write(line.upper())\n\n    <DED><DED><DED><DED>def get_annotation_download_link(self, name, **kwargs):\n        <IND>\"\"\"\n        Parse and test the link to the UCSC annotation file.\n\n        Will check UCSC, Ensembl, NCBI RefSeq and UCSC RefSeq annotation, respectively.\n        More info on the annotation file on: https://genome.ucsc.edu/FAQ/FAQgenes.html#whatdo\n\n        Parameters\n        ----------\n        name : str\n            Genome name\n        \"\"\"\n        gtf_url = f\"http://hgdownload.soe.ucsc.edu/goldenPath/{name}/bigZips/genes/\"\n        txt_url = f\"http://hgdownload.cse.ucsc.edu/goldenPath/{name}/database/\"\n        annot_files = {\n            \"ucsc\": \"knownGene\",\n            \"ensembl\": \"ensGene\",\n            \"ncbi_refseq\": \"ncbiRefSeq\",\n            \"ucsc_refseq\": \"refGene\",\n        }\n\n        # download gtf format if possible, txt format if not\n        gtfs_exists = check_url(gtf_url, 2)\n        base_url = gtf_url + name + \".\" if gtfs_exists else txt_url\n        base_ext = \".gtf.gz\" if gtfs_exists else \".txt.gz\"\n\n        # download specified annotation type if requested\n        file = kwargs.get(\"ucsc_annotation_type\")\n        if file:\n            <IND>link = base_url + annot_files[file.lower()] + base_ext\n            if check_url(link, 2):\n                <IND>return link\n            <DED>logger.warning(f\"Specified annotation type ({file}) not found for {name}.\")\n\n        <DED>else:\n            # download first available annotation type found\n            <IND>for file in annot_files.values():\n                <IND>link = base_url + file + base_ext\n                if check_url(link, 2):\n                    <IND>return link\n\n\n<DED><DED><DED><DED><DED>@register_provider(\"NCBI\")\nclass NcbiProvider(ProviderBase):\n    <IND>\"\"\"\n    NCBI genome provider.\n\n    Uses the assembly reports page to search and list genomes.\n    \"\"\"\n\n    assembly_url = \"https://ftp.ncbi.nlm.nih.gov/genomes/ASSEMBLY_REPORTS/\"\n    provider_specific_install_options = {}\n\n    def __init__(self):\n        <IND>self.name = \"NCBI\"\n        self.provider_status(self.assembly_url)\n        # Populate on init, so that methods can be cached\n        self.genomes = self._get_genomes(self.assembly_url)\n        self.accession_fields = [\"assembly_accession\", \"gbrs_paired_asm\"]\n        self.taxid_fields = [\"species_taxid\", \"taxid\"]\n        self.description_fields = [\n            \"submitter\",\n            \"organism_name\",\n            \"assembly_accession\",\n            \"gbrs_paired_asm\",\n            \"paired_asm_comp\",\n        ]\n\n    <DED>@staticmethod\n    @cache\n    def _get_genomes(assembly_url):\n        <IND>\"\"\"Parse genomes from assembly summary txt files.\"\"\"\n        logger.info(\n            \"Downloading assembly summaries from NCBI, this will take a while...\"\n        )\n\n        def load_summary(url):\n            <IND>\"\"\"\n            lazy loading of the url so we can parse while downloading\n            \"\"\"\n            for row in urlopen(url):\n                <IND>yield row\n\n        <DED><DED>genomes = {}\n        # order is important as asm_name can repeat (overwriting the older name)\n        names = [\n            \"assembly_summary_genbank_historical.txt\",\n            \"assembly_summary_refseq_historical.txt\",\n            \"assembly_summary_genbank.txt\",\n            \"assembly_summary_refseq.txt\",\n        ]\n        for fname in names:\n            <IND>lines = load_summary(f\"{assembly_url}/{fname}\")\n            _ = next(lines)  # line 0 = comment\n            header = (\n                next(lines).decode(\"utf-8\").strip(\"# \").strip(\"\\n\").split(\"\\t\")\n            )  # line 1 = header\n            for line in tqdm(lines, desc=fname[17:-4], unit_scale=1, unit=\" genomes\"):\n                <IND>line = line.decode(\"utf-8\").strip(\"\\n\").split(\"\\t\")\n                if line[19] != \"na\":  # ftp_path must exist\n                    <IND>name = safe(line[15])  # overwrites older asm_names\n                    genomes[name] = dict(zip(header, line))\n        <DED><DED><DED>return genomes\n\n    <DED>def _genome_info_tuple(self, name):\n        <IND>\"\"\"tuple with assembly metadata\"\"\"\n        genome = self.genomes[name]\n        accession = self.assembly_accession(genome)\n        taxid = self.genome_taxid(genome)\n        taxid = str(taxid) if taxid != 0 else \"na\"\n\n        return (\n            name,\n            accession,\n            genome.get(\"organism_name\", \"na\"),\n            taxid,\n            genome.get(\"submitter\", \"na\"),\n        )\n\n    <DED>def get_genome_download_link(self, name, mask=\"soft\", **kwargs):\n        <IND>\"\"\"\n        Return NCBI ftp link to top-level genome sequence\n\n        Parameters\n        ----------\n        name : str\n            Genome name. Current implementation will fail if exact\n            name is not found.\n\n        mask : str , optional\n            Masking level. Options: soft, hard or none. Default is soft.\n\n        Returns\n        ------\n        str with the http/ftp download link.\n        \"\"\"\n        # only soft masked genomes available. can be (un)masked in _post_process_download\n        link = self._ftp_or_html_link(name, file_suffix=\"_genomic.fna.gz\")\n\n        if link:\n            <IND>return link\n\n        <DED>raise GenomeDownloadError(\n            f\"Could not download genome {name} from {self.name}.\\n\"\n            \"URL is broken. Select another genome or provider.\\n\"\n            f\"Broken URL: {link}\"\n        )\n\n    <DED>def _post_process_download(self, name, localname, out_dir, mask=\"soft\"):\n        <IND>\"\"\"\n        Replace accessions with sequence names in fasta file.\n\n        Applies masking.\n\n        Parameters\n        ----------\n        name : str\n            NCBI genome name\n\n        localname : str\n            Custom name for your genome\n\n        out_dir : str\n            Output directory\n\n        mask : str , optional\n            masking level: soft/hard/none, default=soft\n        \"\"\"\n        # Create mapping of accessions to names\n        url = self._ftp_or_html_link(\n            name, file_suffix=\"_assembly_report.txt\", skip_check=True\n        )\n\n        tr = {}\n        with urlopen(url) as response:\n            <IND>for line in response.read().decode(\"utf-8\").splitlines():\n                <IND>if line.startswith(\"#\"):\n                    <IND>continue\n                <DED>vals = line.strip().split(\"\\t\")\n                tr[vals[6]] = vals[0]\n\n        # mask sequence if required\n        <DED><DED>if mask == \"soft\":\n\n            <IND>def mask_cmd(txt):\n                <IND>return txt\n\n        <DED><DED>elif mask == \"hard\":\n            <IND>logger.info(\"NCBI genomes are softmasked by default. Hard masking...\")\n\n            def mask_cmd(txt):\n                <IND>return re.sub(\"[actg]\", \"N\", txt)\n\n        <DED><DED>else:\n            <IND>logger.info(\"NCBI genomes are softmasked by default. Unmasking...\")\n\n            def mask_cmd(txt):\n                <IND>return txt.upper()\n\n        # apply mapping and masking\n        <DED><DED>fa = os.path.join(out_dir, f\"{localname}.fa\")\n        old_fa = os.path.join(out_dir, f\"old_{localname}.fa\")\n        os.rename(fa, old_fa)\n        with open(old_fa) as old, open(fa, \"w\") as new:\n            <IND>for line in old:\n                <IND>if line.startswith(\">\"):\n                    <IND>desc = line.strip()[1:]\n                    name = desc.split(\" \")[0]\n                    new.write(\">{} {}\\n\".format(tr.get(name, name), desc))\n                <DED>else:\n                    <IND>new.write(mask_cmd(line))\n\n    <DED><DED><DED><DED>def get_annotation_download_link(self, name, **kwargs):\n        <IND>\"\"\"\n        Parse and test the link to the NCBI annotation file.\n\n        Parameters\n        ----------\n        name : str\n            Genome name\n        \"\"\"\n        return self._ftp_or_html_link(name, file_suffix=\"_genomic.gff.gz\")\n\n    <DED>def _ftp_or_html_link(self, name, file_suffix, skip_check=False):\n        <IND>\"\"\"\n        NCBI's files are accessible over FTP and HTTPS\n        Try HTTPS first and return the first functioning link\n        \"\"\"\n        genome = self.genomes[safe(name)]\n        ftp_link = genome[\"ftp_path\"]\n        html_link = ftp_link.replace(\"ftp://\", \"https://\")\n        for link in [html_link, ftp_link]:\n            <IND>link += \"/\" + link.split(\"/\")[-1] + file_suffix\n\n            if skip_check or check_url(link, max_tries=2, timeout=10):\n                <IND>return link\n\n\n<DED><DED><DED><DED>@register_provider(\"URL\")\nclass UrlProvider(ProviderBase):\n    <IND>\"\"\"\n    URL genome provider.\n\n    Simply download a genome directly through an url.\n    \"\"\"\n\n    provider_specific_install_options = {\n        \"to_annotation\": {\n            \"long\": \"to-annotation\",\n            \"help\": \"link to the annotation file, required if this is not in the same directory as the fasta file\",\n            \"default\": None,\n        },\n    }\n\n    def __init__(self):\n        <IND>self.name = \"URL\"\n        self.genomes = {}\n\n    <DED>def genome_taxid(self, genome):\n        <IND>return \"na\"\n\n    <DED>def assembly_accession(self, genome):\n        <IND>return \"na\"\n\n    <DED>def search(self, term):\n        <IND>\"\"\"return an empty generator,\n        same as if no genomes were found at the other providers\"\"\"\n        yield from ()\n\n    <DED>def _genome_info_tuple(self, name):\n        <IND>return tuple()\n\n    <DED>def check_name(self, name):\n        <IND>\"\"\"check if genome name can be found for provider\"\"\"\n        return\n\n    <DED>def get_genome_download_link(self, url, mask=None, **kwargs):\n        <IND>return url\n\n    <DED>def get_annotation_download_link(self, name, **kwargs):\n        <IND>\"\"\"\n        check if the linked annotation file is of a supported file type (gtf/gff3/bed)\n        \"\"\"\n        link = kwargs.get(\"to_annotation\")\n        if link:\n            <IND>ext = get_file_info(link)[0]\n            if ext not in [\".gtf\", \".gff\", \".gff3\", \".bed\"]:\n                <IND>raise TypeError(\n                    \"Only (gzipped) gtf, gff and bed files are supported.\\n\"\n                )\n\n            <DED>return link\n\n    <DED><DED>@staticmethod\n    def search_url_for_annotations(url, name):\n        <IND>\"\"\"Attempts to find gtf or gff3 files in the same location as the genome url\"\"\"\n        urldir = os.path.dirname(url)\n        logger.info(\n            \"You have requested the gene annotation to be downloaded. \"\n            \"Genomepy will check the remote directory: \"\n            f\"{urldir} for annotation files...\"\n        )\n\n        def fuzzy_annotation_search(search_name, search_list):\n            <IND>\"\"\"Returns all files containing both name and an annotation extension\"\"\"\n            hits = []\n            for ext in [\"gtf\", \"gff\"]:\n                # .*? = non greedy filler. 3? = optional 3 (for gff3). (\\.gz)? = optional .gz\n                <IND>expr = f\"{search_name}.*?\\.{ext}3?(\\.gz)?\"  # noqa: W605\n                for line in search_list:\n                    <IND>hit = re.search(expr, line, flags=re.IGNORECASE)\n                    if hit:\n                        <IND>hits.append(hit[0])\n            <DED><DED><DED>return hits\n\n        # try to find a GTF or GFF3 file\n        <DED>dirty_list = [str(line) for line in urlopen(urldir).readlines()]\n        fnames = fuzzy_annotation_search(name, dirty_list)\n        if not fnames:\n            <IND>raise FileNotFoundError(\n                \"Could not parse the remote directory. \"\n                \"Please supply a URL using --url-to-annotation.\\n\"\n            )\n\n        <DED>links = [urldir + \"/\" + fname for fname in fnames]\n        return links\n\n    <DED>def download_annotation(self, url, genomes_dir=None, localname=None, **kwargs):\n        <IND>\"\"\"\n        Attempts to download a gtf or gff3 file from the same location as the genome url\n\n        Parameters\n        ----------\n        url : str\n            url of where to download genome from\n\n        genomes_dir : str\n            Directory to install annotation\n\n        localname : str , optional\n            Custom name for your genome\n\n        kwargs: dict , optional:\n            Provider specific options.\n\n            to_annotation : str , optional\n                url to annotation file (only required if this not located in the same directory as the fasta)\n        \"\"\"\n        name = get_localname(url)\n        localname = get_localname(name, localname)\n        genomes_dir = get_genomes_dir(genomes_dir, check_exist=False)\n\n        if kwargs.get(\"to_annotation\"):\n            <IND>links = [self.get_annotation_download_link(None, **kwargs)]\n        <DED>else:\n            # can return multiple possible hits\n            <IND>links = self.search_url_for_annotations(url, name)\n\n        <DED>for link in links:\n            <IND>try:\n                <IND>self.attempt_and_report(name, localname, link, genomes_dir)\n                break\n            <DED>except GenomeDownloadError as e:\n                <IND>if not link == links[-1]:\n                    <IND>logger.info(\n                        \"One of the potential annotations was incompatible with genomepy. \"\n                        \"Attempting another...\"\n                    )\n                    continue\n                <DED>return e\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        return mapping\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "vanheeringen-lab/genomepy",
    "commit": "acb59fbc2678caa2f7a261e53c6656ff9e6e5925",
    "filename": "genomepy/utils.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/vanheeringen-lab-genomepy/genomepy/utils.py",
    "file_hunks_size": 7,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "genomepy/utils.py:144:46 Incompatible variable type [9]: lines is declared to have type `List[typing.Any]` but is used as type `None`.",
    "message": " lines is declared to have type `List[typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 144,
    "warning_line": "def write_readme(readme: str, metadata: dict, lines: list = None):",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "from norns import exceptions\nfrom pyfaidx import Fasta\nfrom tqdm.auto import tqdm\n\nfrom genomepy.exceptions import GenomeDownloadError\n\nconfig = norns.config(\"genomepy\", default=\"cfg/default.yaml\")\n\n\ndef download_file(url, filename):\n    \"\"\"\n    Helper method handling downloading large files from `url` to `filename`.\n    Returns a pointer to `filename`.\n    \"\"\"\n\n    def decorated_pbar(total):\n        \"\"\"Displays a progress bar with download speeds in MB/s.\"\"\"\n        return tqdm(\n            desc=\"Download\",\n            unit_scale=True,\n            unit_divisor=1024,\n            total=total,\n            unit=\"B\",\n        )\n\n    def write_n_update_pbar(data):\n        pbar.update(len(data))\n        f.write(data)\n\n    if url.startswith(\"ftp\"):\n        ftp, target = connect_ftp_link(url)\n        file_size = ftp.size(target)\n        with open(filename, \"wb\") as f:\n            pbar = decorated_pbar(file_size)\n\n            ftp.retrbinary(f\"RETR {target}\", write_n_update_pbar)\n            ftp.quit()  # logout\n\n    else:\n        r = requests.get(url, stream=True)\n        file_size = int(r.headers.get(\"Content-Length\", 0))\n        with open(filename, \"wb\") as f:\n            pbar = decorated_pbar(file_size)\n\n            for chunk in r.iter_content(chunk_size=1024):\n                if chunk:  # filter out keep-alive new chunks\n                    write_n_update_pbar(chunk)\n\n    pbar.close()  # stop updating pbar\n    return filename\n\n\ndef connect_ftp_link(link, timeout=None):\n    \"\"\"\n    anonymous login to ftp\n    accepts link in the form of ftp://ftp.name.domain/... and ftp.name.domain/...\n    \"\"\"\n    link = link.replace(\"ftp://\", \"\")\n    host = link.split(\"/\")[0]\n    target = link.split(host)[1]\n\n    try:\n        ftp = FTP(host, timeout=timeout)\n    except socket.gaierror:\n        raise GenomeDownloadError(f\"FTP host not found: {host}\")\n\n    ftp.login()\n    return ftp, target\n\n\ndef read_readme(readme: str) -> Tuple[dict, list]:\n    \"\"\"\n    parse readme file\n\n    Parameters\n    ----------\n    readme: str\n        filename\n\n    Returns\n    -------\n    tuple\n        metadata : dict with genome metadata\n        lines: list with non-metadata text (such as regex info)\n    \"\"\"\n    metadata = {\n        \"name\": \"na\",\n        \"provider\": \"na\",\n        \"original name\": \"na\",\n        \"original filename\": \"na\",\n        \"assembly_accession\": \"na\",\n        \"tax_id\": \"na\",\n        \"mask\": \"na\",\n        \"genome url\": \"na\",\n        \"annotation url\": \"na\",\n        \"sanitized annotation\": \"no\",\n        \"date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n    }\n    lines = []\n\n    if not os.path.exists(readme):\n        return metadata, lines\n\n    # if the readme exists, overwrite all metadata fields found\n    with open(readme) as f:\n        for line in f.readlines():\n            if \": \" in line:\n                vals = line.strip().split(\": \")\n                metadata[vals[0].strip()] = (\": \".join(vals[1:])).strip()\n            else:\n                line = line.strip(\"\\n\").strip(\" \")\n                # blank lines are allowed, but only one in a row\n                if not (\n                    line == \"\"\n                    and len(lines) > 0\n                    and lines[len(lines) - 1].strip() == \"\"\n                ):\n                    lines.append(line)\n\n    return metadata, lines\n\n\ndef write_readme(readme: str, metadata: dict, lines: list = None):\n    \"\"\"Create a new readme with updated information\"\"\"\n    with open(readme, \"w\") as f:\n        for k, v in metadata.items():\n            print(f\"{k}: {v}\", file=f)\n        if lines:\n            for line in lines:\n                print(line, file=f)\n\n\ndef update_readme(readme: str, updated_metadata: dict = None, extra_lines: list = None):\n    metadata, lines = read_readme(readme)\n    if updated_metadata:\n        metadata = {**metadata, **updated_metadata}\n    if extra_lines:\n        lines = lines + extra_lines\n    write_readme(readme, metadata, lines)\n\n\ndef generate_gap_bed(fname, outname):\n    \"\"\"Generate a BED file with gap locations.\n\n    Parameters\n    ----------\n    fname : str\n        Filename of input FASTA file.\n\n    outname : str\n        Filename of output BED file.\n    \"\"\"\n    f = Fasta(fname)\n    with open(outname, \"w\") as bed:\n        for chrom in f.keys():\n            for m in re.finditer(r\"N+\", f[chrom][:].seq):\n                bed.write(f\"{chrom}\\t{m.start(0)}\\t{m.end(0)}\\n\")\n\n\ndef generate_fa_sizes(fname, outname):\n    \"\"\"Generate a fa.sizes file.\n\n    Parameters\n    ----------\n    fname : str\n        Filename of input FASTA file.\n\n    outname : str\n        Filename of output BED file.\n    \"\"\"\n    f = Fasta(fname)\n    with open(outname, \"w\") as sizes:\n        for seqname, seq in f.items():\n            sizes.write(f\"{seqname}\\t{len(seq)}\\n\")\n\n\ndef _fa_to_file(fasta: Fasta, contigs: list, filepath: str):\n    tmp_dir = mkdtemp(dir=os.path.dirname(filepath))\n    tmpfa = os.path.join(tmp_dir, \"regex.fa\")\n    with open(tmpfa, \"w\") as out:\n        for chrom in contigs:\n            out.write(f\">{fasta[chrom].name}\\n\")\n            out.write(f\"{fasta[chrom][:].seq}\\n\")\n    os.rename(tmpfa, filepath)\n    rm_rf(tmp_dir)\n\n\ndef filter_fasta(\n    infa: str,\n    regex: str = \".*\",\n    invert_match: Optional[bool] = False,\n    outfa: str = None,\n) -> Fasta:\n    \"\"\"Filter fasta file based on regex.\n\n    Parameters\n    ----------\n    infa : str\n        Filename of input fasta file.\n\n    outfa : str, optional\n        Filename of output fasta file.\n        Overwrites infa if left blank.\n\n    regex : str, optional\n        Regular expression used for selecting sequences.\n        Matches everything if left blank.\n\n    invert_match : bool, optional\n        Select all sequence *not* matching regex if set.\n\n    Returns\n    -------\n        fasta : Fasta instance\n            pyfaidx Fasta instance of newly created file\n    \"\"\"\n    fa = Fasta(infa)\n    pattern = re.compile(regex)\n    seqs = [k for k in fa.keys() if bool(pattern.search(k)) is not invert_match]\n    if len(seqs) == 0:\n        raise Exception(\"No sequences left after filtering!\")\n\n    outfa = outfa if outfa else infa\n    _fa_to_file(fa, seqs, outfa)\n    rm_rf(f\"{infa}.fai\")  # old index\n    return Fasta(outfa)\n\n",
        "source_code_len": 6182,
        "target_code": "from norns import exceptions\n\nconfig = norns.config(\"genomepy\", default=\"cfg/default.yaml\")\n\n",
        "target_code_len": 93,
        "diff_format": "@@ -22,227 +14,4 @@\n from norns import exceptions\n-from pyfaidx import Fasta\n-from tqdm.auto import tqdm\n-\n-from genomepy.exceptions import GenomeDownloadError\n \n config = norns.config(\"genomepy\", default=\"cfg/default.yaml\")\n-\n-\n-def download_file(url, filename):\n-    \"\"\"\n-    Helper method handling downloading large files from `url` to `filename`.\n-    Returns a pointer to `filename`.\n-    \"\"\"\n-\n-    def decorated_pbar(total):\n-        \"\"\"Displays a progress bar with download speeds in MB/s.\"\"\"\n-        return tqdm(\n-            desc=\"Download\",\n-            unit_scale=True,\n-            unit_divisor=1024,\n-            total=total,\n-            unit=\"B\",\n-        )\n-\n-    def write_n_update_pbar(data):\n-        pbar.update(len(data))\n-        f.write(data)\n-\n-    if url.startswith(\"ftp\"):\n-        ftp, target = connect_ftp_link(url)\n-        file_size = ftp.size(target)\n-        with open(filename, \"wb\") as f:\n-            pbar = decorated_pbar(file_size)\n-\n-            ftp.retrbinary(f\"RETR {target}\", write_n_update_pbar)\n-            ftp.quit()  # logout\n-\n-    else:\n-        r = requests.get(url, stream=True)\n-        file_size = int(r.headers.get(\"Content-Length\", 0))\n-        with open(filename, \"wb\") as f:\n-            pbar = decorated_pbar(file_size)\n-\n-            for chunk in r.iter_content(chunk_size=1024):\n-                if chunk:  # filter out keep-alive new chunks\n-                    write_n_update_pbar(chunk)\n-\n-    pbar.close()  # stop updating pbar\n-    return filename\n-\n-\n-def connect_ftp_link(link, timeout=None):\n-    \"\"\"\n-    anonymous login to ftp\n-    accepts link in the form of ftp://ftp.name.domain/... and ftp.name.domain/...\n-    \"\"\"\n-    link = link.replace(\"ftp://\", \"\")\n-    host = link.split(\"/\")[0]\n-    target = link.split(host)[1]\n-\n-    try:\n-        ftp = FTP(host, timeout=timeout)\n-    except socket.gaierror:\n-        raise GenomeDownloadError(f\"FTP host not found: {host}\")\n-\n-    ftp.login()\n-    return ftp, target\n-\n-\n-def read_readme(readme: str) -> Tuple[dict, list]:\n-    \"\"\"\n-    parse readme file\n-\n-    Parameters\n-    ----------\n-    readme: str\n-        filename\n-\n-    Returns\n-    -------\n-    tuple\n-        metadata : dict with genome metadata\n-        lines: list with non-metadata text (such as regex info)\n-    \"\"\"\n-    metadata = {\n-        \"name\": \"na\",\n-        \"provider\": \"na\",\n-        \"original name\": \"na\",\n-        \"original filename\": \"na\",\n-        \"assembly_accession\": \"na\",\n-        \"tax_id\": \"na\",\n-        \"mask\": \"na\",\n-        \"genome url\": \"na\",\n-        \"annotation url\": \"na\",\n-        \"sanitized annotation\": \"no\",\n-        \"date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n-    }\n-    lines = []\n-\n-    if not os.path.exists(readme):\n-        return metadata, lines\n-\n-    # if the readme exists, overwrite all metadata fields found\n-    with open(readme) as f:\n-        for line in f.readlines():\n-            if \": \" in line:\n-                vals = line.strip().split(\": \")\n-                metadata[vals[0].strip()] = (\": \".join(vals[1:])).strip()\n-            else:\n-                line = line.strip(\"\\n\").strip(\" \")\n-                # blank lines are allowed, but only one in a row\n-                if not (\n-                    line == \"\"\n-                    and len(lines) > 0\n-                    and lines[len(lines) - 1].strip() == \"\"\n-                ):\n-                    lines.append(line)\n-\n-    return metadata, lines\n-\n-\n-def write_readme(readme: str, metadata: dict, lines: list = None):\n-    \"\"\"Create a new readme with updated information\"\"\"\n-    with open(readme, \"w\") as f:\n-        for k, v in metadata.items():\n-            print(f\"{k}: {v}\", file=f)\n-        if lines:\n-            for line in lines:\n-                print(line, file=f)\n-\n-\n-def update_readme(readme: str, updated_metadata: dict = None, extra_lines: list = None):\n-    metadata, lines = read_readme(readme)\n-    if updated_metadata:\n-        metadata = {**metadata, **updated_metadata}\n-    if extra_lines:\n-        lines = lines + extra_lines\n-    write_readme(readme, metadata, lines)\n-\n-\n-def generate_gap_bed(fname, outname):\n-    \"\"\"Generate a BED file with gap locations.\n-\n-    Parameters\n-    ----------\n-    fname : str\n-        Filename of input FASTA file.\n-\n-    outname : str\n-        Filename of output BED file.\n-    \"\"\"\n-    f = Fasta(fname)\n-    with open(outname, \"w\") as bed:\n-        for chrom in f.keys():\n-            for m in re.finditer(r\"N+\", f[chrom][:].seq):\n-                bed.write(f\"{chrom}\\t{m.start(0)}\\t{m.end(0)}\\n\")\n-\n-\n-def generate_fa_sizes(fname, outname):\n-    \"\"\"Generate a fa.sizes file.\n-\n-    Parameters\n-    ----------\n-    fname : str\n-        Filename of input FASTA file.\n-\n-    outname : str\n-        Filename of output BED file.\n-    \"\"\"\n-    f = Fasta(fname)\n-    with open(outname, \"w\") as sizes:\n-        for seqname, seq in f.items():\n-            sizes.write(f\"{seqname}\\t{len(seq)}\\n\")\n-\n-\n-def _fa_to_file(fasta: Fasta, contigs: list, filepath: str):\n-    tmp_dir = mkdtemp(dir=os.path.dirname(filepath))\n-    tmpfa = os.path.join(tmp_dir, \"regex.fa\")\n-    with open(tmpfa, \"w\") as out:\n-        for chrom in contigs:\n-            out.write(f\">{fasta[chrom].name}\\n\")\n-            out.write(f\"{fasta[chrom][:].seq}\\n\")\n-    os.rename(tmpfa, filepath)\n-    rm_rf(tmp_dir)\n-\n-\n-def filter_fasta(\n-    infa: str,\n-    regex: str = \".*\",\n-    invert_match: Optional[bool] = False,\n-    outfa: str = None,\n-) -> Fasta:\n-    \"\"\"Filter fasta file based on regex.\n-\n-    Parameters\n-    ----------\n-    infa : str\n-        Filename of input fasta file.\n-\n-    outfa : str, optional\n-        Filename of output fasta file.\n-        Overwrites infa if left blank.\n-\n-    regex : str, optional\n-        Regular expression used for selecting sequences.\n-        Matches everything if left blank.\n-\n-    invert_match : bool, optional\n-        Select all sequence *not* matching regex if set.\n-\n-    Returns\n-    -------\n-        fasta : Fasta instance\n-            pyfaidx Fasta instance of newly created file\n-    \"\"\"\n-    fa = Fasta(infa)\n-    pattern = re.compile(regex)\n-    seqs = [k for k in fa.keys() if bool(pattern.search(k)) is not invert_match]\n-    if len(seqs) == 0:\n-        raise Exception(\"No sequences left after filtering!\")\n-\n-    outfa = outfa if outfa else infa\n-    _fa_to_file(fa, seqs, outfa)\n-    rm_rf(f\"{infa}.fai\")  # old index\n-    return Fasta(outfa)\n \n",
        "source_code_with_indent": "from norns import exceptions\nfrom pyfaidx import Fasta\nfrom tqdm.auto import tqdm\n\nfrom genomepy.exceptions import GenomeDownloadError\n\nconfig = norns.config(\"genomepy\", default=\"cfg/default.yaml\")\n\n\ndef download_file(url, filename):\n    <IND>\"\"\"\n    Helper method handling downloading large files from `url` to `filename`.\n    Returns a pointer to `filename`.\n    \"\"\"\n\n    def decorated_pbar(total):\n        <IND>\"\"\"Displays a progress bar with download speeds in MB/s.\"\"\"\n        return tqdm(\n            desc=\"Download\",\n            unit_scale=True,\n            unit_divisor=1024,\n            total=total,\n            unit=\"B\",\n        )\n\n    <DED>def write_n_update_pbar(data):\n        <IND>pbar.update(len(data))\n        f.write(data)\n\n    <DED>if url.startswith(\"ftp\"):\n        <IND>ftp, target = connect_ftp_link(url)\n        file_size = ftp.size(target)\n        with open(filename, \"wb\") as f:\n            <IND>pbar = decorated_pbar(file_size)\n\n            ftp.retrbinary(f\"RETR {target}\", write_n_update_pbar)\n            ftp.quit()  # logout\n\n    <DED><DED>else:\n        <IND>r = requests.get(url, stream=True)\n        file_size = int(r.headers.get(\"Content-Length\", 0))\n        with open(filename, \"wb\") as f:\n            <IND>pbar = decorated_pbar(file_size)\n\n            for chunk in r.iter_content(chunk_size=1024):\n                <IND>if chunk:  # filter out keep-alive new chunks\n                    <IND>write_n_update_pbar(chunk)\n\n    <DED><DED><DED><DED>pbar.close()  # stop updating pbar\n    return filename\n\n\n<DED>def connect_ftp_link(link, timeout=None):\n    <IND>\"\"\"\n    anonymous login to ftp\n    accepts link in the form of ftp://ftp.name.domain/... and ftp.name.domain/...\n    \"\"\"\n    link = link.replace(\"ftp://\", \"\")\n    host = link.split(\"/\")[0]\n    target = link.split(host)[1]\n\n    try:\n        <IND>ftp = FTP(host, timeout=timeout)\n    <DED>except socket.gaierror:\n        <IND>raise GenomeDownloadError(f\"FTP host not found: {host}\")\n\n    <DED>ftp.login()\n    return ftp, target\n\n\n<DED>def read_readme(readme: str) -> Tuple[dict, list]:\n    <IND>\"\"\"\n    parse readme file\n\n    Parameters\n    ----------\n    readme: str\n        filename\n\n    Returns\n    -------\n    tuple\n        metadata : dict with genome metadata\n        lines: list with non-metadata text (such as regex info)\n    \"\"\"\n    metadata = {\n        \"name\": \"na\",\n        \"provider\": \"na\",\n        \"original name\": \"na\",\n        \"original filename\": \"na\",\n        \"assembly_accession\": \"na\",\n        \"tax_id\": \"na\",\n        \"mask\": \"na\",\n        \"genome url\": \"na\",\n        \"annotation url\": \"na\",\n        \"sanitized annotation\": \"no\",\n        \"date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n    }\n    lines = []\n\n    if not os.path.exists(readme):\n        <IND>return metadata, lines\n\n    # if the readme exists, overwrite all metadata fields found\n    <DED>with open(readme) as f:\n        <IND>for line in f.readlines():\n            <IND>if \": \" in line:\n                <IND>vals = line.strip().split(\": \")\n                metadata[vals[0].strip()] = (\": \".join(vals[1:])).strip()\n            <DED>else:\n                <IND>line = line.strip(\"\\n\").strip(\" \")\n                # blank lines are allowed, but only one in a row\n                if not (\n                    line == \"\"\n                    and len(lines) > 0\n                    and lines[len(lines) - 1].strip() == \"\"\n                ):\n                    <IND>lines.append(line)\n\n    <DED><DED><DED><DED>return metadata, lines\n\n\n<DED>def write_readme(readme: str, metadata: dict, lines: list = None):\n    <IND>\"\"\"Create a new readme with updated information\"\"\"\n    with open(readme, \"w\") as f:\n        <IND>for k, v in metadata.items():\n            <IND>print(f\"{k}: {v}\", file=f)\n        <DED>if lines:\n            <IND>for line in lines:\n                <IND>print(line, file=f)\n\n\n<DED><DED><DED><DED>def update_readme(readme: str, updated_metadata: dict = None, extra_lines: list = None):\n    <IND>metadata, lines = read_readme(readme)\n    if updated_metadata:\n        <IND>metadata = {**metadata, **updated_metadata}\n    <DED>if extra_lines:\n        <IND>lines = lines + extra_lines\n    <DED>write_readme(readme, metadata, lines)\n\n\n<DED>def generate_gap_bed(fname, outname):\n    <IND>\"\"\"Generate a BED file with gap locations.\n\n    Parameters\n    ----------\n    fname : str\n        Filename of input FASTA file.\n\n    outname : str\n        Filename of output BED file.\n    \"\"\"\n    f = Fasta(fname)\n    with open(outname, \"w\") as bed:\n        <IND>for chrom in f.keys():\n            <IND>for m in re.finditer(r\"N+\", f[chrom][:].seq):\n                <IND>bed.write(f\"{chrom}\\t{m.start(0)}\\t{m.end(0)}\\n\")\n\n\n<DED><DED><DED><DED>def generate_fa_sizes(fname, outname):\n    <IND>\"\"\"Generate a fa.sizes file.\n\n    Parameters\n    ----------\n    fname : str\n        Filename of input FASTA file.\n\n    outname : str\n        Filename of output BED file.\n    \"\"\"\n    f = Fasta(fname)\n    with open(outname, \"w\") as sizes:\n        <IND>for seqname, seq in f.items():\n            <IND>sizes.write(f\"{seqname}\\t{len(seq)}\\n\")\n\n\n<DED><DED><DED>def _fa_to_file(fasta: Fasta, contigs: list, filepath: str):\n    <IND>tmp_dir = mkdtemp(dir=os.path.dirname(filepath))\n    tmpfa = os.path.join(tmp_dir, \"regex.fa\")\n    with open(tmpfa, \"w\") as out:\n        <IND>for chrom in contigs:\n            <IND>out.write(f\">{fasta[chrom].name}\\n\")\n            out.write(f\"{fasta[chrom][:].seq}\\n\")\n    <DED><DED>os.rename(tmpfa, filepath)\n    rm_rf(tmp_dir)\n\n\n<DED>def filter_fasta(\n    infa: str,\n    regex: str = \".*\",\n    invert_match: Optional[bool] = False,\n    outfa: str = None,\n) -> Fasta:\n    <IND>\"\"\"Filter fasta file based on regex.\n\n    Parameters\n    ----------\n    infa : str\n        Filename of input fasta file.\n\n    outfa : str, optional\n        Filename of output fasta file.\n        Overwrites infa if left blank.\n\n    regex : str, optional\n        Regular expression used for selecting sequences.\n        Matches everything if left blank.\n\n    invert_match : bool, optional\n        Select all sequence *not* matching regex if set.\n\n    Returns\n    -------\n        fasta : Fasta instance\n            pyfaidx Fasta instance of newly created file\n    \"\"\"\n    fa = Fasta(infa)\n    pattern = re.compile(regex)\n    seqs = [k for k in fa.keys() if bool(pattern.search(k)) is not invert_match]\n    if len(seqs) == 0:\n        <IND>raise Exception(\"No sequences left after filtering!\")\n\n    <DED>outfa = outfa if outfa else infa\n    _fa_to_file(fa, seqs, outfa)\n    rm_rf(f\"{infa}.fai\")  # old index\n    return Fasta(outfa)\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "from norns import exceptions\n\nconfig = norns.config(\"genomepy\", default=\"cfg/default.yaml\")\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "vanheeringen-lab/genomepy",
    "commit": "acb59fbc2678caa2f7a261e53c6656ff9e6e5925",
    "filename": "genomepy/utils.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/vanheeringen-lab-genomepy/genomepy/utils.py",
    "file_hunks_size": 7,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "genomepy/utils.py:154:31 Incompatible variable type [9]: updated_metadata is declared to have type `typing.Dict[typing.Any, typing.Any]` but is used as type `None`.",
    "message": " updated_metadata is declared to have type `typing.Dict[typing.Any, typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 154,
    "warning_line": "def update_readme(readme: str, updated_metadata: dict = None, extra_lines: list = None):",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "from norns import exceptions\nfrom pyfaidx import Fasta\nfrom tqdm.auto import tqdm\n\nfrom genomepy.exceptions import GenomeDownloadError\n\nconfig = norns.config(\"genomepy\", default=\"cfg/default.yaml\")\n\n\ndef download_file(url, filename):\n    \"\"\"\n    Helper method handling downloading large files from `url` to `filename`.\n    Returns a pointer to `filename`.\n    \"\"\"\n\n    def decorated_pbar(total):\n        \"\"\"Displays a progress bar with download speeds in MB/s.\"\"\"\n        return tqdm(\n            desc=\"Download\",\n            unit_scale=True,\n            unit_divisor=1024,\n            total=total,\n            unit=\"B\",\n        )\n\n    def write_n_update_pbar(data):\n        pbar.update(len(data))\n        f.write(data)\n\n    if url.startswith(\"ftp\"):\n        ftp, target = connect_ftp_link(url)\n        file_size = ftp.size(target)\n        with open(filename, \"wb\") as f:\n            pbar = decorated_pbar(file_size)\n\n            ftp.retrbinary(f\"RETR {target}\", write_n_update_pbar)\n            ftp.quit()  # logout\n\n    else:\n        r = requests.get(url, stream=True)\n        file_size = int(r.headers.get(\"Content-Length\", 0))\n        with open(filename, \"wb\") as f:\n            pbar = decorated_pbar(file_size)\n\n            for chunk in r.iter_content(chunk_size=1024):\n                if chunk:  # filter out keep-alive new chunks\n                    write_n_update_pbar(chunk)\n\n    pbar.close()  # stop updating pbar\n    return filename\n\n\ndef connect_ftp_link(link, timeout=None):\n    \"\"\"\n    anonymous login to ftp\n    accepts link in the form of ftp://ftp.name.domain/... and ftp.name.domain/...\n    \"\"\"\n    link = link.replace(\"ftp://\", \"\")\n    host = link.split(\"/\")[0]\n    target = link.split(host)[1]\n\n    try:\n        ftp = FTP(host, timeout=timeout)\n    except socket.gaierror:\n        raise GenomeDownloadError(f\"FTP host not found: {host}\")\n\n    ftp.login()\n    return ftp, target\n\n\ndef read_readme(readme: str) -> Tuple[dict, list]:\n    \"\"\"\n    parse readme file\n\n    Parameters\n    ----------\n    readme: str\n        filename\n\n    Returns\n    -------\n    tuple\n        metadata : dict with genome metadata\n        lines: list with non-metadata text (such as regex info)\n    \"\"\"\n    metadata = {\n        \"name\": \"na\",\n        \"provider\": \"na\",\n        \"original name\": \"na\",\n        \"original filename\": \"na\",\n        \"assembly_accession\": \"na\",\n        \"tax_id\": \"na\",\n        \"mask\": \"na\",\n        \"genome url\": \"na\",\n        \"annotation url\": \"na\",\n        \"sanitized annotation\": \"no\",\n        \"date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n    }\n    lines = []\n\n    if not os.path.exists(readme):\n        return metadata, lines\n\n    # if the readme exists, overwrite all metadata fields found\n    with open(readme) as f:\n        for line in f.readlines():\n            if \": \" in line:\n                vals = line.strip().split(\": \")\n                metadata[vals[0].strip()] = (\": \".join(vals[1:])).strip()\n            else:\n                line = line.strip(\"\\n\").strip(\" \")\n                # blank lines are allowed, but only one in a row\n                if not (\n                    line == \"\"\n                    and len(lines) > 0\n                    and lines[len(lines) - 1].strip() == \"\"\n                ):\n                    lines.append(line)\n\n    return metadata, lines\n\n\ndef write_readme(readme: str, metadata: dict, lines: list = None):\n    \"\"\"Create a new readme with updated information\"\"\"\n    with open(readme, \"w\") as f:\n        for k, v in metadata.items():\n            print(f\"{k}: {v}\", file=f)\n        if lines:\n            for line in lines:\n                print(line, file=f)\n\n\ndef update_readme(readme: str, updated_metadata: dict = None, extra_lines: list = None):\n    metadata, lines = read_readme(readme)\n    if updated_metadata:\n        metadata = {**metadata, **updated_metadata}\n    if extra_lines:\n        lines = lines + extra_lines\n    write_readme(readme, metadata, lines)\n\n\ndef generate_gap_bed(fname, outname):\n    \"\"\"Generate a BED file with gap locations.\n\n    Parameters\n    ----------\n    fname : str\n        Filename of input FASTA file.\n\n    outname : str\n        Filename of output BED file.\n    \"\"\"\n    f = Fasta(fname)\n    with open(outname, \"w\") as bed:\n        for chrom in f.keys():\n            for m in re.finditer(r\"N+\", f[chrom][:].seq):\n                bed.write(f\"{chrom}\\t{m.start(0)}\\t{m.end(0)}\\n\")\n\n\ndef generate_fa_sizes(fname, outname):\n    \"\"\"Generate a fa.sizes file.\n\n    Parameters\n    ----------\n    fname : str\n        Filename of input FASTA file.\n\n    outname : str\n        Filename of output BED file.\n    \"\"\"\n    f = Fasta(fname)\n    with open(outname, \"w\") as sizes:\n        for seqname, seq in f.items():\n            sizes.write(f\"{seqname}\\t{len(seq)}\\n\")\n\n\ndef _fa_to_file(fasta: Fasta, contigs: list, filepath: str):\n    tmp_dir = mkdtemp(dir=os.path.dirname(filepath))\n    tmpfa = os.path.join(tmp_dir, \"regex.fa\")\n    with open(tmpfa, \"w\") as out:\n        for chrom in contigs:\n            out.write(f\">{fasta[chrom].name}\\n\")\n            out.write(f\"{fasta[chrom][:].seq}\\n\")\n    os.rename(tmpfa, filepath)\n    rm_rf(tmp_dir)\n\n\ndef filter_fasta(\n    infa: str,\n    regex: str = \".*\",\n    invert_match: Optional[bool] = False,\n    outfa: str = None,\n) -> Fasta:\n    \"\"\"Filter fasta file based on regex.\n\n    Parameters\n    ----------\n    infa : str\n        Filename of input fasta file.\n\n    outfa : str, optional\n        Filename of output fasta file.\n        Overwrites infa if left blank.\n\n    regex : str, optional\n        Regular expression used for selecting sequences.\n        Matches everything if left blank.\n\n    invert_match : bool, optional\n        Select all sequence *not* matching regex if set.\n\n    Returns\n    -------\n        fasta : Fasta instance\n            pyfaidx Fasta instance of newly created file\n    \"\"\"\n    fa = Fasta(infa)\n    pattern = re.compile(regex)\n    seqs = [k for k in fa.keys() if bool(pattern.search(k)) is not invert_match]\n    if len(seqs) == 0:\n        raise Exception(\"No sequences left after filtering!\")\n\n    outfa = outfa if outfa else infa\n    _fa_to_file(fa, seqs, outfa)\n    rm_rf(f\"{infa}.fai\")  # old index\n    return Fasta(outfa)\n\n",
        "source_code_len": 6182,
        "target_code": "from norns import exceptions\n\nconfig = norns.config(\"genomepy\", default=\"cfg/default.yaml\")\n\n",
        "target_code_len": 93,
        "diff_format": "@@ -22,227 +14,4 @@\n from norns import exceptions\n-from pyfaidx import Fasta\n-from tqdm.auto import tqdm\n-\n-from genomepy.exceptions import GenomeDownloadError\n \n config = norns.config(\"genomepy\", default=\"cfg/default.yaml\")\n-\n-\n-def download_file(url, filename):\n-    \"\"\"\n-    Helper method handling downloading large files from `url` to `filename`.\n-    Returns a pointer to `filename`.\n-    \"\"\"\n-\n-    def decorated_pbar(total):\n-        \"\"\"Displays a progress bar with download speeds in MB/s.\"\"\"\n-        return tqdm(\n-            desc=\"Download\",\n-            unit_scale=True,\n-            unit_divisor=1024,\n-            total=total,\n-            unit=\"B\",\n-        )\n-\n-    def write_n_update_pbar(data):\n-        pbar.update(len(data))\n-        f.write(data)\n-\n-    if url.startswith(\"ftp\"):\n-        ftp, target = connect_ftp_link(url)\n-        file_size = ftp.size(target)\n-        with open(filename, \"wb\") as f:\n-            pbar = decorated_pbar(file_size)\n-\n-            ftp.retrbinary(f\"RETR {target}\", write_n_update_pbar)\n-            ftp.quit()  # logout\n-\n-    else:\n-        r = requests.get(url, stream=True)\n-        file_size = int(r.headers.get(\"Content-Length\", 0))\n-        with open(filename, \"wb\") as f:\n-            pbar = decorated_pbar(file_size)\n-\n-            for chunk in r.iter_content(chunk_size=1024):\n-                if chunk:  # filter out keep-alive new chunks\n-                    write_n_update_pbar(chunk)\n-\n-    pbar.close()  # stop updating pbar\n-    return filename\n-\n-\n-def connect_ftp_link(link, timeout=None):\n-    \"\"\"\n-    anonymous login to ftp\n-    accepts link in the form of ftp://ftp.name.domain/... and ftp.name.domain/...\n-    \"\"\"\n-    link = link.replace(\"ftp://\", \"\")\n-    host = link.split(\"/\")[0]\n-    target = link.split(host)[1]\n-\n-    try:\n-        ftp = FTP(host, timeout=timeout)\n-    except socket.gaierror:\n-        raise GenomeDownloadError(f\"FTP host not found: {host}\")\n-\n-    ftp.login()\n-    return ftp, target\n-\n-\n-def read_readme(readme: str) -> Tuple[dict, list]:\n-    \"\"\"\n-    parse readme file\n-\n-    Parameters\n-    ----------\n-    readme: str\n-        filename\n-\n-    Returns\n-    -------\n-    tuple\n-        metadata : dict with genome metadata\n-        lines: list with non-metadata text (such as regex info)\n-    \"\"\"\n-    metadata = {\n-        \"name\": \"na\",\n-        \"provider\": \"na\",\n-        \"original name\": \"na\",\n-        \"original filename\": \"na\",\n-        \"assembly_accession\": \"na\",\n-        \"tax_id\": \"na\",\n-        \"mask\": \"na\",\n-        \"genome url\": \"na\",\n-        \"annotation url\": \"na\",\n-        \"sanitized annotation\": \"no\",\n-        \"date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n-    }\n-    lines = []\n-\n-    if not os.path.exists(readme):\n-        return metadata, lines\n-\n-    # if the readme exists, overwrite all metadata fields found\n-    with open(readme) as f:\n-        for line in f.readlines():\n-            if \": \" in line:\n-                vals = line.strip().split(\": \")\n-                metadata[vals[0].strip()] = (\": \".join(vals[1:])).strip()\n-            else:\n-                line = line.strip(\"\\n\").strip(\" \")\n-                # blank lines are allowed, but only one in a row\n-                if not (\n-                    line == \"\"\n-                    and len(lines) > 0\n-                    and lines[len(lines) - 1].strip() == \"\"\n-                ):\n-                    lines.append(line)\n-\n-    return metadata, lines\n-\n-\n-def write_readme(readme: str, metadata: dict, lines: list = None):\n-    \"\"\"Create a new readme with updated information\"\"\"\n-    with open(readme, \"w\") as f:\n-        for k, v in metadata.items():\n-            print(f\"{k}: {v}\", file=f)\n-        if lines:\n-            for line in lines:\n-                print(line, file=f)\n-\n-\n-def update_readme(readme: str, updated_metadata: dict = None, extra_lines: list = None):\n-    metadata, lines = read_readme(readme)\n-    if updated_metadata:\n-        metadata = {**metadata, **updated_metadata}\n-    if extra_lines:\n-        lines = lines + extra_lines\n-    write_readme(readme, metadata, lines)\n-\n-\n-def generate_gap_bed(fname, outname):\n-    \"\"\"Generate a BED file with gap locations.\n-\n-    Parameters\n-    ----------\n-    fname : str\n-        Filename of input FASTA file.\n-\n-    outname : str\n-        Filename of output BED file.\n-    \"\"\"\n-    f = Fasta(fname)\n-    with open(outname, \"w\") as bed:\n-        for chrom in f.keys():\n-            for m in re.finditer(r\"N+\", f[chrom][:].seq):\n-                bed.write(f\"{chrom}\\t{m.start(0)}\\t{m.end(0)}\\n\")\n-\n-\n-def generate_fa_sizes(fname, outname):\n-    \"\"\"Generate a fa.sizes file.\n-\n-    Parameters\n-    ----------\n-    fname : str\n-        Filename of input FASTA file.\n-\n-    outname : str\n-        Filename of output BED file.\n-    \"\"\"\n-    f = Fasta(fname)\n-    with open(outname, \"w\") as sizes:\n-        for seqname, seq in f.items():\n-            sizes.write(f\"{seqname}\\t{len(seq)}\\n\")\n-\n-\n-def _fa_to_file(fasta: Fasta, contigs: list, filepath: str):\n-    tmp_dir = mkdtemp(dir=os.path.dirname(filepath))\n-    tmpfa = os.path.join(tmp_dir, \"regex.fa\")\n-    with open(tmpfa, \"w\") as out:\n-        for chrom in contigs:\n-            out.write(f\">{fasta[chrom].name}\\n\")\n-            out.write(f\"{fasta[chrom][:].seq}\\n\")\n-    os.rename(tmpfa, filepath)\n-    rm_rf(tmp_dir)\n-\n-\n-def filter_fasta(\n-    infa: str,\n-    regex: str = \".*\",\n-    invert_match: Optional[bool] = False,\n-    outfa: str = None,\n-) -> Fasta:\n-    \"\"\"Filter fasta file based on regex.\n-\n-    Parameters\n-    ----------\n-    infa : str\n-        Filename of input fasta file.\n-\n-    outfa : str, optional\n-        Filename of output fasta file.\n-        Overwrites infa if left blank.\n-\n-    regex : str, optional\n-        Regular expression used for selecting sequences.\n-        Matches everything if left blank.\n-\n-    invert_match : bool, optional\n-        Select all sequence *not* matching regex if set.\n-\n-    Returns\n-    -------\n-        fasta : Fasta instance\n-            pyfaidx Fasta instance of newly created file\n-    \"\"\"\n-    fa = Fasta(infa)\n-    pattern = re.compile(regex)\n-    seqs = [k for k in fa.keys() if bool(pattern.search(k)) is not invert_match]\n-    if len(seqs) == 0:\n-        raise Exception(\"No sequences left after filtering!\")\n-\n-    outfa = outfa if outfa else infa\n-    _fa_to_file(fa, seqs, outfa)\n-    rm_rf(f\"{infa}.fai\")  # old index\n-    return Fasta(outfa)\n \n",
        "source_code_with_indent": "from norns import exceptions\nfrom pyfaidx import Fasta\nfrom tqdm.auto import tqdm\n\nfrom genomepy.exceptions import GenomeDownloadError\n\nconfig = norns.config(\"genomepy\", default=\"cfg/default.yaml\")\n\n\ndef download_file(url, filename):\n    <IND>\"\"\"\n    Helper method handling downloading large files from `url` to `filename`.\n    Returns a pointer to `filename`.\n    \"\"\"\n\n    def decorated_pbar(total):\n        <IND>\"\"\"Displays a progress bar with download speeds in MB/s.\"\"\"\n        return tqdm(\n            desc=\"Download\",\n            unit_scale=True,\n            unit_divisor=1024,\n            total=total,\n            unit=\"B\",\n        )\n\n    <DED>def write_n_update_pbar(data):\n        <IND>pbar.update(len(data))\n        f.write(data)\n\n    <DED>if url.startswith(\"ftp\"):\n        <IND>ftp, target = connect_ftp_link(url)\n        file_size = ftp.size(target)\n        with open(filename, \"wb\") as f:\n            <IND>pbar = decorated_pbar(file_size)\n\n            ftp.retrbinary(f\"RETR {target}\", write_n_update_pbar)\n            ftp.quit()  # logout\n\n    <DED><DED>else:\n        <IND>r = requests.get(url, stream=True)\n        file_size = int(r.headers.get(\"Content-Length\", 0))\n        with open(filename, \"wb\") as f:\n            <IND>pbar = decorated_pbar(file_size)\n\n            for chunk in r.iter_content(chunk_size=1024):\n                <IND>if chunk:  # filter out keep-alive new chunks\n                    <IND>write_n_update_pbar(chunk)\n\n    <DED><DED><DED><DED>pbar.close()  # stop updating pbar\n    return filename\n\n\n<DED>def connect_ftp_link(link, timeout=None):\n    <IND>\"\"\"\n    anonymous login to ftp\n    accepts link in the form of ftp://ftp.name.domain/... and ftp.name.domain/...\n    \"\"\"\n    link = link.replace(\"ftp://\", \"\")\n    host = link.split(\"/\")[0]\n    target = link.split(host)[1]\n\n    try:\n        <IND>ftp = FTP(host, timeout=timeout)\n    <DED>except socket.gaierror:\n        <IND>raise GenomeDownloadError(f\"FTP host not found: {host}\")\n\n    <DED>ftp.login()\n    return ftp, target\n\n\n<DED>def read_readme(readme: str) -> Tuple[dict, list]:\n    <IND>\"\"\"\n    parse readme file\n\n    Parameters\n    ----------\n    readme: str\n        filename\n\n    Returns\n    -------\n    tuple\n        metadata : dict with genome metadata\n        lines: list with non-metadata text (such as regex info)\n    \"\"\"\n    metadata = {\n        \"name\": \"na\",\n        \"provider\": \"na\",\n        \"original name\": \"na\",\n        \"original filename\": \"na\",\n        \"assembly_accession\": \"na\",\n        \"tax_id\": \"na\",\n        \"mask\": \"na\",\n        \"genome url\": \"na\",\n        \"annotation url\": \"na\",\n        \"sanitized annotation\": \"no\",\n        \"date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n    }\n    lines = []\n\n    if not os.path.exists(readme):\n        <IND>return metadata, lines\n\n    # if the readme exists, overwrite all metadata fields found\n    <DED>with open(readme) as f:\n        <IND>for line in f.readlines():\n            <IND>if \": \" in line:\n                <IND>vals = line.strip().split(\": \")\n                metadata[vals[0].strip()] = (\": \".join(vals[1:])).strip()\n            <DED>else:\n                <IND>line = line.strip(\"\\n\").strip(\" \")\n                # blank lines are allowed, but only one in a row\n                if not (\n                    line == \"\"\n                    and len(lines) > 0\n                    and lines[len(lines) - 1].strip() == \"\"\n                ):\n                    <IND>lines.append(line)\n\n    <DED><DED><DED><DED>return metadata, lines\n\n\n<DED>def write_readme(readme: str, metadata: dict, lines: list = None):\n    <IND>\"\"\"Create a new readme with updated information\"\"\"\n    with open(readme, \"w\") as f:\n        <IND>for k, v in metadata.items():\n            <IND>print(f\"{k}: {v}\", file=f)\n        <DED>if lines:\n            <IND>for line in lines:\n                <IND>print(line, file=f)\n\n\n<DED><DED><DED><DED>def update_readme(readme: str, updated_metadata: dict = None, extra_lines: list = None):\n    <IND>metadata, lines = read_readme(readme)\n    if updated_metadata:\n        <IND>metadata = {**metadata, **updated_metadata}\n    <DED>if extra_lines:\n        <IND>lines = lines + extra_lines\n    <DED>write_readme(readme, metadata, lines)\n\n\n<DED>def generate_gap_bed(fname, outname):\n    <IND>\"\"\"Generate a BED file with gap locations.\n\n    Parameters\n    ----------\n    fname : str\n        Filename of input FASTA file.\n\n    outname : str\n        Filename of output BED file.\n    \"\"\"\n    f = Fasta(fname)\n    with open(outname, \"w\") as bed:\n        <IND>for chrom in f.keys():\n            <IND>for m in re.finditer(r\"N+\", f[chrom][:].seq):\n                <IND>bed.write(f\"{chrom}\\t{m.start(0)}\\t{m.end(0)}\\n\")\n\n\n<DED><DED><DED><DED>def generate_fa_sizes(fname, outname):\n    <IND>\"\"\"Generate a fa.sizes file.\n\n    Parameters\n    ----------\n    fname : str\n        Filename of input FASTA file.\n\n    outname : str\n        Filename of output BED file.\n    \"\"\"\n    f = Fasta(fname)\n    with open(outname, \"w\") as sizes:\n        <IND>for seqname, seq in f.items():\n            <IND>sizes.write(f\"{seqname}\\t{len(seq)}\\n\")\n\n\n<DED><DED><DED>def _fa_to_file(fasta: Fasta, contigs: list, filepath: str):\n    <IND>tmp_dir = mkdtemp(dir=os.path.dirname(filepath))\n    tmpfa = os.path.join(tmp_dir, \"regex.fa\")\n    with open(tmpfa, \"w\") as out:\n        <IND>for chrom in contigs:\n            <IND>out.write(f\">{fasta[chrom].name}\\n\")\n            out.write(f\"{fasta[chrom][:].seq}\\n\")\n    <DED><DED>os.rename(tmpfa, filepath)\n    rm_rf(tmp_dir)\n\n\n<DED>def filter_fasta(\n    infa: str,\n    regex: str = \".*\",\n    invert_match: Optional[bool] = False,\n    outfa: str = None,\n) -> Fasta:\n    <IND>\"\"\"Filter fasta file based on regex.\n\n    Parameters\n    ----------\n    infa : str\n        Filename of input fasta file.\n\n    outfa : str, optional\n        Filename of output fasta file.\n        Overwrites infa if left blank.\n\n    regex : str, optional\n        Regular expression used for selecting sequences.\n        Matches everything if left blank.\n\n    invert_match : bool, optional\n        Select all sequence *not* matching regex if set.\n\n    Returns\n    -------\n        fasta : Fasta instance\n            pyfaidx Fasta instance of newly created file\n    \"\"\"\n    fa = Fasta(infa)\n    pattern = re.compile(regex)\n    seqs = [k for k in fa.keys() if bool(pattern.search(k)) is not invert_match]\n    if len(seqs) == 0:\n        <IND>raise Exception(\"No sequences left after filtering!\")\n\n    <DED>outfa = outfa if outfa else infa\n    _fa_to_file(fa, seqs, outfa)\n    rm_rf(f\"{infa}.fai\")  # old index\n    return Fasta(outfa)\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "from norns import exceptions\n\nconfig = norns.config(\"genomepy\", default=\"cfg/default.yaml\")\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "vanheeringen-lab/genomepy",
    "commit": "acb59fbc2678caa2f7a261e53c6656ff9e6e5925",
    "filename": "genomepy/utils.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/vanheeringen-lab-genomepy/genomepy/utils.py",
    "file_hunks_size": 7,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "genomepy/utils.py:154:62 Incompatible variable type [9]: extra_lines is declared to have type `List[typing.Any]` but is used as type `None`.",
    "message": " extra_lines is declared to have type `List[typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 154,
    "warning_line": "def update_readme(readme: str, updated_metadata: dict = None, extra_lines: list = None):",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "from norns import exceptions\nfrom pyfaidx import Fasta\nfrom tqdm.auto import tqdm\n\nfrom genomepy.exceptions import GenomeDownloadError\n\nconfig = norns.config(\"genomepy\", default=\"cfg/default.yaml\")\n\n\ndef download_file(url, filename):\n    \"\"\"\n    Helper method handling downloading large files from `url` to `filename`.\n    Returns a pointer to `filename`.\n    \"\"\"\n\n    def decorated_pbar(total):\n        \"\"\"Displays a progress bar with download speeds in MB/s.\"\"\"\n        return tqdm(\n            desc=\"Download\",\n            unit_scale=True,\n            unit_divisor=1024,\n            total=total,\n            unit=\"B\",\n        )\n\n    def write_n_update_pbar(data):\n        pbar.update(len(data))\n        f.write(data)\n\n    if url.startswith(\"ftp\"):\n        ftp, target = connect_ftp_link(url)\n        file_size = ftp.size(target)\n        with open(filename, \"wb\") as f:\n            pbar = decorated_pbar(file_size)\n\n            ftp.retrbinary(f\"RETR {target}\", write_n_update_pbar)\n            ftp.quit()  # logout\n\n    else:\n        r = requests.get(url, stream=True)\n        file_size = int(r.headers.get(\"Content-Length\", 0))\n        with open(filename, \"wb\") as f:\n            pbar = decorated_pbar(file_size)\n\n            for chunk in r.iter_content(chunk_size=1024):\n                if chunk:  # filter out keep-alive new chunks\n                    write_n_update_pbar(chunk)\n\n    pbar.close()  # stop updating pbar\n    return filename\n\n\ndef connect_ftp_link(link, timeout=None):\n    \"\"\"\n    anonymous login to ftp\n    accepts link in the form of ftp://ftp.name.domain/... and ftp.name.domain/...\n    \"\"\"\n    link = link.replace(\"ftp://\", \"\")\n    host = link.split(\"/\")[0]\n    target = link.split(host)[1]\n\n    try:\n        ftp = FTP(host, timeout=timeout)\n    except socket.gaierror:\n        raise GenomeDownloadError(f\"FTP host not found: {host}\")\n\n    ftp.login()\n    return ftp, target\n\n\ndef read_readme(readme: str) -> Tuple[dict, list]:\n    \"\"\"\n    parse readme file\n\n    Parameters\n    ----------\n    readme: str\n        filename\n\n    Returns\n    -------\n    tuple\n        metadata : dict with genome metadata\n        lines: list with non-metadata text (such as regex info)\n    \"\"\"\n    metadata = {\n        \"name\": \"na\",\n        \"provider\": \"na\",\n        \"original name\": \"na\",\n        \"original filename\": \"na\",\n        \"assembly_accession\": \"na\",\n        \"tax_id\": \"na\",\n        \"mask\": \"na\",\n        \"genome url\": \"na\",\n        \"annotation url\": \"na\",\n        \"sanitized annotation\": \"no\",\n        \"date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n    }\n    lines = []\n\n    if not os.path.exists(readme):\n        return metadata, lines\n\n    # if the readme exists, overwrite all metadata fields found\n    with open(readme) as f:\n        for line in f.readlines():\n            if \": \" in line:\n                vals = line.strip().split(\": \")\n                metadata[vals[0].strip()] = (\": \".join(vals[1:])).strip()\n            else:\n                line = line.strip(\"\\n\").strip(\" \")\n                # blank lines are allowed, but only one in a row\n                if not (\n                    line == \"\"\n                    and len(lines) > 0\n                    and lines[len(lines) - 1].strip() == \"\"\n                ):\n                    lines.append(line)\n\n    return metadata, lines\n\n\ndef write_readme(readme: str, metadata: dict, lines: list = None):\n    \"\"\"Create a new readme with updated information\"\"\"\n    with open(readme, \"w\") as f:\n        for k, v in metadata.items():\n            print(f\"{k}: {v}\", file=f)\n        if lines:\n            for line in lines:\n                print(line, file=f)\n\n\ndef update_readme(readme: str, updated_metadata: dict = None, extra_lines: list = None):\n    metadata, lines = read_readme(readme)\n    if updated_metadata:\n        metadata = {**metadata, **updated_metadata}\n    if extra_lines:\n        lines = lines + extra_lines\n    write_readme(readme, metadata, lines)\n\n\ndef generate_gap_bed(fname, outname):\n    \"\"\"Generate a BED file with gap locations.\n\n    Parameters\n    ----------\n    fname : str\n        Filename of input FASTA file.\n\n    outname : str\n        Filename of output BED file.\n    \"\"\"\n    f = Fasta(fname)\n    with open(outname, \"w\") as bed:\n        for chrom in f.keys():\n            for m in re.finditer(r\"N+\", f[chrom][:].seq):\n                bed.write(f\"{chrom}\\t{m.start(0)}\\t{m.end(0)}\\n\")\n\n\ndef generate_fa_sizes(fname, outname):\n    \"\"\"Generate a fa.sizes file.\n\n    Parameters\n    ----------\n    fname : str\n        Filename of input FASTA file.\n\n    outname : str\n        Filename of output BED file.\n    \"\"\"\n    f = Fasta(fname)\n    with open(outname, \"w\") as sizes:\n        for seqname, seq in f.items():\n            sizes.write(f\"{seqname}\\t{len(seq)}\\n\")\n\n\ndef _fa_to_file(fasta: Fasta, contigs: list, filepath: str):\n    tmp_dir = mkdtemp(dir=os.path.dirname(filepath))\n    tmpfa = os.path.join(tmp_dir, \"regex.fa\")\n    with open(tmpfa, \"w\") as out:\n        for chrom in contigs:\n            out.write(f\">{fasta[chrom].name}\\n\")\n            out.write(f\"{fasta[chrom][:].seq}\\n\")\n    os.rename(tmpfa, filepath)\n    rm_rf(tmp_dir)\n\n\ndef filter_fasta(\n    infa: str,\n    regex: str = \".*\",\n    invert_match: Optional[bool] = False,\n    outfa: str = None,\n) -> Fasta:\n    \"\"\"Filter fasta file based on regex.\n\n    Parameters\n    ----------\n    infa : str\n        Filename of input fasta file.\n\n    outfa : str, optional\n        Filename of output fasta file.\n        Overwrites infa if left blank.\n\n    regex : str, optional\n        Regular expression used for selecting sequences.\n        Matches everything if left blank.\n\n    invert_match : bool, optional\n        Select all sequence *not* matching regex if set.\n\n    Returns\n    -------\n        fasta : Fasta instance\n            pyfaidx Fasta instance of newly created file\n    \"\"\"\n    fa = Fasta(infa)\n    pattern = re.compile(regex)\n    seqs = [k for k in fa.keys() if bool(pattern.search(k)) is not invert_match]\n    if len(seqs) == 0:\n        raise Exception(\"No sequences left after filtering!\")\n\n    outfa = outfa if outfa else infa\n    _fa_to_file(fa, seqs, outfa)\n    rm_rf(f\"{infa}.fai\")  # old index\n    return Fasta(outfa)\n\n",
        "source_code_len": 6182,
        "target_code": "from norns import exceptions\n\nconfig = norns.config(\"genomepy\", default=\"cfg/default.yaml\")\n\n",
        "target_code_len": 93,
        "diff_format": "@@ -22,227 +14,4 @@\n from norns import exceptions\n-from pyfaidx import Fasta\n-from tqdm.auto import tqdm\n-\n-from genomepy.exceptions import GenomeDownloadError\n \n config = norns.config(\"genomepy\", default=\"cfg/default.yaml\")\n-\n-\n-def download_file(url, filename):\n-    \"\"\"\n-    Helper method handling downloading large files from `url` to `filename`.\n-    Returns a pointer to `filename`.\n-    \"\"\"\n-\n-    def decorated_pbar(total):\n-        \"\"\"Displays a progress bar with download speeds in MB/s.\"\"\"\n-        return tqdm(\n-            desc=\"Download\",\n-            unit_scale=True,\n-            unit_divisor=1024,\n-            total=total,\n-            unit=\"B\",\n-        )\n-\n-    def write_n_update_pbar(data):\n-        pbar.update(len(data))\n-        f.write(data)\n-\n-    if url.startswith(\"ftp\"):\n-        ftp, target = connect_ftp_link(url)\n-        file_size = ftp.size(target)\n-        with open(filename, \"wb\") as f:\n-            pbar = decorated_pbar(file_size)\n-\n-            ftp.retrbinary(f\"RETR {target}\", write_n_update_pbar)\n-            ftp.quit()  # logout\n-\n-    else:\n-        r = requests.get(url, stream=True)\n-        file_size = int(r.headers.get(\"Content-Length\", 0))\n-        with open(filename, \"wb\") as f:\n-            pbar = decorated_pbar(file_size)\n-\n-            for chunk in r.iter_content(chunk_size=1024):\n-                if chunk:  # filter out keep-alive new chunks\n-                    write_n_update_pbar(chunk)\n-\n-    pbar.close()  # stop updating pbar\n-    return filename\n-\n-\n-def connect_ftp_link(link, timeout=None):\n-    \"\"\"\n-    anonymous login to ftp\n-    accepts link in the form of ftp://ftp.name.domain/... and ftp.name.domain/...\n-    \"\"\"\n-    link = link.replace(\"ftp://\", \"\")\n-    host = link.split(\"/\")[0]\n-    target = link.split(host)[1]\n-\n-    try:\n-        ftp = FTP(host, timeout=timeout)\n-    except socket.gaierror:\n-        raise GenomeDownloadError(f\"FTP host not found: {host}\")\n-\n-    ftp.login()\n-    return ftp, target\n-\n-\n-def read_readme(readme: str) -> Tuple[dict, list]:\n-    \"\"\"\n-    parse readme file\n-\n-    Parameters\n-    ----------\n-    readme: str\n-        filename\n-\n-    Returns\n-    -------\n-    tuple\n-        metadata : dict with genome metadata\n-        lines: list with non-metadata text (such as regex info)\n-    \"\"\"\n-    metadata = {\n-        \"name\": \"na\",\n-        \"provider\": \"na\",\n-        \"original name\": \"na\",\n-        \"original filename\": \"na\",\n-        \"assembly_accession\": \"na\",\n-        \"tax_id\": \"na\",\n-        \"mask\": \"na\",\n-        \"genome url\": \"na\",\n-        \"annotation url\": \"na\",\n-        \"sanitized annotation\": \"no\",\n-        \"date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n-    }\n-    lines = []\n-\n-    if not os.path.exists(readme):\n-        return metadata, lines\n-\n-    # if the readme exists, overwrite all metadata fields found\n-    with open(readme) as f:\n-        for line in f.readlines():\n-            if \": \" in line:\n-                vals = line.strip().split(\": \")\n-                metadata[vals[0].strip()] = (\": \".join(vals[1:])).strip()\n-            else:\n-                line = line.strip(\"\\n\").strip(\" \")\n-                # blank lines are allowed, but only one in a row\n-                if not (\n-                    line == \"\"\n-                    and len(lines) > 0\n-                    and lines[len(lines) - 1].strip() == \"\"\n-                ):\n-                    lines.append(line)\n-\n-    return metadata, lines\n-\n-\n-def write_readme(readme: str, metadata: dict, lines: list = None):\n-    \"\"\"Create a new readme with updated information\"\"\"\n-    with open(readme, \"w\") as f:\n-        for k, v in metadata.items():\n-            print(f\"{k}: {v}\", file=f)\n-        if lines:\n-            for line in lines:\n-                print(line, file=f)\n-\n-\n-def update_readme(readme: str, updated_metadata: dict = None, extra_lines: list = None):\n-    metadata, lines = read_readme(readme)\n-    if updated_metadata:\n-        metadata = {**metadata, **updated_metadata}\n-    if extra_lines:\n-        lines = lines + extra_lines\n-    write_readme(readme, metadata, lines)\n-\n-\n-def generate_gap_bed(fname, outname):\n-    \"\"\"Generate a BED file with gap locations.\n-\n-    Parameters\n-    ----------\n-    fname : str\n-        Filename of input FASTA file.\n-\n-    outname : str\n-        Filename of output BED file.\n-    \"\"\"\n-    f = Fasta(fname)\n-    with open(outname, \"w\") as bed:\n-        for chrom in f.keys():\n-            for m in re.finditer(r\"N+\", f[chrom][:].seq):\n-                bed.write(f\"{chrom}\\t{m.start(0)}\\t{m.end(0)}\\n\")\n-\n-\n-def generate_fa_sizes(fname, outname):\n-    \"\"\"Generate a fa.sizes file.\n-\n-    Parameters\n-    ----------\n-    fname : str\n-        Filename of input FASTA file.\n-\n-    outname : str\n-        Filename of output BED file.\n-    \"\"\"\n-    f = Fasta(fname)\n-    with open(outname, \"w\") as sizes:\n-        for seqname, seq in f.items():\n-            sizes.write(f\"{seqname}\\t{len(seq)}\\n\")\n-\n-\n-def _fa_to_file(fasta: Fasta, contigs: list, filepath: str):\n-    tmp_dir = mkdtemp(dir=os.path.dirname(filepath))\n-    tmpfa = os.path.join(tmp_dir, \"regex.fa\")\n-    with open(tmpfa, \"w\") as out:\n-        for chrom in contigs:\n-            out.write(f\">{fasta[chrom].name}\\n\")\n-            out.write(f\"{fasta[chrom][:].seq}\\n\")\n-    os.rename(tmpfa, filepath)\n-    rm_rf(tmp_dir)\n-\n-\n-def filter_fasta(\n-    infa: str,\n-    regex: str = \".*\",\n-    invert_match: Optional[bool] = False,\n-    outfa: str = None,\n-) -> Fasta:\n-    \"\"\"Filter fasta file based on regex.\n-\n-    Parameters\n-    ----------\n-    infa : str\n-        Filename of input fasta file.\n-\n-    outfa : str, optional\n-        Filename of output fasta file.\n-        Overwrites infa if left blank.\n-\n-    regex : str, optional\n-        Regular expression used for selecting sequences.\n-        Matches everything if left blank.\n-\n-    invert_match : bool, optional\n-        Select all sequence *not* matching regex if set.\n-\n-    Returns\n-    -------\n-        fasta : Fasta instance\n-            pyfaidx Fasta instance of newly created file\n-    \"\"\"\n-    fa = Fasta(infa)\n-    pattern = re.compile(regex)\n-    seqs = [k for k in fa.keys() if bool(pattern.search(k)) is not invert_match]\n-    if len(seqs) == 0:\n-        raise Exception(\"No sequences left after filtering!\")\n-\n-    outfa = outfa if outfa else infa\n-    _fa_to_file(fa, seqs, outfa)\n-    rm_rf(f\"{infa}.fai\")  # old index\n-    return Fasta(outfa)\n \n",
        "source_code_with_indent": "from norns import exceptions\nfrom pyfaidx import Fasta\nfrom tqdm.auto import tqdm\n\nfrom genomepy.exceptions import GenomeDownloadError\n\nconfig = norns.config(\"genomepy\", default=\"cfg/default.yaml\")\n\n\ndef download_file(url, filename):\n    <IND>\"\"\"\n    Helper method handling downloading large files from `url` to `filename`.\n    Returns a pointer to `filename`.\n    \"\"\"\n\n    def decorated_pbar(total):\n        <IND>\"\"\"Displays a progress bar with download speeds in MB/s.\"\"\"\n        return tqdm(\n            desc=\"Download\",\n            unit_scale=True,\n            unit_divisor=1024,\n            total=total,\n            unit=\"B\",\n        )\n\n    <DED>def write_n_update_pbar(data):\n        <IND>pbar.update(len(data))\n        f.write(data)\n\n    <DED>if url.startswith(\"ftp\"):\n        <IND>ftp, target = connect_ftp_link(url)\n        file_size = ftp.size(target)\n        with open(filename, \"wb\") as f:\n            <IND>pbar = decorated_pbar(file_size)\n\n            ftp.retrbinary(f\"RETR {target}\", write_n_update_pbar)\n            ftp.quit()  # logout\n\n    <DED><DED>else:\n        <IND>r = requests.get(url, stream=True)\n        file_size = int(r.headers.get(\"Content-Length\", 0))\n        with open(filename, \"wb\") as f:\n            <IND>pbar = decorated_pbar(file_size)\n\n            for chunk in r.iter_content(chunk_size=1024):\n                <IND>if chunk:  # filter out keep-alive new chunks\n                    <IND>write_n_update_pbar(chunk)\n\n    <DED><DED><DED><DED>pbar.close()  # stop updating pbar\n    return filename\n\n\n<DED>def connect_ftp_link(link, timeout=None):\n    <IND>\"\"\"\n    anonymous login to ftp\n    accepts link in the form of ftp://ftp.name.domain/... and ftp.name.domain/...\n    \"\"\"\n    link = link.replace(\"ftp://\", \"\")\n    host = link.split(\"/\")[0]\n    target = link.split(host)[1]\n\n    try:\n        <IND>ftp = FTP(host, timeout=timeout)\n    <DED>except socket.gaierror:\n        <IND>raise GenomeDownloadError(f\"FTP host not found: {host}\")\n\n    <DED>ftp.login()\n    return ftp, target\n\n\n<DED>def read_readme(readme: str) -> Tuple[dict, list]:\n    <IND>\"\"\"\n    parse readme file\n\n    Parameters\n    ----------\n    readme: str\n        filename\n\n    Returns\n    -------\n    tuple\n        metadata : dict with genome metadata\n        lines: list with non-metadata text (such as regex info)\n    \"\"\"\n    metadata = {\n        \"name\": \"na\",\n        \"provider\": \"na\",\n        \"original name\": \"na\",\n        \"original filename\": \"na\",\n        \"assembly_accession\": \"na\",\n        \"tax_id\": \"na\",\n        \"mask\": \"na\",\n        \"genome url\": \"na\",\n        \"annotation url\": \"na\",\n        \"sanitized annotation\": \"no\",\n        \"date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n    }\n    lines = []\n\n    if not os.path.exists(readme):\n        <IND>return metadata, lines\n\n    # if the readme exists, overwrite all metadata fields found\n    <DED>with open(readme) as f:\n        <IND>for line in f.readlines():\n            <IND>if \": \" in line:\n                <IND>vals = line.strip().split(\": \")\n                metadata[vals[0].strip()] = (\": \".join(vals[1:])).strip()\n            <DED>else:\n                <IND>line = line.strip(\"\\n\").strip(\" \")\n                # blank lines are allowed, but only one in a row\n                if not (\n                    line == \"\"\n                    and len(lines) > 0\n                    and lines[len(lines) - 1].strip() == \"\"\n                ):\n                    <IND>lines.append(line)\n\n    <DED><DED><DED><DED>return metadata, lines\n\n\n<DED>def write_readme(readme: str, metadata: dict, lines: list = None):\n    <IND>\"\"\"Create a new readme with updated information\"\"\"\n    with open(readme, \"w\") as f:\n        <IND>for k, v in metadata.items():\n            <IND>print(f\"{k}: {v}\", file=f)\n        <DED>if lines:\n            <IND>for line in lines:\n                <IND>print(line, file=f)\n\n\n<DED><DED><DED><DED>def update_readme(readme: str, updated_metadata: dict = None, extra_lines: list = None):\n    <IND>metadata, lines = read_readme(readme)\n    if updated_metadata:\n        <IND>metadata = {**metadata, **updated_metadata}\n    <DED>if extra_lines:\n        <IND>lines = lines + extra_lines\n    <DED>write_readme(readme, metadata, lines)\n\n\n<DED>def generate_gap_bed(fname, outname):\n    <IND>\"\"\"Generate a BED file with gap locations.\n\n    Parameters\n    ----------\n    fname : str\n        Filename of input FASTA file.\n\n    outname : str\n        Filename of output BED file.\n    \"\"\"\n    f = Fasta(fname)\n    with open(outname, \"w\") as bed:\n        <IND>for chrom in f.keys():\n            <IND>for m in re.finditer(r\"N+\", f[chrom][:].seq):\n                <IND>bed.write(f\"{chrom}\\t{m.start(0)}\\t{m.end(0)}\\n\")\n\n\n<DED><DED><DED><DED>def generate_fa_sizes(fname, outname):\n    <IND>\"\"\"Generate a fa.sizes file.\n\n    Parameters\n    ----------\n    fname : str\n        Filename of input FASTA file.\n\n    outname : str\n        Filename of output BED file.\n    \"\"\"\n    f = Fasta(fname)\n    with open(outname, \"w\") as sizes:\n        <IND>for seqname, seq in f.items():\n            <IND>sizes.write(f\"{seqname}\\t{len(seq)}\\n\")\n\n\n<DED><DED><DED>def _fa_to_file(fasta: Fasta, contigs: list, filepath: str):\n    <IND>tmp_dir = mkdtemp(dir=os.path.dirname(filepath))\n    tmpfa = os.path.join(tmp_dir, \"regex.fa\")\n    with open(tmpfa, \"w\") as out:\n        <IND>for chrom in contigs:\n            <IND>out.write(f\">{fasta[chrom].name}\\n\")\n            out.write(f\"{fasta[chrom][:].seq}\\n\")\n    <DED><DED>os.rename(tmpfa, filepath)\n    rm_rf(tmp_dir)\n\n\n<DED>def filter_fasta(\n    infa: str,\n    regex: str = \".*\",\n    invert_match: Optional[bool] = False,\n    outfa: str = None,\n) -> Fasta:\n    <IND>\"\"\"Filter fasta file based on regex.\n\n    Parameters\n    ----------\n    infa : str\n        Filename of input fasta file.\n\n    outfa : str, optional\n        Filename of output fasta file.\n        Overwrites infa if left blank.\n\n    regex : str, optional\n        Regular expression used for selecting sequences.\n        Matches everything if left blank.\n\n    invert_match : bool, optional\n        Select all sequence *not* matching regex if set.\n\n    Returns\n    -------\n        fasta : Fasta instance\n            pyfaidx Fasta instance of newly created file\n    \"\"\"\n    fa = Fasta(infa)\n    pattern = re.compile(regex)\n    seqs = [k for k in fa.keys() if bool(pattern.search(k)) is not invert_match]\n    if len(seqs) == 0:\n        <IND>raise Exception(\"No sequences left after filtering!\")\n\n    <DED>outfa = outfa if outfa else infa\n    _fa_to_file(fa, seqs, outfa)\n    rm_rf(f\"{infa}.fai\")  # old index\n    return Fasta(outfa)\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "from norns import exceptions\n\nconfig = norns.config(\"genomepy\", default=\"cfg/default.yaml\")\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "vanheeringen-lab/genomepy",
    "commit": "acb59fbc2678caa2f7a261e53c6656ff9e6e5925",
    "filename": "genomepy/utils.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/vanheeringen-lab-genomepy/genomepy/utils.py",
    "file_hunks_size": 7,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "genomepy/utils.py:213:4 Incompatible variable type [9]: outfa is declared to have type `str` but is used as type `None`.",
    "message": " outfa is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 213,
    "warning_line": "    outfa: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "from norns import exceptions\nfrom pyfaidx import Fasta\nfrom tqdm.auto import tqdm\n\nfrom genomepy.exceptions import GenomeDownloadError\n\nconfig = norns.config(\"genomepy\", default=\"cfg/default.yaml\")\n\n\ndef download_file(url, filename):\n    \"\"\"\n    Helper method handling downloading large files from `url` to `filename`.\n    Returns a pointer to `filename`.\n    \"\"\"\n\n    def decorated_pbar(total):\n        \"\"\"Displays a progress bar with download speeds in MB/s.\"\"\"\n        return tqdm(\n            desc=\"Download\",\n            unit_scale=True,\n            unit_divisor=1024,\n            total=total,\n            unit=\"B\",\n        )\n\n    def write_n_update_pbar(data):\n        pbar.update(len(data))\n        f.write(data)\n\n    if url.startswith(\"ftp\"):\n        ftp, target = connect_ftp_link(url)\n        file_size = ftp.size(target)\n        with open(filename, \"wb\") as f:\n            pbar = decorated_pbar(file_size)\n\n            ftp.retrbinary(f\"RETR {target}\", write_n_update_pbar)\n            ftp.quit()  # logout\n\n    else:\n        r = requests.get(url, stream=True)\n        file_size = int(r.headers.get(\"Content-Length\", 0))\n        with open(filename, \"wb\") as f:\n            pbar = decorated_pbar(file_size)\n\n            for chunk in r.iter_content(chunk_size=1024):\n                if chunk:  # filter out keep-alive new chunks\n                    write_n_update_pbar(chunk)\n\n    pbar.close()  # stop updating pbar\n    return filename\n\n\ndef connect_ftp_link(link, timeout=None):\n    \"\"\"\n    anonymous login to ftp\n    accepts link in the form of ftp://ftp.name.domain/... and ftp.name.domain/...\n    \"\"\"\n    link = link.replace(\"ftp://\", \"\")\n    host = link.split(\"/\")[0]\n    target = link.split(host)[1]\n\n    try:\n        ftp = FTP(host, timeout=timeout)\n    except socket.gaierror:\n        raise GenomeDownloadError(f\"FTP host not found: {host}\")\n\n    ftp.login()\n    return ftp, target\n\n\ndef read_readme(readme: str) -> Tuple[dict, list]:\n    \"\"\"\n    parse readme file\n\n    Parameters\n    ----------\n    readme: str\n        filename\n\n    Returns\n    -------\n    tuple\n        metadata : dict with genome metadata\n        lines: list with non-metadata text (such as regex info)\n    \"\"\"\n    metadata = {\n        \"name\": \"na\",\n        \"provider\": \"na\",\n        \"original name\": \"na\",\n        \"original filename\": \"na\",\n        \"assembly_accession\": \"na\",\n        \"tax_id\": \"na\",\n        \"mask\": \"na\",\n        \"genome url\": \"na\",\n        \"annotation url\": \"na\",\n        \"sanitized annotation\": \"no\",\n        \"date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n    }\n    lines = []\n\n    if not os.path.exists(readme):\n        return metadata, lines\n\n    # if the readme exists, overwrite all metadata fields found\n    with open(readme) as f:\n        for line in f.readlines():\n            if \": \" in line:\n                vals = line.strip().split(\": \")\n                metadata[vals[0].strip()] = (\": \".join(vals[1:])).strip()\n            else:\n                line = line.strip(\"\\n\").strip(\" \")\n                # blank lines are allowed, but only one in a row\n                if not (\n                    line == \"\"\n                    and len(lines) > 0\n                    and lines[len(lines) - 1].strip() == \"\"\n                ):\n                    lines.append(line)\n\n    return metadata, lines\n\n\ndef write_readme(readme: str, metadata: dict, lines: list = None):\n    \"\"\"Create a new readme with updated information\"\"\"\n    with open(readme, \"w\") as f:\n        for k, v in metadata.items():\n            print(f\"{k}: {v}\", file=f)\n        if lines:\n            for line in lines:\n                print(line, file=f)\n\n\ndef update_readme(readme: str, updated_metadata: dict = None, extra_lines: list = None):\n    metadata, lines = read_readme(readme)\n    if updated_metadata:\n        metadata = {**metadata, **updated_metadata}\n    if extra_lines:\n        lines = lines + extra_lines\n    write_readme(readme, metadata, lines)\n\n\ndef generate_gap_bed(fname, outname):\n    \"\"\"Generate a BED file with gap locations.\n\n    Parameters\n    ----------\n    fname : str\n        Filename of input FASTA file.\n\n    outname : str\n        Filename of output BED file.\n    \"\"\"\n    f = Fasta(fname)\n    with open(outname, \"w\") as bed:\n        for chrom in f.keys():\n            for m in re.finditer(r\"N+\", f[chrom][:].seq):\n                bed.write(f\"{chrom}\\t{m.start(0)}\\t{m.end(0)}\\n\")\n\n\ndef generate_fa_sizes(fname, outname):\n    \"\"\"Generate a fa.sizes file.\n\n    Parameters\n    ----------\n    fname : str\n        Filename of input FASTA file.\n\n    outname : str\n        Filename of output BED file.\n    \"\"\"\n    f = Fasta(fname)\n    with open(outname, \"w\") as sizes:\n        for seqname, seq in f.items():\n            sizes.write(f\"{seqname}\\t{len(seq)}\\n\")\n\n\ndef _fa_to_file(fasta: Fasta, contigs: list, filepath: str):\n    tmp_dir = mkdtemp(dir=os.path.dirname(filepath))\n    tmpfa = os.path.join(tmp_dir, \"regex.fa\")\n    with open(tmpfa, \"w\") as out:\n        for chrom in contigs:\n            out.write(f\">{fasta[chrom].name}\\n\")\n            out.write(f\"{fasta[chrom][:].seq}\\n\")\n    os.rename(tmpfa, filepath)\n    rm_rf(tmp_dir)\n\n\ndef filter_fasta(\n    infa: str,\n    regex: str = \".*\",\n    invert_match: Optional[bool] = False,\n    outfa: str = None,\n) -> Fasta:\n    \"\"\"Filter fasta file based on regex.\n\n    Parameters\n    ----------\n    infa : str\n        Filename of input fasta file.\n\n    outfa : str, optional\n        Filename of output fasta file.\n        Overwrites infa if left blank.\n\n    regex : str, optional\n        Regular expression used for selecting sequences.\n        Matches everything if left blank.\n\n    invert_match : bool, optional\n        Select all sequence *not* matching regex if set.\n\n    Returns\n    -------\n        fasta : Fasta instance\n            pyfaidx Fasta instance of newly created file\n    \"\"\"\n    fa = Fasta(infa)\n    pattern = re.compile(regex)\n    seqs = [k for k in fa.keys() if bool(pattern.search(k)) is not invert_match]\n    if len(seqs) == 0:\n        raise Exception(\"No sequences left after filtering!\")\n\n    outfa = outfa if outfa else infa\n    _fa_to_file(fa, seqs, outfa)\n    rm_rf(f\"{infa}.fai\")  # old index\n    return Fasta(outfa)\n\n",
        "source_code_len": 6182,
        "target_code": "from norns import exceptions\n\nconfig = norns.config(\"genomepy\", default=\"cfg/default.yaml\")\n\n",
        "target_code_len": 93,
        "diff_format": "@@ -22,227 +14,4 @@\n from norns import exceptions\n-from pyfaidx import Fasta\n-from tqdm.auto import tqdm\n-\n-from genomepy.exceptions import GenomeDownloadError\n \n config = norns.config(\"genomepy\", default=\"cfg/default.yaml\")\n-\n-\n-def download_file(url, filename):\n-    \"\"\"\n-    Helper method handling downloading large files from `url` to `filename`.\n-    Returns a pointer to `filename`.\n-    \"\"\"\n-\n-    def decorated_pbar(total):\n-        \"\"\"Displays a progress bar with download speeds in MB/s.\"\"\"\n-        return tqdm(\n-            desc=\"Download\",\n-            unit_scale=True,\n-            unit_divisor=1024,\n-            total=total,\n-            unit=\"B\",\n-        )\n-\n-    def write_n_update_pbar(data):\n-        pbar.update(len(data))\n-        f.write(data)\n-\n-    if url.startswith(\"ftp\"):\n-        ftp, target = connect_ftp_link(url)\n-        file_size = ftp.size(target)\n-        with open(filename, \"wb\") as f:\n-            pbar = decorated_pbar(file_size)\n-\n-            ftp.retrbinary(f\"RETR {target}\", write_n_update_pbar)\n-            ftp.quit()  # logout\n-\n-    else:\n-        r = requests.get(url, stream=True)\n-        file_size = int(r.headers.get(\"Content-Length\", 0))\n-        with open(filename, \"wb\") as f:\n-            pbar = decorated_pbar(file_size)\n-\n-            for chunk in r.iter_content(chunk_size=1024):\n-                if chunk:  # filter out keep-alive new chunks\n-                    write_n_update_pbar(chunk)\n-\n-    pbar.close()  # stop updating pbar\n-    return filename\n-\n-\n-def connect_ftp_link(link, timeout=None):\n-    \"\"\"\n-    anonymous login to ftp\n-    accepts link in the form of ftp://ftp.name.domain/... and ftp.name.domain/...\n-    \"\"\"\n-    link = link.replace(\"ftp://\", \"\")\n-    host = link.split(\"/\")[0]\n-    target = link.split(host)[1]\n-\n-    try:\n-        ftp = FTP(host, timeout=timeout)\n-    except socket.gaierror:\n-        raise GenomeDownloadError(f\"FTP host not found: {host}\")\n-\n-    ftp.login()\n-    return ftp, target\n-\n-\n-def read_readme(readme: str) -> Tuple[dict, list]:\n-    \"\"\"\n-    parse readme file\n-\n-    Parameters\n-    ----------\n-    readme: str\n-        filename\n-\n-    Returns\n-    -------\n-    tuple\n-        metadata : dict with genome metadata\n-        lines: list with non-metadata text (such as regex info)\n-    \"\"\"\n-    metadata = {\n-        \"name\": \"na\",\n-        \"provider\": \"na\",\n-        \"original name\": \"na\",\n-        \"original filename\": \"na\",\n-        \"assembly_accession\": \"na\",\n-        \"tax_id\": \"na\",\n-        \"mask\": \"na\",\n-        \"genome url\": \"na\",\n-        \"annotation url\": \"na\",\n-        \"sanitized annotation\": \"no\",\n-        \"date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n-    }\n-    lines = []\n-\n-    if not os.path.exists(readme):\n-        return metadata, lines\n-\n-    # if the readme exists, overwrite all metadata fields found\n-    with open(readme) as f:\n-        for line in f.readlines():\n-            if \": \" in line:\n-                vals = line.strip().split(\": \")\n-                metadata[vals[0].strip()] = (\": \".join(vals[1:])).strip()\n-            else:\n-                line = line.strip(\"\\n\").strip(\" \")\n-                # blank lines are allowed, but only one in a row\n-                if not (\n-                    line == \"\"\n-                    and len(lines) > 0\n-                    and lines[len(lines) - 1].strip() == \"\"\n-                ):\n-                    lines.append(line)\n-\n-    return metadata, lines\n-\n-\n-def write_readme(readme: str, metadata: dict, lines: list = None):\n-    \"\"\"Create a new readme with updated information\"\"\"\n-    with open(readme, \"w\") as f:\n-        for k, v in metadata.items():\n-            print(f\"{k}: {v}\", file=f)\n-        if lines:\n-            for line in lines:\n-                print(line, file=f)\n-\n-\n-def update_readme(readme: str, updated_metadata: dict = None, extra_lines: list = None):\n-    metadata, lines = read_readme(readme)\n-    if updated_metadata:\n-        metadata = {**metadata, **updated_metadata}\n-    if extra_lines:\n-        lines = lines + extra_lines\n-    write_readme(readme, metadata, lines)\n-\n-\n-def generate_gap_bed(fname, outname):\n-    \"\"\"Generate a BED file with gap locations.\n-\n-    Parameters\n-    ----------\n-    fname : str\n-        Filename of input FASTA file.\n-\n-    outname : str\n-        Filename of output BED file.\n-    \"\"\"\n-    f = Fasta(fname)\n-    with open(outname, \"w\") as bed:\n-        for chrom in f.keys():\n-            for m in re.finditer(r\"N+\", f[chrom][:].seq):\n-                bed.write(f\"{chrom}\\t{m.start(0)}\\t{m.end(0)}\\n\")\n-\n-\n-def generate_fa_sizes(fname, outname):\n-    \"\"\"Generate a fa.sizes file.\n-\n-    Parameters\n-    ----------\n-    fname : str\n-        Filename of input FASTA file.\n-\n-    outname : str\n-        Filename of output BED file.\n-    \"\"\"\n-    f = Fasta(fname)\n-    with open(outname, \"w\") as sizes:\n-        for seqname, seq in f.items():\n-            sizes.write(f\"{seqname}\\t{len(seq)}\\n\")\n-\n-\n-def _fa_to_file(fasta: Fasta, contigs: list, filepath: str):\n-    tmp_dir = mkdtemp(dir=os.path.dirname(filepath))\n-    tmpfa = os.path.join(tmp_dir, \"regex.fa\")\n-    with open(tmpfa, \"w\") as out:\n-        for chrom in contigs:\n-            out.write(f\">{fasta[chrom].name}\\n\")\n-            out.write(f\"{fasta[chrom][:].seq}\\n\")\n-    os.rename(tmpfa, filepath)\n-    rm_rf(tmp_dir)\n-\n-\n-def filter_fasta(\n-    infa: str,\n-    regex: str = \".*\",\n-    invert_match: Optional[bool] = False,\n-    outfa: str = None,\n-) -> Fasta:\n-    \"\"\"Filter fasta file based on regex.\n-\n-    Parameters\n-    ----------\n-    infa : str\n-        Filename of input fasta file.\n-\n-    outfa : str, optional\n-        Filename of output fasta file.\n-        Overwrites infa if left blank.\n-\n-    regex : str, optional\n-        Regular expression used for selecting sequences.\n-        Matches everything if left blank.\n-\n-    invert_match : bool, optional\n-        Select all sequence *not* matching regex if set.\n-\n-    Returns\n-    -------\n-        fasta : Fasta instance\n-            pyfaidx Fasta instance of newly created file\n-    \"\"\"\n-    fa = Fasta(infa)\n-    pattern = re.compile(regex)\n-    seqs = [k for k in fa.keys() if bool(pattern.search(k)) is not invert_match]\n-    if len(seqs) == 0:\n-        raise Exception(\"No sequences left after filtering!\")\n-\n-    outfa = outfa if outfa else infa\n-    _fa_to_file(fa, seqs, outfa)\n-    rm_rf(f\"{infa}.fai\")  # old index\n-    return Fasta(outfa)\n \n",
        "source_code_with_indent": "from norns import exceptions\nfrom pyfaidx import Fasta\nfrom tqdm.auto import tqdm\n\nfrom genomepy.exceptions import GenomeDownloadError\n\nconfig = norns.config(\"genomepy\", default=\"cfg/default.yaml\")\n\n\ndef download_file(url, filename):\n    <IND>\"\"\"\n    Helper method handling downloading large files from `url` to `filename`.\n    Returns a pointer to `filename`.\n    \"\"\"\n\n    def decorated_pbar(total):\n        <IND>\"\"\"Displays a progress bar with download speeds in MB/s.\"\"\"\n        return tqdm(\n            desc=\"Download\",\n            unit_scale=True,\n            unit_divisor=1024,\n            total=total,\n            unit=\"B\",\n        )\n\n    <DED>def write_n_update_pbar(data):\n        <IND>pbar.update(len(data))\n        f.write(data)\n\n    <DED>if url.startswith(\"ftp\"):\n        <IND>ftp, target = connect_ftp_link(url)\n        file_size = ftp.size(target)\n        with open(filename, \"wb\") as f:\n            <IND>pbar = decorated_pbar(file_size)\n\n            ftp.retrbinary(f\"RETR {target}\", write_n_update_pbar)\n            ftp.quit()  # logout\n\n    <DED><DED>else:\n        <IND>r = requests.get(url, stream=True)\n        file_size = int(r.headers.get(\"Content-Length\", 0))\n        with open(filename, \"wb\") as f:\n            <IND>pbar = decorated_pbar(file_size)\n\n            for chunk in r.iter_content(chunk_size=1024):\n                <IND>if chunk:  # filter out keep-alive new chunks\n                    <IND>write_n_update_pbar(chunk)\n\n    <DED><DED><DED><DED>pbar.close()  # stop updating pbar\n    return filename\n\n\n<DED>def connect_ftp_link(link, timeout=None):\n    <IND>\"\"\"\n    anonymous login to ftp\n    accepts link in the form of ftp://ftp.name.domain/... and ftp.name.domain/...\n    \"\"\"\n    link = link.replace(\"ftp://\", \"\")\n    host = link.split(\"/\")[0]\n    target = link.split(host)[1]\n\n    try:\n        <IND>ftp = FTP(host, timeout=timeout)\n    <DED>except socket.gaierror:\n        <IND>raise GenomeDownloadError(f\"FTP host not found: {host}\")\n\n    <DED>ftp.login()\n    return ftp, target\n\n\n<DED>def read_readme(readme: str) -> Tuple[dict, list]:\n    <IND>\"\"\"\n    parse readme file\n\n    Parameters\n    ----------\n    readme: str\n        filename\n\n    Returns\n    -------\n    tuple\n        metadata : dict with genome metadata\n        lines: list with non-metadata text (such as regex info)\n    \"\"\"\n    metadata = {\n        \"name\": \"na\",\n        \"provider\": \"na\",\n        \"original name\": \"na\",\n        \"original filename\": \"na\",\n        \"assembly_accession\": \"na\",\n        \"tax_id\": \"na\",\n        \"mask\": \"na\",\n        \"genome url\": \"na\",\n        \"annotation url\": \"na\",\n        \"sanitized annotation\": \"no\",\n        \"date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n    }\n    lines = []\n\n    if not os.path.exists(readme):\n        <IND>return metadata, lines\n\n    # if the readme exists, overwrite all metadata fields found\n    <DED>with open(readme) as f:\n        <IND>for line in f.readlines():\n            <IND>if \": \" in line:\n                <IND>vals = line.strip().split(\": \")\n                metadata[vals[0].strip()] = (\": \".join(vals[1:])).strip()\n            <DED>else:\n                <IND>line = line.strip(\"\\n\").strip(\" \")\n                # blank lines are allowed, but only one in a row\n                if not (\n                    line == \"\"\n                    and len(lines) > 0\n                    and lines[len(lines) - 1].strip() == \"\"\n                ):\n                    <IND>lines.append(line)\n\n    <DED><DED><DED><DED>return metadata, lines\n\n\n<DED>def write_readme(readme: str, metadata: dict, lines: list = None):\n    <IND>\"\"\"Create a new readme with updated information\"\"\"\n    with open(readme, \"w\") as f:\n        <IND>for k, v in metadata.items():\n            <IND>print(f\"{k}: {v}\", file=f)\n        <DED>if lines:\n            <IND>for line in lines:\n                <IND>print(line, file=f)\n\n\n<DED><DED><DED><DED>def update_readme(readme: str, updated_metadata: dict = None, extra_lines: list = None):\n    <IND>metadata, lines = read_readme(readme)\n    if updated_metadata:\n        <IND>metadata = {**metadata, **updated_metadata}\n    <DED>if extra_lines:\n        <IND>lines = lines + extra_lines\n    <DED>write_readme(readme, metadata, lines)\n\n\n<DED>def generate_gap_bed(fname, outname):\n    <IND>\"\"\"Generate a BED file with gap locations.\n\n    Parameters\n    ----------\n    fname : str\n        Filename of input FASTA file.\n\n    outname : str\n        Filename of output BED file.\n    \"\"\"\n    f = Fasta(fname)\n    with open(outname, \"w\") as bed:\n        <IND>for chrom in f.keys():\n            <IND>for m in re.finditer(r\"N+\", f[chrom][:].seq):\n                <IND>bed.write(f\"{chrom}\\t{m.start(0)}\\t{m.end(0)}\\n\")\n\n\n<DED><DED><DED><DED>def generate_fa_sizes(fname, outname):\n    <IND>\"\"\"Generate a fa.sizes file.\n\n    Parameters\n    ----------\n    fname : str\n        Filename of input FASTA file.\n\n    outname : str\n        Filename of output BED file.\n    \"\"\"\n    f = Fasta(fname)\n    with open(outname, \"w\") as sizes:\n        <IND>for seqname, seq in f.items():\n            <IND>sizes.write(f\"{seqname}\\t{len(seq)}\\n\")\n\n\n<DED><DED><DED>def _fa_to_file(fasta: Fasta, contigs: list, filepath: str):\n    <IND>tmp_dir = mkdtemp(dir=os.path.dirname(filepath))\n    tmpfa = os.path.join(tmp_dir, \"regex.fa\")\n    with open(tmpfa, \"w\") as out:\n        <IND>for chrom in contigs:\n            <IND>out.write(f\">{fasta[chrom].name}\\n\")\n            out.write(f\"{fasta[chrom][:].seq}\\n\")\n    <DED><DED>os.rename(tmpfa, filepath)\n    rm_rf(tmp_dir)\n\n\n<DED>def filter_fasta(\n    infa: str,\n    regex: str = \".*\",\n    invert_match: Optional[bool] = False,\n    outfa: str = None,\n) -> Fasta:\n    <IND>\"\"\"Filter fasta file based on regex.\n\n    Parameters\n    ----------\n    infa : str\n        Filename of input fasta file.\n\n    outfa : str, optional\n        Filename of output fasta file.\n        Overwrites infa if left blank.\n\n    regex : str, optional\n        Regular expression used for selecting sequences.\n        Matches everything if left blank.\n\n    invert_match : bool, optional\n        Select all sequence *not* matching regex if set.\n\n    Returns\n    -------\n        fasta : Fasta instance\n            pyfaidx Fasta instance of newly created file\n    \"\"\"\n    fa = Fasta(infa)\n    pattern = re.compile(regex)\n    seqs = [k for k in fa.keys() if bool(pattern.search(k)) is not invert_match]\n    if len(seqs) == 0:\n        <IND>raise Exception(\"No sequences left after filtering!\")\n\n    <DED>outfa = outfa if outfa else infa\n    _fa_to_file(fa, seqs, outfa)\n    rm_rf(f\"{infa}.fai\")  # old index\n    return Fasta(outfa)\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "from norns import exceptions\n\nconfig = norns.config(\"genomepy\", default=\"cfg/default.yaml\")\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "vanheeringen-lab/genomepy",
    "commit": "acb59fbc2678caa2f7a261e53c6656ff9e6e5925",
    "filename": "genomepy/utils.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/vanheeringen-lab-genomepy/genomepy/utils.py",
    "file_hunks_size": 7,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "genomepy/utils.py:407:36 Invalid type [31]: Expression `(str, bool)` is not a valid type.",
    "message": " Expression `(str, bool)` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 407,
    "warning_line": "def gunzip_and_name(fname: str) -> (str, bool):",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\ndef tar_to_bigfile(fname, outfile):\n    \"\"\"Convert tar of multiple FASTAs to one file.\"\"\"\n    fnames = []\n    # Extract files to temporary directory\n    tmp_dir = mkdtemp(dir=os.path.dirname(outfile))\n    with tarfile.open(fname) as tar:\n        tar.extractall(path=tmp_dir)\n    for root, _, files in os.walk(tmp_dir):\n        fnames += [os.path.join(root, fname) for fname in files]\n\n    # Concatenate\n    with open(outfile, \"w\") as out:\n        for infile in fnames:\n            for line in open(infile):\n                out.write(line)\n\n    rm_rf(tmp_dir)\n\n\ndef gunzip_and_name(fname: str) -> (str, bool):\n    \"\"\"\n    Gunzips the file if gzipped (also works on bgzipped files)\n    Returns up-to-date filename and if it was gunzipped\n    \"\"\"\n    if fname.endswith(\".gz\"):\n        with gzip.open(fname, \"rb\") as f_in:\n            with open(fname[:-3], \"wb\") as f_out:\n                shutil.copyfileobj(f_in, f_out)\n        os.unlink(fname)\n        return fname[:-3], True\n    return fname, False\n\n\ndef gzip_and_name(fname, gzip_file=True):\n    \"\"\"\n    Gzip file if requested\n    Returns up to date filename\n    \"\"\"\n    if gzip_file:\n        with open(fname, \"rb\") as f_in:\n            with gzip.open(fname + \".gz\", \"wb\") as f_out:\n                shutil.copyfileobj(f_in, f_out)\n        os.unlink(fname)\n        fname += \".gz\"\n    return fname\n\n\ndef bgzip_and_name(fname, bgzip=True):\n    \"\"\"\n    Bgzip file if requested\n    Returns up to date filename\n    \"\"\"\n    if bgzip:\n        ret = sp.check_call([\"bgzip\", fname])\n        fname += \".gz\"\n        if ret != 0:\n            raise Exception(f\"Error bgzipping genome {fname}. Is tabix installed?\")\n    return fname\n\n\ndef is_number(term):\n    \"\"\"check if term is a number. Returns bool\"\"\"\n    if isinstance(term, int) or term.isdigit():\n        return True\n\n\ndef try_except_pass(errors, func, *args):\n    \"\"\"try to return FUNC with ARGS, pass on ERRORS\"\"\"\n    try:\n        return func(*args)\n    except errors:\n        pass\n\n\ndef retry(func, tries, *args):\n    \"\"\"\n    Retry functions with potential connection errors.\n\n    *args are passed as variables to func.\n    \"\"\"\n    _try = 1\n    while _try <= tries:\n        try:\n            answer = func(*args)\n            return answer\n        except (urllib.error.URLError, socket.timeout):\n            time.sleep(1)\n            _try += 1\n\n\ndef check_url(url, max_tries=1, timeout=15):\n    \"\"\"Check if URL works. Returns bool\"\"\"\n\n    def _check_url(_url, _timeout):\n        if _url.startswith(\"ftp\"):\n            ftp, target = connect_ftp_link(_url, timeout=_timeout)\n            try:\n                listing = ftp.nlst(target)\n            except all_errors:\n                listing = []\n            ftp.quit()  # logout\n            if listing:\n                return True\n        else:\n            ret = urllib.request.urlopen(_url, timeout=_timeout)\n            if ret.getcode() == 200:\n                return True\n        return False\n\n    return retry(_check_url, max_tries, url, timeout)\n\n\ndef read_url(url):\n    \"\"\"Read a text-based URL.\"\"\"\n    response = urllib.request.urlopen(url)\n    data = response.read()\n    text = data.decode(\"utf-8\")\n    return text\n\n\ndef get_file_info(fname):\n    \"\"\"\n    Returns the lower case file type of a file, and if it is gzipped\n\n    fname: str\n        filename\n    \"\"\"\n    fname = fname.lower()\n    gz = False\n    if fname.endswith(\".gz\"):\n        gz = True\n        fname = fname[:-3]\n    split = os.path.splitext(fname)\n    return split[1], gz\n\n\ndef _open(fname: str, mode: Optional[str] = \"r\"):\n    \"\"\"\n    Return a function to open a (gzipped) file.\n\n    fname: (gzipped) file path\n    mode: (r)ead or (w)rite.\n    \"\"\"\n    if mode not in [\"r\", \"w\"]:\n        raise ValueError(\"mode must be either 'r' or 'w'.\")\n\n    if fname.endswith(\".gz\"):\n        return gzip.open(fname, mode + \"t\")\n    return open(fname, mode)\n\n\ndef file_len(fname):\n    with _open(fname) as f:\n        for i, _ in enumerate(f):  # noqa: B007\n            pass\n    return i + 1\n\n",
        "source_code_len": 3992,
        "target_code": "\ndef try_except_pass(errors, func, *args, **kwargs):\n    \"\"\"try to return FUNC with ARGS, pass on ERRORS\"\"\"\n    try:\n        return func(*args, **kwargs)\n    except errors:\n        pass\n\n",
        "target_code_len": 187,
        "diff_format": "@@ -387,158 +136,8 @@\n \n-def tar_to_bigfile(fname, outfile):\n-    \"\"\"Convert tar of multiple FASTAs to one file.\"\"\"\n-    fnames = []\n-    # Extract files to temporary directory\n-    tmp_dir = mkdtemp(dir=os.path.dirname(outfile))\n-    with tarfile.open(fname) as tar:\n-        tar.extractall(path=tmp_dir)\n-    for root, _, files in os.walk(tmp_dir):\n-        fnames += [os.path.join(root, fname) for fname in files]\n-\n-    # Concatenate\n-    with open(outfile, \"w\") as out:\n-        for infile in fnames:\n-            for line in open(infile):\n-                out.write(line)\n-\n-    rm_rf(tmp_dir)\n-\n-\n-def gunzip_and_name(fname: str) -> (str, bool):\n-    \"\"\"\n-    Gunzips the file if gzipped (also works on bgzipped files)\n-    Returns up-to-date filename and if it was gunzipped\n-    \"\"\"\n-    if fname.endswith(\".gz\"):\n-        with gzip.open(fname, \"rb\") as f_in:\n-            with open(fname[:-3], \"wb\") as f_out:\n-                shutil.copyfileobj(f_in, f_out)\n-        os.unlink(fname)\n-        return fname[:-3], True\n-    return fname, False\n-\n-\n-def gzip_and_name(fname, gzip_file=True):\n-    \"\"\"\n-    Gzip file if requested\n-    Returns up to date filename\n-    \"\"\"\n-    if gzip_file:\n-        with open(fname, \"rb\") as f_in:\n-            with gzip.open(fname + \".gz\", \"wb\") as f_out:\n-                shutil.copyfileobj(f_in, f_out)\n-        os.unlink(fname)\n-        fname += \".gz\"\n-    return fname\n-\n-\n-def bgzip_and_name(fname, bgzip=True):\n-    \"\"\"\n-    Bgzip file if requested\n-    Returns up to date filename\n-    \"\"\"\n-    if bgzip:\n-        ret = sp.check_call([\"bgzip\", fname])\n-        fname += \".gz\"\n-        if ret != 0:\n-            raise Exception(f\"Error bgzipping genome {fname}. Is tabix installed?\")\n-    return fname\n-\n-\n-def is_number(term):\n-    \"\"\"check if term is a number. Returns bool\"\"\"\n-    if isinstance(term, int) or term.isdigit():\n-        return True\n-\n-\n-def try_except_pass(errors, func, *args):\n+def try_except_pass(errors, func, *args, **kwargs):\n     \"\"\"try to return FUNC with ARGS, pass on ERRORS\"\"\"\n     try:\n-        return func(*args)\n+        return func(*args, **kwargs)\n     except errors:\n         pass\n-\n-\n-def retry(func, tries, *args):\n-    \"\"\"\n-    Retry functions with potential connection errors.\n-\n-    *args are passed as variables to func.\n-    \"\"\"\n-    _try = 1\n-    while _try <= tries:\n-        try:\n-            answer = func(*args)\n-            return answer\n-        except (urllib.error.URLError, socket.timeout):\n-            time.sleep(1)\n-            _try += 1\n-\n-\n-def check_url(url, max_tries=1, timeout=15):\n-    \"\"\"Check if URL works. Returns bool\"\"\"\n-\n-    def _check_url(_url, _timeout):\n-        if _url.startswith(\"ftp\"):\n-            ftp, target = connect_ftp_link(_url, timeout=_timeout)\n-            try:\n-                listing = ftp.nlst(target)\n-            except all_errors:\n-                listing = []\n-            ftp.quit()  # logout\n-            if listing:\n-                return True\n-        else:\n-            ret = urllib.request.urlopen(_url, timeout=_timeout)\n-            if ret.getcode() == 200:\n-                return True\n-        return False\n-\n-    return retry(_check_url, max_tries, url, timeout)\n-\n-\n-def read_url(url):\n-    \"\"\"Read a text-based URL.\"\"\"\n-    response = urllib.request.urlopen(url)\n-    data = response.read()\n-    text = data.decode(\"utf-8\")\n-    return text\n-\n-\n-def get_file_info(fname):\n-    \"\"\"\n-    Returns the lower case file type of a file, and if it is gzipped\n-\n-    fname: str\n-        filename\n-    \"\"\"\n-    fname = fname.lower()\n-    gz = False\n-    if fname.endswith(\".gz\"):\n-        gz = True\n-        fname = fname[:-3]\n-    split = os.path.splitext(fname)\n-    return split[1], gz\n-\n-\n-def _open(fname: str, mode: Optional[str] = \"r\"):\n-    \"\"\"\n-    Return a function to open a (gzipped) file.\n-\n-    fname: (gzipped) file path\n-    mode: (r)ead or (w)rite.\n-    \"\"\"\n-    if mode not in [\"r\", \"w\"]:\n-        raise ValueError(\"mode must be either 'r' or 'w'.\")\n-\n-    if fname.endswith(\".gz\"):\n-        return gzip.open(fname, mode + \"t\")\n-    return open(fname, mode)\n-\n-\n-def file_len(fname):\n-    with _open(fname) as f:\n-        for i, _ in enumerate(f):  # noqa: B007\n-            pass\n-    return i + 1\n \n",
        "source_code_with_indent": "\n<DED><DED>def tar_to_bigfile(fname, outfile):\n    <IND>\"\"\"Convert tar of multiple FASTAs to one file.\"\"\"\n    fnames = []\n    # Extract files to temporary directory\n    tmp_dir = mkdtemp(dir=os.path.dirname(outfile))\n    with tarfile.open(fname) as tar:\n        <IND>tar.extractall(path=tmp_dir)\n    <DED>for root, _, files in os.walk(tmp_dir):\n        <IND>fnames += [os.path.join(root, fname) for fname in files]\n\n    # Concatenate\n    <DED>with open(outfile, \"w\") as out:\n        <IND>for infile in fnames:\n            <IND>for line in open(infile):\n                <IND>out.write(line)\n\n    <DED><DED><DED>rm_rf(tmp_dir)\n\n\n<DED>def gunzip_and_name(fname: str) -> (str, bool):\n    <IND>\"\"\"\n    Gunzips the file if gzipped (also works on bgzipped files)\n    Returns up-to-date filename and if it was gunzipped\n    \"\"\"\n    if fname.endswith(\".gz\"):\n        <IND>with gzip.open(fname, \"rb\") as f_in:\n            <IND>with open(fname[:-3], \"wb\") as f_out:\n                <IND>shutil.copyfileobj(f_in, f_out)\n        <DED><DED>os.unlink(fname)\n        return fname[:-3], True\n    <DED>return fname, False\n\n\n<DED>def gzip_and_name(fname, gzip_file=True):\n    <IND>\"\"\"\n    Gzip file if requested\n    Returns up to date filename\n    \"\"\"\n    if gzip_file:\n        <IND>with open(fname, \"rb\") as f_in:\n            <IND>with gzip.open(fname + \".gz\", \"wb\") as f_out:\n                <IND>shutil.copyfileobj(f_in, f_out)\n        <DED><DED>os.unlink(fname)\n        fname += \".gz\"\n    <DED>return fname\n\n\n<DED>def bgzip_and_name(fname, bgzip=True):\n    <IND>\"\"\"\n    Bgzip file if requested\n    Returns up to date filename\n    \"\"\"\n    if bgzip:\n        <IND>ret = sp.check_call([\"bgzip\", fname])\n        fname += \".gz\"\n        if ret != 0:\n            <IND>raise Exception(f\"Error bgzipping genome {fname}. Is tabix installed?\")\n    <DED><DED>return fname\n\n\n<DED>def is_number(term):\n    <IND>\"\"\"check if term is a number. Returns bool\"\"\"\n    if isinstance(term, int) or term.isdigit():\n        <IND>return True\n\n\n<DED><DED>def try_except_pass(errors, func, *args):\n    <IND>\"\"\"try to return FUNC with ARGS, pass on ERRORS\"\"\"\n    try:\n        <IND>return func(*args)\n    <DED>except errors:\n        <IND>pass\n\n\n<DED><DED>def retry(func, tries, *args):\n    <IND>\"\"\"\n    Retry functions with potential connection errors.\n\n    *args are passed as variables to func.\n    \"\"\"\n    _try = 1\n    while _try <= tries:\n        <IND>try:\n            <IND>answer = func(*args)\n            return answer\n        <DED>except (urllib.error.URLError, socket.timeout):\n            <IND>time.sleep(1)\n            _try += 1\n\n\n<DED><DED><DED>def check_url(url, max_tries=1, timeout=15):\n    <IND>\"\"\"Check if URL works. Returns bool\"\"\"\n\n    def _check_url(_url, _timeout):\n        <IND>if _url.startswith(\"ftp\"):\n            <IND>ftp, target = connect_ftp_link(_url, timeout=_timeout)\n            try:\n                <IND>listing = ftp.nlst(target)\n            <DED>except all_errors:\n                <IND>listing = []\n            <DED>ftp.quit()  # logout\n            if listing:\n                <IND>return True\n        <DED><DED>else:\n            <IND>ret = urllib.request.urlopen(_url, timeout=_timeout)\n            if ret.getcode() == 200:\n                <IND>return True\n        <DED><DED>return False\n\n    <DED>return retry(_check_url, max_tries, url, timeout)\n\n\n<DED>def read_url(url):\n    <IND>\"\"\"Read a text-based URL.\"\"\"\n    response = urllib.request.urlopen(url)\n    data = response.read()\n    text = data.decode(\"utf-8\")\n    return text\n\n\n<DED>def get_file_info(fname):\n    <IND>\"\"\"\n    Returns the lower case file type of a file, and if it is gzipped\n\n    fname: str\n        filename\n    \"\"\"\n    fname = fname.lower()\n    gz = False\n    if fname.endswith(\".gz\"):\n        <IND>gz = True\n        fname = fname[:-3]\n    <DED>split = os.path.splitext(fname)\n    return split[1], gz\n\n\n<DED>def _open(fname: str, mode: Optional[str] = \"r\"):\n    <IND>\"\"\"\n    Return a function to open a (gzipped) file.\n\n    fname: (gzipped) file path\n    mode: (r)ead or (w)rite.\n    \"\"\"\n    if mode not in [\"r\", \"w\"]:\n        <IND>raise ValueError(\"mode must be either 'r' or 'w'.\")\n\n    <DED>if fname.endswith(\".gz\"):\n        <IND>return gzip.open(fname, mode + \"t\")\n    <DED>return open(fname, mode)\n\n\n<DED>def file_len(fname):\n    <IND>with _open(fname) as f:\n        <IND>for i, _ in enumerate(f):  # noqa: B007\n            <IND>pass\n    <DED><DED>return i + 1\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n<DED><DED>def try_except_pass(errors, func, *args, **kwargs):\n    <IND>\"\"\"try to return FUNC with ARGS, pass on ERRORS\"\"\"\n    try:\n        <IND>return func(*args, **kwargs)\n    <DED>except errors:\n        <IND>pass\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "vanheeringen-lab/genomepy",
    "commit": "acb59fbc2678caa2f7a261e53c6656ff9e6e5925",
    "filename": "genomepy/utils.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/vanheeringen-lab-genomepy/genomepy/utils.py",
    "file_hunks_size": 7,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "genomepy/utils.py:565:22 Unsupported operand [58]: `<` is not supported for operand types `int` and `Union[List[typing.Any], int]`.",
    "message": " `<` is not supported for operand types `int` and `Union[List[typing.Any], int]`.",
    "rule_id": "Unsupported operand [58]",
    "warning_line_no": 565,
    "warning_line": "        if distance < nearest[0]:"
  },
  {
    "project": "vanheeringen-lab/genomepy",
    "commit": "acb59fbc2678caa2f7a261e53c6656ff9e6e5925",
    "filename": "genomepy/utils.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/vanheeringen-lab-genomepy/genomepy/utils.py",
    "file_hunks_size": 7,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "genomepy/utils.py:567:4 Incompatible variable type [9]: targets is declared to have type `List[typing.Any]` but is used as type `Union[List[typing.Any], int]`.",
    "message": " targets is declared to have type `List[typing.Any]` but is used as type `Union[List[typing.Any], int]`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 567,
    "warning_line": "    targets = nearest[1]"
  }
]