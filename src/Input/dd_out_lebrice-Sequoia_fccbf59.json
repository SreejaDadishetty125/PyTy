[
  {
    "project": "lebrice/Sequoia",
    "commit": "fccbf593f6bf071511c71dcc6d471a585795cb1f",
    "filename": "sequoia/methods/aux_tasks/ewc.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/lebrice-Sequoia/sequoia/methods/aux_tasks/ewc.py",
    "file_hunks_size": 10,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "sequoia/methods/aux_tasks/ewc.py:151:68 Unsupported operand [58]: `>` is not supported for operand types `int` and `Optional[int]`.",
    "message": " `>` is not supported for operand types `int` and `Optional[int]`.",
    "rule_id": "Unsupported operand [58]",
    "warning_line_no": 151,
    "warning_line": "        elif self._model.training and (task_id is None or task_id > self.previous_task):",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    def on_task_switch(self, task_id: Optional[int]):\n        \"\"\" Executed when the task switches (to either a known or unknown task).\n        \"\"\"\n        if not self.enabled:\n            return\n\n        logger.info(f\"On task switch called: task_id={task_id}\")\n\n        if self._shared_net is None:\n            logger.info(\n                f\"On task switch called: task_id={task_id}, EWC cannot be \"\n                f\"applied as there are no shared weights.\"\n            )\n\n        elif self.previous_task is None and self.n_switches == 0 and not task_id:\n            self.previous_task = task_id\n            logger.info(\"Starting the first task, no EWC update.\")\n            self.n_switches += 1\n\n        elif self._model.training and (task_id is None or task_id > self.previous_task):\n            # we dont want to go here at test time.\n            # NOTE: We also switch between unknown tasks.\n            logger.info(\n                f\"Switching tasks: {self.previous_task} -> {task_id}: \"\n                f\"Updating the EWC 'anchor' weights.\"\n            )\n            self.previous_task = task_id\n            device = self._model.config.device\n            self.previous_model_weights = (\n                PVector.from_model(self._shared_net.to(device)).clone().detach()\n            )\n\n            # Create a Dataloader from the stored observations.\n            obs_type: Type[Observations] = type(self.observation_collector[0])\n            dataset = [obs.as_namedtuple() for obs in self.observation_collector]\n            # Or, alternatively (see the note below on why we don't use this):\n            # stacked_observations: Observations = obs_type.stack(self.observation_collector)\n            # dataset = TensorDataset(*stacked_observations.as_namedtuple())\n\n            # NOTE: This is equivalent to just using the same batch size as during\n            # training, as each Observations in the list is already a batch.\n            # NOTE: We keep the same batch size here as during training because for\n            # instance in RL, it would be weird to suddenly give some new batch size,\n            # since the buffers would get cleared and re-created just for these forward\n            # passes\n            dataloader = DataLoader(dataset, batch_size=None, collate_fn=None)\n\n            # Create the parameters to be passed to the FIM function. These may vary a\n            # bit, depending on if we're being applied in a classification setting or in\n            # a regression setting (not done yet)\n            variant: str\n            if isinstance(self._model.output_head, ClassificationHead):\n                variant = \"classif_logits\"\n                n_output = self._model.action_space.n\n\n                def fim_function(*inputs) -> Tensor:\n                    observations = obs_type(*inputs).to(self._model.device)\n                    forward_pass: ForwardPass = self._model(observations)\n                    actions = forward_pass.actions\n                    return actions.logits\n\n            elif isinstance(self._model.output_head, RegressionHead):\n                # NOTE: This hasn't been tested yet.\n                variant = \"regression\"\n                n_output = flatdim(self._model.action_space)\n\n                def fim_function(*inputs) -> Tensor:\n                    observations = obs_type(*inputs).to(self._model.device)\n                    forward_pass: ForwardPass = self._model(observations)\n                    actions = forward_pass.actions\n                    return actions.y_pred\n\n            else:\n                raise NotImplementedError(\"TODO\")\n\n            new_fim = FIM(\n                model=self._shared_net,\n                loader=dataloader,\n                representation=self.options.fim_representation,\n                n_output=n_output,\n                variant=variant,\n                function=fim_function,\n                device=self._model.device,\n            )\n\n            # TODO: There was maybe an idea to use another fisher information matrix for\n            # the critic in A2C, but not doing that atm.\n            new_fims = [new_fim]\n            self.consolidate(new_fims, task=self.previous_task)\n            self.n_switches += 1\n            self.observation_collector.clear()\n\n    @property\n    def _shared_net(self) -> Optional[nn.Module]:\n        \"\"\"\n        Returns 'None' if there is not shared network part, othervise returns the shared net\n        \"\"\"\n        if self._model.encoder is None:\n            return None\n        elif isinstance(self._model.encoder, nn.Sequential):\n            if len(self._model.encoder) == 0:\n                return None\n        return self._model.encoder\n\n    def get_loss(self, forward_pass: ForwardPass, y: Tensor = None) -> Loss:\n        \"\"\" Gets the EWC loss.\n        \"\"\"\n        if self._model.training:      \n            self.observation_collector.append(forward_pass.observations)\n\n        if self.previous_task is None or not self.enabled or self._shared_net is None:\n            # We're in the first task: do nothing.\n            return Loss(name=self.name)\n\n        loss = 0.0\n        v_current = PVector.from_model(self._shared_net)\n\n        for fim in self.fisher_information_matrices:\n            diff = v_current - self.previous_model_weights\n            loss += fim.vTMv(diff)\n        self._i += 1\n        ewc_loss = Loss(name=self.name, loss=loss)\n        return ewc_loss\n",
        "source_code_len": 5397,
        "target_code": "\n    def get_current_model_weights(self) -> PVector:\n        return PVector.from_model(self.model.shared_modules())\n",
        "target_code_len": 116,
        "diff_format": "@@ -131,124 +312,3 @@\n \n-    def on_task_switch(self, task_id: Optional[int]):\n-        \"\"\" Executed when the task switches (to either a known or unknown task).\n-        \"\"\"\n-        if not self.enabled:\n-            return\n-\n-        logger.info(f\"On task switch called: task_id={task_id}\")\n-\n-        if self._shared_net is None:\n-            logger.info(\n-                f\"On task switch called: task_id={task_id}, EWC cannot be \"\n-                f\"applied as there are no shared weights.\"\n-            )\n-\n-        elif self.previous_task is None and self.n_switches == 0 and not task_id:\n-            self.previous_task = task_id\n-            logger.info(\"Starting the first task, no EWC update.\")\n-            self.n_switches += 1\n-\n-        elif self._model.training and (task_id is None or task_id > self.previous_task):\n-            # we dont want to go here at test time.\n-            # NOTE: We also switch between unknown tasks.\n-            logger.info(\n-                f\"Switching tasks: {self.previous_task} -> {task_id}: \"\n-                f\"Updating the EWC 'anchor' weights.\"\n-            )\n-            self.previous_task = task_id\n-            device = self._model.config.device\n-            self.previous_model_weights = (\n-                PVector.from_model(self._shared_net.to(device)).clone().detach()\n-            )\n-\n-            # Create a Dataloader from the stored observations.\n-            obs_type: Type[Observations] = type(self.observation_collector[0])\n-            dataset = [obs.as_namedtuple() for obs in self.observation_collector]\n-            # Or, alternatively (see the note below on why we don't use this):\n-            # stacked_observations: Observations = obs_type.stack(self.observation_collector)\n-            # dataset = TensorDataset(*stacked_observations.as_namedtuple())\n-\n-            # NOTE: This is equivalent to just using the same batch size as during\n-            # training, as each Observations in the list is already a batch.\n-            # NOTE: We keep the same batch size here as during training because for\n-            # instance in RL, it would be weird to suddenly give some new batch size,\n-            # since the buffers would get cleared and re-created just for these forward\n-            # passes\n-            dataloader = DataLoader(dataset, batch_size=None, collate_fn=None)\n-\n-            # Create the parameters to be passed to the FIM function. These may vary a\n-            # bit, depending on if we're being applied in a classification setting or in\n-            # a regression setting (not done yet)\n-            variant: str\n-            if isinstance(self._model.output_head, ClassificationHead):\n-                variant = \"classif_logits\"\n-                n_output = self._model.action_space.n\n-\n-                def fim_function(*inputs) -> Tensor:\n-                    observations = obs_type(*inputs).to(self._model.device)\n-                    forward_pass: ForwardPass = self._model(observations)\n-                    actions = forward_pass.actions\n-                    return actions.logits\n-\n-            elif isinstance(self._model.output_head, RegressionHead):\n-                # NOTE: This hasn't been tested yet.\n-                variant = \"regression\"\n-                n_output = flatdim(self._model.action_space)\n-\n-                def fim_function(*inputs) -> Tensor:\n-                    observations = obs_type(*inputs).to(self._model.device)\n-                    forward_pass: ForwardPass = self._model(observations)\n-                    actions = forward_pass.actions\n-                    return actions.y_pred\n-\n-            else:\n-                raise NotImplementedError(\"TODO\")\n-\n-            new_fim = FIM(\n-                model=self._shared_net,\n-                loader=dataloader,\n-                representation=self.options.fim_representation,\n-                n_output=n_output,\n-                variant=variant,\n-                function=fim_function,\n-                device=self._model.device,\n-            )\n-\n-            # TODO: There was maybe an idea to use another fisher information matrix for\n-            # the critic in A2C, but not doing that atm.\n-            new_fims = [new_fim]\n-            self.consolidate(new_fims, task=self.previous_task)\n-            self.n_switches += 1\n-            self.observation_collector.clear()\n-\n-    @property\n-    def _shared_net(self) -> Optional[nn.Module]:\n-        \"\"\"\n-        Returns 'None' if there is not shared network part, othervise returns the shared net\n-        \"\"\"\n-        if self._model.encoder is None:\n-            return None\n-        elif isinstance(self._model.encoder, nn.Sequential):\n-            if len(self._model.encoder) == 0:\n-                return None\n-        return self._model.encoder\n-\n-    def get_loss(self, forward_pass: ForwardPass, y: Tensor = None) -> Loss:\n-        \"\"\" Gets the EWC loss.\n-        \"\"\"\n-        if self._model.training:      \n-            self.observation_collector.append(forward_pass.observations)\n-\n-        if self.previous_task is None or not self.enabled or self._shared_net is None:\n-            # We're in the first task: do nothing.\n-            return Loss(name=self.name)\n-\n-        loss = 0.0\n-        v_current = PVector.from_model(self._shared_net)\n-\n-        for fim in self.fisher_information_matrices:\n-            diff = v_current - self.previous_model_weights\n-            loss += fim.vTMv(diff)\n-        self._i += 1\n-        ewc_loss = Loss(name=self.name, loss=loss)\n-        return ewc_loss\n+    def get_current_model_weights(self) -> PVector:\n+        return PVector.from_model(self.model.shared_modules())\n",
        "source_code_with_indent": "\n    <DED><DED><DED>def on_task_switch(self, task_id: Optional[int]):\n        <IND>\"\"\" Executed when the task switches (to either a known or unknown task).\n        \"\"\"\n        if not self.enabled:\n            <IND>return\n\n        <DED>logger.info(f\"On task switch called: task_id={task_id}\")\n\n        if self._shared_net is None:\n            <IND>logger.info(\n                f\"On task switch called: task_id={task_id}, EWC cannot be \"\n                f\"applied as there are no shared weights.\"\n            )\n\n        <DED>elif self.previous_task is None and self.n_switches == 0 and not task_id:\n            <IND>self.previous_task = task_id\n            logger.info(\"Starting the first task, no EWC update.\")\n            self.n_switches += 1\n\n        <DED>elif self._model.training and (task_id is None or task_id > self.previous_task):\n            # we dont want to go here at test time.\n            # NOTE: We also switch between unknown tasks.\n            <IND>logger.info(\n                f\"Switching tasks: {self.previous_task} -> {task_id}: \"\n                f\"Updating the EWC 'anchor' weights.\"\n            )\n            self.previous_task = task_id\n            device = self._model.config.device\n            self.previous_model_weights = (\n                PVector.from_model(self._shared_net.to(device)).clone().detach()\n            )\n\n            # Create a Dataloader from the stored observations.\n            obs_type: Type[Observations] = type(self.observation_collector[0])\n            dataset = [obs.as_namedtuple() for obs in self.observation_collector]\n            # Or, alternatively (see the note below on why we don't use this):\n            # stacked_observations: Observations = obs_type.stack(self.observation_collector)\n            # dataset = TensorDataset(*stacked_observations.as_namedtuple())\n\n            # NOTE: This is equivalent to just using the same batch size as during\n            # training, as each Observations in the list is already a batch.\n            # NOTE: We keep the same batch size here as during training because for\n            # instance in RL, it would be weird to suddenly give some new batch size,\n            # since the buffers would get cleared and re-created just for these forward\n            # passes\n            dataloader = DataLoader(dataset, batch_size=None, collate_fn=None)\n\n            # Create the parameters to be passed to the FIM function. These may vary a\n            # bit, depending on if we're being applied in a classification setting or in\n            # a regression setting (not done yet)\n            variant: str\n            if isinstance(self._model.output_head, ClassificationHead):\n                <IND>variant = \"classif_logits\"\n                n_output = self._model.action_space.n\n\n                def fim_function(*inputs) -> Tensor:\n                    <IND>observations = obs_type(*inputs).to(self._model.device)\n                    forward_pass: ForwardPass = self._model(observations)\n                    actions = forward_pass.actions\n                    return actions.logits\n\n            <DED><DED>elif isinstance(self._model.output_head, RegressionHead):\n                # NOTE: This hasn't been tested yet.\n                <IND>variant = \"regression\"\n                n_output = flatdim(self._model.action_space)\n\n                def fim_function(*inputs) -> Tensor:\n                    <IND>observations = obs_type(*inputs).to(self._model.device)\n                    forward_pass: ForwardPass = self._model(observations)\n                    actions = forward_pass.actions\n                    return actions.y_pred\n\n            <DED><DED>else:\n                <IND>raise NotImplementedError(\"TODO\")\n\n            <DED>new_fim = FIM(\n                model=self._shared_net,\n                loader=dataloader,\n                representation=self.options.fim_representation,\n                n_output=n_output,\n                variant=variant,\n                function=fim_function,\n                device=self._model.device,\n            )\n\n            # TODO: There was maybe an idea to use another fisher information matrix for\n            # the critic in A2C, but not doing that atm.\n            new_fims = [new_fim]\n            self.consolidate(new_fims, task=self.previous_task)\n            self.n_switches += 1\n            self.observation_collector.clear()\n\n    <DED><DED>@property\n    def _shared_net(self) -> Optional[nn.Module]:\n        <IND>\"\"\"\n        Returns 'None' if there is not shared network part, othervise returns the shared net\n        \"\"\"\n        if self._model.encoder is None:\n            <IND>return None\n        <DED>elif isinstance(self._model.encoder, nn.Sequential):\n            <IND>if len(self._model.encoder) == 0:\n                <IND>return None\n        <DED><DED>return self._model.encoder\n\n    <DED>def get_loss(self, forward_pass: ForwardPass, y: Tensor = None) -> Loss:\n        <IND>\"\"\" Gets the EWC loss.\n        \"\"\"\n        if self._model.training:      \n            <IND>self.observation_collector.append(forward_pass.observations)\n\n        <DED>if self.previous_task is None or not self.enabled or self._shared_net is None:\n            # We're in the first task: do nothing.\n            <IND>return Loss(name=self.name)\n\n        <DED>loss = 0.0\n        v_current = PVector.from_model(self._shared_net)\n\n        for fim in self.fisher_information_matrices:\n            <IND>diff = v_current - self.previous_model_weights\n            loss += fim.vTMv(diff)\n        <DED>self._i += 1\n        ewc_loss = Loss(name=self.name, loss=loss)\n        return ewc_loss\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED><DED><DED>def get_current_model_weights(self) -> PVector:\n        <IND>return PVector.from_model(self.model.shared_modules())\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  }
]