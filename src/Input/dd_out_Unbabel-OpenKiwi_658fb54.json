[
  {
    "project": "Unbabel/OpenKiwi",
    "commit": "658fb5427252d78c4d83048138ce64857149cabe",
    "filename": "kiwi/modules/common/attention.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Unbabel-OpenKiwi/kiwi/modules/common/attention.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "kiwi/modules/common/attention.py:126:17 Call error [29]: `Attention` is not a function.",
    "message": " `Attention` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 126,
    "warning_line": "    out, probs = attn(qs, qs, qs)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        return o_attn, p_attn\n\n\nif __name__ == '__main__':\n    from kiwi.utils.tensors import sequence_mask\n    from kiwi.modules.common.scorer import (\n        DotProductScorer,\n        GeneralScorer,\n        OperationScorer,\n        MLPScorer,\n    )\n\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, query_size)\n\n    # set of query vectors\n    qs = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    # values vectors (same shape as keys)\n    vs = torch.randn(batch_size, source_len, keys_size)\n\n    # values vectors with same size as query vectors\n    vq = torch.randn(batch_size, source_len, query_size)\n\n    # self attention on target (decoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # self attention on source (encoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # masked self attention on target (decoder)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs, mask=mask)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # masked self attention on source (encoder)\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks, mask=mask)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - multiplicative attention\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # masked encoder attend to decoder - multiplicative attention\n    # this is odd but we can do it anyway :-)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n    out, probs = attn(ks, qs, qs, mask=mask)\n    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n\n    # masked decoder attend to encoder - multiplicative attention\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks, mask=mask)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - concat attention\n    attn = Attention(\n        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n    )\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_len": 3940,
        "target_code": "        return o_attn, p_attn\n",
        "target_code_len": 30,
        "diff_format": "@@ -84,100 +84,1 @@\n         return o_attn, p_attn\n-\n-\n-if __name__ == '__main__':\n-    from kiwi.utils.tensors import sequence_mask\n-    from kiwi.modules.common.scorer import (\n-        DotProductScorer,\n-        GeneralScorer,\n-        OperationScorer,\n-        MLPScorer,\n-    )\n-\n-    torch.manual_seed(1)\n-    torch.cuda.manual_seed(1)\n-\n-    batch_size = 4\n-    source_len = 7\n-    target_len = 3\n-    query_size = 10\n-    keys_size = 20\n-    attn_size = 15\n-\n-    # query vectors\n-    q = torch.randn(batch_size, query_size)\n-\n-    # set of query vectors\n-    qs = torch.randn(batch_size, target_len, query_size)\n-\n-    # keys vectors (a key vector for each encoder word)\n-    ks = torch.randn(batch_size, source_len, keys_size)\n-\n-    # keys vectors with same size as query vectors\n-    kq = torch.randn(batch_size, source_len, query_size)\n-\n-    # values vectors (same shape as keys)\n-    vs = torch.randn(batch_size, source_len, keys_size)\n-\n-    # values vectors with same size as query vectors\n-    vq = torch.randn(batch_size, source_len, query_size)\n-\n-    # self attention on target (decoder)\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(qs, qs, qs)\n-    assert list(out.shape) == list(qs.shape)\n-    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n-\n-    # self attention on source (encoder)\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(ks, ks, ks)\n-    assert list(out.shape) == list(ks.shape)\n-    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n-\n-    # masked self attention on target (decoder)\n-    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(qs, qs, qs, mask=mask)\n-    assert list(out.shape) == list(qs.shape)\n-    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n-\n-    # masked self attention on source (encoder)\n-    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(ks, ks, ks, mask=mask)\n-    assert list(out.shape) == list(ks.shape)\n-    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder - multiplicative attention\n-    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # masked encoder attend to decoder - multiplicative attention\n-    # this is odd but we can do it anyway :-)\n-    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n-    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n-    out, probs = attn(ks, qs, qs, mask=mask)\n-    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n-    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n-\n-    # masked decoder attend to encoder - multiplicative attention\n-    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n-    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n-    out, probs = attn(qs, ks, ks, mask=mask)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder - concat attention\n-    attn = Attention(\n-        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n-    )\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n-    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_with_indent": "        return o_attn, p_attn\n\n\n<DED><DED>if __name__ == '__main__':\n    <IND>from kiwi.utils.tensors import sequence_mask\n    from kiwi.modules.common.scorer import (\n        DotProductScorer,\n        GeneralScorer,\n        OperationScorer,\n        MLPScorer,\n    )\n\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, query_size)\n\n    # set of query vectors\n    qs = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    # values vectors (same shape as keys)\n    vs = torch.randn(batch_size, source_len, keys_size)\n\n    # values vectors with same size as query vectors\n    vq = torch.randn(batch_size, source_len, query_size)\n\n    # self attention on target (decoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # self attention on source (encoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # masked self attention on target (decoder)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs, mask=mask)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # masked self attention on source (encoder)\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks, mask=mask)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - multiplicative attention\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # masked encoder attend to decoder - multiplicative attention\n    # this is odd but we can do it anyway :-)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n    out, probs = attn(ks, qs, qs, mask=mask)\n    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n\n    # masked decoder attend to encoder - multiplicative attention\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks, mask=mask)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - concat attention\n    attn = Attention(\n        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n    )\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        return o_attn, p_attn\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "Unbabel/OpenKiwi",
    "commit": "658fb5427252d78c4d83048138ce64857149cabe",
    "filename": "kiwi/modules/common/attention.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Unbabel-OpenKiwi/kiwi/modules/common/attention.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "kiwi/modules/common/attention.py:132:17 Call error [29]: `Attention` is not a function.",
    "message": " `Attention` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 132,
    "warning_line": "    out, probs = attn(ks, ks, ks)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        return o_attn, p_attn\n\n\nif __name__ == '__main__':\n    from kiwi.utils.tensors import sequence_mask\n    from kiwi.modules.common.scorer import (\n        DotProductScorer,\n        GeneralScorer,\n        OperationScorer,\n        MLPScorer,\n    )\n\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, query_size)\n\n    # set of query vectors\n    qs = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    # values vectors (same shape as keys)\n    vs = torch.randn(batch_size, source_len, keys_size)\n\n    # values vectors with same size as query vectors\n    vq = torch.randn(batch_size, source_len, query_size)\n\n    # self attention on target (decoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # self attention on source (encoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # masked self attention on target (decoder)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs, mask=mask)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # masked self attention on source (encoder)\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks, mask=mask)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - multiplicative attention\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # masked encoder attend to decoder - multiplicative attention\n    # this is odd but we can do it anyway :-)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n    out, probs = attn(ks, qs, qs, mask=mask)\n    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n\n    # masked decoder attend to encoder - multiplicative attention\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks, mask=mask)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - concat attention\n    attn = Attention(\n        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n    )\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_len": 3940,
        "target_code": "        return o_attn, p_attn\n",
        "target_code_len": 30,
        "diff_format": "@@ -84,100 +84,1 @@\n         return o_attn, p_attn\n-\n-\n-if __name__ == '__main__':\n-    from kiwi.utils.tensors import sequence_mask\n-    from kiwi.modules.common.scorer import (\n-        DotProductScorer,\n-        GeneralScorer,\n-        OperationScorer,\n-        MLPScorer,\n-    )\n-\n-    torch.manual_seed(1)\n-    torch.cuda.manual_seed(1)\n-\n-    batch_size = 4\n-    source_len = 7\n-    target_len = 3\n-    query_size = 10\n-    keys_size = 20\n-    attn_size = 15\n-\n-    # query vectors\n-    q = torch.randn(batch_size, query_size)\n-\n-    # set of query vectors\n-    qs = torch.randn(batch_size, target_len, query_size)\n-\n-    # keys vectors (a key vector for each encoder word)\n-    ks = torch.randn(batch_size, source_len, keys_size)\n-\n-    # keys vectors with same size as query vectors\n-    kq = torch.randn(batch_size, source_len, query_size)\n-\n-    # values vectors (same shape as keys)\n-    vs = torch.randn(batch_size, source_len, keys_size)\n-\n-    # values vectors with same size as query vectors\n-    vq = torch.randn(batch_size, source_len, query_size)\n-\n-    # self attention on target (decoder)\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(qs, qs, qs)\n-    assert list(out.shape) == list(qs.shape)\n-    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n-\n-    # self attention on source (encoder)\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(ks, ks, ks)\n-    assert list(out.shape) == list(ks.shape)\n-    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n-\n-    # masked self attention on target (decoder)\n-    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(qs, qs, qs, mask=mask)\n-    assert list(out.shape) == list(qs.shape)\n-    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n-\n-    # masked self attention on source (encoder)\n-    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(ks, ks, ks, mask=mask)\n-    assert list(out.shape) == list(ks.shape)\n-    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder - multiplicative attention\n-    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # masked encoder attend to decoder - multiplicative attention\n-    # this is odd but we can do it anyway :-)\n-    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n-    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n-    out, probs = attn(ks, qs, qs, mask=mask)\n-    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n-    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n-\n-    # masked decoder attend to encoder - multiplicative attention\n-    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n-    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n-    out, probs = attn(qs, ks, ks, mask=mask)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder - concat attention\n-    attn = Attention(\n-        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n-    )\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n-    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_with_indent": "        return o_attn, p_attn\n\n\n<DED><DED>if __name__ == '__main__':\n    <IND>from kiwi.utils.tensors import sequence_mask\n    from kiwi.modules.common.scorer import (\n        DotProductScorer,\n        GeneralScorer,\n        OperationScorer,\n        MLPScorer,\n    )\n\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, query_size)\n\n    # set of query vectors\n    qs = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    # values vectors (same shape as keys)\n    vs = torch.randn(batch_size, source_len, keys_size)\n\n    # values vectors with same size as query vectors\n    vq = torch.randn(batch_size, source_len, query_size)\n\n    # self attention on target (decoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # self attention on source (encoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # masked self attention on target (decoder)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs, mask=mask)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # masked self attention on source (encoder)\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks, mask=mask)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - multiplicative attention\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # masked encoder attend to decoder - multiplicative attention\n    # this is odd but we can do it anyway :-)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n    out, probs = attn(ks, qs, qs, mask=mask)\n    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n\n    # masked decoder attend to encoder - multiplicative attention\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks, mask=mask)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - concat attention\n    attn = Attention(\n        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n    )\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        return o_attn, p_attn\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "Unbabel/OpenKiwi",
    "commit": "658fb5427252d78c4d83048138ce64857149cabe",
    "filename": "kiwi/modules/common/attention.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Unbabel-OpenKiwi/kiwi/modules/common/attention.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "kiwi/modules/common/attention.py:139:17 Call error [29]: `Attention` is not a function.",
    "message": " `Attention` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 139,
    "warning_line": "    out, probs = attn(qs, qs, qs, mask=mask)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        return o_attn, p_attn\n\n\nif __name__ == '__main__':\n    from kiwi.utils.tensors import sequence_mask\n    from kiwi.modules.common.scorer import (\n        DotProductScorer,\n        GeneralScorer,\n        OperationScorer,\n        MLPScorer,\n    )\n\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, query_size)\n\n    # set of query vectors\n    qs = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    # values vectors (same shape as keys)\n    vs = torch.randn(batch_size, source_len, keys_size)\n\n    # values vectors with same size as query vectors\n    vq = torch.randn(batch_size, source_len, query_size)\n\n    # self attention on target (decoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # self attention on source (encoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # masked self attention on target (decoder)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs, mask=mask)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # masked self attention on source (encoder)\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks, mask=mask)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - multiplicative attention\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # masked encoder attend to decoder - multiplicative attention\n    # this is odd but we can do it anyway :-)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n    out, probs = attn(ks, qs, qs, mask=mask)\n    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n\n    # masked decoder attend to encoder - multiplicative attention\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks, mask=mask)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - concat attention\n    attn = Attention(\n        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n    )\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_len": 3940,
        "target_code": "        return o_attn, p_attn\n",
        "target_code_len": 30,
        "diff_format": "@@ -84,100 +84,1 @@\n         return o_attn, p_attn\n-\n-\n-if __name__ == '__main__':\n-    from kiwi.utils.tensors import sequence_mask\n-    from kiwi.modules.common.scorer import (\n-        DotProductScorer,\n-        GeneralScorer,\n-        OperationScorer,\n-        MLPScorer,\n-    )\n-\n-    torch.manual_seed(1)\n-    torch.cuda.manual_seed(1)\n-\n-    batch_size = 4\n-    source_len = 7\n-    target_len = 3\n-    query_size = 10\n-    keys_size = 20\n-    attn_size = 15\n-\n-    # query vectors\n-    q = torch.randn(batch_size, query_size)\n-\n-    # set of query vectors\n-    qs = torch.randn(batch_size, target_len, query_size)\n-\n-    # keys vectors (a key vector for each encoder word)\n-    ks = torch.randn(batch_size, source_len, keys_size)\n-\n-    # keys vectors with same size as query vectors\n-    kq = torch.randn(batch_size, source_len, query_size)\n-\n-    # values vectors (same shape as keys)\n-    vs = torch.randn(batch_size, source_len, keys_size)\n-\n-    # values vectors with same size as query vectors\n-    vq = torch.randn(batch_size, source_len, query_size)\n-\n-    # self attention on target (decoder)\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(qs, qs, qs)\n-    assert list(out.shape) == list(qs.shape)\n-    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n-\n-    # self attention on source (encoder)\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(ks, ks, ks)\n-    assert list(out.shape) == list(ks.shape)\n-    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n-\n-    # masked self attention on target (decoder)\n-    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(qs, qs, qs, mask=mask)\n-    assert list(out.shape) == list(qs.shape)\n-    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n-\n-    # masked self attention on source (encoder)\n-    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(ks, ks, ks, mask=mask)\n-    assert list(out.shape) == list(ks.shape)\n-    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder - multiplicative attention\n-    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # masked encoder attend to decoder - multiplicative attention\n-    # this is odd but we can do it anyway :-)\n-    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n-    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n-    out, probs = attn(ks, qs, qs, mask=mask)\n-    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n-    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n-\n-    # masked decoder attend to encoder - multiplicative attention\n-    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n-    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n-    out, probs = attn(qs, ks, ks, mask=mask)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder - concat attention\n-    attn = Attention(\n-        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n-    )\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n-    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_with_indent": "        return o_attn, p_attn\n\n\n<DED><DED>if __name__ == '__main__':\n    <IND>from kiwi.utils.tensors import sequence_mask\n    from kiwi.modules.common.scorer import (\n        DotProductScorer,\n        GeneralScorer,\n        OperationScorer,\n        MLPScorer,\n    )\n\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, query_size)\n\n    # set of query vectors\n    qs = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    # values vectors (same shape as keys)\n    vs = torch.randn(batch_size, source_len, keys_size)\n\n    # values vectors with same size as query vectors\n    vq = torch.randn(batch_size, source_len, query_size)\n\n    # self attention on target (decoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # self attention on source (encoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # masked self attention on target (decoder)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs, mask=mask)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # masked self attention on source (encoder)\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks, mask=mask)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - multiplicative attention\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # masked encoder attend to decoder - multiplicative attention\n    # this is odd but we can do it anyway :-)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n    out, probs = attn(ks, qs, qs, mask=mask)\n    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n\n    # masked decoder attend to encoder - multiplicative attention\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks, mask=mask)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - concat attention\n    attn = Attention(\n        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n    )\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        return o_attn, p_attn\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "Unbabel/OpenKiwi",
    "commit": "658fb5427252d78c4d83048138ce64857149cabe",
    "filename": "kiwi/modules/common/attention.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Unbabel-OpenKiwi/kiwi/modules/common/attention.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "kiwi/modules/common/attention.py:146:17 Call error [29]: `Attention` is not a function.",
    "message": " `Attention` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 146,
    "warning_line": "    out, probs = attn(ks, ks, ks, mask=mask)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        return o_attn, p_attn\n\n\nif __name__ == '__main__':\n    from kiwi.utils.tensors import sequence_mask\n    from kiwi.modules.common.scorer import (\n        DotProductScorer,\n        GeneralScorer,\n        OperationScorer,\n        MLPScorer,\n    )\n\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, query_size)\n\n    # set of query vectors\n    qs = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    # values vectors (same shape as keys)\n    vs = torch.randn(batch_size, source_len, keys_size)\n\n    # values vectors with same size as query vectors\n    vq = torch.randn(batch_size, source_len, query_size)\n\n    # self attention on target (decoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # self attention on source (encoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # masked self attention on target (decoder)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs, mask=mask)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # masked self attention on source (encoder)\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks, mask=mask)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - multiplicative attention\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # masked encoder attend to decoder - multiplicative attention\n    # this is odd but we can do it anyway :-)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n    out, probs = attn(ks, qs, qs, mask=mask)\n    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n\n    # masked decoder attend to encoder - multiplicative attention\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks, mask=mask)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - concat attention\n    attn = Attention(\n        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n    )\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_len": 3940,
        "target_code": "        return o_attn, p_attn\n",
        "target_code_len": 30,
        "diff_format": "@@ -84,100 +84,1 @@\n         return o_attn, p_attn\n-\n-\n-if __name__ == '__main__':\n-    from kiwi.utils.tensors import sequence_mask\n-    from kiwi.modules.common.scorer import (\n-        DotProductScorer,\n-        GeneralScorer,\n-        OperationScorer,\n-        MLPScorer,\n-    )\n-\n-    torch.manual_seed(1)\n-    torch.cuda.manual_seed(1)\n-\n-    batch_size = 4\n-    source_len = 7\n-    target_len = 3\n-    query_size = 10\n-    keys_size = 20\n-    attn_size = 15\n-\n-    # query vectors\n-    q = torch.randn(batch_size, query_size)\n-\n-    # set of query vectors\n-    qs = torch.randn(batch_size, target_len, query_size)\n-\n-    # keys vectors (a key vector for each encoder word)\n-    ks = torch.randn(batch_size, source_len, keys_size)\n-\n-    # keys vectors with same size as query vectors\n-    kq = torch.randn(batch_size, source_len, query_size)\n-\n-    # values vectors (same shape as keys)\n-    vs = torch.randn(batch_size, source_len, keys_size)\n-\n-    # values vectors with same size as query vectors\n-    vq = torch.randn(batch_size, source_len, query_size)\n-\n-    # self attention on target (decoder)\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(qs, qs, qs)\n-    assert list(out.shape) == list(qs.shape)\n-    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n-\n-    # self attention on source (encoder)\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(ks, ks, ks)\n-    assert list(out.shape) == list(ks.shape)\n-    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n-\n-    # masked self attention on target (decoder)\n-    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(qs, qs, qs, mask=mask)\n-    assert list(out.shape) == list(qs.shape)\n-    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n-\n-    # masked self attention on source (encoder)\n-    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(ks, ks, ks, mask=mask)\n-    assert list(out.shape) == list(ks.shape)\n-    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder - multiplicative attention\n-    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # masked encoder attend to decoder - multiplicative attention\n-    # this is odd but we can do it anyway :-)\n-    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n-    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n-    out, probs = attn(ks, qs, qs, mask=mask)\n-    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n-    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n-\n-    # masked decoder attend to encoder - multiplicative attention\n-    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n-    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n-    out, probs = attn(qs, ks, ks, mask=mask)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder - concat attention\n-    attn = Attention(\n-        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n-    )\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n-    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_with_indent": "        return o_attn, p_attn\n\n\n<DED><DED>if __name__ == '__main__':\n    <IND>from kiwi.utils.tensors import sequence_mask\n    from kiwi.modules.common.scorer import (\n        DotProductScorer,\n        GeneralScorer,\n        OperationScorer,\n        MLPScorer,\n    )\n\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, query_size)\n\n    # set of query vectors\n    qs = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    # values vectors (same shape as keys)\n    vs = torch.randn(batch_size, source_len, keys_size)\n\n    # values vectors with same size as query vectors\n    vq = torch.randn(batch_size, source_len, query_size)\n\n    # self attention on target (decoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # self attention on source (encoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # masked self attention on target (decoder)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs, mask=mask)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # masked self attention on source (encoder)\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks, mask=mask)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - multiplicative attention\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # masked encoder attend to decoder - multiplicative attention\n    # this is odd but we can do it anyway :-)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n    out, probs = attn(ks, qs, qs, mask=mask)\n    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n\n    # masked decoder attend to encoder - multiplicative attention\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks, mask=mask)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - concat attention\n    attn = Attention(\n        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n    )\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        return o_attn, p_attn\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "Unbabel/OpenKiwi",
    "commit": "658fb5427252d78c4d83048138ce64857149cabe",
    "filename": "kiwi/modules/common/attention.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Unbabel-OpenKiwi/kiwi/modules/common/attention.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "kiwi/modules/common/attention.py:152:17 Call error [29]: `Attention` is not a function.",
    "message": " `Attention` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 152,
    "warning_line": "    out, probs = attn(qs, ks, ks)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        return o_attn, p_attn\n\n\nif __name__ == '__main__':\n    from kiwi.utils.tensors import sequence_mask\n    from kiwi.modules.common.scorer import (\n        DotProductScorer,\n        GeneralScorer,\n        OperationScorer,\n        MLPScorer,\n    )\n\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, query_size)\n\n    # set of query vectors\n    qs = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    # values vectors (same shape as keys)\n    vs = torch.randn(batch_size, source_len, keys_size)\n\n    # values vectors with same size as query vectors\n    vq = torch.randn(batch_size, source_len, query_size)\n\n    # self attention on target (decoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # self attention on source (encoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # masked self attention on target (decoder)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs, mask=mask)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # masked self attention on source (encoder)\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks, mask=mask)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - multiplicative attention\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # masked encoder attend to decoder - multiplicative attention\n    # this is odd but we can do it anyway :-)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n    out, probs = attn(ks, qs, qs, mask=mask)\n    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n\n    # masked decoder attend to encoder - multiplicative attention\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks, mask=mask)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - concat attention\n    attn = Attention(\n        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n    )\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_len": 3940,
        "target_code": "        return o_attn, p_attn\n",
        "target_code_len": 30,
        "diff_format": "@@ -84,100 +84,1 @@\n         return o_attn, p_attn\n-\n-\n-if __name__ == '__main__':\n-    from kiwi.utils.tensors import sequence_mask\n-    from kiwi.modules.common.scorer import (\n-        DotProductScorer,\n-        GeneralScorer,\n-        OperationScorer,\n-        MLPScorer,\n-    )\n-\n-    torch.manual_seed(1)\n-    torch.cuda.manual_seed(1)\n-\n-    batch_size = 4\n-    source_len = 7\n-    target_len = 3\n-    query_size = 10\n-    keys_size = 20\n-    attn_size = 15\n-\n-    # query vectors\n-    q = torch.randn(batch_size, query_size)\n-\n-    # set of query vectors\n-    qs = torch.randn(batch_size, target_len, query_size)\n-\n-    # keys vectors (a key vector for each encoder word)\n-    ks = torch.randn(batch_size, source_len, keys_size)\n-\n-    # keys vectors with same size as query vectors\n-    kq = torch.randn(batch_size, source_len, query_size)\n-\n-    # values vectors (same shape as keys)\n-    vs = torch.randn(batch_size, source_len, keys_size)\n-\n-    # values vectors with same size as query vectors\n-    vq = torch.randn(batch_size, source_len, query_size)\n-\n-    # self attention on target (decoder)\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(qs, qs, qs)\n-    assert list(out.shape) == list(qs.shape)\n-    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n-\n-    # self attention on source (encoder)\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(ks, ks, ks)\n-    assert list(out.shape) == list(ks.shape)\n-    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n-\n-    # masked self attention on target (decoder)\n-    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(qs, qs, qs, mask=mask)\n-    assert list(out.shape) == list(qs.shape)\n-    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n-\n-    # masked self attention on source (encoder)\n-    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(ks, ks, ks, mask=mask)\n-    assert list(out.shape) == list(ks.shape)\n-    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder - multiplicative attention\n-    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # masked encoder attend to decoder - multiplicative attention\n-    # this is odd but we can do it anyway :-)\n-    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n-    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n-    out, probs = attn(ks, qs, qs, mask=mask)\n-    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n-    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n-\n-    # masked decoder attend to encoder - multiplicative attention\n-    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n-    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n-    out, probs = attn(qs, ks, ks, mask=mask)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder - concat attention\n-    attn = Attention(\n-        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n-    )\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n-    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_with_indent": "        return o_attn, p_attn\n\n\n<DED><DED>if __name__ == '__main__':\n    <IND>from kiwi.utils.tensors import sequence_mask\n    from kiwi.modules.common.scorer import (\n        DotProductScorer,\n        GeneralScorer,\n        OperationScorer,\n        MLPScorer,\n    )\n\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, query_size)\n\n    # set of query vectors\n    qs = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    # values vectors (same shape as keys)\n    vs = torch.randn(batch_size, source_len, keys_size)\n\n    # values vectors with same size as query vectors\n    vq = torch.randn(batch_size, source_len, query_size)\n\n    # self attention on target (decoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # self attention on source (encoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # masked self attention on target (decoder)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs, mask=mask)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # masked self attention on source (encoder)\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks, mask=mask)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - multiplicative attention\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # masked encoder attend to decoder - multiplicative attention\n    # this is odd but we can do it anyway :-)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n    out, probs = attn(ks, qs, qs, mask=mask)\n    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n\n    # masked decoder attend to encoder - multiplicative attention\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks, mask=mask)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - concat attention\n    attn = Attention(\n        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n    )\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        return o_attn, p_attn\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "Unbabel/OpenKiwi",
    "commit": "658fb5427252d78c4d83048138ce64857149cabe",
    "filename": "kiwi/modules/common/attention.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Unbabel-OpenKiwi/kiwi/modules/common/attention.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "kiwi/modules/common/attention.py:160:17 Call error [29]: `Attention` is not a function.",
    "message": " `Attention` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 160,
    "warning_line": "    out, probs = attn(ks, qs, qs, mask=mask)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        return o_attn, p_attn\n\n\nif __name__ == '__main__':\n    from kiwi.utils.tensors import sequence_mask\n    from kiwi.modules.common.scorer import (\n        DotProductScorer,\n        GeneralScorer,\n        OperationScorer,\n        MLPScorer,\n    )\n\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, query_size)\n\n    # set of query vectors\n    qs = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    # values vectors (same shape as keys)\n    vs = torch.randn(batch_size, source_len, keys_size)\n\n    # values vectors with same size as query vectors\n    vq = torch.randn(batch_size, source_len, query_size)\n\n    # self attention on target (decoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # self attention on source (encoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # masked self attention on target (decoder)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs, mask=mask)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # masked self attention on source (encoder)\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks, mask=mask)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - multiplicative attention\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # masked encoder attend to decoder - multiplicative attention\n    # this is odd but we can do it anyway :-)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n    out, probs = attn(ks, qs, qs, mask=mask)\n    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n\n    # masked decoder attend to encoder - multiplicative attention\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks, mask=mask)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - concat attention\n    attn = Attention(\n        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n    )\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_len": 3940,
        "target_code": "        return o_attn, p_attn\n",
        "target_code_len": 30,
        "diff_format": "@@ -84,100 +84,1 @@\n         return o_attn, p_attn\n-\n-\n-if __name__ == '__main__':\n-    from kiwi.utils.tensors import sequence_mask\n-    from kiwi.modules.common.scorer import (\n-        DotProductScorer,\n-        GeneralScorer,\n-        OperationScorer,\n-        MLPScorer,\n-    )\n-\n-    torch.manual_seed(1)\n-    torch.cuda.manual_seed(1)\n-\n-    batch_size = 4\n-    source_len = 7\n-    target_len = 3\n-    query_size = 10\n-    keys_size = 20\n-    attn_size = 15\n-\n-    # query vectors\n-    q = torch.randn(batch_size, query_size)\n-\n-    # set of query vectors\n-    qs = torch.randn(batch_size, target_len, query_size)\n-\n-    # keys vectors (a key vector for each encoder word)\n-    ks = torch.randn(batch_size, source_len, keys_size)\n-\n-    # keys vectors with same size as query vectors\n-    kq = torch.randn(batch_size, source_len, query_size)\n-\n-    # values vectors (same shape as keys)\n-    vs = torch.randn(batch_size, source_len, keys_size)\n-\n-    # values vectors with same size as query vectors\n-    vq = torch.randn(batch_size, source_len, query_size)\n-\n-    # self attention on target (decoder)\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(qs, qs, qs)\n-    assert list(out.shape) == list(qs.shape)\n-    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n-\n-    # self attention on source (encoder)\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(ks, ks, ks)\n-    assert list(out.shape) == list(ks.shape)\n-    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n-\n-    # masked self attention on target (decoder)\n-    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(qs, qs, qs, mask=mask)\n-    assert list(out.shape) == list(qs.shape)\n-    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n-\n-    # masked self attention on source (encoder)\n-    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(ks, ks, ks, mask=mask)\n-    assert list(out.shape) == list(ks.shape)\n-    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder - multiplicative attention\n-    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # masked encoder attend to decoder - multiplicative attention\n-    # this is odd but we can do it anyway :-)\n-    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n-    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n-    out, probs = attn(ks, qs, qs, mask=mask)\n-    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n-    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n-\n-    # masked decoder attend to encoder - multiplicative attention\n-    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n-    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n-    out, probs = attn(qs, ks, ks, mask=mask)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder - concat attention\n-    attn = Attention(\n-        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n-    )\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n-    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_with_indent": "        return o_attn, p_attn\n\n\n<DED><DED>if __name__ == '__main__':\n    <IND>from kiwi.utils.tensors import sequence_mask\n    from kiwi.modules.common.scorer import (\n        DotProductScorer,\n        GeneralScorer,\n        OperationScorer,\n        MLPScorer,\n    )\n\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, query_size)\n\n    # set of query vectors\n    qs = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    # values vectors (same shape as keys)\n    vs = torch.randn(batch_size, source_len, keys_size)\n\n    # values vectors with same size as query vectors\n    vq = torch.randn(batch_size, source_len, query_size)\n\n    # self attention on target (decoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # self attention on source (encoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # masked self attention on target (decoder)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs, mask=mask)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # masked self attention on source (encoder)\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks, mask=mask)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - multiplicative attention\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # masked encoder attend to decoder - multiplicative attention\n    # this is odd but we can do it anyway :-)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n    out, probs = attn(ks, qs, qs, mask=mask)\n    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n\n    # masked decoder attend to encoder - multiplicative attention\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks, mask=mask)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - concat attention\n    attn = Attention(\n        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n    )\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        return o_attn, p_attn\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "Unbabel/OpenKiwi",
    "commit": "658fb5427252d78c4d83048138ce64857149cabe",
    "filename": "kiwi/modules/common/attention.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Unbabel-OpenKiwi/kiwi/modules/common/attention.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "kiwi/modules/common/attention.py:167:17 Call error [29]: `Attention` is not a function.",
    "message": " `Attention` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 167,
    "warning_line": "    out, probs = attn(qs, ks, ks, mask=mask)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        return o_attn, p_attn\n\n\nif __name__ == '__main__':\n    from kiwi.utils.tensors import sequence_mask\n    from kiwi.modules.common.scorer import (\n        DotProductScorer,\n        GeneralScorer,\n        OperationScorer,\n        MLPScorer,\n    )\n\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, query_size)\n\n    # set of query vectors\n    qs = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    # values vectors (same shape as keys)\n    vs = torch.randn(batch_size, source_len, keys_size)\n\n    # values vectors with same size as query vectors\n    vq = torch.randn(batch_size, source_len, query_size)\n\n    # self attention on target (decoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # self attention on source (encoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # masked self attention on target (decoder)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs, mask=mask)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # masked self attention on source (encoder)\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks, mask=mask)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - multiplicative attention\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # masked encoder attend to decoder - multiplicative attention\n    # this is odd but we can do it anyway :-)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n    out, probs = attn(ks, qs, qs, mask=mask)\n    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n\n    # masked decoder attend to encoder - multiplicative attention\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks, mask=mask)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - concat attention\n    attn = Attention(\n        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n    )\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_len": 3940,
        "target_code": "        return o_attn, p_attn\n",
        "target_code_len": 30,
        "diff_format": "@@ -84,100 +84,1 @@\n         return o_attn, p_attn\n-\n-\n-if __name__ == '__main__':\n-    from kiwi.utils.tensors import sequence_mask\n-    from kiwi.modules.common.scorer import (\n-        DotProductScorer,\n-        GeneralScorer,\n-        OperationScorer,\n-        MLPScorer,\n-    )\n-\n-    torch.manual_seed(1)\n-    torch.cuda.manual_seed(1)\n-\n-    batch_size = 4\n-    source_len = 7\n-    target_len = 3\n-    query_size = 10\n-    keys_size = 20\n-    attn_size = 15\n-\n-    # query vectors\n-    q = torch.randn(batch_size, query_size)\n-\n-    # set of query vectors\n-    qs = torch.randn(batch_size, target_len, query_size)\n-\n-    # keys vectors (a key vector for each encoder word)\n-    ks = torch.randn(batch_size, source_len, keys_size)\n-\n-    # keys vectors with same size as query vectors\n-    kq = torch.randn(batch_size, source_len, query_size)\n-\n-    # values vectors (same shape as keys)\n-    vs = torch.randn(batch_size, source_len, keys_size)\n-\n-    # values vectors with same size as query vectors\n-    vq = torch.randn(batch_size, source_len, query_size)\n-\n-    # self attention on target (decoder)\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(qs, qs, qs)\n-    assert list(out.shape) == list(qs.shape)\n-    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n-\n-    # self attention on source (encoder)\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(ks, ks, ks)\n-    assert list(out.shape) == list(ks.shape)\n-    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n-\n-    # masked self attention on target (decoder)\n-    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(qs, qs, qs, mask=mask)\n-    assert list(out.shape) == list(qs.shape)\n-    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n-\n-    # masked self attention on source (encoder)\n-    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(ks, ks, ks, mask=mask)\n-    assert list(out.shape) == list(ks.shape)\n-    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder - multiplicative attention\n-    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # masked encoder attend to decoder - multiplicative attention\n-    # this is odd but we can do it anyway :-)\n-    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n-    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n-    out, probs = attn(ks, qs, qs, mask=mask)\n-    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n-    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n-\n-    # masked decoder attend to encoder - multiplicative attention\n-    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n-    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n-    out, probs = attn(qs, ks, ks, mask=mask)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder - concat attention\n-    attn = Attention(\n-        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n-    )\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n-    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_with_indent": "        return o_attn, p_attn\n\n\n<DED><DED>if __name__ == '__main__':\n    <IND>from kiwi.utils.tensors import sequence_mask\n    from kiwi.modules.common.scorer import (\n        DotProductScorer,\n        GeneralScorer,\n        OperationScorer,\n        MLPScorer,\n    )\n\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, query_size)\n\n    # set of query vectors\n    qs = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    # values vectors (same shape as keys)\n    vs = torch.randn(batch_size, source_len, keys_size)\n\n    # values vectors with same size as query vectors\n    vq = torch.randn(batch_size, source_len, query_size)\n\n    # self attention on target (decoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # self attention on source (encoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # masked self attention on target (decoder)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs, mask=mask)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # masked self attention on source (encoder)\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks, mask=mask)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - multiplicative attention\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # masked encoder attend to decoder - multiplicative attention\n    # this is odd but we can do it anyway :-)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n    out, probs = attn(ks, qs, qs, mask=mask)\n    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n\n    # masked decoder attend to encoder - multiplicative attention\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks, mask=mask)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - concat attention\n    attn = Attention(\n        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n    )\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        return o_attn, p_attn\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "Unbabel/OpenKiwi",
    "commit": "658fb5427252d78c4d83048138ce64857149cabe",
    "filename": "kiwi/modules/common/attention.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Unbabel-OpenKiwi/kiwi/modules/common/attention.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "kiwi/modules/common/attention.py:175:17 Call error [29]: `Attention` is not a function.",
    "message": " `Attention` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 175,
    "warning_line": "    out, probs = attn(qs, ks, ks)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        return o_attn, p_attn\n\n\nif __name__ == '__main__':\n    from kiwi.utils.tensors import sequence_mask\n    from kiwi.modules.common.scorer import (\n        DotProductScorer,\n        GeneralScorer,\n        OperationScorer,\n        MLPScorer,\n    )\n\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, query_size)\n\n    # set of query vectors\n    qs = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    # values vectors (same shape as keys)\n    vs = torch.randn(batch_size, source_len, keys_size)\n\n    # values vectors with same size as query vectors\n    vq = torch.randn(batch_size, source_len, query_size)\n\n    # self attention on target (decoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # self attention on source (encoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # masked self attention on target (decoder)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs, mask=mask)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # masked self attention on source (encoder)\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks, mask=mask)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - multiplicative attention\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # masked encoder attend to decoder - multiplicative attention\n    # this is odd but we can do it anyway :-)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n    out, probs = attn(ks, qs, qs, mask=mask)\n    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n\n    # masked decoder attend to encoder - multiplicative attention\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks, mask=mask)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - concat attention\n    attn = Attention(\n        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n    )\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_len": 3940,
        "target_code": "        return o_attn, p_attn\n",
        "target_code_len": 30,
        "diff_format": "@@ -84,100 +84,1 @@\n         return o_attn, p_attn\n-\n-\n-if __name__ == '__main__':\n-    from kiwi.utils.tensors import sequence_mask\n-    from kiwi.modules.common.scorer import (\n-        DotProductScorer,\n-        GeneralScorer,\n-        OperationScorer,\n-        MLPScorer,\n-    )\n-\n-    torch.manual_seed(1)\n-    torch.cuda.manual_seed(1)\n-\n-    batch_size = 4\n-    source_len = 7\n-    target_len = 3\n-    query_size = 10\n-    keys_size = 20\n-    attn_size = 15\n-\n-    # query vectors\n-    q = torch.randn(batch_size, query_size)\n-\n-    # set of query vectors\n-    qs = torch.randn(batch_size, target_len, query_size)\n-\n-    # keys vectors (a key vector for each encoder word)\n-    ks = torch.randn(batch_size, source_len, keys_size)\n-\n-    # keys vectors with same size as query vectors\n-    kq = torch.randn(batch_size, source_len, query_size)\n-\n-    # values vectors (same shape as keys)\n-    vs = torch.randn(batch_size, source_len, keys_size)\n-\n-    # values vectors with same size as query vectors\n-    vq = torch.randn(batch_size, source_len, query_size)\n-\n-    # self attention on target (decoder)\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(qs, qs, qs)\n-    assert list(out.shape) == list(qs.shape)\n-    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n-\n-    # self attention on source (encoder)\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(ks, ks, ks)\n-    assert list(out.shape) == list(ks.shape)\n-    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n-\n-    # masked self attention on target (decoder)\n-    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(qs, qs, qs, mask=mask)\n-    assert list(out.shape) == list(qs.shape)\n-    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n-\n-    # masked self attention on source (encoder)\n-    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(ks, ks, ks, mask=mask)\n-    assert list(out.shape) == list(ks.shape)\n-    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder - multiplicative attention\n-    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # masked encoder attend to decoder - multiplicative attention\n-    # this is odd but we can do it anyway :-)\n-    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n-    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n-    out, probs = attn(ks, qs, qs, mask=mask)\n-    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n-    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n-\n-    # masked decoder attend to encoder - multiplicative attention\n-    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n-    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n-    out, probs = attn(qs, ks, ks, mask=mask)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder - concat attention\n-    attn = Attention(\n-        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n-    )\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n-    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_with_indent": "        return o_attn, p_attn\n\n\n<DED><DED>if __name__ == '__main__':\n    <IND>from kiwi.utils.tensors import sequence_mask\n    from kiwi.modules.common.scorer import (\n        DotProductScorer,\n        GeneralScorer,\n        OperationScorer,\n        MLPScorer,\n    )\n\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, query_size)\n\n    # set of query vectors\n    qs = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    # values vectors (same shape as keys)\n    vs = torch.randn(batch_size, source_len, keys_size)\n\n    # values vectors with same size as query vectors\n    vq = torch.randn(batch_size, source_len, query_size)\n\n    # self attention on target (decoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # self attention on source (encoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # masked self attention on target (decoder)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs, mask=mask)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # masked self attention on source (encoder)\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks, mask=mask)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - multiplicative attention\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # masked encoder attend to decoder - multiplicative attention\n    # this is odd but we can do it anyway :-)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n    out, probs = attn(ks, qs, qs, mask=mask)\n    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n\n    # masked decoder attend to encoder - multiplicative attention\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks, mask=mask)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - concat attention\n    attn = Attention(\n        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n    )\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        return o_attn, p_attn\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "Unbabel/OpenKiwi",
    "commit": "658fb5427252d78c4d83048138ce64857149cabe",
    "filename": "kiwi/modules/common/attention.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Unbabel-OpenKiwi/kiwi/modules/common/attention.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "kiwi/modules/common/attention.py:181:17 Call error [29]: `Attention` is not a function.",
    "message": " `Attention` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 181,
    "warning_line": "    out, probs = attn(qs, ks, ks)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        return o_attn, p_attn\n\n\nif __name__ == '__main__':\n    from kiwi.utils.tensors import sequence_mask\n    from kiwi.modules.common.scorer import (\n        DotProductScorer,\n        GeneralScorer,\n        OperationScorer,\n        MLPScorer,\n    )\n\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, query_size)\n\n    # set of query vectors\n    qs = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    # values vectors (same shape as keys)\n    vs = torch.randn(batch_size, source_len, keys_size)\n\n    # values vectors with same size as query vectors\n    vq = torch.randn(batch_size, source_len, query_size)\n\n    # self attention on target (decoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # self attention on source (encoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # masked self attention on target (decoder)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs, mask=mask)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # masked self attention on source (encoder)\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks, mask=mask)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - multiplicative attention\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # masked encoder attend to decoder - multiplicative attention\n    # this is odd but we can do it anyway :-)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n    out, probs = attn(ks, qs, qs, mask=mask)\n    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n\n    # masked decoder attend to encoder - multiplicative attention\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks, mask=mask)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - concat attention\n    attn = Attention(\n        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n    )\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_len": 3940,
        "target_code": "        return o_attn, p_attn\n",
        "target_code_len": 30,
        "diff_format": "@@ -84,100 +84,1 @@\n         return o_attn, p_attn\n-\n-\n-if __name__ == '__main__':\n-    from kiwi.utils.tensors import sequence_mask\n-    from kiwi.modules.common.scorer import (\n-        DotProductScorer,\n-        GeneralScorer,\n-        OperationScorer,\n-        MLPScorer,\n-    )\n-\n-    torch.manual_seed(1)\n-    torch.cuda.manual_seed(1)\n-\n-    batch_size = 4\n-    source_len = 7\n-    target_len = 3\n-    query_size = 10\n-    keys_size = 20\n-    attn_size = 15\n-\n-    # query vectors\n-    q = torch.randn(batch_size, query_size)\n-\n-    # set of query vectors\n-    qs = torch.randn(batch_size, target_len, query_size)\n-\n-    # keys vectors (a key vector for each encoder word)\n-    ks = torch.randn(batch_size, source_len, keys_size)\n-\n-    # keys vectors with same size as query vectors\n-    kq = torch.randn(batch_size, source_len, query_size)\n-\n-    # values vectors (same shape as keys)\n-    vs = torch.randn(batch_size, source_len, keys_size)\n-\n-    # values vectors with same size as query vectors\n-    vq = torch.randn(batch_size, source_len, query_size)\n-\n-    # self attention on target (decoder)\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(qs, qs, qs)\n-    assert list(out.shape) == list(qs.shape)\n-    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n-\n-    # self attention on source (encoder)\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(ks, ks, ks)\n-    assert list(out.shape) == list(ks.shape)\n-    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n-\n-    # masked self attention on target (decoder)\n-    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(qs, qs, qs, mask=mask)\n-    assert list(out.shape) == list(qs.shape)\n-    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n-\n-    # masked self attention on source (encoder)\n-    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n-    attn = Attention(DotProductScorer(), dropout=0.1)\n-    out, probs = attn(ks, ks, ks, mask=mask)\n-    assert list(out.shape) == list(ks.shape)\n-    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder - multiplicative attention\n-    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # masked encoder attend to decoder - multiplicative attention\n-    # this is odd but we can do it anyway :-)\n-    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n-    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n-    out, probs = attn(ks, qs, qs, mask=mask)\n-    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n-    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n-\n-    # masked decoder attend to encoder - multiplicative attention\n-    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n-    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n-    out, probs = attn(qs, ks, ks, mask=mask)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder - concat attention\n-    attn = Attention(\n-        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n-    )\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n-\n-    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n-    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n-    out, probs = attn(qs, ks, ks)\n-    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n-    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_with_indent": "        return o_attn, p_attn\n\n\n<DED><DED>if __name__ == '__main__':\n    <IND>from kiwi.utils.tensors import sequence_mask\n    from kiwi.modules.common.scorer import (\n        DotProductScorer,\n        GeneralScorer,\n        OperationScorer,\n        MLPScorer,\n    )\n\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, query_size)\n\n    # set of query vectors\n    qs = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    # values vectors (same shape as keys)\n    vs = torch.randn(batch_size, source_len, keys_size)\n\n    # values vectors with same size as query vectors\n    vq = torch.randn(batch_size, source_len, query_size)\n\n    # self attention on target (decoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # self attention on source (encoder)\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # masked self attention on target (decoder)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(qs, qs, qs, mask=mask)\n    assert list(out.shape) == list(qs.shape)\n    assert list(probs.shape) == [batch_size, qs.shape[1], qs.shape[1]]\n\n    # masked self attention on source (encoder)\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(DotProductScorer(), dropout=0.1)\n    out, probs = attn(ks, ks, ks, mask=mask)\n    assert list(out.shape) == list(ks.shape)\n    assert list(probs.shape) == [batch_size, ks.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - multiplicative attention\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # masked encoder attend to decoder - multiplicative attention\n    # this is odd but we can do it anyway :-)\n    mask = sequence_mask(torch.LongTensor([2, 1, 2, 3]))\n    attn = Attention(GeneralScorer(keys_size, query_size), dropout=0.1)\n    out, probs = attn(ks, qs, qs, mask=mask)\n    assert list(out.shape) == [batch_size, ks.shape[1], qs.shape[-1]]\n    assert list(probs.shape) == [batch_size, ks.shape[1], qs.shape[1]]\n\n    # masked decoder attend to encoder - multiplicative attention\n    mask = sequence_mask(torch.LongTensor([5, 3, 7, 4]))\n    attn = Attention(GeneralScorer(query_size, keys_size), dropout=0.1)\n    out, probs = attn(qs, ks, ks, mask=mask)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder - concat attention\n    attn = Attention(\n        OperationScorer(query_size, keys_size, attn_size, op='concat'), dropout=0.1\n    )\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n\n    # decoder attend to encoder using a mlp with two hidden layers of 5 neurons\n    attn = Attention(MLPScorer(query_size, keys_size, layer_sizes=[5, 5]), dropout=0.1)\n    out, probs = attn(qs, ks, ks)\n    assert list(out.shape) == [batch_size, qs.shape[1], ks.shape[-1]]\n    assert list(probs.shape) == [batch_size, qs.shape[1], ks.shape[1]]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        return o_attn, p_attn\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "Unbabel/OpenKiwi",
    "commit": "658fb5427252d78c4d83048138ce64857149cabe",
    "filename": "kiwi/modules/common/scorer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Unbabel-OpenKiwi/kiwi/modules/common/scorer.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "kiwi/modules/common/scorer.py:198:10 Call error [29]: `DotProductScorer` is not a function.",
    "message": " `DotProductScorer` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 198,
    "warning_line": "    out = DotProductScorer()(q, kq)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        return x.squeeze(-1)  # remove last dimension\n\n\nif __name__ == '__main__':\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    out = DotProductScorer()(q, kq)\n    assert list(out.shape) == [batch_size, q.shape[1], kq.shape[1]]\n\n    out = GeneralScorer(query_size, keys_size)(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)\n    assert list(out.shape) == [batch_size, q.shape[1], q.shape[1]]\n\n    out = MLPScorer(\n        query_size, keys_size, layer_sizes=[10, 5, 5], activation=nn.Sigmoid\n    )(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n",
        "source_code_len": 1690,
        "target_code": "        return x.squeeze(-1)  # remove last dimension\n",
        "target_code_len": 54,
        "diff_format": "@@ -175,48 +92,1 @@\n         return x.squeeze(-1)  # remove last dimension\n-\n-\n-if __name__ == '__main__':\n-    torch.manual_seed(1)\n-    torch.cuda.manual_seed(1)\n-\n-    batch_size = 4\n-    source_len = 7\n-    target_len = 3\n-    query_size = 10\n-    keys_size = 20\n-    attn_size = 15\n-\n-    # query vectors\n-    q = torch.randn(batch_size, target_len, query_size)\n-\n-    # keys vectors (a key vector for each encoder word)\n-    ks = torch.randn(batch_size, source_len, keys_size)\n-\n-    # keys vectors with same size as query vectors\n-    kq = torch.randn(batch_size, source_len, query_size)\n-\n-    out = DotProductScorer()(q, kq)\n-    assert list(out.shape) == [batch_size, q.shape[1], kq.shape[1]]\n-\n-    out = GeneralScorer(query_size, keys_size)(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)\n-    assert list(out.shape) == [batch_size, q.shape[1], q.shape[1]]\n-\n-    out = MLPScorer(\n-        query_size, keys_size, layer_sizes=[10, 5, 5], activation=nn.Sigmoid\n-    )(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n",
        "source_code_with_indent": "        <DED>return x.squeeze(-1)  # remove last dimension\n\n\n<DED><DED>if __name__ == '__main__':\n    <IND>torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    out = DotProductScorer()(q, kq)\n    assert list(out.shape) == [batch_size, q.shape[1], kq.shape[1]]\n\n    out = GeneralScorer(query_size, keys_size)(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)\n    assert list(out.shape) == [batch_size, q.shape[1], q.shape[1]]\n\n    out = MLPScorer(\n        query_size, keys_size, layer_sizes=[10, 5, 5], activation=nn.Sigmoid\n    )(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <DED>return x.squeeze(-1)  # remove last dimension\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "Unbabel/OpenKiwi",
    "commit": "658fb5427252d78c4d83048138ce64857149cabe",
    "filename": "kiwi/modules/common/scorer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Unbabel-OpenKiwi/kiwi/modules/common/scorer.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "kiwi/modules/common/scorer.py:201:10 Call error [29]: `GeneralScorer` is not a function.",
    "message": " `GeneralScorer` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 201,
    "warning_line": "    out = GeneralScorer(query_size, keys_size)(q, ks)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        return x.squeeze(-1)  # remove last dimension\n\n\nif __name__ == '__main__':\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    out = DotProductScorer()(q, kq)\n    assert list(out.shape) == [batch_size, q.shape[1], kq.shape[1]]\n\n    out = GeneralScorer(query_size, keys_size)(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)\n    assert list(out.shape) == [batch_size, q.shape[1], q.shape[1]]\n\n    out = MLPScorer(\n        query_size, keys_size, layer_sizes=[10, 5, 5], activation=nn.Sigmoid\n    )(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n",
        "source_code_len": 1690,
        "target_code": "        return x.squeeze(-1)  # remove last dimension\n",
        "target_code_len": 54,
        "diff_format": "@@ -175,48 +92,1 @@\n         return x.squeeze(-1)  # remove last dimension\n-\n-\n-if __name__ == '__main__':\n-    torch.manual_seed(1)\n-    torch.cuda.manual_seed(1)\n-\n-    batch_size = 4\n-    source_len = 7\n-    target_len = 3\n-    query_size = 10\n-    keys_size = 20\n-    attn_size = 15\n-\n-    # query vectors\n-    q = torch.randn(batch_size, target_len, query_size)\n-\n-    # keys vectors (a key vector for each encoder word)\n-    ks = torch.randn(batch_size, source_len, keys_size)\n-\n-    # keys vectors with same size as query vectors\n-    kq = torch.randn(batch_size, source_len, query_size)\n-\n-    out = DotProductScorer()(q, kq)\n-    assert list(out.shape) == [batch_size, q.shape[1], kq.shape[1]]\n-\n-    out = GeneralScorer(query_size, keys_size)(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)\n-    assert list(out.shape) == [batch_size, q.shape[1], q.shape[1]]\n-\n-    out = MLPScorer(\n-        query_size, keys_size, layer_sizes=[10, 5, 5], activation=nn.Sigmoid\n-    )(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n",
        "source_code_with_indent": "        <DED>return x.squeeze(-1)  # remove last dimension\n\n\n<DED><DED>if __name__ == '__main__':\n    <IND>torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    out = DotProductScorer()(q, kq)\n    assert list(out.shape) == [batch_size, q.shape[1], kq.shape[1]]\n\n    out = GeneralScorer(query_size, keys_size)(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)\n    assert list(out.shape) == [batch_size, q.shape[1], q.shape[1]]\n\n    out = MLPScorer(\n        query_size, keys_size, layer_sizes=[10, 5, 5], activation=nn.Sigmoid\n    )(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <DED>return x.squeeze(-1)  # remove last dimension\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "Unbabel/OpenKiwi",
    "commit": "658fb5427252d78c4d83048138ce64857149cabe",
    "filename": "kiwi/modules/common/scorer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Unbabel-OpenKiwi/kiwi/modules/common/scorer.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "kiwi/modules/common/scorer.py:204:10 Call error [29]: `OperationScorer` is not a function.",
    "message": " `OperationScorer` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 204,
    "warning_line": "    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        return x.squeeze(-1)  # remove last dimension\n\n\nif __name__ == '__main__':\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    out = DotProductScorer()(q, kq)\n    assert list(out.shape) == [batch_size, q.shape[1], kq.shape[1]]\n\n    out = GeneralScorer(query_size, keys_size)(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)\n    assert list(out.shape) == [batch_size, q.shape[1], q.shape[1]]\n\n    out = MLPScorer(\n        query_size, keys_size, layer_sizes=[10, 5, 5], activation=nn.Sigmoid\n    )(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n",
        "source_code_len": 1690,
        "target_code": "        return x.squeeze(-1)  # remove last dimension\n",
        "target_code_len": 54,
        "diff_format": "@@ -175,48 +92,1 @@\n         return x.squeeze(-1)  # remove last dimension\n-\n-\n-if __name__ == '__main__':\n-    torch.manual_seed(1)\n-    torch.cuda.manual_seed(1)\n-\n-    batch_size = 4\n-    source_len = 7\n-    target_len = 3\n-    query_size = 10\n-    keys_size = 20\n-    attn_size = 15\n-\n-    # query vectors\n-    q = torch.randn(batch_size, target_len, query_size)\n-\n-    # keys vectors (a key vector for each encoder word)\n-    ks = torch.randn(batch_size, source_len, keys_size)\n-\n-    # keys vectors with same size as query vectors\n-    kq = torch.randn(batch_size, source_len, query_size)\n-\n-    out = DotProductScorer()(q, kq)\n-    assert list(out.shape) == [batch_size, q.shape[1], kq.shape[1]]\n-\n-    out = GeneralScorer(query_size, keys_size)(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)\n-    assert list(out.shape) == [batch_size, q.shape[1], q.shape[1]]\n-\n-    out = MLPScorer(\n-        query_size, keys_size, layer_sizes=[10, 5, 5], activation=nn.Sigmoid\n-    )(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n",
        "source_code_with_indent": "        <DED>return x.squeeze(-1)  # remove last dimension\n\n\n<DED><DED>if __name__ == '__main__':\n    <IND>torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    out = DotProductScorer()(q, kq)\n    assert list(out.shape) == [batch_size, q.shape[1], kq.shape[1]]\n\n    out = GeneralScorer(query_size, keys_size)(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)\n    assert list(out.shape) == [batch_size, q.shape[1], q.shape[1]]\n\n    out = MLPScorer(\n        query_size, keys_size, layer_sizes=[10, 5, 5], activation=nn.Sigmoid\n    )(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <DED>return x.squeeze(-1)  # remove last dimension\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "Unbabel/OpenKiwi",
    "commit": "658fb5427252d78c4d83048138ce64857149cabe",
    "filename": "kiwi/modules/common/scorer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Unbabel-OpenKiwi/kiwi/modules/common/scorer.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "kiwi/modules/common/scorer.py:207:10 Call error [29]: `OperationScorer` is not a function.",
    "message": " `OperationScorer` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 207,
    "warning_line": "    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        return x.squeeze(-1)  # remove last dimension\n\n\nif __name__ == '__main__':\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    out = DotProductScorer()(q, kq)\n    assert list(out.shape) == [batch_size, q.shape[1], kq.shape[1]]\n\n    out = GeneralScorer(query_size, keys_size)(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)\n    assert list(out.shape) == [batch_size, q.shape[1], q.shape[1]]\n\n    out = MLPScorer(\n        query_size, keys_size, layer_sizes=[10, 5, 5], activation=nn.Sigmoid\n    )(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n",
        "source_code_len": 1690,
        "target_code": "        return x.squeeze(-1)  # remove last dimension\n",
        "target_code_len": 54,
        "diff_format": "@@ -175,48 +92,1 @@\n         return x.squeeze(-1)  # remove last dimension\n-\n-\n-if __name__ == '__main__':\n-    torch.manual_seed(1)\n-    torch.cuda.manual_seed(1)\n-\n-    batch_size = 4\n-    source_len = 7\n-    target_len = 3\n-    query_size = 10\n-    keys_size = 20\n-    attn_size = 15\n-\n-    # query vectors\n-    q = torch.randn(batch_size, target_len, query_size)\n-\n-    # keys vectors (a key vector for each encoder word)\n-    ks = torch.randn(batch_size, source_len, keys_size)\n-\n-    # keys vectors with same size as query vectors\n-    kq = torch.randn(batch_size, source_len, query_size)\n-\n-    out = DotProductScorer()(q, kq)\n-    assert list(out.shape) == [batch_size, q.shape[1], kq.shape[1]]\n-\n-    out = GeneralScorer(query_size, keys_size)(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)\n-    assert list(out.shape) == [batch_size, q.shape[1], q.shape[1]]\n-\n-    out = MLPScorer(\n-        query_size, keys_size, layer_sizes=[10, 5, 5], activation=nn.Sigmoid\n-    )(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n",
        "source_code_with_indent": "        <DED>return x.squeeze(-1)  # remove last dimension\n\n\n<DED><DED>if __name__ == '__main__':\n    <IND>torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    out = DotProductScorer()(q, kq)\n    assert list(out.shape) == [batch_size, q.shape[1], kq.shape[1]]\n\n    out = GeneralScorer(query_size, keys_size)(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)\n    assert list(out.shape) == [batch_size, q.shape[1], q.shape[1]]\n\n    out = MLPScorer(\n        query_size, keys_size, layer_sizes=[10, 5, 5], activation=nn.Sigmoid\n    )(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <DED>return x.squeeze(-1)  # remove last dimension\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "Unbabel/OpenKiwi",
    "commit": "658fb5427252d78c4d83048138ce64857149cabe",
    "filename": "kiwi/modules/common/scorer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Unbabel-OpenKiwi/kiwi/modules/common/scorer.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "kiwi/modules/common/scorer.py:210:10 Call error [29]: `OperationScorer` is not a function.",
    "message": " `OperationScorer` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 210,
    "warning_line": "    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        return x.squeeze(-1)  # remove last dimension\n\n\nif __name__ == '__main__':\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    out = DotProductScorer()(q, kq)\n    assert list(out.shape) == [batch_size, q.shape[1], kq.shape[1]]\n\n    out = GeneralScorer(query_size, keys_size)(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)\n    assert list(out.shape) == [batch_size, q.shape[1], q.shape[1]]\n\n    out = MLPScorer(\n        query_size, keys_size, layer_sizes=[10, 5, 5], activation=nn.Sigmoid\n    )(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n",
        "source_code_len": 1690,
        "target_code": "        return x.squeeze(-1)  # remove last dimension\n",
        "target_code_len": 54,
        "diff_format": "@@ -175,48 +92,1 @@\n         return x.squeeze(-1)  # remove last dimension\n-\n-\n-if __name__ == '__main__':\n-    torch.manual_seed(1)\n-    torch.cuda.manual_seed(1)\n-\n-    batch_size = 4\n-    source_len = 7\n-    target_len = 3\n-    query_size = 10\n-    keys_size = 20\n-    attn_size = 15\n-\n-    # query vectors\n-    q = torch.randn(batch_size, target_len, query_size)\n-\n-    # keys vectors (a key vector for each encoder word)\n-    ks = torch.randn(batch_size, source_len, keys_size)\n-\n-    # keys vectors with same size as query vectors\n-    kq = torch.randn(batch_size, source_len, query_size)\n-\n-    out = DotProductScorer()(q, kq)\n-    assert list(out.shape) == [batch_size, q.shape[1], kq.shape[1]]\n-\n-    out = GeneralScorer(query_size, keys_size)(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)\n-    assert list(out.shape) == [batch_size, q.shape[1], q.shape[1]]\n-\n-    out = MLPScorer(\n-        query_size, keys_size, layer_sizes=[10, 5, 5], activation=nn.Sigmoid\n-    )(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n",
        "source_code_with_indent": "        <DED>return x.squeeze(-1)  # remove last dimension\n\n\n<DED><DED>if __name__ == '__main__':\n    <IND>torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    out = DotProductScorer()(q, kq)\n    assert list(out.shape) == [batch_size, q.shape[1], kq.shape[1]]\n\n    out = GeneralScorer(query_size, keys_size)(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)\n    assert list(out.shape) == [batch_size, q.shape[1], q.shape[1]]\n\n    out = MLPScorer(\n        query_size, keys_size, layer_sizes=[10, 5, 5], activation=nn.Sigmoid\n    )(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <DED>return x.squeeze(-1)  # remove last dimension\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "Unbabel/OpenKiwi",
    "commit": "658fb5427252d78c4d83048138ce64857149cabe",
    "filename": "kiwi/modules/common/scorer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Unbabel-OpenKiwi/kiwi/modules/common/scorer.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "kiwi/modules/common/scorer.py:213:10 Call error [29]: `OperationScorer` is not a function.",
    "message": " `OperationScorer` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 213,
    "warning_line": "    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        return x.squeeze(-1)  # remove last dimension\n\n\nif __name__ == '__main__':\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    out = DotProductScorer()(q, kq)\n    assert list(out.shape) == [batch_size, q.shape[1], kq.shape[1]]\n\n    out = GeneralScorer(query_size, keys_size)(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)\n    assert list(out.shape) == [batch_size, q.shape[1], q.shape[1]]\n\n    out = MLPScorer(\n        query_size, keys_size, layer_sizes=[10, 5, 5], activation=nn.Sigmoid\n    )(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n",
        "source_code_len": 1690,
        "target_code": "        return x.squeeze(-1)  # remove last dimension\n",
        "target_code_len": 54,
        "diff_format": "@@ -175,48 +92,1 @@\n         return x.squeeze(-1)  # remove last dimension\n-\n-\n-if __name__ == '__main__':\n-    torch.manual_seed(1)\n-    torch.cuda.manual_seed(1)\n-\n-    batch_size = 4\n-    source_len = 7\n-    target_len = 3\n-    query_size = 10\n-    keys_size = 20\n-    attn_size = 15\n-\n-    # query vectors\n-    q = torch.randn(batch_size, target_len, query_size)\n-\n-    # keys vectors (a key vector for each encoder word)\n-    ks = torch.randn(batch_size, source_len, keys_size)\n-\n-    # keys vectors with same size as query vectors\n-    kq = torch.randn(batch_size, source_len, query_size)\n-\n-    out = DotProductScorer()(q, kq)\n-    assert list(out.shape) == [batch_size, q.shape[1], kq.shape[1]]\n-\n-    out = GeneralScorer(query_size, keys_size)(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)\n-    assert list(out.shape) == [batch_size, q.shape[1], q.shape[1]]\n-\n-    out = MLPScorer(\n-        query_size, keys_size, layer_sizes=[10, 5, 5], activation=nn.Sigmoid\n-    )(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n",
        "source_code_with_indent": "        <DED>return x.squeeze(-1)  # remove last dimension\n\n\n<DED><DED>if __name__ == '__main__':\n    <IND>torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    out = DotProductScorer()(q, kq)\n    assert list(out.shape) == [batch_size, q.shape[1], kq.shape[1]]\n\n    out = GeneralScorer(query_size, keys_size)(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)\n    assert list(out.shape) == [batch_size, q.shape[1], q.shape[1]]\n\n    out = MLPScorer(\n        query_size, keys_size, layer_sizes=[10, 5, 5], activation=nn.Sigmoid\n    )(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <DED>return x.squeeze(-1)  # remove last dimension\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "Unbabel/OpenKiwi",
    "commit": "658fb5427252d78c4d83048138ce64857149cabe",
    "filename": "kiwi/modules/common/scorer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Unbabel-OpenKiwi/kiwi/modules/common/scorer.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "kiwi/modules/common/scorer.py:216:10 Call error [29]: `MLPScorer` is not a function.",
    "message": " `MLPScorer` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 216,
    "warning_line": "    out = MLPScorer(",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        return x.squeeze(-1)  # remove last dimension\n\n\nif __name__ == '__main__':\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    out = DotProductScorer()(q, kq)\n    assert list(out.shape) == [batch_size, q.shape[1], kq.shape[1]]\n\n    out = GeneralScorer(query_size, keys_size)(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)\n    assert list(out.shape) == [batch_size, q.shape[1], q.shape[1]]\n\n    out = MLPScorer(\n        query_size, keys_size, layer_sizes=[10, 5, 5], activation=nn.Sigmoid\n    )(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n",
        "source_code_len": 1690,
        "target_code": "        return x.squeeze(-1)  # remove last dimension\n",
        "target_code_len": 54,
        "diff_format": "@@ -175,48 +92,1 @@\n         return x.squeeze(-1)  # remove last dimension\n-\n-\n-if __name__ == '__main__':\n-    torch.manual_seed(1)\n-    torch.cuda.manual_seed(1)\n-\n-    batch_size = 4\n-    source_len = 7\n-    target_len = 3\n-    query_size = 10\n-    keys_size = 20\n-    attn_size = 15\n-\n-    # query vectors\n-    q = torch.randn(batch_size, target_len, query_size)\n-\n-    # keys vectors (a key vector for each encoder word)\n-    ks = torch.randn(batch_size, source_len, keys_size)\n-\n-    # keys vectors with same size as query vectors\n-    kq = torch.randn(batch_size, source_len, query_size)\n-\n-    out = DotProductScorer()(q, kq)\n-    assert list(out.shape) == [batch_size, q.shape[1], kq.shape[1]]\n-\n-    out = GeneralScorer(query_size, keys_size)(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)\n-    assert list(out.shape) == [batch_size, q.shape[1], q.shape[1]]\n-\n-    out = MLPScorer(\n-        query_size, keys_size, layer_sizes=[10, 5, 5], activation=nn.Sigmoid\n-    )(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n",
        "source_code_with_indent": "        <DED>return x.squeeze(-1)  # remove last dimension\n\n\n<DED><DED>if __name__ == '__main__':\n    <IND>torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    out = DotProductScorer()(q, kq)\n    assert list(out.shape) == [batch_size, q.shape[1], kq.shape[1]]\n\n    out = GeneralScorer(query_size, keys_size)(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)\n    assert list(out.shape) == [batch_size, q.shape[1], q.shape[1]]\n\n    out = MLPScorer(\n        query_size, keys_size, layer_sizes=[10, 5, 5], activation=nn.Sigmoid\n    )(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <DED>return x.squeeze(-1)  # remove last dimension\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "Unbabel/OpenKiwi",
    "commit": "658fb5427252d78c4d83048138ce64857149cabe",
    "filename": "kiwi/modules/common/scorer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Unbabel-OpenKiwi/kiwi/modules/common/scorer.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "kiwi/modules/common/scorer.py:221:10 Call error [29]: `MLPScorer` is not a function.",
    "message": " `MLPScorer` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 221,
    "warning_line": "    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        return x.squeeze(-1)  # remove last dimension\n\n\nif __name__ == '__main__':\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    out = DotProductScorer()(q, kq)\n    assert list(out.shape) == [batch_size, q.shape[1], kq.shape[1]]\n\n    out = GeneralScorer(query_size, keys_size)(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)\n    assert list(out.shape) == [batch_size, q.shape[1], q.shape[1]]\n\n    out = MLPScorer(\n        query_size, keys_size, layer_sizes=[10, 5, 5], activation=nn.Sigmoid\n    )(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n",
        "source_code_len": 1690,
        "target_code": "        return x.squeeze(-1)  # remove last dimension\n",
        "target_code_len": 54,
        "diff_format": "@@ -175,48 +92,1 @@\n         return x.squeeze(-1)  # remove last dimension\n-\n-\n-if __name__ == '__main__':\n-    torch.manual_seed(1)\n-    torch.cuda.manual_seed(1)\n-\n-    batch_size = 4\n-    source_len = 7\n-    target_len = 3\n-    query_size = 10\n-    keys_size = 20\n-    attn_size = 15\n-\n-    # query vectors\n-    q = torch.randn(batch_size, target_len, query_size)\n-\n-    # keys vectors (a key vector for each encoder word)\n-    ks = torch.randn(batch_size, source_len, keys_size)\n-\n-    # keys vectors with same size as query vectors\n-    kq = torch.randn(batch_size, source_len, query_size)\n-\n-    out = DotProductScorer()(q, kq)\n-    assert list(out.shape) == [batch_size, q.shape[1], kq.shape[1]]\n-\n-    out = GeneralScorer(query_size, keys_size)(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)\n-    assert list(out.shape) == [batch_size, q.shape[1], q.shape[1]]\n-\n-    out = MLPScorer(\n-        query_size, keys_size, layer_sizes=[10, 5, 5], activation=nn.Sigmoid\n-    )(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n-\n-    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)\n-    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n",
        "source_code_with_indent": "        <DED>return x.squeeze(-1)  # remove last dimension\n\n\n<DED><DED>if __name__ == '__main__':\n    <IND>torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    batch_size = 4\n    source_len = 7\n    target_len = 3\n    query_size = 10\n    keys_size = 20\n    attn_size = 15\n\n    # query vectors\n    q = torch.randn(batch_size, target_len, query_size)\n\n    # keys vectors (a key vector for each encoder word)\n    ks = torch.randn(batch_size, source_len, keys_size)\n\n    # keys vectors with same size as query vectors\n    kq = torch.randn(batch_size, source_len, query_size)\n\n    out = DotProductScorer()(q, kq)\n    assert list(out.shape) == [batch_size, q.shape[1], kq.shape[1]]\n\n    out = GeneralScorer(query_size, keys_size)(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='add')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='mul')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, keys_size, attn_size, op='concat')(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = OperationScorer(query_size, query_size, attn_size, op='add')(q, q)\n    assert list(out.shape) == [batch_size, q.shape[1], q.shape[1]]\n\n    out = MLPScorer(\n        query_size, keys_size, layer_sizes=[10, 5, 5], activation=nn.Sigmoid\n    )(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n\n    out = MLPScorer(query_size, keys_size, layer_sizes=[10, 5, 5])(q, ks)\n    assert list(out.shape) == [batch_size, q.shape[1], ks.shape[1]]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <DED>return x.squeeze(-1)  # remove last dimension\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "Unbabel/OpenKiwi",
    "commit": "658fb5427252d78c4d83048138ce64857149cabe",
    "filename": "kiwi/training/optimizers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Unbabel-OpenKiwi/kiwi/training/optimizers.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "kiwi/training/optimizers.py:107:19 Incompatible variable type [9]: closure is declared to have type `typing.Callable[..., typing.Any]` but is used as type `None`.",
    "message": " closure is declared to have type `typing.Callable[..., typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 107,
    "warning_line": "    def step(self, closure: Callable = None):"
  },
  {
    "project": "Unbabel/OpenKiwi",
    "commit": "658fb5427252d78c4d83048138ce64857149cabe",
    "filename": "kiwi/utils/migrations/v0_to_v2.py",
    "min_patch_found": false,
    "full_warning_msg": "kiwi/utils/migrations/v0_to_v2.py:397:36 Incompatible parameter type [6]: Expected `str` for 2nd positional only parameter to call `dict.__setitem__` but got `Dict[typing.Any, typing.Any]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/Unbabel-OpenKiwi/kiwi/utils/migrations/v0_to_v2.py'",
    "dd_fail": true
  },
  {
    "project": "Unbabel/OpenKiwi",
    "commit": "658fb5427252d78c4d83048138ce64857149cabe",
    "filename": "kiwi/utils/migrations/v0_to_v2.py",
    "min_patch_found": false,
    "full_warning_msg": "kiwi/utils/migrations/v0_to_v2.py:401:39 Incompatible parameter type [6]: Expected `str` for 2nd positional only parameter to call `dict.__setitem__` but got `Dict[typing.Any, typing.Any]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/Unbabel-OpenKiwi/kiwi/utils/migrations/v0_to_v2.py'",
    "dd_fail": true
  },
  {
    "project": "Unbabel/OpenKiwi",
    "commit": "658fb5427252d78c4d83048138ce64857149cabe",
    "filename": "kiwi/utils/migrations/v0_to_v2.py",
    "min_patch_found": false,
    "full_warning_msg": "kiwi/utils/migrations/v0_to_v2.py:425:36 Incompatible parameter type [6]: Expected `str` for 2nd positional only parameter to call `dict.__setitem__` but got `Dict[typing.Any, typing.Any]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/Unbabel-OpenKiwi/kiwi/utils/migrations/v0_to_v2.py'",
    "dd_fail": true
  },
  {
    "project": "Unbabel/OpenKiwi",
    "commit": "658fb5427252d78c4d83048138ce64857149cabe",
    "filename": "kiwi/utils/migrations/v0_to_v2.py",
    "min_patch_found": false,
    "full_warning_msg": "kiwi/utils/migrations/v0_to_v2.py:429:39 Incompatible parameter type [6]: Expected `str` for 2nd positional only parameter to call `dict.__setitem__` but got `Dict[typing.Any, typing.Any]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/Unbabel-OpenKiwi/kiwi/utils/migrations/v0_to_v2.py'",
    "dd_fail": true
  }
]