[
  {
    "project": "pfnet-research/chainer-compiler",
    "commit": "cd16c9b2e3ca28f13be62778a484d6a7d5c047b4",
    "filename": "elichika/sandbox.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/pfnet-research-chainer-compiler/elichika/sandbox.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "elichika/sandbox.py:119:8 Incompatible variable type [9]: m is declared to have type `SimpleFunc` but is used as type `MultiLayerPerceptron`.",
    "message": " m is declared to have type `SimpleFunc` but is used as type `MultiLayerPerceptron`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 119,
    "warning_line": "        m = MultiLayerPerceptron(10, 10, 10)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nclass MultiLayerPerceptron(chainer.Chain):\n    def __init__(self, n_in, n_hidden, n_out):\n        super(MultiLayerPerceptron, self).__init__()\n        with self.init_scope():\n            self.layer1 = L.Linear(n_in, n_hidden)\n            self.layer2 = L.Linear(n_hidden, n_hidden)\n            self.layer3 = L.Linear(n_hidden, n_out)\n\n    def forward(self, x):\n        # Forward propagation\n        self.x = x\n        h1 = F.relu(self.layer1(x))\n        h2 = F.relu(self.layer2(h1))\n        return self.layer3(h2)\n\nclass DynamicCond(chainer.Chain):\n    def forward(self, x, cond):\n        if cond:\n            x = 3\n        else:\n            x = 10\n        return x\n\nclass D(chainer.Chain):\n    def forward(self):\n        for i in range(4):\n            o = i\n        return o\n\nclass SimpleFunc(chainer.Chain):\n    def forward(self):\n        a,b = self.get_value()\n        return a + b\n\n    def get_value(self):\n        return 1.0, 2.0\n\nclass Conv(chainer.Chain):\n\n    def __init__(self):\n        super(Conv, self).__init__()\n        with self.init_scope():\n            # TODO Add more tests\n            self.l1 = L.Convolution2D(None, 6, (5, 7), stride=(2, 3))\n\n    def forward(self, x):\n        y1 = self.l1(x)\n        return y1\n\nclass IsNot(chainer.Chain):\n    def forward(self, x, y):\n        return x is not y\n\nclass ListGen(chainer.Chain):\n    def forward(self, x):\n        self.z = -x\n        self.y = []\n        self.y.append(1)\n        self.y.append(2)\n        self.y.append(3)\n        self.z3 = [int(w) for w in self.y]\n        self.z2 = self.y[1]\n        self.z = self.y[1:2]\n\nclass StaticCondTrue(chainer.Chain):\n    def forward(self, x):\n        if True:\n            x += 3\n        else:\n            x += 10\n        return x\n\nclass UpdateSelf(chainer.Chain):\n    def forward(self, x, cond):\n        self.x = x\n        if cond:\n            self.x += 10\n        return self.x\n\n",
        "source_code_len": 1887,
        "target_code": "\nclass SoftmaxAxis(chainer.Chain):\n    def forward(self, x):\n        return F.softmax(x, axis=2)\n\n\n",
        "target_code_len": 99,
        "diff_format": "@@ -10,80 +10,6 @@\n \n-class MultiLayerPerceptron(chainer.Chain):\n-    def __init__(self, n_in, n_hidden, n_out):\n-        super(MultiLayerPerceptron, self).__init__()\n-        with self.init_scope():\n-            self.layer1 = L.Linear(n_in, n_hidden)\n-            self.layer2 = L.Linear(n_hidden, n_hidden)\n-            self.layer3 = L.Linear(n_hidden, n_out)\n+class SoftmaxAxis(chainer.Chain):\n+    def forward(self, x):\n+        return F.softmax(x, axis=2)\n \n-    def forward(self, x):\n-        # Forward propagation\n-        self.x = x\n-        h1 = F.relu(self.layer1(x))\n-        h2 = F.relu(self.layer2(h1))\n-        return self.layer3(h2)\n-\n-class DynamicCond(chainer.Chain):\n-    def forward(self, x, cond):\n-        if cond:\n-            x = 3\n-        else:\n-            x = 10\n-        return x\n-\n-class D(chainer.Chain):\n-    def forward(self):\n-        for i in range(4):\n-            o = i\n-        return o\n-\n-class SimpleFunc(chainer.Chain):\n-    def forward(self):\n-        a,b = self.get_value()\n-        return a + b\n-\n-    def get_value(self):\n-        return 1.0, 2.0\n-\n-class Conv(chainer.Chain):\n-\n-    def __init__(self):\n-        super(Conv, self).__init__()\n-        with self.init_scope():\n-            # TODO Add more tests\n-            self.l1 = L.Convolution2D(None, 6, (5, 7), stride=(2, 3))\n-\n-    def forward(self, x):\n-        y1 = self.l1(x)\n-        return y1\n-\n-class IsNot(chainer.Chain):\n-    def forward(self, x, y):\n-        return x is not y\n-\n-class ListGen(chainer.Chain):\n-    def forward(self, x):\n-        self.z = -x\n-        self.y = []\n-        self.y.append(1)\n-        self.y.append(2)\n-        self.y.append(3)\n-        self.z3 = [int(w) for w in self.y]\n-        self.z2 = self.y[1]\n-        self.z = self.y[1:2]\n-\n-class StaticCondTrue(chainer.Chain):\n-    def forward(self, x):\n-        if True:\n-            x += 3\n-        else:\n-            x += 10\n-        return x\n-\n-class UpdateSelf(chainer.Chain):\n-    def forward(self, x, cond):\n-        self.x = x\n-        if cond:\n-            self.x += 10\n-        return self.x\n \n",
        "source_code_with_indent": "\nclass MultiLayerPerceptron(chainer.Chain):\n    <IND>def __init__(self, n_in, n_hidden, n_out):\n        <IND>super(MultiLayerPerceptron, self).__init__()\n        with self.init_scope():\n            <IND>self.layer1 = L.Linear(n_in, n_hidden)\n            self.layer2 = L.Linear(n_hidden, n_hidden)\n            self.layer3 = L.Linear(n_hidden, n_out)\n\n    <DED><DED>def forward(self, x):\n        # Forward propagation\n        <IND>self.x = x\n        h1 = F.relu(self.layer1(x))\n        h2 = F.relu(self.layer2(h1))\n        return self.layer3(h2)\n\n<DED><DED>class DynamicCond(chainer.Chain):\n    <IND>def forward(self, x, cond):\n        <IND>if cond:\n            <IND>x = 3\n        <DED>else:\n            <IND>x = 10\n        <DED>return x\n\n<DED><DED>class D(chainer.Chain):\n    <IND>def forward(self):\n        <IND>for i in range(4):\n            <IND>o = i\n        <DED>return o\n\n<DED><DED>class SimpleFunc(chainer.Chain):\n    <IND>def forward(self):\n        <IND>a,b = self.get_value()\n        return a + b\n\n    <DED>def get_value(self):\n        <IND>return 1.0, 2.0\n\n<DED><DED>class Conv(chainer.Chain):\n\n    <IND>def __init__(self):\n        <IND>super(Conv, self).__init__()\n        with self.init_scope():\n            # TODO Add more tests\n            <IND>self.l1 = L.Convolution2D(None, 6, (5, 7), stride=(2, 3))\n\n    <DED><DED>def forward(self, x):\n        <IND>y1 = self.l1(x)\n        return y1\n\n<DED><DED>class IsNot(chainer.Chain):\n    <IND>def forward(self, x, y):\n        <IND>return x is not y\n\n<DED><DED>class ListGen(chainer.Chain):\n    <IND>def forward(self, x):\n        <IND>self.z = -x\n        self.y = []\n        self.y.append(1)\n        self.y.append(2)\n        self.y.append(3)\n        self.z3 = [int(w) for w in self.y]\n        self.z2 = self.y[1]\n        self.z = self.y[1:2]\n\n<DED><DED>class StaticCondTrue(chainer.Chain):\n    <IND>def forward(self, x):\n        <IND>if True:\n            <IND>x += 3\n        <DED>else:\n            <IND>x += 10\n        <DED>return x\n\n<DED><DED>class UpdateSelf(chainer.Chain):\n    <IND>def forward(self, x, cond):\n        <IND>self.x = x\n        if cond:\n            <IND>self.x += 10\n        <DED>return self.x\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\nclass SoftmaxAxis(chainer.Chain):\n    <IND>def forward(self, x):\n        <IND>return F.softmax(x, axis=2)\n\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    #resnet50 = chainer.links.ResNet50Layers()\n    export(UpdateSelf(), [42, True], 'result/UpdateSelf')\n\n    all = False\n    if all:\n        m = SimpleFunc()\n        export(m, [], 'result/SimpleFunc')\n\n        m = MultiLayerPerceptron(10, 10, 10)\n        export(m, [np.zeros((10))], 'result/MLP')\n\n        m = DynamicCond()\n        export(m, [0.0, True], 'result/DynamicCond')\n\n        m = UpdateSelf()\n        export(m, [10.0], 'result/UpdateSelf')\n",
        "source_code_len": 452,
        "target_code": "\n    np.random.seed(314)\n    a = np.random.rand(3, 5, 4).astype(np.float32)\n\n    export(SoftmaxAxis(), [a], 'result/SoftmaxAxis')\n",
        "target_code_len": 130,
        "diff_format": "@@ -110,17 +36,5 @@\n \n-    #resnet50 = chainer.links.ResNet50Layers()\n-    export(UpdateSelf(), [42, True], 'result/UpdateSelf')\n+    np.random.seed(314)\n+    a = np.random.rand(3, 5, 4).astype(np.float32)\n \n-    all = False\n-    if all:\n-        m = SimpleFunc()\n-        export(m, [], 'result/SimpleFunc')\n-\n-        m = MultiLayerPerceptron(10, 10, 10)\n-        export(m, [np.zeros((10))], 'result/MLP')\n-\n-        m = DynamicCond()\n-        export(m, [0.0, True], 'result/DynamicCond')\n-\n-        m = UpdateSelf()\n-        export(m, [10.0], 'result/UpdateSelf')\n+    export(SoftmaxAxis(), [a], 'result/SoftmaxAxis')\n",
        "source_code_with_indent": "\n    #resnet50 = chainer.links.ResNet50Layers()\n    export(UpdateSelf(), [42, True], 'result/UpdateSelf')\n\n    all = False\n    if all:\n        <IND>m = SimpleFunc()\n        export(m, [], 'result/SimpleFunc')\n\n        m = MultiLayerPerceptron(10, 10, 10)\n        export(m, [np.zeros((10))], 'result/MLP')\n\n        m = DynamicCond()\n        export(m, [0.0, True], 'result/DynamicCond')\n\n        m = UpdateSelf()\n        export(m, [10.0], 'result/UpdateSelf')\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    np.random.seed(314)\n    a = np.random.rand(3, 5, 4).astype(np.float32)\n\n    export(SoftmaxAxis(), [a], 'result/SoftmaxAxis')\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "pfnet-research/chainer-compiler",
    "commit": "cd16c9b2e3ca28f13be62778a484d6a7d5c047b4",
    "filename": "elichika/sandbox.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/pfnet-research-chainer-compiler/elichika/sandbox.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "elichika/sandbox.py:122:8 Incompatible variable type [9]: m is declared to have type `SimpleFunc` but is used as type `DynamicCond`.",
    "message": " m is declared to have type `SimpleFunc` but is used as type `DynamicCond`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 122,
    "warning_line": "        m = DynamicCond()",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nclass MultiLayerPerceptron(chainer.Chain):\n    def __init__(self, n_in, n_hidden, n_out):\n        super(MultiLayerPerceptron, self).__init__()\n        with self.init_scope():\n            self.layer1 = L.Linear(n_in, n_hidden)\n            self.layer2 = L.Linear(n_hidden, n_hidden)\n            self.layer3 = L.Linear(n_hidden, n_out)\n\n    def forward(self, x):\n        # Forward propagation\n        self.x = x\n        h1 = F.relu(self.layer1(x))\n        h2 = F.relu(self.layer2(h1))\n        return self.layer3(h2)\n\nclass DynamicCond(chainer.Chain):\n    def forward(self, x, cond):\n        if cond:\n            x = 3\n        else:\n            x = 10\n        return x\n\nclass D(chainer.Chain):\n    def forward(self):\n        for i in range(4):\n            o = i\n        return o\n\nclass SimpleFunc(chainer.Chain):\n    def forward(self):\n        a,b = self.get_value()\n        return a + b\n\n    def get_value(self):\n        return 1.0, 2.0\n\nclass Conv(chainer.Chain):\n\n    def __init__(self):\n        super(Conv, self).__init__()\n        with self.init_scope():\n            # TODO Add more tests\n            self.l1 = L.Convolution2D(None, 6, (5, 7), stride=(2, 3))\n\n    def forward(self, x):\n        y1 = self.l1(x)\n        return y1\n\nclass IsNot(chainer.Chain):\n    def forward(self, x, y):\n        return x is not y\n\nclass ListGen(chainer.Chain):\n    def forward(self, x):\n        self.z = -x\n        self.y = []\n        self.y.append(1)\n        self.y.append(2)\n        self.y.append(3)\n        self.z3 = [int(w) for w in self.y]\n        self.z2 = self.y[1]\n        self.z = self.y[1:2]\n\nclass StaticCondTrue(chainer.Chain):\n    def forward(self, x):\n        if True:\n            x += 3\n        else:\n            x += 10\n        return x\n\nclass UpdateSelf(chainer.Chain):\n    def forward(self, x, cond):\n        self.x = x\n        if cond:\n            self.x += 10\n        return self.x\n\n",
        "source_code_len": 1887,
        "target_code": "\nclass SoftmaxAxis(chainer.Chain):\n    def forward(self, x):\n        return F.softmax(x, axis=2)\n\n\n",
        "target_code_len": 99,
        "diff_format": "@@ -10,80 +10,6 @@\n \n-class MultiLayerPerceptron(chainer.Chain):\n-    def __init__(self, n_in, n_hidden, n_out):\n-        super(MultiLayerPerceptron, self).__init__()\n-        with self.init_scope():\n-            self.layer1 = L.Linear(n_in, n_hidden)\n-            self.layer2 = L.Linear(n_hidden, n_hidden)\n-            self.layer3 = L.Linear(n_hidden, n_out)\n+class SoftmaxAxis(chainer.Chain):\n+    def forward(self, x):\n+        return F.softmax(x, axis=2)\n \n-    def forward(self, x):\n-        # Forward propagation\n-        self.x = x\n-        h1 = F.relu(self.layer1(x))\n-        h2 = F.relu(self.layer2(h1))\n-        return self.layer3(h2)\n-\n-class DynamicCond(chainer.Chain):\n-    def forward(self, x, cond):\n-        if cond:\n-            x = 3\n-        else:\n-            x = 10\n-        return x\n-\n-class D(chainer.Chain):\n-    def forward(self):\n-        for i in range(4):\n-            o = i\n-        return o\n-\n-class SimpleFunc(chainer.Chain):\n-    def forward(self):\n-        a,b = self.get_value()\n-        return a + b\n-\n-    def get_value(self):\n-        return 1.0, 2.0\n-\n-class Conv(chainer.Chain):\n-\n-    def __init__(self):\n-        super(Conv, self).__init__()\n-        with self.init_scope():\n-            # TODO Add more tests\n-            self.l1 = L.Convolution2D(None, 6, (5, 7), stride=(2, 3))\n-\n-    def forward(self, x):\n-        y1 = self.l1(x)\n-        return y1\n-\n-class IsNot(chainer.Chain):\n-    def forward(self, x, y):\n-        return x is not y\n-\n-class ListGen(chainer.Chain):\n-    def forward(self, x):\n-        self.z = -x\n-        self.y = []\n-        self.y.append(1)\n-        self.y.append(2)\n-        self.y.append(3)\n-        self.z3 = [int(w) for w in self.y]\n-        self.z2 = self.y[1]\n-        self.z = self.y[1:2]\n-\n-class StaticCondTrue(chainer.Chain):\n-    def forward(self, x):\n-        if True:\n-            x += 3\n-        else:\n-            x += 10\n-        return x\n-\n-class UpdateSelf(chainer.Chain):\n-    def forward(self, x, cond):\n-        self.x = x\n-        if cond:\n-            self.x += 10\n-        return self.x\n \n",
        "source_code_with_indent": "\nclass MultiLayerPerceptron(chainer.Chain):\n    <IND>def __init__(self, n_in, n_hidden, n_out):\n        <IND>super(MultiLayerPerceptron, self).__init__()\n        with self.init_scope():\n            <IND>self.layer1 = L.Linear(n_in, n_hidden)\n            self.layer2 = L.Linear(n_hidden, n_hidden)\n            self.layer3 = L.Linear(n_hidden, n_out)\n\n    <DED><DED>def forward(self, x):\n        # Forward propagation\n        <IND>self.x = x\n        h1 = F.relu(self.layer1(x))\n        h2 = F.relu(self.layer2(h1))\n        return self.layer3(h2)\n\n<DED><DED>class DynamicCond(chainer.Chain):\n    <IND>def forward(self, x, cond):\n        <IND>if cond:\n            <IND>x = 3\n        <DED>else:\n            <IND>x = 10\n        <DED>return x\n\n<DED><DED>class D(chainer.Chain):\n    <IND>def forward(self):\n        <IND>for i in range(4):\n            <IND>o = i\n        <DED>return o\n\n<DED><DED>class SimpleFunc(chainer.Chain):\n    <IND>def forward(self):\n        <IND>a,b = self.get_value()\n        return a + b\n\n    <DED>def get_value(self):\n        <IND>return 1.0, 2.0\n\n<DED><DED>class Conv(chainer.Chain):\n\n    <IND>def __init__(self):\n        <IND>super(Conv, self).__init__()\n        with self.init_scope():\n            # TODO Add more tests\n            <IND>self.l1 = L.Convolution2D(None, 6, (5, 7), stride=(2, 3))\n\n    <DED><DED>def forward(self, x):\n        <IND>y1 = self.l1(x)\n        return y1\n\n<DED><DED>class IsNot(chainer.Chain):\n    <IND>def forward(self, x, y):\n        <IND>return x is not y\n\n<DED><DED>class ListGen(chainer.Chain):\n    <IND>def forward(self, x):\n        <IND>self.z = -x\n        self.y = []\n        self.y.append(1)\n        self.y.append(2)\n        self.y.append(3)\n        self.z3 = [int(w) for w in self.y]\n        self.z2 = self.y[1]\n        self.z = self.y[1:2]\n\n<DED><DED>class StaticCondTrue(chainer.Chain):\n    <IND>def forward(self, x):\n        <IND>if True:\n            <IND>x += 3\n        <DED>else:\n            <IND>x += 10\n        <DED>return x\n\n<DED><DED>class UpdateSelf(chainer.Chain):\n    <IND>def forward(self, x, cond):\n        <IND>self.x = x\n        if cond:\n            <IND>self.x += 10\n        <DED>return self.x\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\nclass SoftmaxAxis(chainer.Chain):\n    <IND>def forward(self, x):\n        <IND>return F.softmax(x, axis=2)\n\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    #resnet50 = chainer.links.ResNet50Layers()\n    export(UpdateSelf(), [42, True], 'result/UpdateSelf')\n\n    all = False\n    if all:\n        m = SimpleFunc()\n        export(m, [], 'result/SimpleFunc')\n\n        m = MultiLayerPerceptron(10, 10, 10)\n        export(m, [np.zeros((10))], 'result/MLP')\n\n        m = DynamicCond()\n        export(m, [0.0, True], 'result/DynamicCond')\n\n        m = UpdateSelf()\n        export(m, [10.0], 'result/UpdateSelf')\n",
        "source_code_len": 452,
        "target_code": "\n    np.random.seed(314)\n    a = np.random.rand(3, 5, 4).astype(np.float32)\n\n    export(SoftmaxAxis(), [a], 'result/SoftmaxAxis')\n",
        "target_code_len": 130,
        "diff_format": "@@ -110,17 +36,5 @@\n \n-    #resnet50 = chainer.links.ResNet50Layers()\n-    export(UpdateSelf(), [42, True], 'result/UpdateSelf')\n+    np.random.seed(314)\n+    a = np.random.rand(3, 5, 4).astype(np.float32)\n \n-    all = False\n-    if all:\n-        m = SimpleFunc()\n-        export(m, [], 'result/SimpleFunc')\n-\n-        m = MultiLayerPerceptron(10, 10, 10)\n-        export(m, [np.zeros((10))], 'result/MLP')\n-\n-        m = DynamicCond()\n-        export(m, [0.0, True], 'result/DynamicCond')\n-\n-        m = UpdateSelf()\n-        export(m, [10.0], 'result/UpdateSelf')\n+    export(SoftmaxAxis(), [a], 'result/SoftmaxAxis')\n",
        "source_code_with_indent": "\n    #resnet50 = chainer.links.ResNet50Layers()\n    export(UpdateSelf(), [42, True], 'result/UpdateSelf')\n\n    all = False\n    if all:\n        <IND>m = SimpleFunc()\n        export(m, [], 'result/SimpleFunc')\n\n        m = MultiLayerPerceptron(10, 10, 10)\n        export(m, [np.zeros((10))], 'result/MLP')\n\n        m = DynamicCond()\n        export(m, [0.0, True], 'result/DynamicCond')\n\n        m = UpdateSelf()\n        export(m, [10.0], 'result/UpdateSelf')\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    np.random.seed(314)\n    a = np.random.rand(3, 5, 4).astype(np.float32)\n\n    export(SoftmaxAxis(), [a], 'result/SoftmaxAxis')\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "pfnet-research/chainer-compiler",
    "commit": "cd16c9b2e3ca28f13be62778a484d6a7d5c047b4",
    "filename": "elichika/sandbox.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/pfnet-research-chainer-compiler/elichika/sandbox.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "elichika/sandbox.py:125:8 Incompatible variable type [9]: m is declared to have type `SimpleFunc` but is used as type `UpdateSelf`.",
    "message": " m is declared to have type `SimpleFunc` but is used as type `UpdateSelf`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 125,
    "warning_line": "        m = UpdateSelf()",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nclass MultiLayerPerceptron(chainer.Chain):\n    def __init__(self, n_in, n_hidden, n_out):\n        super(MultiLayerPerceptron, self).__init__()\n        with self.init_scope():\n            self.layer1 = L.Linear(n_in, n_hidden)\n            self.layer2 = L.Linear(n_hidden, n_hidden)\n            self.layer3 = L.Linear(n_hidden, n_out)\n\n    def forward(self, x):\n        # Forward propagation\n        self.x = x\n        h1 = F.relu(self.layer1(x))\n        h2 = F.relu(self.layer2(h1))\n        return self.layer3(h2)\n\nclass DynamicCond(chainer.Chain):\n    def forward(self, x, cond):\n        if cond:\n            x = 3\n        else:\n            x = 10\n        return x\n\nclass D(chainer.Chain):\n    def forward(self):\n        for i in range(4):\n            o = i\n        return o\n\nclass SimpleFunc(chainer.Chain):\n    def forward(self):\n        a,b = self.get_value()\n        return a + b\n\n    def get_value(self):\n        return 1.0, 2.0\n\nclass Conv(chainer.Chain):\n\n    def __init__(self):\n        super(Conv, self).__init__()\n        with self.init_scope():\n            # TODO Add more tests\n            self.l1 = L.Convolution2D(None, 6, (5, 7), stride=(2, 3))\n\n    def forward(self, x):\n        y1 = self.l1(x)\n        return y1\n\nclass IsNot(chainer.Chain):\n    def forward(self, x, y):\n        return x is not y\n\nclass ListGen(chainer.Chain):\n    def forward(self, x):\n        self.z = -x\n        self.y = []\n        self.y.append(1)\n        self.y.append(2)\n        self.y.append(3)\n        self.z3 = [int(w) for w in self.y]\n        self.z2 = self.y[1]\n        self.z = self.y[1:2]\n\nclass StaticCondTrue(chainer.Chain):\n    def forward(self, x):\n        if True:\n            x += 3\n        else:\n            x += 10\n        return x\n\nclass UpdateSelf(chainer.Chain):\n    def forward(self, x, cond):\n        self.x = x\n        if cond:\n            self.x += 10\n        return self.x\n\n",
        "source_code_len": 1887,
        "target_code": "\nclass SoftmaxAxis(chainer.Chain):\n    def forward(self, x):\n        return F.softmax(x, axis=2)\n\n\n",
        "target_code_len": 99,
        "diff_format": "@@ -10,80 +10,6 @@\n \n-class MultiLayerPerceptron(chainer.Chain):\n-    def __init__(self, n_in, n_hidden, n_out):\n-        super(MultiLayerPerceptron, self).__init__()\n-        with self.init_scope():\n-            self.layer1 = L.Linear(n_in, n_hidden)\n-            self.layer2 = L.Linear(n_hidden, n_hidden)\n-            self.layer3 = L.Linear(n_hidden, n_out)\n+class SoftmaxAxis(chainer.Chain):\n+    def forward(self, x):\n+        return F.softmax(x, axis=2)\n \n-    def forward(self, x):\n-        # Forward propagation\n-        self.x = x\n-        h1 = F.relu(self.layer1(x))\n-        h2 = F.relu(self.layer2(h1))\n-        return self.layer3(h2)\n-\n-class DynamicCond(chainer.Chain):\n-    def forward(self, x, cond):\n-        if cond:\n-            x = 3\n-        else:\n-            x = 10\n-        return x\n-\n-class D(chainer.Chain):\n-    def forward(self):\n-        for i in range(4):\n-            o = i\n-        return o\n-\n-class SimpleFunc(chainer.Chain):\n-    def forward(self):\n-        a,b = self.get_value()\n-        return a + b\n-\n-    def get_value(self):\n-        return 1.0, 2.0\n-\n-class Conv(chainer.Chain):\n-\n-    def __init__(self):\n-        super(Conv, self).__init__()\n-        with self.init_scope():\n-            # TODO Add more tests\n-            self.l1 = L.Convolution2D(None, 6, (5, 7), stride=(2, 3))\n-\n-    def forward(self, x):\n-        y1 = self.l1(x)\n-        return y1\n-\n-class IsNot(chainer.Chain):\n-    def forward(self, x, y):\n-        return x is not y\n-\n-class ListGen(chainer.Chain):\n-    def forward(self, x):\n-        self.z = -x\n-        self.y = []\n-        self.y.append(1)\n-        self.y.append(2)\n-        self.y.append(3)\n-        self.z3 = [int(w) for w in self.y]\n-        self.z2 = self.y[1]\n-        self.z = self.y[1:2]\n-\n-class StaticCondTrue(chainer.Chain):\n-    def forward(self, x):\n-        if True:\n-            x += 3\n-        else:\n-            x += 10\n-        return x\n-\n-class UpdateSelf(chainer.Chain):\n-    def forward(self, x, cond):\n-        self.x = x\n-        if cond:\n-            self.x += 10\n-        return self.x\n \n",
        "source_code_with_indent": "\nclass MultiLayerPerceptron(chainer.Chain):\n    <IND>def __init__(self, n_in, n_hidden, n_out):\n        <IND>super(MultiLayerPerceptron, self).__init__()\n        with self.init_scope():\n            <IND>self.layer1 = L.Linear(n_in, n_hidden)\n            self.layer2 = L.Linear(n_hidden, n_hidden)\n            self.layer3 = L.Linear(n_hidden, n_out)\n\n    <DED><DED>def forward(self, x):\n        # Forward propagation\n        <IND>self.x = x\n        h1 = F.relu(self.layer1(x))\n        h2 = F.relu(self.layer2(h1))\n        return self.layer3(h2)\n\n<DED><DED>class DynamicCond(chainer.Chain):\n    <IND>def forward(self, x, cond):\n        <IND>if cond:\n            <IND>x = 3\n        <DED>else:\n            <IND>x = 10\n        <DED>return x\n\n<DED><DED>class D(chainer.Chain):\n    <IND>def forward(self):\n        <IND>for i in range(4):\n            <IND>o = i\n        <DED>return o\n\n<DED><DED>class SimpleFunc(chainer.Chain):\n    <IND>def forward(self):\n        <IND>a,b = self.get_value()\n        return a + b\n\n    <DED>def get_value(self):\n        <IND>return 1.0, 2.0\n\n<DED><DED>class Conv(chainer.Chain):\n\n    <IND>def __init__(self):\n        <IND>super(Conv, self).__init__()\n        with self.init_scope():\n            # TODO Add more tests\n            <IND>self.l1 = L.Convolution2D(None, 6, (5, 7), stride=(2, 3))\n\n    <DED><DED>def forward(self, x):\n        <IND>y1 = self.l1(x)\n        return y1\n\n<DED><DED>class IsNot(chainer.Chain):\n    <IND>def forward(self, x, y):\n        <IND>return x is not y\n\n<DED><DED>class ListGen(chainer.Chain):\n    <IND>def forward(self, x):\n        <IND>self.z = -x\n        self.y = []\n        self.y.append(1)\n        self.y.append(2)\n        self.y.append(3)\n        self.z3 = [int(w) for w in self.y]\n        self.z2 = self.y[1]\n        self.z = self.y[1:2]\n\n<DED><DED>class StaticCondTrue(chainer.Chain):\n    <IND>def forward(self, x):\n        <IND>if True:\n            <IND>x += 3\n        <DED>else:\n            <IND>x += 10\n        <DED>return x\n\n<DED><DED>class UpdateSelf(chainer.Chain):\n    <IND>def forward(self, x, cond):\n        <IND>self.x = x\n        if cond:\n            <IND>self.x += 10\n        <DED>return self.x\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\nclass SoftmaxAxis(chainer.Chain):\n    <IND>def forward(self, x):\n        <IND>return F.softmax(x, axis=2)\n\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    #resnet50 = chainer.links.ResNet50Layers()\n    export(UpdateSelf(), [42, True], 'result/UpdateSelf')\n\n    all = False\n    if all:\n        m = SimpleFunc()\n        export(m, [], 'result/SimpleFunc')\n\n        m = MultiLayerPerceptron(10, 10, 10)\n        export(m, [np.zeros((10))], 'result/MLP')\n\n        m = DynamicCond()\n        export(m, [0.0, True], 'result/DynamicCond')\n\n        m = UpdateSelf()\n        export(m, [10.0], 'result/UpdateSelf')\n",
        "source_code_len": 452,
        "target_code": "\n    np.random.seed(314)\n    a = np.random.rand(3, 5, 4).astype(np.float32)\n\n    export(SoftmaxAxis(), [a], 'result/SoftmaxAxis')\n",
        "target_code_len": 130,
        "diff_format": "@@ -110,17 +36,5 @@\n \n-    #resnet50 = chainer.links.ResNet50Layers()\n-    export(UpdateSelf(), [42, True], 'result/UpdateSelf')\n+    np.random.seed(314)\n+    a = np.random.rand(3, 5, 4).astype(np.float32)\n \n-    all = False\n-    if all:\n-        m = SimpleFunc()\n-        export(m, [], 'result/SimpleFunc')\n-\n-        m = MultiLayerPerceptron(10, 10, 10)\n-        export(m, [np.zeros((10))], 'result/MLP')\n-\n-        m = DynamicCond()\n-        export(m, [0.0, True], 'result/DynamicCond')\n-\n-        m = UpdateSelf()\n-        export(m, [10.0], 'result/UpdateSelf')\n+    export(SoftmaxAxis(), [a], 'result/SoftmaxAxis')\n",
        "source_code_with_indent": "\n    #resnet50 = chainer.links.ResNet50Layers()\n    export(UpdateSelf(), [42, True], 'result/UpdateSelf')\n\n    all = False\n    if all:\n        <IND>m = SimpleFunc()\n        export(m, [], 'result/SimpleFunc')\n\n        m = MultiLayerPerceptron(10, 10, 10)\n        export(m, [np.zeros((10))], 'result/MLP')\n\n        m = DynamicCond()\n        export(m, [0.0, True], 'result/DynamicCond')\n\n        m = UpdateSelf()\n        export(m, [10.0], 'result/UpdateSelf')\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    np.random.seed(314)\n    a = np.random.rand(3, 5, 4).astype(np.float32)\n\n    export(SoftmaxAxis(), [a], 'result/SoftmaxAxis')\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  }
]