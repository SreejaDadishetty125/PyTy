[
  {
    "project": "norse/norse",
    "commit": "db79ee7f4c2486c7b7af6787a4d8cd60c06241c9",
    "filename": "norse/benchmark/main.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/norse-norse/norse/benchmark/main.py",
    "file_hunks_size": 4,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "norse/benchmark/main.py:38:21 Unbound name [10]: Name `BenchmarkParameters` is used but not defined in the current scope.",
    "message": " Name `BenchmarkParameters` is used but not defined in the current scope.",
    "rule_id": "Unbound name [10]",
    "warning_line_no": 38,
    "warning_line": "    model: Callable[[BenchmarkParameters], float],",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nfrom benchmark import *\n\n",
        "source_code_len": 26,
        "target_code": "\nfrom .benchmark import *\n\n",
        "target_code_len": 27,
        "diff_format": "@@ -13,3 +17,3 @@\n \n-from benchmark import *\n+from .benchmark import *\n \n",
        "source_code_with_indent": "\nfrom benchmark import *\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\nfrom .benchmark import *\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "norse/norse",
    "commit": "db79ee7f4c2486c7b7af6787a4d8cd60c06241c9",
    "filename": "norse/benchmark/main.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/norse-norse/norse/benchmark/main.py",
    "file_hunks_size": 4,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "norse/benchmark/main.py:39:25 Unbound name [10]: Name `BenchmarkData` is used but not defined in the current scope.",
    "message": " Name `BenchmarkData` is used but not defined in the current scope.",
    "rule_id": "Unbound name [10]",
    "warning_line_no": 39,
    "warning_line": "    collector: Callable[[BenchmarkData], dict],",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nfrom benchmark import *\n\n",
        "source_code_len": 26,
        "target_code": "\nfrom .benchmark import *\n\n",
        "target_code_len": 27,
        "diff_format": "@@ -13,3 +17,3 @@\n \n-from benchmark import *\n+from .benchmark import *\n \n",
        "source_code_with_indent": "\nfrom benchmark import *\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\nfrom .benchmark import *\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "norse/norse",
    "commit": "db79ee7f4c2486c7b7af6787a4d8cd60c06241c9",
    "filename": "norse/benchmark/main.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/norse-norse/norse/benchmark/main.py",
    "file_hunks_size": 4,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "norse/benchmark/main.py:40:12 Unbound name [10]: Name `BenchmarkConfig` is used but not defined in the current scope.",
    "message": " Name `BenchmarkConfig` is used but not defined in the current scope.",
    "rule_id": "Unbound name [10]",
    "warning_line_no": 40,
    "warning_line": "    config: BenchmarkConfig,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nfrom benchmark import *\n\n",
        "source_code_len": 26,
        "target_code": "\nfrom .benchmark import *\n\n",
        "target_code_len": 27,
        "diff_format": "@@ -13,3 +17,3 @@\n \n-from benchmark import *\n+from .benchmark import *\n \n",
        "source_code_with_indent": "\nfrom benchmark import *\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\nfrom .benchmark import *\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "norse/norse",
    "commit": "db79ee7f4c2486c7b7af6787a4d8cd60c06241c9",
    "filename": "norse/torch/functional/lif_refrac.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/norse-norse/norse/torch/functional/lif_refrac.py",
    "file_hunks_size": 4,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "norse/torch/functional/lif_refrac.py:121:54 Incompatible parameter type [6]: Expected `LIFRefracState` for 1st positional only parameter to call `compute_refractory_update` but got `LIFRefracFeedForwardState`.",
    "message": " Expected `LIFRefracState` for 1st positional only parameter to call `compute_refractory_update` but got `LIFRefracFeedForwardState`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 121,
    "warning_line": "    v_new, z_new, rho_new = compute_refractory_update(state, z_new, s_new.v, p)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "import torch\n\nfrom .lif import (\n    LIFParameters,\n",
        "source_code_len": 52,
        "target_code": "from typing import NamedTuple, Tuple, overload\n\nimport torch\n\nfrom norse.torch.functional.lif import (\n    LIFParameters,\n",
        "target_code_len": 122,
        "diff_format": "@@ -1,4 +1,6 @@\n+from typing import NamedTuple, Tuple, overload\n+\n import torch\n \n-from .lif import (\n+from norse.torch.functional.lif import (\n     LIFParameters,\n",
        "source_code_with_indent": "import torch\n\nfrom .lif import (\n    LIFParameters,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "from typing import NamedTuple, Tuple, overload\n\nimport torch\n\nfrom norse.torch.functional.lif import (\n    LIFParameters,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\ndef compute_refractory_update(\n    state: LIFRefracState,\n    z_new: torch.Tensor,\n",
        "source_code_len": 84,
        "target_code": "\nclass LIFRefracFeedForwardState(NamedTuple):\n    \"\"\"State of a feed forward LIF neuron with absolute refractory period.\n\n    Parameters:\n        lif (LIFFeedForwardState): state of the feed forward LIF\n                                   neuron integration\n        rho (torch.Tensor): refractory state (count towards zero)\n    \"\"\"\n\n    lif: LIFFeedForwardState\n    rho: torch.Tensor\n\n\n@overload\ndef compute_refractory_update(\n    state: LIFRefracState,\n    z_new: torch.Tensor,\n    v_new: torch.Tensor,\n    p: LIFRefracParameters = LIFRefracParameters(),\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    ...\n\n\n@overload\ndef compute_refractory_update(\n    state: LIFRefracFeedForwardState,\n    z_new: torch.Tensor,\n    v_new: torch.Tensor,\n    p: LIFRefracParameters = LIFRefracParameters(),\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    ...\n\n\ndef compute_refractory_update(\n    state,\n    z_new: torch.Tensor,\n",
        "target_code_len": 931,
        "diff_format": "@@ -38,4 +38,37 @@\n \n+class LIFRefracFeedForwardState(NamedTuple):\n+    \"\"\"State of a feed forward LIF neuron with absolute refractory period.\n+\n+    Parameters:\n+        lif (LIFFeedForwardState): state of the feed forward LIF\n+                                   neuron integration\n+        rho (torch.Tensor): refractory state (count towards zero)\n+    \"\"\"\n+\n+    lif: LIFFeedForwardState\n+    rho: torch.Tensor\n+\n+\n+@overload\n def compute_refractory_update(\n     state: LIFRefracState,\n+    z_new: torch.Tensor,\n+    v_new: torch.Tensor,\n+    p: LIFRefracParameters = LIFRefracParameters(),\n+) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n+    ...\n+\n+\n+@overload\n+def compute_refractory_update(\n+    state: LIFRefracFeedForwardState,\n+    z_new: torch.Tensor,\n+    v_new: torch.Tensor,\n+    p: LIFRefracParameters = LIFRefracParameters(),\n+) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n+    ...\n+\n+\n+def compute_refractory_update(\n+    state,\n     z_new: torch.Tensor,\n",
        "source_code_with_indent": "\n<DED>def compute_refractory_update(\n    state: LIFRefracState,\n    z_new: torch.Tensor,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n<DED>class LIFRefracFeedForwardState(NamedTuple):\n    <IND>\"\"\"State of a feed forward LIF neuron with absolute refractory period.\n\n    Parameters:\n        lif (LIFFeedForwardState): state of the feed forward LIF\n                                   neuron integration\n        rho (torch.Tensor): refractory state (count towards zero)\n    \"\"\"\n\n    lif: LIFFeedForwardState\n    rho: torch.Tensor\n\n\n<DED>@overload\ndef compute_refractory_update(\n    state: LIFRefracState,\n    z_new: torch.Tensor,\n    v_new: torch.Tensor,\n    p: LIFRefracParameters = LIFRefracParameters(),\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    <IND>...\n\n\n<DED>@overload\ndef compute_refractory_update(\n    state: LIFRefracFeedForwardState,\n    z_new: torch.Tensor,\n    v_new: torch.Tensor,\n    p: LIFRefracParameters = LIFRefracParameters(),\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    <IND>...\n\n\n<DED>def compute_refractory_update(\n    state,\n    z_new: torch.Tensor,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "norse/norse",
    "commit": "db79ee7f4c2486c7b7af6787a4d8cd60c06241c9",
    "filename": "norse/torch/functional/regularization.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/norse-norse/norse/torch/functional/regularization.py",
    "file_hunks_size": 7,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "norse/torch/functional/regularization.py:66:6 Invalid type [31]: Expression `(torch.Tensor, $local_norse?torch?functional?regularization$T)` is not a valid type.",
    "message": " Expression `(torch.Tensor, $local_norse?torch?functional?regularization$T)` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 66,
    "warning_line": ") -> (torch.Tensor, T):",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\"\"\"\nfrom typing import Any, Callable, NewType, Optional, TypeVar\nimport torch\n\n# The type of regularisation state, e. g. ints for accumulating spikes or\n# torch.Tensor for accumulating membrane potential.\nT = TypeVar(\"T\")\n\n",
        "source_code_len": 223,
        "target_code": "\"\"\"\nfrom typing import Any, Callable, NewType, Optional, Tuple\n\nimport torch\n\n",
        "target_code_len": 78,
        "diff_format": "@@ -12,8 +12,5 @@\n \"\"\"\n-from typing import Any, Callable, NewType, Optional, TypeVar\n+from typing import Any, Callable, NewType, Optional, Tuple\n+\n import torch\n-\n-# The type of regularisation state, e. g. ints for accumulating spikes or\n-# torch.Tensor for accumulating membrane potential.\n-T = TypeVar(\"T\")\n \n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\nfrom typing import Any, Callable, NewType, Optional, TypeVar\nimport torch\n\n# The type of regularisation state, e. g. ints for accumulating spikes or\n# torch.Tensor for accumulating membrane potential.\nT = TypeVar(\"T\")\n\n",
        "target_code_with_indent": "\nfrom typing import Any, Callable, NewType, Optional, Tuple\n\nimport torch\n\n"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "# and return an updated regularization state\nAccumulator = NewType(\"Accumulator\", Callable[[torch.Tensor, Any, Optional[T]], T])\n\n\ndef spike_accumulator(z: torch.Tensor, s: Any, state: int = 0) -> int:\n    \"\"\"\n",
        "source_code_len": 210,
        "target_code": "# and return an updated regularization state\nAccumulator = NewType(\"Accumulator\", Callable[[torch.Tensor, Any, Optional[Any]], Any])\n\n\ndef spike_accumulator(\n    z: torch.Tensor, _: Any, state: Optional[torch.Tensor] = None\n) -> int:\n    \"\"\"\n",
        "target_code_len": 242,
        "diff_format": "@@ -21,6 +18,8 @@\n # and return an updated regularization state\n-Accumulator = NewType(\"Accumulator\", Callable[[torch.Tensor, Any, Optional[T]], T])\n+Accumulator = NewType(\"Accumulator\", Callable[[torch.Tensor, Any, Optional[Any]], Any])\n \n \n-def spike_accumulator(z: torch.Tensor, s: Any, state: int = 0) -> int:\n+def spike_accumulator(\n+    z: torch.Tensor, _: Any, state: Optional[torch.Tensor] = None\n+) -> int:\n     \"\"\"\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "# and return an updated regularization state\nAccumulator = NewType(\"Accumulator\", Callable[[torch.Tensor, Any, Optional[T]], T])\n\n\ndef spike_accumulator(z: torch.Tensor, s: Any, state: int = 0) -> int:\n    <IND>",
        "target_code_with_indent": "# and return an updated regularization state\nAccumulator = NewType(\"Accumulator\", Callable[[torch.Tensor, Any, Optional[Any]], Any])\n\n\ndef spike_accumulator(\n    z: torch.Tensor, _: Any, state: Optional[torch.Tensor] = None\n) -> int:\n    <IND>"
      },
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    accumulator: Accumulator = spike_accumulator,\n    state: T = None,\n) -> (torch.Tensor, T):\n    \"\"\"\n",
        "source_code_len": 103,
        "target_code": "    accumulator: Accumulator = spike_accumulator,\n    state: Any = None,\n) -> Tuple[torch.Tensor, Any]:\n    \"\"\"\n",
        "target_code_len": 112,
        "diff_format": "@@ -64,4 +65,4 @@\n     accumulator: Accumulator = spike_accumulator,\n-    state: T = None,\n-) -> (torch.Tensor, T):\n+    state: Any = None,\n+) -> Tuple[torch.Tensor, Any]:\n     \"\"\"\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "    accumulator: Accumulator = spike_accumulator,\n    state: T = None,\n) -> (torch.Tensor, T):\n    <IND>",
        "target_code_with_indent": "    accumulator: Accumulator = spike_accumulator,\n    state: Any = None,\n) -> Tuple[torch.Tensor, Any]:\n    <IND>"
      }
    ]
  }
]