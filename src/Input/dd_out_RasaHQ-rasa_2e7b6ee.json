[
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "rasa/core/policies/policy.py",
    "min_patch_found": false,
    "full_warning_msg": "rasa/core/policies/policy.py:214:12 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `SingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "rasa/core/policies/unexpected_intent_policy.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/RasaHQ-rasa/rasa/core/policies/unexpected_intent_policy.py",
    "file_hunks_size": 2,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "rasa/core/policies/unexpected_intent_policy.py:337:12 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `rasa.core.featurizers.tracker_featurizers.MaxHistoryTrackerFeaturizer2.__init__` but got `IntentTokenizerSingleStateFeaturizer`.",
    "message": " Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `rasa.core.featurizers.tracker_featurizers.MaxHistoryTrackerFeaturizer2.__init__` but got `IntentTokenizerSingleStateFeaturizer`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 337,
    "warning_line": "            IntentTokenizerSingleStateFeaturizer(),"
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "rasa/core/run.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/RasaHQ-rasa/rasa/core/run.py",
    "file_hunks_size": 6,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "rasa/core/run.py:264:12 Incompatible parameter type [6]: Expected `Union[rasa.core.nlg.generator.NaturalLanguageGenerator, rasa.utils.endpoints.EndpointConfig]` for 5th parameter `generator` to call `agent.load_agent` but got `Optional[rasa.utils.endpoints.EndpointConfig]`.",
    "message": " Expected `Union[rasa.core.nlg.generator.NaturalLanguageGenerator, rasa.utils.endpoints.EndpointConfig]` for 5th parameter `generator` to call `agent.load_agent` but got `Optional[rasa.utils.endpoints.EndpointConfig]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 264,
    "warning_line": "            generator=endpoints.nlg,"
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "rasa/nlu/model.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/RasaHQ-rasa/rasa/nlu/model.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "rasa/nlu/model.py:166:45 Incompatible parameter type [6]: Expected `List[Optional[str]]` for 1st positional only parameter to call `components.validate_requirements` but got `List[str]`.",
    "message": " Expected `List[Optional[str]]` for 1st positional only parameter to call `components.validate_requirements` but got `List[str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 166,
    "warning_line": "            components.validate_requirements(cfg.component_names)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        write_json_to_file(filename, metadata, indent=4)\n\n\nclass Trainer:\n    \"\"\"Trainer will load the data and train all components.\n\n    Requires a pipeline specification and configuration to use for\n    the training.\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: RasaNLUModelConfig,\n        component_builder: Optional[ComponentBuilder] = None,\n        skip_validation: bool = False,\n        model_to_finetune: Optional[\"Interpreter\"] = None,\n    ) -> None:\n\n        self.config = cfg\n        self.skip_validation = skip_validation\n        self.training_data = None  # type: Optional[TrainingData]\n\n        if component_builder is None:\n            # If no builder is passed, every interpreter creation will result in\n            # a new builder. hence, no components are reused.\n            component_builder = components.ComponentBuilder()\n\n        # Before instantiating the component classes, lets check if all\n        # required packages are available\n        if not self.skip_validation:\n            components.validate_requirements(cfg.component_names)\n\n        if model_to_finetune:\n            self.pipeline = model_to_finetune.pipeline\n        else:\n            self.pipeline = self._build_pipeline(cfg, component_builder)\n\n    def _build_pipeline(\n        self, cfg: RasaNLUModelConfig, component_builder: ComponentBuilder\n    ) -> List[Component]:\n        \"\"\"Transform the passed names of the pipeline components into classes.\"\"\"\n        pipeline = []\n\n        # Transform the passed names of the pipeline components into classes\n        for index, pipeline_component in enumerate(cfg.pipeline):\n            component_cfg = cfg.for_component(index)\n            component = component_builder.create_component(component_cfg, cfg)\n            components.validate_component_keys(component, pipeline_component)\n            pipeline.append(component)\n\n        if not self.skip_validation:\n            components.validate_pipeline(pipeline)\n\n        return pipeline\n\n    def train(self, data: TrainingData, **kwargs: Any) -> \"Interpreter\":\n        \"\"\"Trains the underlying pipeline using the provided training data.\"\"\"\n\n        self.training_data = data\n\n        self.training_data.validate()\n\n        context = kwargs\n\n        for component in self.pipeline:\n            updates = component.provide_context()\n            if updates:\n                context.update(updates)\n\n        # Before the training starts: check that all arguments are provided\n        if not self.skip_validation:\n            components.validate_required_components_from_data(\n                self.pipeline, self.training_data\n            )\n\n        # Warn if there is an obvious case of competing entity extractors\n        components.warn_of_competing_extractors(self.pipeline)\n        components.warn_of_competition_with_regex_extractor(\n            self.pipeline, self.training_data\n        )\n\n        # data gets modified internally during the training - hence the copy\n        working_data: TrainingData = copy.deepcopy(data)\n\n        for i, component in enumerate(self.pipeline):\n            logger.info(f\"Starting to train component {component.name}\")\n            component.prepare_partial_processing(self.pipeline[:i], context)\n            component.train(working_data, self.config, **context)\n            logger.info(\"Finished training component.\")\n\n        return Interpreter(self.pipeline, context)\n\n    @staticmethod\n    def _file_name(index: int, name: Text) -> Text:\n        return f\"component_{index}_{name}\"\n\n    def persist(\n        self,\n        path: Text,\n        persistor: Optional[Persistor] = None,\n        fixed_model_name: Text = None,\n        persist_nlu_training_data: bool = False,\n    ) -> Text:\n        \"\"\"Persist all components of the pipeline to the passed path.\n\n        Returns the directory of the persisted model.\"\"\"\n\n        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n        metadata = {\"language\": self.config[\"language\"], \"pipeline\": []}\n\n        if fixed_model_name:\n            model_name = fixed_model_name\n        else:\n            model_name = NLU_MODEL_NAME_PREFIX + timestamp\n\n        path = os.path.abspath(path)\n        dir_name = os.path.join(path, model_name)\n\n        rasa.shared.utils.io.create_directory(dir_name)\n\n        if self.training_data and persist_nlu_training_data:\n            metadata.update(self.training_data.persist(dir_name))\n\n        for i, component in enumerate(self.pipeline):\n            file_name = self._file_name(i, component.name)\n            update = component.persist(file_name, dir_name)\n            component_meta = component.component_config\n            if update:\n                component_meta.update(update)\n            component_meta[\n                \"class\"\n            ] = rasa.shared.utils.common.module_path_from_instance(component)\n\n            metadata[\"pipeline\"].append(component_meta)\n\n        Metadata(metadata).persist(dir_name)\n\n        if persistor is not None:\n            persistor.persist(dir_name, model_name)\n        logger.info(\n            \"Successfully saved model into '{}'\".format(os.path.abspath(dir_name))\n        )\n        return dir_name\n\n\nclass Interpreter:\n    \"\"\"Use a trained pipeline of components to parse text messages.\"\"\"\n\n    # Defines all attributes (& default values)\n    # that will be returned by `parse`\n    @staticmethod\n    def default_output_attributes() -> Dict[Text, Any]:\n        return {\n            TEXT: \"\",\n            INTENT: {INTENT_NAME_KEY: None, PREDICTED_CONFIDENCE_KEY: 0.0},\n            ENTITIES: [],\n        }\n\n    @staticmethod\n    def ensure_model_compatibility(\n        metadata: Metadata, version_to_check: Optional[Text] = None\n    ) -> None:\n        from packaging import version\n\n        if version_to_check is None:\n            version_to_check = MINIMUM_COMPATIBLE_VERSION\n\n        model_version = metadata.get(\"rasa_version\", \"0.0.0\")\n        if version.parse(model_version) < version.parse(version_to_check):\n            raise UnsupportedModelError(\n                f\"The model version is trained using Rasa Open Source {model_version} \"\n                f\"and is not compatible with your current installation \"\n                f\"({rasa.__version__}). \"\n                f\"This means that you either need to retrain your model \"\n                f\"or revert back to the Rasa version that trained the model \"\n                f\"to ensure that the versions match up again.\"\n            )\n\n    @staticmethod\n    def load(\n        model_dir: Text,\n        component_builder: Optional[ComponentBuilder] = None,\n        skip_validation: bool = False,\n        new_config: Optional[Dict] = None,\n        finetuning_epoch_fraction: float = 1.0,\n    ) -> \"Interpreter\":\n        \"\"\"Create an interpreter based on a persisted model.\n\n        Args:\n            skip_validation: If set to `True`, does not check that all\n                required packages for the components are installed\n                before loading them.\n            model_dir: The path of the model to load\n            component_builder: The\n                :class:`rasa.nlu.components.ComponentBuilder` to use.\n            new_config: Optional new config to use for the new epochs.\n            finetuning_epoch_fraction: Value to multiply all epochs by.\n\n        Returns:\n            An interpreter that uses the loaded model.\n        \"\"\"\n        model_metadata = Metadata.load(model_dir)\n\n        if new_config:\n            Interpreter._update_metadata_epochs(\n                model_metadata, new_config, finetuning_epoch_fraction\n            )\n\n        Interpreter.ensure_model_compatibility(model_metadata)\n        return Interpreter.create(\n            model_dir,\n            model_metadata,\n            component_builder,\n            skip_validation,\n            should_finetune=new_config is not None,\n        )\n\n    @staticmethod\n    def _get_default_value_for_component(name: Text, key: Text) -> Any:\n        from rasa.nlu.registry import get_component_class\n\n        return get_component_class(name).defaults[key]\n\n    @staticmethod\n    def _update_metadata_epochs(\n        model_metadata: Metadata,\n        new_config: Optional[Dict] = None,\n        finetuning_epoch_fraction: float = 1.0,\n    ) -> Metadata:\n        new_config = new_config or {}\n        for old_component_config, new_component_config in zip(\n            model_metadata.metadata[\"pipeline\"], new_config[\"pipeline\"]\n        ):\n            if EPOCHS in old_component_config:\n                new_epochs = new_component_config.get(\n                    EPOCHS,\n                    Interpreter._get_default_value_for_component(\n                        old_component_config[\"class\"], EPOCHS\n                    ),\n                )\n                old_component_config[EPOCHS] = ceil(\n                    new_epochs * finetuning_epoch_fraction\n                )\n        return model_metadata\n\n    @staticmethod\n    def create(\n        model_dir: Text,\n        model_metadata: Metadata,\n        component_builder: Optional[ComponentBuilder] = None,\n        skip_validation: bool = False,\n        should_finetune: bool = False,\n    ) -> \"Interpreter\":\n        \"\"\"Create model and components defined by the provided metadata.\n\n        Args:\n            model_dir: The directory containing the model.\n            model_metadata: The metadata describing each component.\n            component_builder: The\n                :class:`rasa.nlu.components.ComponentBuilder` to use.\n            skip_validation: If set to `True`, does not check that all\n                required packages for the components are installed\n                before loading them.\n            should_finetune: Indicates if the model components will be fine-tuned.\n\n        Returns:\n            An interpreter that uses the created model.\n        \"\"\"\n        context: Dict[Text, Any] = {\"should_finetune\": should_finetune}\n\n        if component_builder is None:\n            # If no builder is passed, every interpreter creation will result\n            # in a new builder. hence, no components are reused.\n            component_builder = components.ComponentBuilder()\n\n        pipeline = []\n\n        # Before instantiating the component classes,\n        # lets check if all required packages are available\n        if not skip_validation:\n            components.validate_requirements(model_metadata.component_classes)\n\n        for i in range(model_metadata.number_of_components):\n            component_meta = model_metadata.for_component(i)\n            component = component_builder.load_component(\n                component_meta, model_dir, model_metadata, **context\n            )\n            try:\n                updates = component.provide_context()\n                if updates:\n                    context.update(updates)\n                pipeline.append(component)\n            except components.MissingArgumentError as e:\n                raise Exception(\n                    \"Failed to initialize component '{}'. \"\n                    \"{}\".format(component.name, e)\n                )\n\n        return Interpreter(pipeline, context, model_metadata)\n\n    def __init__(\n        self,\n        pipeline: List[Component],\n        context: Optional[Dict[Text, Any]],\n        model_metadata: Optional[Metadata] = None,\n    ) -> None:\n\n        self.pipeline = pipeline\n        self.context = context if context is not None else {}\n        self.model_metadata = model_metadata\n        self.has_already_warned_of_overlapping_entities = False\n\n    def parse(\n        self,\n        text: Text,\n        time: Optional[datetime.datetime] = None,\n        only_output_properties: bool = True,\n    ) -> Dict[Text, Any]:\n        \"\"\"Parse the input text, classify it and return pipeline result.\n\n        The pipeline result usually contains intent and entities.\"\"\"\n\n        if not text:\n            # Not all components are able to handle empty strings. So we need\n            # to prevent that... This default return will not contain all\n            # output attributes of all components, but in the end, no one\n            # should pass an empty string in the first place.\n            output = self.default_output_attributes()\n            output[\"text\"] = \"\"\n            return output\n\n        timestamp = int(time.timestamp()) if time else None\n        data = self.default_output_attributes()\n        data[TEXT] = text\n\n        message = Message(data=data, time=timestamp, output_properties={TEXT_TOKENS})\n\n        for component in self.pipeline:\n            component.process(message, **self.context)\n\n        if not self.has_already_warned_of_overlapping_entities:\n            self.warn_of_overlapping_entities(message)\n\n        output = self.default_output_attributes()\n        output.update(message.as_dict(only_output_properties=only_output_properties))\n        return output\n\n    def featurize_message(self, message: Message) -> Message:\n        \"\"\"\n        Tokenize and featurize the input message\n        Args:\n            message: message storing text to process;\n        Returns:\n            message: it contains the tokens and features which are the output of the\n            NLU pipeline;\n        \"\"\"\n\n        for component in self.pipeline:\n            if not isinstance(component, (EntityExtractor, IntentClassifier)):\n                component.process(message, **self.context)\n        return message\n\n    def warn_of_overlapping_entities(self, message: Message) -> None:\n        \"\"\"Issues a warning when there are overlapping entity annotations.\n\n        This warning is only issued once per Interpreter life time.\n\n        Args:\n            message: user message with all processing metadata such as entities\n        \"\"\"\n        overlapping_entity_pairs = message.find_overlapping_entities()\n        if len(overlapping_entity_pairs) > 0:\n            message_text = message.get(\"text\")\n            first_pair = overlapping_entity_pairs[0]\n            entity_1 = first_pair[0]\n            entity_2 = first_pair[1]\n            rasa.shared.utils.io.raise_warning(\n                f\"Parsing of message: '{message_text}' lead to overlapping \"\n                f\"entities: {entity_1['value']} of type \"\n                f\"{entity_1['entity']} extracted by \"\n                f\"{entity_1['extractor']} overlaps with \"\n                f\"{entity_2['value']} of type {entity_2['entity']} extracted by \"\n                f\"{entity_2['extractor']}. This can lead to unintended filling of \"\n                f\"slots. Please refer to the documentation section on entity \"\n                f\"extractors and entities getting extracted multiple times:\"\n                f\"{DOCS_URL_COMPONENTS}#entity-extractors\"\n            )\n            self.has_already_warned_of_overlapping_entities = True\n",
        "source_code_len": 14724,
        "target_code": "        write_json_to_file(filename, metadata, indent=4)\n",
        "target_code_len": 57,
        "diff_format": "@@ -136,388 +117,1 @@\n         write_json_to_file(filename, metadata, indent=4)\n-\n-\n-class Trainer:\n-    \"\"\"Trainer will load the data and train all components.\n-\n-    Requires a pipeline specification and configuration to use for\n-    the training.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        cfg: RasaNLUModelConfig,\n-        component_builder: Optional[ComponentBuilder] = None,\n-        skip_validation: bool = False,\n-        model_to_finetune: Optional[\"Interpreter\"] = None,\n-    ) -> None:\n-\n-        self.config = cfg\n-        self.skip_validation = skip_validation\n-        self.training_data = None  # type: Optional[TrainingData]\n-\n-        if component_builder is None:\n-            # If no builder is passed, every interpreter creation will result in\n-            # a new builder. hence, no components are reused.\n-            component_builder = components.ComponentBuilder()\n-\n-        # Before instantiating the component classes, lets check if all\n-        # required packages are available\n-        if not self.skip_validation:\n-            components.validate_requirements(cfg.component_names)\n-\n-        if model_to_finetune:\n-            self.pipeline = model_to_finetune.pipeline\n-        else:\n-            self.pipeline = self._build_pipeline(cfg, component_builder)\n-\n-    def _build_pipeline(\n-        self, cfg: RasaNLUModelConfig, component_builder: ComponentBuilder\n-    ) -> List[Component]:\n-        \"\"\"Transform the passed names of the pipeline components into classes.\"\"\"\n-        pipeline = []\n-\n-        # Transform the passed names of the pipeline components into classes\n-        for index, pipeline_component in enumerate(cfg.pipeline):\n-            component_cfg = cfg.for_component(index)\n-            component = component_builder.create_component(component_cfg, cfg)\n-            components.validate_component_keys(component, pipeline_component)\n-            pipeline.append(component)\n-\n-        if not self.skip_validation:\n-            components.validate_pipeline(pipeline)\n-\n-        return pipeline\n-\n-    def train(self, data: TrainingData, **kwargs: Any) -> \"Interpreter\":\n-        \"\"\"Trains the underlying pipeline using the provided training data.\"\"\"\n-\n-        self.training_data = data\n-\n-        self.training_data.validate()\n-\n-        context = kwargs\n-\n-        for component in self.pipeline:\n-            updates = component.provide_context()\n-            if updates:\n-                context.update(updates)\n-\n-        # Before the training starts: check that all arguments are provided\n-        if not self.skip_validation:\n-            components.validate_required_components_from_data(\n-                self.pipeline, self.training_data\n-            )\n-\n-        # Warn if there is an obvious case of competing entity extractors\n-        components.warn_of_competing_extractors(self.pipeline)\n-        components.warn_of_competition_with_regex_extractor(\n-            self.pipeline, self.training_data\n-        )\n-\n-        # data gets modified internally during the training - hence the copy\n-        working_data: TrainingData = copy.deepcopy(data)\n-\n-        for i, component in enumerate(self.pipeline):\n-            logger.info(f\"Starting to train component {component.name}\")\n-            component.prepare_partial_processing(self.pipeline[:i], context)\n-            component.train(working_data, self.config, **context)\n-            logger.info(\"Finished training component.\")\n-\n-        return Interpreter(self.pipeline, context)\n-\n-    @staticmethod\n-    def _file_name(index: int, name: Text) -> Text:\n-        return f\"component_{index}_{name}\"\n-\n-    def persist(\n-        self,\n-        path: Text,\n-        persistor: Optional[Persistor] = None,\n-        fixed_model_name: Text = None,\n-        persist_nlu_training_data: bool = False,\n-    ) -> Text:\n-        \"\"\"Persist all components of the pipeline to the passed path.\n-\n-        Returns the directory of the persisted model.\"\"\"\n-\n-        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n-        metadata = {\"language\": self.config[\"language\"], \"pipeline\": []}\n-\n-        if fixed_model_name:\n-            model_name = fixed_model_name\n-        else:\n-            model_name = NLU_MODEL_NAME_PREFIX + timestamp\n-\n-        path = os.path.abspath(path)\n-        dir_name = os.path.join(path, model_name)\n-\n-        rasa.shared.utils.io.create_directory(dir_name)\n-\n-        if self.training_data and persist_nlu_training_data:\n-            metadata.update(self.training_data.persist(dir_name))\n-\n-        for i, component in enumerate(self.pipeline):\n-            file_name = self._file_name(i, component.name)\n-            update = component.persist(file_name, dir_name)\n-            component_meta = component.component_config\n-            if update:\n-                component_meta.update(update)\n-            component_meta[\n-                \"class\"\n-            ] = rasa.shared.utils.common.module_path_from_instance(component)\n-\n-            metadata[\"pipeline\"].append(component_meta)\n-\n-        Metadata(metadata).persist(dir_name)\n-\n-        if persistor is not None:\n-            persistor.persist(dir_name, model_name)\n-        logger.info(\n-            \"Successfully saved model into '{}'\".format(os.path.abspath(dir_name))\n-        )\n-        return dir_name\n-\n-\n-class Interpreter:\n-    \"\"\"Use a trained pipeline of components to parse text messages.\"\"\"\n-\n-    # Defines all attributes (& default values)\n-    # that will be returned by `parse`\n-    @staticmethod\n-    def default_output_attributes() -> Dict[Text, Any]:\n-        return {\n-            TEXT: \"\",\n-            INTENT: {INTENT_NAME_KEY: None, PREDICTED_CONFIDENCE_KEY: 0.0},\n-            ENTITIES: [],\n-        }\n-\n-    @staticmethod\n-    def ensure_model_compatibility(\n-        metadata: Metadata, version_to_check: Optional[Text] = None\n-    ) -> None:\n-        from packaging import version\n-\n-        if version_to_check is None:\n-            version_to_check = MINIMUM_COMPATIBLE_VERSION\n-\n-        model_version = metadata.get(\"rasa_version\", \"0.0.0\")\n-        if version.parse(model_version) < version.parse(version_to_check):\n-            raise UnsupportedModelError(\n-                f\"The model version is trained using Rasa Open Source {model_version} \"\n-                f\"and is not compatible with your current installation \"\n-                f\"({rasa.__version__}). \"\n-                f\"This means that you either need to retrain your model \"\n-                f\"or revert back to the Rasa version that trained the model \"\n-                f\"to ensure that the versions match up again.\"\n-            )\n-\n-    @staticmethod\n-    def load(\n-        model_dir: Text,\n-        component_builder: Optional[ComponentBuilder] = None,\n-        skip_validation: bool = False,\n-        new_config: Optional[Dict] = None,\n-        finetuning_epoch_fraction: float = 1.0,\n-    ) -> \"Interpreter\":\n-        \"\"\"Create an interpreter based on a persisted model.\n-\n-        Args:\n-            skip_validation: If set to `True`, does not check that all\n-                required packages for the components are installed\n-                before loading them.\n-            model_dir: The path of the model to load\n-            component_builder: The\n-                :class:`rasa.nlu.components.ComponentBuilder` to use.\n-            new_config: Optional new config to use for the new epochs.\n-            finetuning_epoch_fraction: Value to multiply all epochs by.\n-\n-        Returns:\n-            An interpreter that uses the loaded model.\n-        \"\"\"\n-        model_metadata = Metadata.load(model_dir)\n-\n-        if new_config:\n-            Interpreter._update_metadata_epochs(\n-                model_metadata, new_config, finetuning_epoch_fraction\n-            )\n-\n-        Interpreter.ensure_model_compatibility(model_metadata)\n-        return Interpreter.create(\n-            model_dir,\n-            model_metadata,\n-            component_builder,\n-            skip_validation,\n-            should_finetune=new_config is not None,\n-        )\n-\n-    @staticmethod\n-    def _get_default_value_for_component(name: Text, key: Text) -> Any:\n-        from rasa.nlu.registry import get_component_class\n-\n-        return get_component_class(name).defaults[key]\n-\n-    @staticmethod\n-    def _update_metadata_epochs(\n-        model_metadata: Metadata,\n-        new_config: Optional[Dict] = None,\n-        finetuning_epoch_fraction: float = 1.0,\n-    ) -> Metadata:\n-        new_config = new_config or {}\n-        for old_component_config, new_component_config in zip(\n-            model_metadata.metadata[\"pipeline\"], new_config[\"pipeline\"]\n-        ):\n-            if EPOCHS in old_component_config:\n-                new_epochs = new_component_config.get(\n-                    EPOCHS,\n-                    Interpreter._get_default_value_for_component(\n-                        old_component_config[\"class\"], EPOCHS\n-                    ),\n-                )\n-                old_component_config[EPOCHS] = ceil(\n-                    new_epochs * finetuning_epoch_fraction\n-                )\n-        return model_metadata\n-\n-    @staticmethod\n-    def create(\n-        model_dir: Text,\n-        model_metadata: Metadata,\n-        component_builder: Optional[ComponentBuilder] = None,\n-        skip_validation: bool = False,\n-        should_finetune: bool = False,\n-    ) -> \"Interpreter\":\n-        \"\"\"Create model and components defined by the provided metadata.\n-\n-        Args:\n-            model_dir: The directory containing the model.\n-            model_metadata: The metadata describing each component.\n-            component_builder: The\n-                :class:`rasa.nlu.components.ComponentBuilder` to use.\n-            skip_validation: If set to `True`, does not check that all\n-                required packages for the components are installed\n-                before loading them.\n-            should_finetune: Indicates if the model components will be fine-tuned.\n-\n-        Returns:\n-            An interpreter that uses the created model.\n-        \"\"\"\n-        context: Dict[Text, Any] = {\"should_finetune\": should_finetune}\n-\n-        if component_builder is None:\n-            # If no builder is passed, every interpreter creation will result\n-            # in a new builder. hence, no components are reused.\n-            component_builder = components.ComponentBuilder()\n-\n-        pipeline = []\n-\n-        # Before instantiating the component classes,\n-        # lets check if all required packages are available\n-        if not skip_validation:\n-            components.validate_requirements(model_metadata.component_classes)\n-\n-        for i in range(model_metadata.number_of_components):\n-            component_meta = model_metadata.for_component(i)\n-            component = component_builder.load_component(\n-                component_meta, model_dir, model_metadata, **context\n-            )\n-            try:\n-                updates = component.provide_context()\n-                if updates:\n-                    context.update(updates)\n-                pipeline.append(component)\n-            except components.MissingArgumentError as e:\n-                raise Exception(\n-                    \"Failed to initialize component '{}'. \"\n-                    \"{}\".format(component.name, e)\n-                )\n-\n-        return Interpreter(pipeline, context, model_metadata)\n-\n-    def __init__(\n-        self,\n-        pipeline: List[Component],\n-        context: Optional[Dict[Text, Any]],\n-        model_metadata: Optional[Metadata] = None,\n-    ) -> None:\n-\n-        self.pipeline = pipeline\n-        self.context = context if context is not None else {}\n-        self.model_metadata = model_metadata\n-        self.has_already_warned_of_overlapping_entities = False\n-\n-    def parse(\n-        self,\n-        text: Text,\n-        time: Optional[datetime.datetime] = None,\n-        only_output_properties: bool = True,\n-    ) -> Dict[Text, Any]:\n-        \"\"\"Parse the input text, classify it and return pipeline result.\n-\n-        The pipeline result usually contains intent and entities.\"\"\"\n-\n-        if not text:\n-            # Not all components are able to handle empty strings. So we need\n-            # to prevent that... This default return will not contain all\n-            # output attributes of all components, but in the end, no one\n-            # should pass an empty string in the first place.\n-            output = self.default_output_attributes()\n-            output[\"text\"] = \"\"\n-            return output\n-\n-        timestamp = int(time.timestamp()) if time else None\n-        data = self.default_output_attributes()\n-        data[TEXT] = text\n-\n-        message = Message(data=data, time=timestamp, output_properties={TEXT_TOKENS})\n-\n-        for component in self.pipeline:\n-            component.process(message, **self.context)\n-\n-        if not self.has_already_warned_of_overlapping_entities:\n-            self.warn_of_overlapping_entities(message)\n-\n-        output = self.default_output_attributes()\n-        output.update(message.as_dict(only_output_properties=only_output_properties))\n-        return output\n-\n-    def featurize_message(self, message: Message) -> Message:\n-        \"\"\"\n-        Tokenize and featurize the input message\n-        Args:\n-            message: message storing text to process;\n-        Returns:\n-            message: it contains the tokens and features which are the output of the\n-            NLU pipeline;\n-        \"\"\"\n-\n-        for component in self.pipeline:\n-            if not isinstance(component, (EntityExtractor, IntentClassifier)):\n-                component.process(message, **self.context)\n-        return message\n-\n-    def warn_of_overlapping_entities(self, message: Message) -> None:\n-        \"\"\"Issues a warning when there are overlapping entity annotations.\n-\n-        This warning is only issued once per Interpreter life time.\n-\n-        Args:\n-            message: user message with all processing metadata such as entities\n-        \"\"\"\n-        overlapping_entity_pairs = message.find_overlapping_entities()\n-        if len(overlapping_entity_pairs) > 0:\n-            message_text = message.get(\"text\")\n-            first_pair = overlapping_entity_pairs[0]\n-            entity_1 = first_pair[0]\n-            entity_2 = first_pair[1]\n-            rasa.shared.utils.io.raise_warning(\n-                f\"Parsing of message: '{message_text}' lead to overlapping \"\n-                f\"entities: {entity_1['value']} of type \"\n-                f\"{entity_1['entity']} extracted by \"\n-                f\"{entity_1['extractor']} overlaps with \"\n-                f\"{entity_2['value']} of type {entity_2['entity']} extracted by \"\n-                f\"{entity_2['extractor']}. This can lead to unintended filling of \"\n-                f\"slots. Please refer to the documentation section on entity \"\n-                f\"extractors and entities getting extracted multiple times:\"\n-                f\"{DOCS_URL_COMPONENTS}#entity-extractors\"\n-            )\n-            self.has_already_warned_of_overlapping_entities = True\n",
        "source_code_with_indent": "        write_json_to_file(filename, metadata, indent=4)\n\n\n<DED><DED>class Trainer:\n    <IND>\"\"\"Trainer will load the data and train all components.\n\n    Requires a pipeline specification and configuration to use for\n    the training.\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: RasaNLUModelConfig,\n        component_builder: Optional[ComponentBuilder] = None,\n        skip_validation: bool = False,\n        model_to_finetune: Optional[\"Interpreter\"] = None,\n    ) -> None:\n\n        <IND>self.config = cfg\n        self.skip_validation = skip_validation\n        self.training_data = None  # type: Optional[TrainingData]\n\n        if component_builder is None:\n            # If no builder is passed, every interpreter creation will result in\n            # a new builder. hence, no components are reused.\n            <IND>component_builder = components.ComponentBuilder()\n\n        # Before instantiating the component classes, lets check if all\n        # required packages are available\n        <DED>if not self.skip_validation:\n            <IND>components.validate_requirements(cfg.component_names)\n\n        <DED>if model_to_finetune:\n            <IND>self.pipeline = model_to_finetune.pipeline\n        <DED>else:\n            <IND>self.pipeline = self._build_pipeline(cfg, component_builder)\n\n    <DED><DED>def _build_pipeline(\n        self, cfg: RasaNLUModelConfig, component_builder: ComponentBuilder\n    ) -> List[Component]:\n        <IND>\"\"\"Transform the passed names of the pipeline components into classes.\"\"\"\n        pipeline = []\n\n        # Transform the passed names of the pipeline components into classes\n        for index, pipeline_component in enumerate(cfg.pipeline):\n            <IND>component_cfg = cfg.for_component(index)\n            component = component_builder.create_component(component_cfg, cfg)\n            components.validate_component_keys(component, pipeline_component)\n            pipeline.append(component)\n\n        <DED>if not self.skip_validation:\n            <IND>components.validate_pipeline(pipeline)\n\n        <DED>return pipeline\n\n    <DED>def train(self, data: TrainingData, **kwargs: Any) -> \"Interpreter\":\n        <IND>\"\"\"Trains the underlying pipeline using the provided training data.\"\"\"\n\n        self.training_data = data\n\n        self.training_data.validate()\n\n        context = kwargs\n\n        for component in self.pipeline:\n            <IND>updates = component.provide_context()\n            if updates:\n                <IND>context.update(updates)\n\n        # Before the training starts: check that all arguments are provided\n        <DED><DED>if not self.skip_validation:\n            <IND>components.validate_required_components_from_data(\n                self.pipeline, self.training_data\n            )\n\n        # Warn if there is an obvious case of competing entity extractors\n        <DED>components.warn_of_competing_extractors(self.pipeline)\n        components.warn_of_competition_with_regex_extractor(\n            self.pipeline, self.training_data\n        )\n\n        # data gets modified internally during the training - hence the copy\n        working_data: TrainingData = copy.deepcopy(data)\n\n        for i, component in enumerate(self.pipeline):\n            <IND>logger.info(f\"Starting to train component {component.name}\")\n            component.prepare_partial_processing(self.pipeline[:i], context)\n            component.train(working_data, self.config, **context)\n            logger.info(\"Finished training component.\")\n\n        <DED>return Interpreter(self.pipeline, context)\n\n    <DED>@staticmethod\n    def _file_name(index: int, name: Text) -> Text:\n        <IND>return f\"component_{index}_{name}\"\n\n    <DED>def persist(\n        self,\n        path: Text,\n        persistor: Optional[Persistor] = None,\n        fixed_model_name: Text = None,\n        persist_nlu_training_data: bool = False,\n    ) -> Text:\n        <IND>\"\"\"Persist all components of the pipeline to the passed path.\n\n        Returns the directory of the persisted model.\"\"\"\n\n        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n        metadata = {\"language\": self.config[\"language\"], \"pipeline\": []}\n\n        if fixed_model_name:\n            <IND>model_name = fixed_model_name\n        <DED>else:\n            <IND>model_name = NLU_MODEL_NAME_PREFIX + timestamp\n\n        <DED>path = os.path.abspath(path)\n        dir_name = os.path.join(path, model_name)\n\n        rasa.shared.utils.io.create_directory(dir_name)\n\n        if self.training_data and persist_nlu_training_data:\n            <IND>metadata.update(self.training_data.persist(dir_name))\n\n        <DED>for i, component in enumerate(self.pipeline):\n            <IND>file_name = self._file_name(i, component.name)\n            update = component.persist(file_name, dir_name)\n            component_meta = component.component_config\n            if update:\n                <IND>component_meta.update(update)\n            <DED>component_meta[\n                \"class\"\n            ] = rasa.shared.utils.common.module_path_from_instance(component)\n\n            metadata[\"pipeline\"].append(component_meta)\n\n        <DED>Metadata(metadata).persist(dir_name)\n\n        if persistor is not None:\n            <IND>persistor.persist(dir_name, model_name)\n        <DED>logger.info(\n            \"Successfully saved model into '{}'\".format(os.path.abspath(dir_name))\n        )\n        return dir_name\n\n\n<DED><DED>class Interpreter:\n    <IND>\"\"\"Use a trained pipeline of components to parse text messages.\"\"\"\n\n    # Defines all attributes (& default values)\n    # that will be returned by `parse`\n    @staticmethod\n    def default_output_attributes() -> Dict[Text, Any]:\n        <IND>return {\n            TEXT: \"\",\n            INTENT: {INTENT_NAME_KEY: None, PREDICTED_CONFIDENCE_KEY: 0.0},\n            ENTITIES: [],\n        }\n\n    <DED>@staticmethod\n    def ensure_model_compatibility(\n        metadata: Metadata, version_to_check: Optional[Text] = None\n    ) -> None:\n        <IND>from packaging import version\n\n        if version_to_check is None:\n            <IND>version_to_check = MINIMUM_COMPATIBLE_VERSION\n\n        <DED>model_version = metadata.get(\"rasa_version\", \"0.0.0\")\n        if version.parse(model_version) < version.parse(version_to_check):\n            <IND>raise UnsupportedModelError(\n                f\"The model version is trained using Rasa Open Source {model_version} \"\n                f\"and is not compatible with your current installation \"\n                f\"({rasa.__version__}). \"\n                f\"This means that you either need to retrain your model \"\n                f\"or revert back to the Rasa version that trained the model \"\n                f\"to ensure that the versions match up again.\"\n            )\n\n    <DED><DED>@staticmethod\n    def load(\n        model_dir: Text,\n        component_builder: Optional[ComponentBuilder] = None,\n        skip_validation: bool = False,\n        new_config: Optional[Dict] = None,\n        finetuning_epoch_fraction: float = 1.0,\n    ) -> \"Interpreter\":\n        <IND>\"\"\"Create an interpreter based on a persisted model.\n\n        Args:\n            skip_validation: If set to `True`, does not check that all\n                required packages for the components are installed\n                before loading them.\n            model_dir: The path of the model to load\n            component_builder: The\n                :class:`rasa.nlu.components.ComponentBuilder` to use.\n            new_config: Optional new config to use for the new epochs.\n            finetuning_epoch_fraction: Value to multiply all epochs by.\n\n        Returns:\n            An interpreter that uses the loaded model.\n        \"\"\"\n        model_metadata = Metadata.load(model_dir)\n\n        if new_config:\n            <IND>Interpreter._update_metadata_epochs(\n                model_metadata, new_config, finetuning_epoch_fraction\n            )\n\n        <DED>Interpreter.ensure_model_compatibility(model_metadata)\n        return Interpreter.create(\n            model_dir,\n            model_metadata,\n            component_builder,\n            skip_validation,\n            should_finetune=new_config is not None,\n        )\n\n    <DED>@staticmethod\n    def _get_default_value_for_component(name: Text, key: Text) -> Any:\n        <IND>from rasa.nlu.registry import get_component_class\n\n        return get_component_class(name).defaults[key]\n\n    <DED>@staticmethod\n    def _update_metadata_epochs(\n        model_metadata: Metadata,\n        new_config: Optional[Dict] = None,\n        finetuning_epoch_fraction: float = 1.0,\n    ) -> Metadata:\n        <IND>new_config = new_config or {}\n        for old_component_config, new_component_config in zip(\n            model_metadata.metadata[\"pipeline\"], new_config[\"pipeline\"]\n        ):\n            <IND>if EPOCHS in old_component_config:\n                <IND>new_epochs = new_component_config.get(\n                    EPOCHS,\n                    Interpreter._get_default_value_for_component(\n                        old_component_config[\"class\"], EPOCHS\n                    ),\n                )\n                old_component_config[EPOCHS] = ceil(\n                    new_epochs * finetuning_epoch_fraction\n                )\n        <DED><DED>return model_metadata\n\n    <DED>@staticmethod\n    def create(\n        model_dir: Text,\n        model_metadata: Metadata,\n        component_builder: Optional[ComponentBuilder] = None,\n        skip_validation: bool = False,\n        should_finetune: bool = False,\n    ) -> \"Interpreter\":\n        <IND>\"\"\"Create model and components defined by the provided metadata.\n\n        Args:\n            model_dir: The directory containing the model.\n            model_metadata: The metadata describing each component.\n            component_builder: The\n                :class:`rasa.nlu.components.ComponentBuilder` to use.\n            skip_validation: If set to `True`, does not check that all\n                required packages for the components are installed\n                before loading them.\n            should_finetune: Indicates if the model components will be fine-tuned.\n\n        Returns:\n            An interpreter that uses the created model.\n        \"\"\"\n        context: Dict[Text, Any] = {\"should_finetune\": should_finetune}\n\n        if component_builder is None:\n            # If no builder is passed, every interpreter creation will result\n            # in a new builder. hence, no components are reused.\n            <IND>component_builder = components.ComponentBuilder()\n\n        <DED>pipeline = []\n\n        # Before instantiating the component classes,\n        # lets check if all required packages are available\n        if not skip_validation:\n            <IND>components.validate_requirements(model_metadata.component_classes)\n\n        <DED>for i in range(model_metadata.number_of_components):\n            <IND>component_meta = model_metadata.for_component(i)\n            component = component_builder.load_component(\n                component_meta, model_dir, model_metadata, **context\n            )\n            try:\n                <IND>updates = component.provide_context()\n                if updates:\n                    <IND>context.update(updates)\n                <DED>pipeline.append(component)\n            <DED>except components.MissingArgumentError as e:\n                <IND>raise Exception(\n                    \"Failed to initialize component '{}'. \"\n                    \"{}\".format(component.name, e)\n                )\n\n        <DED><DED>return Interpreter(pipeline, context, model_metadata)\n\n    <DED>def __init__(\n        self,\n        pipeline: List[Component],\n        context: Optional[Dict[Text, Any]],\n        model_metadata: Optional[Metadata] = None,\n    ) -> None:\n\n        <IND>self.pipeline = pipeline\n        self.context = context if context is not None else {}\n        self.model_metadata = model_metadata\n        self.has_already_warned_of_overlapping_entities = False\n\n    <DED>def parse(\n        self,\n        text: Text,\n        time: Optional[datetime.datetime] = None,\n        only_output_properties: bool = True,\n    ) -> Dict[Text, Any]:\n        <IND>\"\"\"Parse the input text, classify it and return pipeline result.\n\n        The pipeline result usually contains intent and entities.\"\"\"\n\n        if not text:\n            # Not all components are able to handle empty strings. So we need\n            # to prevent that... This default return will not contain all\n            # output attributes of all components, but in the end, no one\n            # should pass an empty string in the first place.\n            <IND>output = self.default_output_attributes()\n            output[\"text\"] = \"\"\n            return output\n\n        <DED>timestamp = int(time.timestamp()) if time else None\n        data = self.default_output_attributes()\n        data[TEXT] = text\n\n        message = Message(data=data, time=timestamp, output_properties={TEXT_TOKENS})\n\n        for component in self.pipeline:\n            <IND>component.process(message, **self.context)\n\n        <DED>if not self.has_already_warned_of_overlapping_entities:\n            <IND>self.warn_of_overlapping_entities(message)\n\n        <DED>output = self.default_output_attributes()\n        output.update(message.as_dict(only_output_properties=only_output_properties))\n        return output\n\n    <DED>def featurize_message(self, message: Message) -> Message:\n        <IND>\"\"\"\n        Tokenize and featurize the input message\n        Args:\n            message: message storing text to process;\n        Returns:\n            message: it contains the tokens and features which are the output of the\n            NLU pipeline;\n        \"\"\"\n\n        for component in self.pipeline:\n            <IND>if not isinstance(component, (EntityExtractor, IntentClassifier)):\n                <IND>component.process(message, **self.context)\n        <DED><DED>return message\n\n    <DED>def warn_of_overlapping_entities(self, message: Message) -> None:\n        <IND>\"\"\"Issues a warning when there are overlapping entity annotations.\n\n        This warning is only issued once per Interpreter life time.\n\n        Args:\n            message: user message with all processing metadata such as entities\n        \"\"\"\n        overlapping_entity_pairs = message.find_overlapping_entities()\n        if len(overlapping_entity_pairs) > 0:\n            <IND>message_text = message.get(\"text\")\n            first_pair = overlapping_entity_pairs[0]\n            entity_1 = first_pair[0]\n            entity_2 = first_pair[1]\n            rasa.shared.utils.io.raise_warning(\n                f\"Parsing of message: '{message_text}' lead to overlapping \"\n                f\"entities: {entity_1['value']} of type \"\n                f\"{entity_1['entity']} extracted by \"\n                f\"{entity_1['extractor']} overlaps with \"\n                f\"{entity_2['value']} of type {entity_2['entity']} extracted by \"\n                f\"{entity_2['extractor']}. This can lead to unintended filling of \"\n                f\"slots. Please refer to the documentation section on entity \"\n                f\"extractors and entities getting extracted multiple times:\"\n                f\"{DOCS_URL_COMPONENTS}#entity-extractors\"\n            )\n            self.has_already_warned_of_overlapping_entities = True\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        write_json_to_file(filename, metadata, indent=4)\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "rasa/nlu/model.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/RasaHQ-rasa/rasa/nlu/model.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "rasa/nlu/model.py:208:31 Incompatible parameter type [6]: Expected `TrainingData` for 2nd positional only parameter to call `components.validate_required_components_from_data` but got `Optional[TrainingData]`.",
    "message": " Expected `TrainingData` for 2nd positional only parameter to call `components.validate_required_components_from_data` but got `Optional[TrainingData]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 208,
    "warning_line": "                self.pipeline, self.training_data",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        write_json_to_file(filename, metadata, indent=4)\n\n\nclass Trainer:\n    \"\"\"Trainer will load the data and train all components.\n\n    Requires a pipeline specification and configuration to use for\n    the training.\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: RasaNLUModelConfig,\n        component_builder: Optional[ComponentBuilder] = None,\n        skip_validation: bool = False,\n        model_to_finetune: Optional[\"Interpreter\"] = None,\n    ) -> None:\n\n        self.config = cfg\n        self.skip_validation = skip_validation\n        self.training_data = None  # type: Optional[TrainingData]\n\n        if component_builder is None:\n            # If no builder is passed, every interpreter creation will result in\n            # a new builder. hence, no components are reused.\n            component_builder = components.ComponentBuilder()\n\n        # Before instantiating the component classes, lets check if all\n        # required packages are available\n        if not self.skip_validation:\n            components.validate_requirements(cfg.component_names)\n\n        if model_to_finetune:\n            self.pipeline = model_to_finetune.pipeline\n        else:\n            self.pipeline = self._build_pipeline(cfg, component_builder)\n\n    def _build_pipeline(\n        self, cfg: RasaNLUModelConfig, component_builder: ComponentBuilder\n    ) -> List[Component]:\n        \"\"\"Transform the passed names of the pipeline components into classes.\"\"\"\n        pipeline = []\n\n        # Transform the passed names of the pipeline components into classes\n        for index, pipeline_component in enumerate(cfg.pipeline):\n            component_cfg = cfg.for_component(index)\n            component = component_builder.create_component(component_cfg, cfg)\n            components.validate_component_keys(component, pipeline_component)\n            pipeline.append(component)\n\n        if not self.skip_validation:\n            components.validate_pipeline(pipeline)\n\n        return pipeline\n\n    def train(self, data: TrainingData, **kwargs: Any) -> \"Interpreter\":\n        \"\"\"Trains the underlying pipeline using the provided training data.\"\"\"\n\n        self.training_data = data\n\n        self.training_data.validate()\n\n        context = kwargs\n\n        for component in self.pipeline:\n            updates = component.provide_context()\n            if updates:\n                context.update(updates)\n\n        # Before the training starts: check that all arguments are provided\n        if not self.skip_validation:\n            components.validate_required_components_from_data(\n                self.pipeline, self.training_data\n            )\n\n        # Warn if there is an obvious case of competing entity extractors\n        components.warn_of_competing_extractors(self.pipeline)\n        components.warn_of_competition_with_regex_extractor(\n            self.pipeline, self.training_data\n        )\n\n        # data gets modified internally during the training - hence the copy\n        working_data: TrainingData = copy.deepcopy(data)\n\n        for i, component in enumerate(self.pipeline):\n            logger.info(f\"Starting to train component {component.name}\")\n            component.prepare_partial_processing(self.pipeline[:i], context)\n            component.train(working_data, self.config, **context)\n            logger.info(\"Finished training component.\")\n\n        return Interpreter(self.pipeline, context)\n\n    @staticmethod\n    def _file_name(index: int, name: Text) -> Text:\n        return f\"component_{index}_{name}\"\n\n    def persist(\n        self,\n        path: Text,\n        persistor: Optional[Persistor] = None,\n        fixed_model_name: Text = None,\n        persist_nlu_training_data: bool = False,\n    ) -> Text:\n        \"\"\"Persist all components of the pipeline to the passed path.\n\n        Returns the directory of the persisted model.\"\"\"\n\n        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n        metadata = {\"language\": self.config[\"language\"], \"pipeline\": []}\n\n        if fixed_model_name:\n            model_name = fixed_model_name\n        else:\n            model_name = NLU_MODEL_NAME_PREFIX + timestamp\n\n        path = os.path.abspath(path)\n        dir_name = os.path.join(path, model_name)\n\n        rasa.shared.utils.io.create_directory(dir_name)\n\n        if self.training_data and persist_nlu_training_data:\n            metadata.update(self.training_data.persist(dir_name))\n\n        for i, component in enumerate(self.pipeline):\n            file_name = self._file_name(i, component.name)\n            update = component.persist(file_name, dir_name)\n            component_meta = component.component_config\n            if update:\n                component_meta.update(update)\n            component_meta[\n                \"class\"\n            ] = rasa.shared.utils.common.module_path_from_instance(component)\n\n            metadata[\"pipeline\"].append(component_meta)\n\n        Metadata(metadata).persist(dir_name)\n\n        if persistor is not None:\n            persistor.persist(dir_name, model_name)\n        logger.info(\n            \"Successfully saved model into '{}'\".format(os.path.abspath(dir_name))\n        )\n        return dir_name\n\n\nclass Interpreter:\n    \"\"\"Use a trained pipeline of components to parse text messages.\"\"\"\n\n    # Defines all attributes (& default values)\n    # that will be returned by `parse`\n    @staticmethod\n    def default_output_attributes() -> Dict[Text, Any]:\n        return {\n            TEXT: \"\",\n            INTENT: {INTENT_NAME_KEY: None, PREDICTED_CONFIDENCE_KEY: 0.0},\n            ENTITIES: [],\n        }\n\n    @staticmethod\n    def ensure_model_compatibility(\n        metadata: Metadata, version_to_check: Optional[Text] = None\n    ) -> None:\n        from packaging import version\n\n        if version_to_check is None:\n            version_to_check = MINIMUM_COMPATIBLE_VERSION\n\n        model_version = metadata.get(\"rasa_version\", \"0.0.0\")\n        if version.parse(model_version) < version.parse(version_to_check):\n            raise UnsupportedModelError(\n                f\"The model version is trained using Rasa Open Source {model_version} \"\n                f\"and is not compatible with your current installation \"\n                f\"({rasa.__version__}). \"\n                f\"This means that you either need to retrain your model \"\n                f\"or revert back to the Rasa version that trained the model \"\n                f\"to ensure that the versions match up again.\"\n            )\n\n    @staticmethod\n    def load(\n        model_dir: Text,\n        component_builder: Optional[ComponentBuilder] = None,\n        skip_validation: bool = False,\n        new_config: Optional[Dict] = None,\n        finetuning_epoch_fraction: float = 1.0,\n    ) -> \"Interpreter\":\n        \"\"\"Create an interpreter based on a persisted model.\n\n        Args:\n            skip_validation: If set to `True`, does not check that all\n                required packages for the components are installed\n                before loading them.\n            model_dir: The path of the model to load\n            component_builder: The\n                :class:`rasa.nlu.components.ComponentBuilder` to use.\n            new_config: Optional new config to use for the new epochs.\n            finetuning_epoch_fraction: Value to multiply all epochs by.\n\n        Returns:\n            An interpreter that uses the loaded model.\n        \"\"\"\n        model_metadata = Metadata.load(model_dir)\n\n        if new_config:\n            Interpreter._update_metadata_epochs(\n                model_metadata, new_config, finetuning_epoch_fraction\n            )\n\n        Interpreter.ensure_model_compatibility(model_metadata)\n        return Interpreter.create(\n            model_dir,\n            model_metadata,\n            component_builder,\n            skip_validation,\n            should_finetune=new_config is not None,\n        )\n\n    @staticmethod\n    def _get_default_value_for_component(name: Text, key: Text) -> Any:\n        from rasa.nlu.registry import get_component_class\n\n        return get_component_class(name).defaults[key]\n\n    @staticmethod\n    def _update_metadata_epochs(\n        model_metadata: Metadata,\n        new_config: Optional[Dict] = None,\n        finetuning_epoch_fraction: float = 1.0,\n    ) -> Metadata:\n        new_config = new_config or {}\n        for old_component_config, new_component_config in zip(\n            model_metadata.metadata[\"pipeline\"], new_config[\"pipeline\"]\n        ):\n            if EPOCHS in old_component_config:\n                new_epochs = new_component_config.get(\n                    EPOCHS,\n                    Interpreter._get_default_value_for_component(\n                        old_component_config[\"class\"], EPOCHS\n                    ),\n                )\n                old_component_config[EPOCHS] = ceil(\n                    new_epochs * finetuning_epoch_fraction\n                )\n        return model_metadata\n\n    @staticmethod\n    def create(\n        model_dir: Text,\n        model_metadata: Metadata,\n        component_builder: Optional[ComponentBuilder] = None,\n        skip_validation: bool = False,\n        should_finetune: bool = False,\n    ) -> \"Interpreter\":\n        \"\"\"Create model and components defined by the provided metadata.\n\n        Args:\n            model_dir: The directory containing the model.\n            model_metadata: The metadata describing each component.\n            component_builder: The\n                :class:`rasa.nlu.components.ComponentBuilder` to use.\n            skip_validation: If set to `True`, does not check that all\n                required packages for the components are installed\n                before loading them.\n            should_finetune: Indicates if the model components will be fine-tuned.\n\n        Returns:\n            An interpreter that uses the created model.\n        \"\"\"\n        context: Dict[Text, Any] = {\"should_finetune\": should_finetune}\n\n        if component_builder is None:\n            # If no builder is passed, every interpreter creation will result\n            # in a new builder. hence, no components are reused.\n            component_builder = components.ComponentBuilder()\n\n        pipeline = []\n\n        # Before instantiating the component classes,\n        # lets check if all required packages are available\n        if not skip_validation:\n            components.validate_requirements(model_metadata.component_classes)\n\n        for i in range(model_metadata.number_of_components):\n            component_meta = model_metadata.for_component(i)\n            component = component_builder.load_component(\n                component_meta, model_dir, model_metadata, **context\n            )\n            try:\n                updates = component.provide_context()\n                if updates:\n                    context.update(updates)\n                pipeline.append(component)\n            except components.MissingArgumentError as e:\n                raise Exception(\n                    \"Failed to initialize component '{}'. \"\n                    \"{}\".format(component.name, e)\n                )\n\n        return Interpreter(pipeline, context, model_metadata)\n\n    def __init__(\n        self,\n        pipeline: List[Component],\n        context: Optional[Dict[Text, Any]],\n        model_metadata: Optional[Metadata] = None,\n    ) -> None:\n\n        self.pipeline = pipeline\n        self.context = context if context is not None else {}\n        self.model_metadata = model_metadata\n        self.has_already_warned_of_overlapping_entities = False\n\n    def parse(\n        self,\n        text: Text,\n        time: Optional[datetime.datetime] = None,\n        only_output_properties: bool = True,\n    ) -> Dict[Text, Any]:\n        \"\"\"Parse the input text, classify it and return pipeline result.\n\n        The pipeline result usually contains intent and entities.\"\"\"\n\n        if not text:\n            # Not all components are able to handle empty strings. So we need\n            # to prevent that... This default return will not contain all\n            # output attributes of all components, but in the end, no one\n            # should pass an empty string in the first place.\n            output = self.default_output_attributes()\n            output[\"text\"] = \"\"\n            return output\n\n        timestamp = int(time.timestamp()) if time else None\n        data = self.default_output_attributes()\n        data[TEXT] = text\n\n        message = Message(data=data, time=timestamp, output_properties={TEXT_TOKENS})\n\n        for component in self.pipeline:\n            component.process(message, **self.context)\n\n        if not self.has_already_warned_of_overlapping_entities:\n            self.warn_of_overlapping_entities(message)\n\n        output = self.default_output_attributes()\n        output.update(message.as_dict(only_output_properties=only_output_properties))\n        return output\n\n    def featurize_message(self, message: Message) -> Message:\n        \"\"\"\n        Tokenize and featurize the input message\n        Args:\n            message: message storing text to process;\n        Returns:\n            message: it contains the tokens and features which are the output of the\n            NLU pipeline;\n        \"\"\"\n\n        for component in self.pipeline:\n            if not isinstance(component, (EntityExtractor, IntentClassifier)):\n                component.process(message, **self.context)\n        return message\n\n    def warn_of_overlapping_entities(self, message: Message) -> None:\n        \"\"\"Issues a warning when there are overlapping entity annotations.\n\n        This warning is only issued once per Interpreter life time.\n\n        Args:\n            message: user message with all processing metadata such as entities\n        \"\"\"\n        overlapping_entity_pairs = message.find_overlapping_entities()\n        if len(overlapping_entity_pairs) > 0:\n            message_text = message.get(\"text\")\n            first_pair = overlapping_entity_pairs[0]\n            entity_1 = first_pair[0]\n            entity_2 = first_pair[1]\n            rasa.shared.utils.io.raise_warning(\n                f\"Parsing of message: '{message_text}' lead to overlapping \"\n                f\"entities: {entity_1['value']} of type \"\n                f\"{entity_1['entity']} extracted by \"\n                f\"{entity_1['extractor']} overlaps with \"\n                f\"{entity_2['value']} of type {entity_2['entity']} extracted by \"\n                f\"{entity_2['extractor']}. This can lead to unintended filling of \"\n                f\"slots. Please refer to the documentation section on entity \"\n                f\"extractors and entities getting extracted multiple times:\"\n                f\"{DOCS_URL_COMPONENTS}#entity-extractors\"\n            )\n            self.has_already_warned_of_overlapping_entities = True\n",
        "source_code_len": 14724,
        "target_code": "        write_json_to_file(filename, metadata, indent=4)\n",
        "target_code_len": 57,
        "diff_format": "@@ -136,388 +117,1 @@\n         write_json_to_file(filename, metadata, indent=4)\n-\n-\n-class Trainer:\n-    \"\"\"Trainer will load the data and train all components.\n-\n-    Requires a pipeline specification and configuration to use for\n-    the training.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        cfg: RasaNLUModelConfig,\n-        component_builder: Optional[ComponentBuilder] = None,\n-        skip_validation: bool = False,\n-        model_to_finetune: Optional[\"Interpreter\"] = None,\n-    ) -> None:\n-\n-        self.config = cfg\n-        self.skip_validation = skip_validation\n-        self.training_data = None  # type: Optional[TrainingData]\n-\n-        if component_builder is None:\n-            # If no builder is passed, every interpreter creation will result in\n-            # a new builder. hence, no components are reused.\n-            component_builder = components.ComponentBuilder()\n-\n-        # Before instantiating the component classes, lets check if all\n-        # required packages are available\n-        if not self.skip_validation:\n-            components.validate_requirements(cfg.component_names)\n-\n-        if model_to_finetune:\n-            self.pipeline = model_to_finetune.pipeline\n-        else:\n-            self.pipeline = self._build_pipeline(cfg, component_builder)\n-\n-    def _build_pipeline(\n-        self, cfg: RasaNLUModelConfig, component_builder: ComponentBuilder\n-    ) -> List[Component]:\n-        \"\"\"Transform the passed names of the pipeline components into classes.\"\"\"\n-        pipeline = []\n-\n-        # Transform the passed names of the pipeline components into classes\n-        for index, pipeline_component in enumerate(cfg.pipeline):\n-            component_cfg = cfg.for_component(index)\n-            component = component_builder.create_component(component_cfg, cfg)\n-            components.validate_component_keys(component, pipeline_component)\n-            pipeline.append(component)\n-\n-        if not self.skip_validation:\n-            components.validate_pipeline(pipeline)\n-\n-        return pipeline\n-\n-    def train(self, data: TrainingData, **kwargs: Any) -> \"Interpreter\":\n-        \"\"\"Trains the underlying pipeline using the provided training data.\"\"\"\n-\n-        self.training_data = data\n-\n-        self.training_data.validate()\n-\n-        context = kwargs\n-\n-        for component in self.pipeline:\n-            updates = component.provide_context()\n-            if updates:\n-                context.update(updates)\n-\n-        # Before the training starts: check that all arguments are provided\n-        if not self.skip_validation:\n-            components.validate_required_components_from_data(\n-                self.pipeline, self.training_data\n-            )\n-\n-        # Warn if there is an obvious case of competing entity extractors\n-        components.warn_of_competing_extractors(self.pipeline)\n-        components.warn_of_competition_with_regex_extractor(\n-            self.pipeline, self.training_data\n-        )\n-\n-        # data gets modified internally during the training - hence the copy\n-        working_data: TrainingData = copy.deepcopy(data)\n-\n-        for i, component in enumerate(self.pipeline):\n-            logger.info(f\"Starting to train component {component.name}\")\n-            component.prepare_partial_processing(self.pipeline[:i], context)\n-            component.train(working_data, self.config, **context)\n-            logger.info(\"Finished training component.\")\n-\n-        return Interpreter(self.pipeline, context)\n-\n-    @staticmethod\n-    def _file_name(index: int, name: Text) -> Text:\n-        return f\"component_{index}_{name}\"\n-\n-    def persist(\n-        self,\n-        path: Text,\n-        persistor: Optional[Persistor] = None,\n-        fixed_model_name: Text = None,\n-        persist_nlu_training_data: bool = False,\n-    ) -> Text:\n-        \"\"\"Persist all components of the pipeline to the passed path.\n-\n-        Returns the directory of the persisted model.\"\"\"\n-\n-        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n-        metadata = {\"language\": self.config[\"language\"], \"pipeline\": []}\n-\n-        if fixed_model_name:\n-            model_name = fixed_model_name\n-        else:\n-            model_name = NLU_MODEL_NAME_PREFIX + timestamp\n-\n-        path = os.path.abspath(path)\n-        dir_name = os.path.join(path, model_name)\n-\n-        rasa.shared.utils.io.create_directory(dir_name)\n-\n-        if self.training_data and persist_nlu_training_data:\n-            metadata.update(self.training_data.persist(dir_name))\n-\n-        for i, component in enumerate(self.pipeline):\n-            file_name = self._file_name(i, component.name)\n-            update = component.persist(file_name, dir_name)\n-            component_meta = component.component_config\n-            if update:\n-                component_meta.update(update)\n-            component_meta[\n-                \"class\"\n-            ] = rasa.shared.utils.common.module_path_from_instance(component)\n-\n-            metadata[\"pipeline\"].append(component_meta)\n-\n-        Metadata(metadata).persist(dir_name)\n-\n-        if persistor is not None:\n-            persistor.persist(dir_name, model_name)\n-        logger.info(\n-            \"Successfully saved model into '{}'\".format(os.path.abspath(dir_name))\n-        )\n-        return dir_name\n-\n-\n-class Interpreter:\n-    \"\"\"Use a trained pipeline of components to parse text messages.\"\"\"\n-\n-    # Defines all attributes (& default values)\n-    # that will be returned by `parse`\n-    @staticmethod\n-    def default_output_attributes() -> Dict[Text, Any]:\n-        return {\n-            TEXT: \"\",\n-            INTENT: {INTENT_NAME_KEY: None, PREDICTED_CONFIDENCE_KEY: 0.0},\n-            ENTITIES: [],\n-        }\n-\n-    @staticmethod\n-    def ensure_model_compatibility(\n-        metadata: Metadata, version_to_check: Optional[Text] = None\n-    ) -> None:\n-        from packaging import version\n-\n-        if version_to_check is None:\n-            version_to_check = MINIMUM_COMPATIBLE_VERSION\n-\n-        model_version = metadata.get(\"rasa_version\", \"0.0.0\")\n-        if version.parse(model_version) < version.parse(version_to_check):\n-            raise UnsupportedModelError(\n-                f\"The model version is trained using Rasa Open Source {model_version} \"\n-                f\"and is not compatible with your current installation \"\n-                f\"({rasa.__version__}). \"\n-                f\"This means that you either need to retrain your model \"\n-                f\"or revert back to the Rasa version that trained the model \"\n-                f\"to ensure that the versions match up again.\"\n-            )\n-\n-    @staticmethod\n-    def load(\n-        model_dir: Text,\n-        component_builder: Optional[ComponentBuilder] = None,\n-        skip_validation: bool = False,\n-        new_config: Optional[Dict] = None,\n-        finetuning_epoch_fraction: float = 1.0,\n-    ) -> \"Interpreter\":\n-        \"\"\"Create an interpreter based on a persisted model.\n-\n-        Args:\n-            skip_validation: If set to `True`, does not check that all\n-                required packages for the components are installed\n-                before loading them.\n-            model_dir: The path of the model to load\n-            component_builder: The\n-                :class:`rasa.nlu.components.ComponentBuilder` to use.\n-            new_config: Optional new config to use for the new epochs.\n-            finetuning_epoch_fraction: Value to multiply all epochs by.\n-\n-        Returns:\n-            An interpreter that uses the loaded model.\n-        \"\"\"\n-        model_metadata = Metadata.load(model_dir)\n-\n-        if new_config:\n-            Interpreter._update_metadata_epochs(\n-                model_metadata, new_config, finetuning_epoch_fraction\n-            )\n-\n-        Interpreter.ensure_model_compatibility(model_metadata)\n-        return Interpreter.create(\n-            model_dir,\n-            model_metadata,\n-            component_builder,\n-            skip_validation,\n-            should_finetune=new_config is not None,\n-        )\n-\n-    @staticmethod\n-    def _get_default_value_for_component(name: Text, key: Text) -> Any:\n-        from rasa.nlu.registry import get_component_class\n-\n-        return get_component_class(name).defaults[key]\n-\n-    @staticmethod\n-    def _update_metadata_epochs(\n-        model_metadata: Metadata,\n-        new_config: Optional[Dict] = None,\n-        finetuning_epoch_fraction: float = 1.0,\n-    ) -> Metadata:\n-        new_config = new_config or {}\n-        for old_component_config, new_component_config in zip(\n-            model_metadata.metadata[\"pipeline\"], new_config[\"pipeline\"]\n-        ):\n-            if EPOCHS in old_component_config:\n-                new_epochs = new_component_config.get(\n-                    EPOCHS,\n-                    Interpreter._get_default_value_for_component(\n-                        old_component_config[\"class\"], EPOCHS\n-                    ),\n-                )\n-                old_component_config[EPOCHS] = ceil(\n-                    new_epochs * finetuning_epoch_fraction\n-                )\n-        return model_metadata\n-\n-    @staticmethod\n-    def create(\n-        model_dir: Text,\n-        model_metadata: Metadata,\n-        component_builder: Optional[ComponentBuilder] = None,\n-        skip_validation: bool = False,\n-        should_finetune: bool = False,\n-    ) -> \"Interpreter\":\n-        \"\"\"Create model and components defined by the provided metadata.\n-\n-        Args:\n-            model_dir: The directory containing the model.\n-            model_metadata: The metadata describing each component.\n-            component_builder: The\n-                :class:`rasa.nlu.components.ComponentBuilder` to use.\n-            skip_validation: If set to `True`, does not check that all\n-                required packages for the components are installed\n-                before loading them.\n-            should_finetune: Indicates if the model components will be fine-tuned.\n-\n-        Returns:\n-            An interpreter that uses the created model.\n-        \"\"\"\n-        context: Dict[Text, Any] = {\"should_finetune\": should_finetune}\n-\n-        if component_builder is None:\n-            # If no builder is passed, every interpreter creation will result\n-            # in a new builder. hence, no components are reused.\n-            component_builder = components.ComponentBuilder()\n-\n-        pipeline = []\n-\n-        # Before instantiating the component classes,\n-        # lets check if all required packages are available\n-        if not skip_validation:\n-            components.validate_requirements(model_metadata.component_classes)\n-\n-        for i in range(model_metadata.number_of_components):\n-            component_meta = model_metadata.for_component(i)\n-            component = component_builder.load_component(\n-                component_meta, model_dir, model_metadata, **context\n-            )\n-            try:\n-                updates = component.provide_context()\n-                if updates:\n-                    context.update(updates)\n-                pipeline.append(component)\n-            except components.MissingArgumentError as e:\n-                raise Exception(\n-                    \"Failed to initialize component '{}'. \"\n-                    \"{}\".format(component.name, e)\n-                )\n-\n-        return Interpreter(pipeline, context, model_metadata)\n-\n-    def __init__(\n-        self,\n-        pipeline: List[Component],\n-        context: Optional[Dict[Text, Any]],\n-        model_metadata: Optional[Metadata] = None,\n-    ) -> None:\n-\n-        self.pipeline = pipeline\n-        self.context = context if context is not None else {}\n-        self.model_metadata = model_metadata\n-        self.has_already_warned_of_overlapping_entities = False\n-\n-    def parse(\n-        self,\n-        text: Text,\n-        time: Optional[datetime.datetime] = None,\n-        only_output_properties: bool = True,\n-    ) -> Dict[Text, Any]:\n-        \"\"\"Parse the input text, classify it and return pipeline result.\n-\n-        The pipeline result usually contains intent and entities.\"\"\"\n-\n-        if not text:\n-            # Not all components are able to handle empty strings. So we need\n-            # to prevent that... This default return will not contain all\n-            # output attributes of all components, but in the end, no one\n-            # should pass an empty string in the first place.\n-            output = self.default_output_attributes()\n-            output[\"text\"] = \"\"\n-            return output\n-\n-        timestamp = int(time.timestamp()) if time else None\n-        data = self.default_output_attributes()\n-        data[TEXT] = text\n-\n-        message = Message(data=data, time=timestamp, output_properties={TEXT_TOKENS})\n-\n-        for component in self.pipeline:\n-            component.process(message, **self.context)\n-\n-        if not self.has_already_warned_of_overlapping_entities:\n-            self.warn_of_overlapping_entities(message)\n-\n-        output = self.default_output_attributes()\n-        output.update(message.as_dict(only_output_properties=only_output_properties))\n-        return output\n-\n-    def featurize_message(self, message: Message) -> Message:\n-        \"\"\"\n-        Tokenize and featurize the input message\n-        Args:\n-            message: message storing text to process;\n-        Returns:\n-            message: it contains the tokens and features which are the output of the\n-            NLU pipeline;\n-        \"\"\"\n-\n-        for component in self.pipeline:\n-            if not isinstance(component, (EntityExtractor, IntentClassifier)):\n-                component.process(message, **self.context)\n-        return message\n-\n-    def warn_of_overlapping_entities(self, message: Message) -> None:\n-        \"\"\"Issues a warning when there are overlapping entity annotations.\n-\n-        This warning is only issued once per Interpreter life time.\n-\n-        Args:\n-            message: user message with all processing metadata such as entities\n-        \"\"\"\n-        overlapping_entity_pairs = message.find_overlapping_entities()\n-        if len(overlapping_entity_pairs) > 0:\n-            message_text = message.get(\"text\")\n-            first_pair = overlapping_entity_pairs[0]\n-            entity_1 = first_pair[0]\n-            entity_2 = first_pair[1]\n-            rasa.shared.utils.io.raise_warning(\n-                f\"Parsing of message: '{message_text}' lead to overlapping \"\n-                f\"entities: {entity_1['value']} of type \"\n-                f\"{entity_1['entity']} extracted by \"\n-                f\"{entity_1['extractor']} overlaps with \"\n-                f\"{entity_2['value']} of type {entity_2['entity']} extracted by \"\n-                f\"{entity_2['extractor']}. This can lead to unintended filling of \"\n-                f\"slots. Please refer to the documentation section on entity \"\n-                f\"extractors and entities getting extracted multiple times:\"\n-                f\"{DOCS_URL_COMPONENTS}#entity-extractors\"\n-            )\n-            self.has_already_warned_of_overlapping_entities = True\n",
        "source_code_with_indent": "        write_json_to_file(filename, metadata, indent=4)\n\n\n<DED><DED>class Trainer:\n    <IND>\"\"\"Trainer will load the data and train all components.\n\n    Requires a pipeline specification and configuration to use for\n    the training.\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: RasaNLUModelConfig,\n        component_builder: Optional[ComponentBuilder] = None,\n        skip_validation: bool = False,\n        model_to_finetune: Optional[\"Interpreter\"] = None,\n    ) -> None:\n\n        <IND>self.config = cfg\n        self.skip_validation = skip_validation\n        self.training_data = None  # type: Optional[TrainingData]\n\n        if component_builder is None:\n            # If no builder is passed, every interpreter creation will result in\n            # a new builder. hence, no components are reused.\n            <IND>component_builder = components.ComponentBuilder()\n\n        # Before instantiating the component classes, lets check if all\n        # required packages are available\n        <DED>if not self.skip_validation:\n            <IND>components.validate_requirements(cfg.component_names)\n\n        <DED>if model_to_finetune:\n            <IND>self.pipeline = model_to_finetune.pipeline\n        <DED>else:\n            <IND>self.pipeline = self._build_pipeline(cfg, component_builder)\n\n    <DED><DED>def _build_pipeline(\n        self, cfg: RasaNLUModelConfig, component_builder: ComponentBuilder\n    ) -> List[Component]:\n        <IND>\"\"\"Transform the passed names of the pipeline components into classes.\"\"\"\n        pipeline = []\n\n        # Transform the passed names of the pipeline components into classes\n        for index, pipeline_component in enumerate(cfg.pipeline):\n            <IND>component_cfg = cfg.for_component(index)\n            component = component_builder.create_component(component_cfg, cfg)\n            components.validate_component_keys(component, pipeline_component)\n            pipeline.append(component)\n\n        <DED>if not self.skip_validation:\n            <IND>components.validate_pipeline(pipeline)\n\n        <DED>return pipeline\n\n    <DED>def train(self, data: TrainingData, **kwargs: Any) -> \"Interpreter\":\n        <IND>\"\"\"Trains the underlying pipeline using the provided training data.\"\"\"\n\n        self.training_data = data\n\n        self.training_data.validate()\n\n        context = kwargs\n\n        for component in self.pipeline:\n            <IND>updates = component.provide_context()\n            if updates:\n                <IND>context.update(updates)\n\n        # Before the training starts: check that all arguments are provided\n        <DED><DED>if not self.skip_validation:\n            <IND>components.validate_required_components_from_data(\n                self.pipeline, self.training_data\n            )\n\n        # Warn if there is an obvious case of competing entity extractors\n        <DED>components.warn_of_competing_extractors(self.pipeline)\n        components.warn_of_competition_with_regex_extractor(\n            self.pipeline, self.training_data\n        )\n\n        # data gets modified internally during the training - hence the copy\n        working_data: TrainingData = copy.deepcopy(data)\n\n        for i, component in enumerate(self.pipeline):\n            <IND>logger.info(f\"Starting to train component {component.name}\")\n            component.prepare_partial_processing(self.pipeline[:i], context)\n            component.train(working_data, self.config, **context)\n            logger.info(\"Finished training component.\")\n\n        <DED>return Interpreter(self.pipeline, context)\n\n    <DED>@staticmethod\n    def _file_name(index: int, name: Text) -> Text:\n        <IND>return f\"component_{index}_{name}\"\n\n    <DED>def persist(\n        self,\n        path: Text,\n        persistor: Optional[Persistor] = None,\n        fixed_model_name: Text = None,\n        persist_nlu_training_data: bool = False,\n    ) -> Text:\n        <IND>\"\"\"Persist all components of the pipeline to the passed path.\n\n        Returns the directory of the persisted model.\"\"\"\n\n        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n        metadata = {\"language\": self.config[\"language\"], \"pipeline\": []}\n\n        if fixed_model_name:\n            <IND>model_name = fixed_model_name\n        <DED>else:\n            <IND>model_name = NLU_MODEL_NAME_PREFIX + timestamp\n\n        <DED>path = os.path.abspath(path)\n        dir_name = os.path.join(path, model_name)\n\n        rasa.shared.utils.io.create_directory(dir_name)\n\n        if self.training_data and persist_nlu_training_data:\n            <IND>metadata.update(self.training_data.persist(dir_name))\n\n        <DED>for i, component in enumerate(self.pipeline):\n            <IND>file_name = self._file_name(i, component.name)\n            update = component.persist(file_name, dir_name)\n            component_meta = component.component_config\n            if update:\n                <IND>component_meta.update(update)\n            <DED>component_meta[\n                \"class\"\n            ] = rasa.shared.utils.common.module_path_from_instance(component)\n\n            metadata[\"pipeline\"].append(component_meta)\n\n        <DED>Metadata(metadata).persist(dir_name)\n\n        if persistor is not None:\n            <IND>persistor.persist(dir_name, model_name)\n        <DED>logger.info(\n            \"Successfully saved model into '{}'\".format(os.path.abspath(dir_name))\n        )\n        return dir_name\n\n\n<DED><DED>class Interpreter:\n    <IND>\"\"\"Use a trained pipeline of components to parse text messages.\"\"\"\n\n    # Defines all attributes (& default values)\n    # that will be returned by `parse`\n    @staticmethod\n    def default_output_attributes() -> Dict[Text, Any]:\n        <IND>return {\n            TEXT: \"\",\n            INTENT: {INTENT_NAME_KEY: None, PREDICTED_CONFIDENCE_KEY: 0.0},\n            ENTITIES: [],\n        }\n\n    <DED>@staticmethod\n    def ensure_model_compatibility(\n        metadata: Metadata, version_to_check: Optional[Text] = None\n    ) -> None:\n        <IND>from packaging import version\n\n        if version_to_check is None:\n            <IND>version_to_check = MINIMUM_COMPATIBLE_VERSION\n\n        <DED>model_version = metadata.get(\"rasa_version\", \"0.0.0\")\n        if version.parse(model_version) < version.parse(version_to_check):\n            <IND>raise UnsupportedModelError(\n                f\"The model version is trained using Rasa Open Source {model_version} \"\n                f\"and is not compatible with your current installation \"\n                f\"({rasa.__version__}). \"\n                f\"This means that you either need to retrain your model \"\n                f\"or revert back to the Rasa version that trained the model \"\n                f\"to ensure that the versions match up again.\"\n            )\n\n    <DED><DED>@staticmethod\n    def load(\n        model_dir: Text,\n        component_builder: Optional[ComponentBuilder] = None,\n        skip_validation: bool = False,\n        new_config: Optional[Dict] = None,\n        finetuning_epoch_fraction: float = 1.0,\n    ) -> \"Interpreter\":\n        <IND>\"\"\"Create an interpreter based on a persisted model.\n\n        Args:\n            skip_validation: If set to `True`, does not check that all\n                required packages for the components are installed\n                before loading them.\n            model_dir: The path of the model to load\n            component_builder: The\n                :class:`rasa.nlu.components.ComponentBuilder` to use.\n            new_config: Optional new config to use for the new epochs.\n            finetuning_epoch_fraction: Value to multiply all epochs by.\n\n        Returns:\n            An interpreter that uses the loaded model.\n        \"\"\"\n        model_metadata = Metadata.load(model_dir)\n\n        if new_config:\n            <IND>Interpreter._update_metadata_epochs(\n                model_metadata, new_config, finetuning_epoch_fraction\n            )\n\n        <DED>Interpreter.ensure_model_compatibility(model_metadata)\n        return Interpreter.create(\n            model_dir,\n            model_metadata,\n            component_builder,\n            skip_validation,\n            should_finetune=new_config is not None,\n        )\n\n    <DED>@staticmethod\n    def _get_default_value_for_component(name: Text, key: Text) -> Any:\n        <IND>from rasa.nlu.registry import get_component_class\n\n        return get_component_class(name).defaults[key]\n\n    <DED>@staticmethod\n    def _update_metadata_epochs(\n        model_metadata: Metadata,\n        new_config: Optional[Dict] = None,\n        finetuning_epoch_fraction: float = 1.0,\n    ) -> Metadata:\n        <IND>new_config = new_config or {}\n        for old_component_config, new_component_config in zip(\n            model_metadata.metadata[\"pipeline\"], new_config[\"pipeline\"]\n        ):\n            <IND>if EPOCHS in old_component_config:\n                <IND>new_epochs = new_component_config.get(\n                    EPOCHS,\n                    Interpreter._get_default_value_for_component(\n                        old_component_config[\"class\"], EPOCHS\n                    ),\n                )\n                old_component_config[EPOCHS] = ceil(\n                    new_epochs * finetuning_epoch_fraction\n                )\n        <DED><DED>return model_metadata\n\n    <DED>@staticmethod\n    def create(\n        model_dir: Text,\n        model_metadata: Metadata,\n        component_builder: Optional[ComponentBuilder] = None,\n        skip_validation: bool = False,\n        should_finetune: bool = False,\n    ) -> \"Interpreter\":\n        <IND>\"\"\"Create model and components defined by the provided metadata.\n\n        Args:\n            model_dir: The directory containing the model.\n            model_metadata: The metadata describing each component.\n            component_builder: The\n                :class:`rasa.nlu.components.ComponentBuilder` to use.\n            skip_validation: If set to `True`, does not check that all\n                required packages for the components are installed\n                before loading them.\n            should_finetune: Indicates if the model components will be fine-tuned.\n\n        Returns:\n            An interpreter that uses the created model.\n        \"\"\"\n        context: Dict[Text, Any] = {\"should_finetune\": should_finetune}\n\n        if component_builder is None:\n            # If no builder is passed, every interpreter creation will result\n            # in a new builder. hence, no components are reused.\n            <IND>component_builder = components.ComponentBuilder()\n\n        <DED>pipeline = []\n\n        # Before instantiating the component classes,\n        # lets check if all required packages are available\n        if not skip_validation:\n            <IND>components.validate_requirements(model_metadata.component_classes)\n\n        <DED>for i in range(model_metadata.number_of_components):\n            <IND>component_meta = model_metadata.for_component(i)\n            component = component_builder.load_component(\n                component_meta, model_dir, model_metadata, **context\n            )\n            try:\n                <IND>updates = component.provide_context()\n                if updates:\n                    <IND>context.update(updates)\n                <DED>pipeline.append(component)\n            <DED>except components.MissingArgumentError as e:\n                <IND>raise Exception(\n                    \"Failed to initialize component '{}'. \"\n                    \"{}\".format(component.name, e)\n                )\n\n        <DED><DED>return Interpreter(pipeline, context, model_metadata)\n\n    <DED>def __init__(\n        self,\n        pipeline: List[Component],\n        context: Optional[Dict[Text, Any]],\n        model_metadata: Optional[Metadata] = None,\n    ) -> None:\n\n        <IND>self.pipeline = pipeline\n        self.context = context if context is not None else {}\n        self.model_metadata = model_metadata\n        self.has_already_warned_of_overlapping_entities = False\n\n    <DED>def parse(\n        self,\n        text: Text,\n        time: Optional[datetime.datetime] = None,\n        only_output_properties: bool = True,\n    ) -> Dict[Text, Any]:\n        <IND>\"\"\"Parse the input text, classify it and return pipeline result.\n\n        The pipeline result usually contains intent and entities.\"\"\"\n\n        if not text:\n            # Not all components are able to handle empty strings. So we need\n            # to prevent that... This default return will not contain all\n            # output attributes of all components, but in the end, no one\n            # should pass an empty string in the first place.\n            <IND>output = self.default_output_attributes()\n            output[\"text\"] = \"\"\n            return output\n\n        <DED>timestamp = int(time.timestamp()) if time else None\n        data = self.default_output_attributes()\n        data[TEXT] = text\n\n        message = Message(data=data, time=timestamp, output_properties={TEXT_TOKENS})\n\n        for component in self.pipeline:\n            <IND>component.process(message, **self.context)\n\n        <DED>if not self.has_already_warned_of_overlapping_entities:\n            <IND>self.warn_of_overlapping_entities(message)\n\n        <DED>output = self.default_output_attributes()\n        output.update(message.as_dict(only_output_properties=only_output_properties))\n        return output\n\n    <DED>def featurize_message(self, message: Message) -> Message:\n        <IND>\"\"\"\n        Tokenize and featurize the input message\n        Args:\n            message: message storing text to process;\n        Returns:\n            message: it contains the tokens and features which are the output of the\n            NLU pipeline;\n        \"\"\"\n\n        for component in self.pipeline:\n            <IND>if not isinstance(component, (EntityExtractor, IntentClassifier)):\n                <IND>component.process(message, **self.context)\n        <DED><DED>return message\n\n    <DED>def warn_of_overlapping_entities(self, message: Message) -> None:\n        <IND>\"\"\"Issues a warning when there are overlapping entity annotations.\n\n        This warning is only issued once per Interpreter life time.\n\n        Args:\n            message: user message with all processing metadata such as entities\n        \"\"\"\n        overlapping_entity_pairs = message.find_overlapping_entities()\n        if len(overlapping_entity_pairs) > 0:\n            <IND>message_text = message.get(\"text\")\n            first_pair = overlapping_entity_pairs[0]\n            entity_1 = first_pair[0]\n            entity_2 = first_pair[1]\n            rasa.shared.utils.io.raise_warning(\n                f\"Parsing of message: '{message_text}' lead to overlapping \"\n                f\"entities: {entity_1['value']} of type \"\n                f\"{entity_1['entity']} extracted by \"\n                f\"{entity_1['extractor']} overlaps with \"\n                f\"{entity_2['value']} of type {entity_2['entity']} extracted by \"\n                f\"{entity_2['extractor']}. This can lead to unintended filling of \"\n                f\"slots. Please refer to the documentation section on entity \"\n                f\"extractors and entities getting extracted multiple times:\"\n                f\"{DOCS_URL_COMPONENTS}#entity-extractors\"\n            )\n            self.has_already_warned_of_overlapping_entities = True\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        write_json_to_file(filename, metadata, indent=4)\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "rasa/nlu/model.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/RasaHQ-rasa/rasa/nlu/model.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "rasa/nlu/model.py:214:27 Incompatible parameter type [6]: Expected `TrainingData` for 2nd positional only parameter to call `components.warn_of_competition_with_regex_extractor` but got `Optional[TrainingData]`.",
    "message": " Expected `TrainingData` for 2nd positional only parameter to call `components.warn_of_competition_with_regex_extractor` but got `Optional[TrainingData]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 214,
    "warning_line": "            self.pipeline, self.training_data",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        write_json_to_file(filename, metadata, indent=4)\n\n\nclass Trainer:\n    \"\"\"Trainer will load the data and train all components.\n\n    Requires a pipeline specification and configuration to use for\n    the training.\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: RasaNLUModelConfig,\n        component_builder: Optional[ComponentBuilder] = None,\n        skip_validation: bool = False,\n        model_to_finetune: Optional[\"Interpreter\"] = None,\n    ) -> None:\n\n        self.config = cfg\n        self.skip_validation = skip_validation\n        self.training_data = None  # type: Optional[TrainingData]\n\n        if component_builder is None:\n            # If no builder is passed, every interpreter creation will result in\n            # a new builder. hence, no components are reused.\n            component_builder = components.ComponentBuilder()\n\n        # Before instantiating the component classes, lets check if all\n        # required packages are available\n        if not self.skip_validation:\n            components.validate_requirements(cfg.component_names)\n\n        if model_to_finetune:\n            self.pipeline = model_to_finetune.pipeline\n        else:\n            self.pipeline = self._build_pipeline(cfg, component_builder)\n\n    def _build_pipeline(\n        self, cfg: RasaNLUModelConfig, component_builder: ComponentBuilder\n    ) -> List[Component]:\n        \"\"\"Transform the passed names of the pipeline components into classes.\"\"\"\n        pipeline = []\n\n        # Transform the passed names of the pipeline components into classes\n        for index, pipeline_component in enumerate(cfg.pipeline):\n            component_cfg = cfg.for_component(index)\n            component = component_builder.create_component(component_cfg, cfg)\n            components.validate_component_keys(component, pipeline_component)\n            pipeline.append(component)\n\n        if not self.skip_validation:\n            components.validate_pipeline(pipeline)\n\n        return pipeline\n\n    def train(self, data: TrainingData, **kwargs: Any) -> \"Interpreter\":\n        \"\"\"Trains the underlying pipeline using the provided training data.\"\"\"\n\n        self.training_data = data\n\n        self.training_data.validate()\n\n        context = kwargs\n\n        for component in self.pipeline:\n            updates = component.provide_context()\n            if updates:\n                context.update(updates)\n\n        # Before the training starts: check that all arguments are provided\n        if not self.skip_validation:\n            components.validate_required_components_from_data(\n                self.pipeline, self.training_data\n            )\n\n        # Warn if there is an obvious case of competing entity extractors\n        components.warn_of_competing_extractors(self.pipeline)\n        components.warn_of_competition_with_regex_extractor(\n            self.pipeline, self.training_data\n        )\n\n        # data gets modified internally during the training - hence the copy\n        working_data: TrainingData = copy.deepcopy(data)\n\n        for i, component in enumerate(self.pipeline):\n            logger.info(f\"Starting to train component {component.name}\")\n            component.prepare_partial_processing(self.pipeline[:i], context)\n            component.train(working_data, self.config, **context)\n            logger.info(\"Finished training component.\")\n\n        return Interpreter(self.pipeline, context)\n\n    @staticmethod\n    def _file_name(index: int, name: Text) -> Text:\n        return f\"component_{index}_{name}\"\n\n    def persist(\n        self,\n        path: Text,\n        persistor: Optional[Persistor] = None,\n        fixed_model_name: Text = None,\n        persist_nlu_training_data: bool = False,\n    ) -> Text:\n        \"\"\"Persist all components of the pipeline to the passed path.\n\n        Returns the directory of the persisted model.\"\"\"\n\n        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n        metadata = {\"language\": self.config[\"language\"], \"pipeline\": []}\n\n        if fixed_model_name:\n            model_name = fixed_model_name\n        else:\n            model_name = NLU_MODEL_NAME_PREFIX + timestamp\n\n        path = os.path.abspath(path)\n        dir_name = os.path.join(path, model_name)\n\n        rasa.shared.utils.io.create_directory(dir_name)\n\n        if self.training_data and persist_nlu_training_data:\n            metadata.update(self.training_data.persist(dir_name))\n\n        for i, component in enumerate(self.pipeline):\n            file_name = self._file_name(i, component.name)\n            update = component.persist(file_name, dir_name)\n            component_meta = component.component_config\n            if update:\n                component_meta.update(update)\n            component_meta[\n                \"class\"\n            ] = rasa.shared.utils.common.module_path_from_instance(component)\n\n            metadata[\"pipeline\"].append(component_meta)\n\n        Metadata(metadata).persist(dir_name)\n\n        if persistor is not None:\n            persistor.persist(dir_name, model_name)\n        logger.info(\n            \"Successfully saved model into '{}'\".format(os.path.abspath(dir_name))\n        )\n        return dir_name\n\n\nclass Interpreter:\n    \"\"\"Use a trained pipeline of components to parse text messages.\"\"\"\n\n    # Defines all attributes (& default values)\n    # that will be returned by `parse`\n    @staticmethod\n    def default_output_attributes() -> Dict[Text, Any]:\n        return {\n            TEXT: \"\",\n            INTENT: {INTENT_NAME_KEY: None, PREDICTED_CONFIDENCE_KEY: 0.0},\n            ENTITIES: [],\n        }\n\n    @staticmethod\n    def ensure_model_compatibility(\n        metadata: Metadata, version_to_check: Optional[Text] = None\n    ) -> None:\n        from packaging import version\n\n        if version_to_check is None:\n            version_to_check = MINIMUM_COMPATIBLE_VERSION\n\n        model_version = metadata.get(\"rasa_version\", \"0.0.0\")\n        if version.parse(model_version) < version.parse(version_to_check):\n            raise UnsupportedModelError(\n                f\"The model version is trained using Rasa Open Source {model_version} \"\n                f\"and is not compatible with your current installation \"\n                f\"({rasa.__version__}). \"\n                f\"This means that you either need to retrain your model \"\n                f\"or revert back to the Rasa version that trained the model \"\n                f\"to ensure that the versions match up again.\"\n            )\n\n    @staticmethod\n    def load(\n        model_dir: Text,\n        component_builder: Optional[ComponentBuilder] = None,\n        skip_validation: bool = False,\n        new_config: Optional[Dict] = None,\n        finetuning_epoch_fraction: float = 1.0,\n    ) -> \"Interpreter\":\n        \"\"\"Create an interpreter based on a persisted model.\n\n        Args:\n            skip_validation: If set to `True`, does not check that all\n                required packages for the components are installed\n                before loading them.\n            model_dir: The path of the model to load\n            component_builder: The\n                :class:`rasa.nlu.components.ComponentBuilder` to use.\n            new_config: Optional new config to use for the new epochs.\n            finetuning_epoch_fraction: Value to multiply all epochs by.\n\n        Returns:\n            An interpreter that uses the loaded model.\n        \"\"\"\n        model_metadata = Metadata.load(model_dir)\n\n        if new_config:\n            Interpreter._update_metadata_epochs(\n                model_metadata, new_config, finetuning_epoch_fraction\n            )\n\n        Interpreter.ensure_model_compatibility(model_metadata)\n        return Interpreter.create(\n            model_dir,\n            model_metadata,\n            component_builder,\n            skip_validation,\n            should_finetune=new_config is not None,\n        )\n\n    @staticmethod\n    def _get_default_value_for_component(name: Text, key: Text) -> Any:\n        from rasa.nlu.registry import get_component_class\n\n        return get_component_class(name).defaults[key]\n\n    @staticmethod\n    def _update_metadata_epochs(\n        model_metadata: Metadata,\n        new_config: Optional[Dict] = None,\n        finetuning_epoch_fraction: float = 1.0,\n    ) -> Metadata:\n        new_config = new_config or {}\n        for old_component_config, new_component_config in zip(\n            model_metadata.metadata[\"pipeline\"], new_config[\"pipeline\"]\n        ):\n            if EPOCHS in old_component_config:\n                new_epochs = new_component_config.get(\n                    EPOCHS,\n                    Interpreter._get_default_value_for_component(\n                        old_component_config[\"class\"], EPOCHS\n                    ),\n                )\n                old_component_config[EPOCHS] = ceil(\n                    new_epochs * finetuning_epoch_fraction\n                )\n        return model_metadata\n\n    @staticmethod\n    def create(\n        model_dir: Text,\n        model_metadata: Metadata,\n        component_builder: Optional[ComponentBuilder] = None,\n        skip_validation: bool = False,\n        should_finetune: bool = False,\n    ) -> \"Interpreter\":\n        \"\"\"Create model and components defined by the provided metadata.\n\n        Args:\n            model_dir: The directory containing the model.\n            model_metadata: The metadata describing each component.\n            component_builder: The\n                :class:`rasa.nlu.components.ComponentBuilder` to use.\n            skip_validation: If set to `True`, does not check that all\n                required packages for the components are installed\n                before loading them.\n            should_finetune: Indicates if the model components will be fine-tuned.\n\n        Returns:\n            An interpreter that uses the created model.\n        \"\"\"\n        context: Dict[Text, Any] = {\"should_finetune\": should_finetune}\n\n        if component_builder is None:\n            # If no builder is passed, every interpreter creation will result\n            # in a new builder. hence, no components are reused.\n            component_builder = components.ComponentBuilder()\n\n        pipeline = []\n\n        # Before instantiating the component classes,\n        # lets check if all required packages are available\n        if not skip_validation:\n            components.validate_requirements(model_metadata.component_classes)\n\n        for i in range(model_metadata.number_of_components):\n            component_meta = model_metadata.for_component(i)\n            component = component_builder.load_component(\n                component_meta, model_dir, model_metadata, **context\n            )\n            try:\n                updates = component.provide_context()\n                if updates:\n                    context.update(updates)\n                pipeline.append(component)\n            except components.MissingArgumentError as e:\n                raise Exception(\n                    \"Failed to initialize component '{}'. \"\n                    \"{}\".format(component.name, e)\n                )\n\n        return Interpreter(pipeline, context, model_metadata)\n\n    def __init__(\n        self,\n        pipeline: List[Component],\n        context: Optional[Dict[Text, Any]],\n        model_metadata: Optional[Metadata] = None,\n    ) -> None:\n\n        self.pipeline = pipeline\n        self.context = context if context is not None else {}\n        self.model_metadata = model_metadata\n        self.has_already_warned_of_overlapping_entities = False\n\n    def parse(\n        self,\n        text: Text,\n        time: Optional[datetime.datetime] = None,\n        only_output_properties: bool = True,\n    ) -> Dict[Text, Any]:\n        \"\"\"Parse the input text, classify it and return pipeline result.\n\n        The pipeline result usually contains intent and entities.\"\"\"\n\n        if not text:\n            # Not all components are able to handle empty strings. So we need\n            # to prevent that... This default return will not contain all\n            # output attributes of all components, but in the end, no one\n            # should pass an empty string in the first place.\n            output = self.default_output_attributes()\n            output[\"text\"] = \"\"\n            return output\n\n        timestamp = int(time.timestamp()) if time else None\n        data = self.default_output_attributes()\n        data[TEXT] = text\n\n        message = Message(data=data, time=timestamp, output_properties={TEXT_TOKENS})\n\n        for component in self.pipeline:\n            component.process(message, **self.context)\n\n        if not self.has_already_warned_of_overlapping_entities:\n            self.warn_of_overlapping_entities(message)\n\n        output = self.default_output_attributes()\n        output.update(message.as_dict(only_output_properties=only_output_properties))\n        return output\n\n    def featurize_message(self, message: Message) -> Message:\n        \"\"\"\n        Tokenize and featurize the input message\n        Args:\n            message: message storing text to process;\n        Returns:\n            message: it contains the tokens and features which are the output of the\n            NLU pipeline;\n        \"\"\"\n\n        for component in self.pipeline:\n            if not isinstance(component, (EntityExtractor, IntentClassifier)):\n                component.process(message, **self.context)\n        return message\n\n    def warn_of_overlapping_entities(self, message: Message) -> None:\n        \"\"\"Issues a warning when there are overlapping entity annotations.\n\n        This warning is only issued once per Interpreter life time.\n\n        Args:\n            message: user message with all processing metadata such as entities\n        \"\"\"\n        overlapping_entity_pairs = message.find_overlapping_entities()\n        if len(overlapping_entity_pairs) > 0:\n            message_text = message.get(\"text\")\n            first_pair = overlapping_entity_pairs[0]\n            entity_1 = first_pair[0]\n            entity_2 = first_pair[1]\n            rasa.shared.utils.io.raise_warning(\n                f\"Parsing of message: '{message_text}' lead to overlapping \"\n                f\"entities: {entity_1['value']} of type \"\n                f\"{entity_1['entity']} extracted by \"\n                f\"{entity_1['extractor']} overlaps with \"\n                f\"{entity_2['value']} of type {entity_2['entity']} extracted by \"\n                f\"{entity_2['extractor']}. This can lead to unintended filling of \"\n                f\"slots. Please refer to the documentation section on entity \"\n                f\"extractors and entities getting extracted multiple times:\"\n                f\"{DOCS_URL_COMPONENTS}#entity-extractors\"\n            )\n            self.has_already_warned_of_overlapping_entities = True\n",
        "source_code_len": 14724,
        "target_code": "        write_json_to_file(filename, metadata, indent=4)\n",
        "target_code_len": 57,
        "diff_format": "@@ -136,388 +117,1 @@\n         write_json_to_file(filename, metadata, indent=4)\n-\n-\n-class Trainer:\n-    \"\"\"Trainer will load the data and train all components.\n-\n-    Requires a pipeline specification and configuration to use for\n-    the training.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        cfg: RasaNLUModelConfig,\n-        component_builder: Optional[ComponentBuilder] = None,\n-        skip_validation: bool = False,\n-        model_to_finetune: Optional[\"Interpreter\"] = None,\n-    ) -> None:\n-\n-        self.config = cfg\n-        self.skip_validation = skip_validation\n-        self.training_data = None  # type: Optional[TrainingData]\n-\n-        if component_builder is None:\n-            # If no builder is passed, every interpreter creation will result in\n-            # a new builder. hence, no components are reused.\n-            component_builder = components.ComponentBuilder()\n-\n-        # Before instantiating the component classes, lets check if all\n-        # required packages are available\n-        if not self.skip_validation:\n-            components.validate_requirements(cfg.component_names)\n-\n-        if model_to_finetune:\n-            self.pipeline = model_to_finetune.pipeline\n-        else:\n-            self.pipeline = self._build_pipeline(cfg, component_builder)\n-\n-    def _build_pipeline(\n-        self, cfg: RasaNLUModelConfig, component_builder: ComponentBuilder\n-    ) -> List[Component]:\n-        \"\"\"Transform the passed names of the pipeline components into classes.\"\"\"\n-        pipeline = []\n-\n-        # Transform the passed names of the pipeline components into classes\n-        for index, pipeline_component in enumerate(cfg.pipeline):\n-            component_cfg = cfg.for_component(index)\n-            component = component_builder.create_component(component_cfg, cfg)\n-            components.validate_component_keys(component, pipeline_component)\n-            pipeline.append(component)\n-\n-        if not self.skip_validation:\n-            components.validate_pipeline(pipeline)\n-\n-        return pipeline\n-\n-    def train(self, data: TrainingData, **kwargs: Any) -> \"Interpreter\":\n-        \"\"\"Trains the underlying pipeline using the provided training data.\"\"\"\n-\n-        self.training_data = data\n-\n-        self.training_data.validate()\n-\n-        context = kwargs\n-\n-        for component in self.pipeline:\n-            updates = component.provide_context()\n-            if updates:\n-                context.update(updates)\n-\n-        # Before the training starts: check that all arguments are provided\n-        if not self.skip_validation:\n-            components.validate_required_components_from_data(\n-                self.pipeline, self.training_data\n-            )\n-\n-        # Warn if there is an obvious case of competing entity extractors\n-        components.warn_of_competing_extractors(self.pipeline)\n-        components.warn_of_competition_with_regex_extractor(\n-            self.pipeline, self.training_data\n-        )\n-\n-        # data gets modified internally during the training - hence the copy\n-        working_data: TrainingData = copy.deepcopy(data)\n-\n-        for i, component in enumerate(self.pipeline):\n-            logger.info(f\"Starting to train component {component.name}\")\n-            component.prepare_partial_processing(self.pipeline[:i], context)\n-            component.train(working_data, self.config, **context)\n-            logger.info(\"Finished training component.\")\n-\n-        return Interpreter(self.pipeline, context)\n-\n-    @staticmethod\n-    def _file_name(index: int, name: Text) -> Text:\n-        return f\"component_{index}_{name}\"\n-\n-    def persist(\n-        self,\n-        path: Text,\n-        persistor: Optional[Persistor] = None,\n-        fixed_model_name: Text = None,\n-        persist_nlu_training_data: bool = False,\n-    ) -> Text:\n-        \"\"\"Persist all components of the pipeline to the passed path.\n-\n-        Returns the directory of the persisted model.\"\"\"\n-\n-        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n-        metadata = {\"language\": self.config[\"language\"], \"pipeline\": []}\n-\n-        if fixed_model_name:\n-            model_name = fixed_model_name\n-        else:\n-            model_name = NLU_MODEL_NAME_PREFIX + timestamp\n-\n-        path = os.path.abspath(path)\n-        dir_name = os.path.join(path, model_name)\n-\n-        rasa.shared.utils.io.create_directory(dir_name)\n-\n-        if self.training_data and persist_nlu_training_data:\n-            metadata.update(self.training_data.persist(dir_name))\n-\n-        for i, component in enumerate(self.pipeline):\n-            file_name = self._file_name(i, component.name)\n-            update = component.persist(file_name, dir_name)\n-            component_meta = component.component_config\n-            if update:\n-                component_meta.update(update)\n-            component_meta[\n-                \"class\"\n-            ] = rasa.shared.utils.common.module_path_from_instance(component)\n-\n-            metadata[\"pipeline\"].append(component_meta)\n-\n-        Metadata(metadata).persist(dir_name)\n-\n-        if persistor is not None:\n-            persistor.persist(dir_name, model_name)\n-        logger.info(\n-            \"Successfully saved model into '{}'\".format(os.path.abspath(dir_name))\n-        )\n-        return dir_name\n-\n-\n-class Interpreter:\n-    \"\"\"Use a trained pipeline of components to parse text messages.\"\"\"\n-\n-    # Defines all attributes (& default values)\n-    # that will be returned by `parse`\n-    @staticmethod\n-    def default_output_attributes() -> Dict[Text, Any]:\n-        return {\n-            TEXT: \"\",\n-            INTENT: {INTENT_NAME_KEY: None, PREDICTED_CONFIDENCE_KEY: 0.0},\n-            ENTITIES: [],\n-        }\n-\n-    @staticmethod\n-    def ensure_model_compatibility(\n-        metadata: Metadata, version_to_check: Optional[Text] = None\n-    ) -> None:\n-        from packaging import version\n-\n-        if version_to_check is None:\n-            version_to_check = MINIMUM_COMPATIBLE_VERSION\n-\n-        model_version = metadata.get(\"rasa_version\", \"0.0.0\")\n-        if version.parse(model_version) < version.parse(version_to_check):\n-            raise UnsupportedModelError(\n-                f\"The model version is trained using Rasa Open Source {model_version} \"\n-                f\"and is not compatible with your current installation \"\n-                f\"({rasa.__version__}). \"\n-                f\"This means that you either need to retrain your model \"\n-                f\"or revert back to the Rasa version that trained the model \"\n-                f\"to ensure that the versions match up again.\"\n-            )\n-\n-    @staticmethod\n-    def load(\n-        model_dir: Text,\n-        component_builder: Optional[ComponentBuilder] = None,\n-        skip_validation: bool = False,\n-        new_config: Optional[Dict] = None,\n-        finetuning_epoch_fraction: float = 1.0,\n-    ) -> \"Interpreter\":\n-        \"\"\"Create an interpreter based on a persisted model.\n-\n-        Args:\n-            skip_validation: If set to `True`, does not check that all\n-                required packages for the components are installed\n-                before loading them.\n-            model_dir: The path of the model to load\n-            component_builder: The\n-                :class:`rasa.nlu.components.ComponentBuilder` to use.\n-            new_config: Optional new config to use for the new epochs.\n-            finetuning_epoch_fraction: Value to multiply all epochs by.\n-\n-        Returns:\n-            An interpreter that uses the loaded model.\n-        \"\"\"\n-        model_metadata = Metadata.load(model_dir)\n-\n-        if new_config:\n-            Interpreter._update_metadata_epochs(\n-                model_metadata, new_config, finetuning_epoch_fraction\n-            )\n-\n-        Interpreter.ensure_model_compatibility(model_metadata)\n-        return Interpreter.create(\n-            model_dir,\n-            model_metadata,\n-            component_builder,\n-            skip_validation,\n-            should_finetune=new_config is not None,\n-        )\n-\n-    @staticmethod\n-    def _get_default_value_for_component(name: Text, key: Text) -> Any:\n-        from rasa.nlu.registry import get_component_class\n-\n-        return get_component_class(name).defaults[key]\n-\n-    @staticmethod\n-    def _update_metadata_epochs(\n-        model_metadata: Metadata,\n-        new_config: Optional[Dict] = None,\n-        finetuning_epoch_fraction: float = 1.0,\n-    ) -> Metadata:\n-        new_config = new_config or {}\n-        for old_component_config, new_component_config in zip(\n-            model_metadata.metadata[\"pipeline\"], new_config[\"pipeline\"]\n-        ):\n-            if EPOCHS in old_component_config:\n-                new_epochs = new_component_config.get(\n-                    EPOCHS,\n-                    Interpreter._get_default_value_for_component(\n-                        old_component_config[\"class\"], EPOCHS\n-                    ),\n-                )\n-                old_component_config[EPOCHS] = ceil(\n-                    new_epochs * finetuning_epoch_fraction\n-                )\n-        return model_metadata\n-\n-    @staticmethod\n-    def create(\n-        model_dir: Text,\n-        model_metadata: Metadata,\n-        component_builder: Optional[ComponentBuilder] = None,\n-        skip_validation: bool = False,\n-        should_finetune: bool = False,\n-    ) -> \"Interpreter\":\n-        \"\"\"Create model and components defined by the provided metadata.\n-\n-        Args:\n-            model_dir: The directory containing the model.\n-            model_metadata: The metadata describing each component.\n-            component_builder: The\n-                :class:`rasa.nlu.components.ComponentBuilder` to use.\n-            skip_validation: If set to `True`, does not check that all\n-                required packages for the components are installed\n-                before loading them.\n-            should_finetune: Indicates if the model components will be fine-tuned.\n-\n-        Returns:\n-            An interpreter that uses the created model.\n-        \"\"\"\n-        context: Dict[Text, Any] = {\"should_finetune\": should_finetune}\n-\n-        if component_builder is None:\n-            # If no builder is passed, every interpreter creation will result\n-            # in a new builder. hence, no components are reused.\n-            component_builder = components.ComponentBuilder()\n-\n-        pipeline = []\n-\n-        # Before instantiating the component classes,\n-        # lets check if all required packages are available\n-        if not skip_validation:\n-            components.validate_requirements(model_metadata.component_classes)\n-\n-        for i in range(model_metadata.number_of_components):\n-            component_meta = model_metadata.for_component(i)\n-            component = component_builder.load_component(\n-                component_meta, model_dir, model_metadata, **context\n-            )\n-            try:\n-                updates = component.provide_context()\n-                if updates:\n-                    context.update(updates)\n-                pipeline.append(component)\n-            except components.MissingArgumentError as e:\n-                raise Exception(\n-                    \"Failed to initialize component '{}'. \"\n-                    \"{}\".format(component.name, e)\n-                )\n-\n-        return Interpreter(pipeline, context, model_metadata)\n-\n-    def __init__(\n-        self,\n-        pipeline: List[Component],\n-        context: Optional[Dict[Text, Any]],\n-        model_metadata: Optional[Metadata] = None,\n-    ) -> None:\n-\n-        self.pipeline = pipeline\n-        self.context = context if context is not None else {}\n-        self.model_metadata = model_metadata\n-        self.has_already_warned_of_overlapping_entities = False\n-\n-    def parse(\n-        self,\n-        text: Text,\n-        time: Optional[datetime.datetime] = None,\n-        only_output_properties: bool = True,\n-    ) -> Dict[Text, Any]:\n-        \"\"\"Parse the input text, classify it and return pipeline result.\n-\n-        The pipeline result usually contains intent and entities.\"\"\"\n-\n-        if not text:\n-            # Not all components are able to handle empty strings. So we need\n-            # to prevent that... This default return will not contain all\n-            # output attributes of all components, but in the end, no one\n-            # should pass an empty string in the first place.\n-            output = self.default_output_attributes()\n-            output[\"text\"] = \"\"\n-            return output\n-\n-        timestamp = int(time.timestamp()) if time else None\n-        data = self.default_output_attributes()\n-        data[TEXT] = text\n-\n-        message = Message(data=data, time=timestamp, output_properties={TEXT_TOKENS})\n-\n-        for component in self.pipeline:\n-            component.process(message, **self.context)\n-\n-        if not self.has_already_warned_of_overlapping_entities:\n-            self.warn_of_overlapping_entities(message)\n-\n-        output = self.default_output_attributes()\n-        output.update(message.as_dict(only_output_properties=only_output_properties))\n-        return output\n-\n-    def featurize_message(self, message: Message) -> Message:\n-        \"\"\"\n-        Tokenize and featurize the input message\n-        Args:\n-            message: message storing text to process;\n-        Returns:\n-            message: it contains the tokens and features which are the output of the\n-            NLU pipeline;\n-        \"\"\"\n-\n-        for component in self.pipeline:\n-            if not isinstance(component, (EntityExtractor, IntentClassifier)):\n-                component.process(message, **self.context)\n-        return message\n-\n-    def warn_of_overlapping_entities(self, message: Message) -> None:\n-        \"\"\"Issues a warning when there are overlapping entity annotations.\n-\n-        This warning is only issued once per Interpreter life time.\n-\n-        Args:\n-            message: user message with all processing metadata such as entities\n-        \"\"\"\n-        overlapping_entity_pairs = message.find_overlapping_entities()\n-        if len(overlapping_entity_pairs) > 0:\n-            message_text = message.get(\"text\")\n-            first_pair = overlapping_entity_pairs[0]\n-            entity_1 = first_pair[0]\n-            entity_2 = first_pair[1]\n-            rasa.shared.utils.io.raise_warning(\n-                f\"Parsing of message: '{message_text}' lead to overlapping \"\n-                f\"entities: {entity_1['value']} of type \"\n-                f\"{entity_1['entity']} extracted by \"\n-                f\"{entity_1['extractor']} overlaps with \"\n-                f\"{entity_2['value']} of type {entity_2['entity']} extracted by \"\n-                f\"{entity_2['extractor']}. This can lead to unintended filling of \"\n-                f\"slots. Please refer to the documentation section on entity \"\n-                f\"extractors and entities getting extracted multiple times:\"\n-                f\"{DOCS_URL_COMPONENTS}#entity-extractors\"\n-            )\n-            self.has_already_warned_of_overlapping_entities = True\n",
        "source_code_with_indent": "        write_json_to_file(filename, metadata, indent=4)\n\n\n<DED><DED>class Trainer:\n    <IND>\"\"\"Trainer will load the data and train all components.\n\n    Requires a pipeline specification and configuration to use for\n    the training.\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: RasaNLUModelConfig,\n        component_builder: Optional[ComponentBuilder] = None,\n        skip_validation: bool = False,\n        model_to_finetune: Optional[\"Interpreter\"] = None,\n    ) -> None:\n\n        <IND>self.config = cfg\n        self.skip_validation = skip_validation\n        self.training_data = None  # type: Optional[TrainingData]\n\n        if component_builder is None:\n            # If no builder is passed, every interpreter creation will result in\n            # a new builder. hence, no components are reused.\n            <IND>component_builder = components.ComponentBuilder()\n\n        # Before instantiating the component classes, lets check if all\n        # required packages are available\n        <DED>if not self.skip_validation:\n            <IND>components.validate_requirements(cfg.component_names)\n\n        <DED>if model_to_finetune:\n            <IND>self.pipeline = model_to_finetune.pipeline\n        <DED>else:\n            <IND>self.pipeline = self._build_pipeline(cfg, component_builder)\n\n    <DED><DED>def _build_pipeline(\n        self, cfg: RasaNLUModelConfig, component_builder: ComponentBuilder\n    ) -> List[Component]:\n        <IND>\"\"\"Transform the passed names of the pipeline components into classes.\"\"\"\n        pipeline = []\n\n        # Transform the passed names of the pipeline components into classes\n        for index, pipeline_component in enumerate(cfg.pipeline):\n            <IND>component_cfg = cfg.for_component(index)\n            component = component_builder.create_component(component_cfg, cfg)\n            components.validate_component_keys(component, pipeline_component)\n            pipeline.append(component)\n\n        <DED>if not self.skip_validation:\n            <IND>components.validate_pipeline(pipeline)\n\n        <DED>return pipeline\n\n    <DED>def train(self, data: TrainingData, **kwargs: Any) -> \"Interpreter\":\n        <IND>\"\"\"Trains the underlying pipeline using the provided training data.\"\"\"\n\n        self.training_data = data\n\n        self.training_data.validate()\n\n        context = kwargs\n\n        for component in self.pipeline:\n            <IND>updates = component.provide_context()\n            if updates:\n                <IND>context.update(updates)\n\n        # Before the training starts: check that all arguments are provided\n        <DED><DED>if not self.skip_validation:\n            <IND>components.validate_required_components_from_data(\n                self.pipeline, self.training_data\n            )\n\n        # Warn if there is an obvious case of competing entity extractors\n        <DED>components.warn_of_competing_extractors(self.pipeline)\n        components.warn_of_competition_with_regex_extractor(\n            self.pipeline, self.training_data\n        )\n\n        # data gets modified internally during the training - hence the copy\n        working_data: TrainingData = copy.deepcopy(data)\n\n        for i, component in enumerate(self.pipeline):\n            <IND>logger.info(f\"Starting to train component {component.name}\")\n            component.prepare_partial_processing(self.pipeline[:i], context)\n            component.train(working_data, self.config, **context)\n            logger.info(\"Finished training component.\")\n\n        <DED>return Interpreter(self.pipeline, context)\n\n    <DED>@staticmethod\n    def _file_name(index: int, name: Text) -> Text:\n        <IND>return f\"component_{index}_{name}\"\n\n    <DED>def persist(\n        self,\n        path: Text,\n        persistor: Optional[Persistor] = None,\n        fixed_model_name: Text = None,\n        persist_nlu_training_data: bool = False,\n    ) -> Text:\n        <IND>\"\"\"Persist all components of the pipeline to the passed path.\n\n        Returns the directory of the persisted model.\"\"\"\n\n        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n        metadata = {\"language\": self.config[\"language\"], \"pipeline\": []}\n\n        if fixed_model_name:\n            <IND>model_name = fixed_model_name\n        <DED>else:\n            <IND>model_name = NLU_MODEL_NAME_PREFIX + timestamp\n\n        <DED>path = os.path.abspath(path)\n        dir_name = os.path.join(path, model_name)\n\n        rasa.shared.utils.io.create_directory(dir_name)\n\n        if self.training_data and persist_nlu_training_data:\n            <IND>metadata.update(self.training_data.persist(dir_name))\n\n        <DED>for i, component in enumerate(self.pipeline):\n            <IND>file_name = self._file_name(i, component.name)\n            update = component.persist(file_name, dir_name)\n            component_meta = component.component_config\n            if update:\n                <IND>component_meta.update(update)\n            <DED>component_meta[\n                \"class\"\n            ] = rasa.shared.utils.common.module_path_from_instance(component)\n\n            metadata[\"pipeline\"].append(component_meta)\n\n        <DED>Metadata(metadata).persist(dir_name)\n\n        if persistor is not None:\n            <IND>persistor.persist(dir_name, model_name)\n        <DED>logger.info(\n            \"Successfully saved model into '{}'\".format(os.path.abspath(dir_name))\n        )\n        return dir_name\n\n\n<DED><DED>class Interpreter:\n    <IND>\"\"\"Use a trained pipeline of components to parse text messages.\"\"\"\n\n    # Defines all attributes (& default values)\n    # that will be returned by `parse`\n    @staticmethod\n    def default_output_attributes() -> Dict[Text, Any]:\n        <IND>return {\n            TEXT: \"\",\n            INTENT: {INTENT_NAME_KEY: None, PREDICTED_CONFIDENCE_KEY: 0.0},\n            ENTITIES: [],\n        }\n\n    <DED>@staticmethod\n    def ensure_model_compatibility(\n        metadata: Metadata, version_to_check: Optional[Text] = None\n    ) -> None:\n        <IND>from packaging import version\n\n        if version_to_check is None:\n            <IND>version_to_check = MINIMUM_COMPATIBLE_VERSION\n\n        <DED>model_version = metadata.get(\"rasa_version\", \"0.0.0\")\n        if version.parse(model_version) < version.parse(version_to_check):\n            <IND>raise UnsupportedModelError(\n                f\"The model version is trained using Rasa Open Source {model_version} \"\n                f\"and is not compatible with your current installation \"\n                f\"({rasa.__version__}). \"\n                f\"This means that you either need to retrain your model \"\n                f\"or revert back to the Rasa version that trained the model \"\n                f\"to ensure that the versions match up again.\"\n            )\n\n    <DED><DED>@staticmethod\n    def load(\n        model_dir: Text,\n        component_builder: Optional[ComponentBuilder] = None,\n        skip_validation: bool = False,\n        new_config: Optional[Dict] = None,\n        finetuning_epoch_fraction: float = 1.0,\n    ) -> \"Interpreter\":\n        <IND>\"\"\"Create an interpreter based on a persisted model.\n\n        Args:\n            skip_validation: If set to `True`, does not check that all\n                required packages for the components are installed\n                before loading them.\n            model_dir: The path of the model to load\n            component_builder: The\n                :class:`rasa.nlu.components.ComponentBuilder` to use.\n            new_config: Optional new config to use for the new epochs.\n            finetuning_epoch_fraction: Value to multiply all epochs by.\n\n        Returns:\n            An interpreter that uses the loaded model.\n        \"\"\"\n        model_metadata = Metadata.load(model_dir)\n\n        if new_config:\n            <IND>Interpreter._update_metadata_epochs(\n                model_metadata, new_config, finetuning_epoch_fraction\n            )\n\n        <DED>Interpreter.ensure_model_compatibility(model_metadata)\n        return Interpreter.create(\n            model_dir,\n            model_metadata,\n            component_builder,\n            skip_validation,\n            should_finetune=new_config is not None,\n        )\n\n    <DED>@staticmethod\n    def _get_default_value_for_component(name: Text, key: Text) -> Any:\n        <IND>from rasa.nlu.registry import get_component_class\n\n        return get_component_class(name).defaults[key]\n\n    <DED>@staticmethod\n    def _update_metadata_epochs(\n        model_metadata: Metadata,\n        new_config: Optional[Dict] = None,\n        finetuning_epoch_fraction: float = 1.0,\n    ) -> Metadata:\n        <IND>new_config = new_config or {}\n        for old_component_config, new_component_config in zip(\n            model_metadata.metadata[\"pipeline\"], new_config[\"pipeline\"]\n        ):\n            <IND>if EPOCHS in old_component_config:\n                <IND>new_epochs = new_component_config.get(\n                    EPOCHS,\n                    Interpreter._get_default_value_for_component(\n                        old_component_config[\"class\"], EPOCHS\n                    ),\n                )\n                old_component_config[EPOCHS] = ceil(\n                    new_epochs * finetuning_epoch_fraction\n                )\n        <DED><DED>return model_metadata\n\n    <DED>@staticmethod\n    def create(\n        model_dir: Text,\n        model_metadata: Metadata,\n        component_builder: Optional[ComponentBuilder] = None,\n        skip_validation: bool = False,\n        should_finetune: bool = False,\n    ) -> \"Interpreter\":\n        <IND>\"\"\"Create model and components defined by the provided metadata.\n\n        Args:\n            model_dir: The directory containing the model.\n            model_metadata: The metadata describing each component.\n            component_builder: The\n                :class:`rasa.nlu.components.ComponentBuilder` to use.\n            skip_validation: If set to `True`, does not check that all\n                required packages for the components are installed\n                before loading them.\n            should_finetune: Indicates if the model components will be fine-tuned.\n\n        Returns:\n            An interpreter that uses the created model.\n        \"\"\"\n        context: Dict[Text, Any] = {\"should_finetune\": should_finetune}\n\n        if component_builder is None:\n            # If no builder is passed, every interpreter creation will result\n            # in a new builder. hence, no components are reused.\n            <IND>component_builder = components.ComponentBuilder()\n\n        <DED>pipeline = []\n\n        # Before instantiating the component classes,\n        # lets check if all required packages are available\n        if not skip_validation:\n            <IND>components.validate_requirements(model_metadata.component_classes)\n\n        <DED>for i in range(model_metadata.number_of_components):\n            <IND>component_meta = model_metadata.for_component(i)\n            component = component_builder.load_component(\n                component_meta, model_dir, model_metadata, **context\n            )\n            try:\n                <IND>updates = component.provide_context()\n                if updates:\n                    <IND>context.update(updates)\n                <DED>pipeline.append(component)\n            <DED>except components.MissingArgumentError as e:\n                <IND>raise Exception(\n                    \"Failed to initialize component '{}'. \"\n                    \"{}\".format(component.name, e)\n                )\n\n        <DED><DED>return Interpreter(pipeline, context, model_metadata)\n\n    <DED>def __init__(\n        self,\n        pipeline: List[Component],\n        context: Optional[Dict[Text, Any]],\n        model_metadata: Optional[Metadata] = None,\n    ) -> None:\n\n        <IND>self.pipeline = pipeline\n        self.context = context if context is not None else {}\n        self.model_metadata = model_metadata\n        self.has_already_warned_of_overlapping_entities = False\n\n    <DED>def parse(\n        self,\n        text: Text,\n        time: Optional[datetime.datetime] = None,\n        only_output_properties: bool = True,\n    ) -> Dict[Text, Any]:\n        <IND>\"\"\"Parse the input text, classify it and return pipeline result.\n\n        The pipeline result usually contains intent and entities.\"\"\"\n\n        if not text:\n            # Not all components are able to handle empty strings. So we need\n            # to prevent that... This default return will not contain all\n            # output attributes of all components, but in the end, no one\n            # should pass an empty string in the first place.\n            <IND>output = self.default_output_attributes()\n            output[\"text\"] = \"\"\n            return output\n\n        <DED>timestamp = int(time.timestamp()) if time else None\n        data = self.default_output_attributes()\n        data[TEXT] = text\n\n        message = Message(data=data, time=timestamp, output_properties={TEXT_TOKENS})\n\n        for component in self.pipeline:\n            <IND>component.process(message, **self.context)\n\n        <DED>if not self.has_already_warned_of_overlapping_entities:\n            <IND>self.warn_of_overlapping_entities(message)\n\n        <DED>output = self.default_output_attributes()\n        output.update(message.as_dict(only_output_properties=only_output_properties))\n        return output\n\n    <DED>def featurize_message(self, message: Message) -> Message:\n        <IND>\"\"\"\n        Tokenize and featurize the input message\n        Args:\n            message: message storing text to process;\n        Returns:\n            message: it contains the tokens and features which are the output of the\n            NLU pipeline;\n        \"\"\"\n\n        for component in self.pipeline:\n            <IND>if not isinstance(component, (EntityExtractor, IntentClassifier)):\n                <IND>component.process(message, **self.context)\n        <DED><DED>return message\n\n    <DED>def warn_of_overlapping_entities(self, message: Message) -> None:\n        <IND>\"\"\"Issues a warning when there are overlapping entity annotations.\n\n        This warning is only issued once per Interpreter life time.\n\n        Args:\n            message: user message with all processing metadata such as entities\n        \"\"\"\n        overlapping_entity_pairs = message.find_overlapping_entities()\n        if len(overlapping_entity_pairs) > 0:\n            <IND>message_text = message.get(\"text\")\n            first_pair = overlapping_entity_pairs[0]\n            entity_1 = first_pair[0]\n            entity_2 = first_pair[1]\n            rasa.shared.utils.io.raise_warning(\n                f\"Parsing of message: '{message_text}' lead to overlapping \"\n                f\"entities: {entity_1['value']} of type \"\n                f\"{entity_1['entity']} extracted by \"\n                f\"{entity_1['extractor']} overlaps with \"\n                f\"{entity_2['value']} of type {entity_2['entity']} extracted by \"\n                f\"{entity_2['extractor']}. This can lead to unintended filling of \"\n                f\"slots. Please refer to the documentation section on entity \"\n                f\"extractors and entities getting extracted multiple times:\"\n                f\"{DOCS_URL_COMPONENTS}#entity-extractors\"\n            )\n            self.has_already_warned_of_overlapping_entities = True\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        write_json_to_file(filename, metadata, indent=4)\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "rasa/nlu/model.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/RasaHQ-rasa/rasa/nlu/model.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "rasa/nlu/model.py:236:8 Incompatible variable type [9]: fixed_model_name is declared to have type `str` but is used as type `None`.",
    "message": " fixed_model_name is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 236,
    "warning_line": "        fixed_model_name: Text = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        write_json_to_file(filename, metadata, indent=4)\n\n\nclass Trainer:\n    \"\"\"Trainer will load the data and train all components.\n\n    Requires a pipeline specification and configuration to use for\n    the training.\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: RasaNLUModelConfig,\n        component_builder: Optional[ComponentBuilder] = None,\n        skip_validation: bool = False,\n        model_to_finetune: Optional[\"Interpreter\"] = None,\n    ) -> None:\n\n        self.config = cfg\n        self.skip_validation = skip_validation\n        self.training_data = None  # type: Optional[TrainingData]\n\n        if component_builder is None:\n            # If no builder is passed, every interpreter creation will result in\n            # a new builder. hence, no components are reused.\n            component_builder = components.ComponentBuilder()\n\n        # Before instantiating the component classes, lets check if all\n        # required packages are available\n        if not self.skip_validation:\n            components.validate_requirements(cfg.component_names)\n\n        if model_to_finetune:\n            self.pipeline = model_to_finetune.pipeline\n        else:\n            self.pipeline = self._build_pipeline(cfg, component_builder)\n\n    def _build_pipeline(\n        self, cfg: RasaNLUModelConfig, component_builder: ComponentBuilder\n    ) -> List[Component]:\n        \"\"\"Transform the passed names of the pipeline components into classes.\"\"\"\n        pipeline = []\n\n        # Transform the passed names of the pipeline components into classes\n        for index, pipeline_component in enumerate(cfg.pipeline):\n            component_cfg = cfg.for_component(index)\n            component = component_builder.create_component(component_cfg, cfg)\n            components.validate_component_keys(component, pipeline_component)\n            pipeline.append(component)\n\n        if not self.skip_validation:\n            components.validate_pipeline(pipeline)\n\n        return pipeline\n\n    def train(self, data: TrainingData, **kwargs: Any) -> \"Interpreter\":\n        \"\"\"Trains the underlying pipeline using the provided training data.\"\"\"\n\n        self.training_data = data\n\n        self.training_data.validate()\n\n        context = kwargs\n\n        for component in self.pipeline:\n            updates = component.provide_context()\n            if updates:\n                context.update(updates)\n\n        # Before the training starts: check that all arguments are provided\n        if not self.skip_validation:\n            components.validate_required_components_from_data(\n                self.pipeline, self.training_data\n            )\n\n        # Warn if there is an obvious case of competing entity extractors\n        components.warn_of_competing_extractors(self.pipeline)\n        components.warn_of_competition_with_regex_extractor(\n            self.pipeline, self.training_data\n        )\n\n        # data gets modified internally during the training - hence the copy\n        working_data: TrainingData = copy.deepcopy(data)\n\n        for i, component in enumerate(self.pipeline):\n            logger.info(f\"Starting to train component {component.name}\")\n            component.prepare_partial_processing(self.pipeline[:i], context)\n            component.train(working_data, self.config, **context)\n            logger.info(\"Finished training component.\")\n\n        return Interpreter(self.pipeline, context)\n\n    @staticmethod\n    def _file_name(index: int, name: Text) -> Text:\n        return f\"component_{index}_{name}\"\n\n    def persist(\n        self,\n        path: Text,\n        persistor: Optional[Persistor] = None,\n        fixed_model_name: Text = None,\n        persist_nlu_training_data: bool = False,\n    ) -> Text:\n        \"\"\"Persist all components of the pipeline to the passed path.\n\n        Returns the directory of the persisted model.\"\"\"\n\n        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n        metadata = {\"language\": self.config[\"language\"], \"pipeline\": []}\n\n        if fixed_model_name:\n            model_name = fixed_model_name\n        else:\n            model_name = NLU_MODEL_NAME_PREFIX + timestamp\n\n        path = os.path.abspath(path)\n        dir_name = os.path.join(path, model_name)\n\n        rasa.shared.utils.io.create_directory(dir_name)\n\n        if self.training_data and persist_nlu_training_data:\n            metadata.update(self.training_data.persist(dir_name))\n\n        for i, component in enumerate(self.pipeline):\n            file_name = self._file_name(i, component.name)\n            update = component.persist(file_name, dir_name)\n            component_meta = component.component_config\n            if update:\n                component_meta.update(update)\n            component_meta[\n                \"class\"\n            ] = rasa.shared.utils.common.module_path_from_instance(component)\n\n            metadata[\"pipeline\"].append(component_meta)\n\n        Metadata(metadata).persist(dir_name)\n\n        if persistor is not None:\n            persistor.persist(dir_name, model_name)\n        logger.info(\n            \"Successfully saved model into '{}'\".format(os.path.abspath(dir_name))\n        )\n        return dir_name\n\n\nclass Interpreter:\n    \"\"\"Use a trained pipeline of components to parse text messages.\"\"\"\n\n    # Defines all attributes (& default values)\n    # that will be returned by `parse`\n    @staticmethod\n    def default_output_attributes() -> Dict[Text, Any]:\n        return {\n            TEXT: \"\",\n            INTENT: {INTENT_NAME_KEY: None, PREDICTED_CONFIDENCE_KEY: 0.0},\n            ENTITIES: [],\n        }\n\n    @staticmethod\n    def ensure_model_compatibility(\n        metadata: Metadata, version_to_check: Optional[Text] = None\n    ) -> None:\n        from packaging import version\n\n        if version_to_check is None:\n            version_to_check = MINIMUM_COMPATIBLE_VERSION\n\n        model_version = metadata.get(\"rasa_version\", \"0.0.0\")\n        if version.parse(model_version) < version.parse(version_to_check):\n            raise UnsupportedModelError(\n                f\"The model version is trained using Rasa Open Source {model_version} \"\n                f\"and is not compatible with your current installation \"\n                f\"({rasa.__version__}). \"\n                f\"This means that you either need to retrain your model \"\n                f\"or revert back to the Rasa version that trained the model \"\n                f\"to ensure that the versions match up again.\"\n            )\n\n    @staticmethod\n    def load(\n        model_dir: Text,\n        component_builder: Optional[ComponentBuilder] = None,\n        skip_validation: bool = False,\n        new_config: Optional[Dict] = None,\n        finetuning_epoch_fraction: float = 1.0,\n    ) -> \"Interpreter\":\n        \"\"\"Create an interpreter based on a persisted model.\n\n        Args:\n            skip_validation: If set to `True`, does not check that all\n                required packages for the components are installed\n                before loading them.\n            model_dir: The path of the model to load\n            component_builder: The\n                :class:`rasa.nlu.components.ComponentBuilder` to use.\n            new_config: Optional new config to use for the new epochs.\n            finetuning_epoch_fraction: Value to multiply all epochs by.\n\n        Returns:\n            An interpreter that uses the loaded model.\n        \"\"\"\n        model_metadata = Metadata.load(model_dir)\n\n        if new_config:\n            Interpreter._update_metadata_epochs(\n                model_metadata, new_config, finetuning_epoch_fraction\n            )\n\n        Interpreter.ensure_model_compatibility(model_metadata)\n        return Interpreter.create(\n            model_dir,\n            model_metadata,\n            component_builder,\n            skip_validation,\n            should_finetune=new_config is not None,\n        )\n\n    @staticmethod\n    def _get_default_value_for_component(name: Text, key: Text) -> Any:\n        from rasa.nlu.registry import get_component_class\n\n        return get_component_class(name).defaults[key]\n\n    @staticmethod\n    def _update_metadata_epochs(\n        model_metadata: Metadata,\n        new_config: Optional[Dict] = None,\n        finetuning_epoch_fraction: float = 1.0,\n    ) -> Metadata:\n        new_config = new_config or {}\n        for old_component_config, new_component_config in zip(\n            model_metadata.metadata[\"pipeline\"], new_config[\"pipeline\"]\n        ):\n            if EPOCHS in old_component_config:\n                new_epochs = new_component_config.get(\n                    EPOCHS,\n                    Interpreter._get_default_value_for_component(\n                        old_component_config[\"class\"], EPOCHS\n                    ),\n                )\n                old_component_config[EPOCHS] = ceil(\n                    new_epochs * finetuning_epoch_fraction\n                )\n        return model_metadata\n\n    @staticmethod\n    def create(\n        model_dir: Text,\n        model_metadata: Metadata,\n        component_builder: Optional[ComponentBuilder] = None,\n        skip_validation: bool = False,\n        should_finetune: bool = False,\n    ) -> \"Interpreter\":\n        \"\"\"Create model and components defined by the provided metadata.\n\n        Args:\n            model_dir: The directory containing the model.\n            model_metadata: The metadata describing each component.\n            component_builder: The\n                :class:`rasa.nlu.components.ComponentBuilder` to use.\n            skip_validation: If set to `True`, does not check that all\n                required packages for the components are installed\n                before loading them.\n            should_finetune: Indicates if the model components will be fine-tuned.\n\n        Returns:\n            An interpreter that uses the created model.\n        \"\"\"\n        context: Dict[Text, Any] = {\"should_finetune\": should_finetune}\n\n        if component_builder is None:\n            # If no builder is passed, every interpreter creation will result\n            # in a new builder. hence, no components are reused.\n            component_builder = components.ComponentBuilder()\n\n        pipeline = []\n\n        # Before instantiating the component classes,\n        # lets check if all required packages are available\n        if not skip_validation:\n            components.validate_requirements(model_metadata.component_classes)\n\n        for i in range(model_metadata.number_of_components):\n            component_meta = model_metadata.for_component(i)\n            component = component_builder.load_component(\n                component_meta, model_dir, model_metadata, **context\n            )\n            try:\n                updates = component.provide_context()\n                if updates:\n                    context.update(updates)\n                pipeline.append(component)\n            except components.MissingArgumentError as e:\n                raise Exception(\n                    \"Failed to initialize component '{}'. \"\n                    \"{}\".format(component.name, e)\n                )\n\n        return Interpreter(pipeline, context, model_metadata)\n\n    def __init__(\n        self,\n        pipeline: List[Component],\n        context: Optional[Dict[Text, Any]],\n        model_metadata: Optional[Metadata] = None,\n    ) -> None:\n\n        self.pipeline = pipeline\n        self.context = context if context is not None else {}\n        self.model_metadata = model_metadata\n        self.has_already_warned_of_overlapping_entities = False\n\n    def parse(\n        self,\n        text: Text,\n        time: Optional[datetime.datetime] = None,\n        only_output_properties: bool = True,\n    ) -> Dict[Text, Any]:\n        \"\"\"Parse the input text, classify it and return pipeline result.\n\n        The pipeline result usually contains intent and entities.\"\"\"\n\n        if not text:\n            # Not all components are able to handle empty strings. So we need\n            # to prevent that... This default return will not contain all\n            # output attributes of all components, but in the end, no one\n            # should pass an empty string in the first place.\n            output = self.default_output_attributes()\n            output[\"text\"] = \"\"\n            return output\n\n        timestamp = int(time.timestamp()) if time else None\n        data = self.default_output_attributes()\n        data[TEXT] = text\n\n        message = Message(data=data, time=timestamp, output_properties={TEXT_TOKENS})\n\n        for component in self.pipeline:\n            component.process(message, **self.context)\n\n        if not self.has_already_warned_of_overlapping_entities:\n            self.warn_of_overlapping_entities(message)\n\n        output = self.default_output_attributes()\n        output.update(message.as_dict(only_output_properties=only_output_properties))\n        return output\n\n    def featurize_message(self, message: Message) -> Message:\n        \"\"\"\n        Tokenize and featurize the input message\n        Args:\n            message: message storing text to process;\n        Returns:\n            message: it contains the tokens and features which are the output of the\n            NLU pipeline;\n        \"\"\"\n\n        for component in self.pipeline:\n            if not isinstance(component, (EntityExtractor, IntentClassifier)):\n                component.process(message, **self.context)\n        return message\n\n    def warn_of_overlapping_entities(self, message: Message) -> None:\n        \"\"\"Issues a warning when there are overlapping entity annotations.\n\n        This warning is only issued once per Interpreter life time.\n\n        Args:\n            message: user message with all processing metadata such as entities\n        \"\"\"\n        overlapping_entity_pairs = message.find_overlapping_entities()\n        if len(overlapping_entity_pairs) > 0:\n            message_text = message.get(\"text\")\n            first_pair = overlapping_entity_pairs[0]\n            entity_1 = first_pair[0]\n            entity_2 = first_pair[1]\n            rasa.shared.utils.io.raise_warning(\n                f\"Parsing of message: '{message_text}' lead to overlapping \"\n                f\"entities: {entity_1['value']} of type \"\n                f\"{entity_1['entity']} extracted by \"\n                f\"{entity_1['extractor']} overlaps with \"\n                f\"{entity_2['value']} of type {entity_2['entity']} extracted by \"\n                f\"{entity_2['extractor']}. This can lead to unintended filling of \"\n                f\"slots. Please refer to the documentation section on entity \"\n                f\"extractors and entities getting extracted multiple times:\"\n                f\"{DOCS_URL_COMPONENTS}#entity-extractors\"\n            )\n            self.has_already_warned_of_overlapping_entities = True\n",
        "source_code_len": 14724,
        "target_code": "        write_json_to_file(filename, metadata, indent=4)\n",
        "target_code_len": 57,
        "diff_format": "@@ -136,388 +117,1 @@\n         write_json_to_file(filename, metadata, indent=4)\n-\n-\n-class Trainer:\n-    \"\"\"Trainer will load the data and train all components.\n-\n-    Requires a pipeline specification and configuration to use for\n-    the training.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        cfg: RasaNLUModelConfig,\n-        component_builder: Optional[ComponentBuilder] = None,\n-        skip_validation: bool = False,\n-        model_to_finetune: Optional[\"Interpreter\"] = None,\n-    ) -> None:\n-\n-        self.config = cfg\n-        self.skip_validation = skip_validation\n-        self.training_data = None  # type: Optional[TrainingData]\n-\n-        if component_builder is None:\n-            # If no builder is passed, every interpreter creation will result in\n-            # a new builder. hence, no components are reused.\n-            component_builder = components.ComponentBuilder()\n-\n-        # Before instantiating the component classes, lets check if all\n-        # required packages are available\n-        if not self.skip_validation:\n-            components.validate_requirements(cfg.component_names)\n-\n-        if model_to_finetune:\n-            self.pipeline = model_to_finetune.pipeline\n-        else:\n-            self.pipeline = self._build_pipeline(cfg, component_builder)\n-\n-    def _build_pipeline(\n-        self, cfg: RasaNLUModelConfig, component_builder: ComponentBuilder\n-    ) -> List[Component]:\n-        \"\"\"Transform the passed names of the pipeline components into classes.\"\"\"\n-        pipeline = []\n-\n-        # Transform the passed names of the pipeline components into classes\n-        for index, pipeline_component in enumerate(cfg.pipeline):\n-            component_cfg = cfg.for_component(index)\n-            component = component_builder.create_component(component_cfg, cfg)\n-            components.validate_component_keys(component, pipeline_component)\n-            pipeline.append(component)\n-\n-        if not self.skip_validation:\n-            components.validate_pipeline(pipeline)\n-\n-        return pipeline\n-\n-    def train(self, data: TrainingData, **kwargs: Any) -> \"Interpreter\":\n-        \"\"\"Trains the underlying pipeline using the provided training data.\"\"\"\n-\n-        self.training_data = data\n-\n-        self.training_data.validate()\n-\n-        context = kwargs\n-\n-        for component in self.pipeline:\n-            updates = component.provide_context()\n-            if updates:\n-                context.update(updates)\n-\n-        # Before the training starts: check that all arguments are provided\n-        if not self.skip_validation:\n-            components.validate_required_components_from_data(\n-                self.pipeline, self.training_data\n-            )\n-\n-        # Warn if there is an obvious case of competing entity extractors\n-        components.warn_of_competing_extractors(self.pipeline)\n-        components.warn_of_competition_with_regex_extractor(\n-            self.pipeline, self.training_data\n-        )\n-\n-        # data gets modified internally during the training - hence the copy\n-        working_data: TrainingData = copy.deepcopy(data)\n-\n-        for i, component in enumerate(self.pipeline):\n-            logger.info(f\"Starting to train component {component.name}\")\n-            component.prepare_partial_processing(self.pipeline[:i], context)\n-            component.train(working_data, self.config, **context)\n-            logger.info(\"Finished training component.\")\n-\n-        return Interpreter(self.pipeline, context)\n-\n-    @staticmethod\n-    def _file_name(index: int, name: Text) -> Text:\n-        return f\"component_{index}_{name}\"\n-\n-    def persist(\n-        self,\n-        path: Text,\n-        persistor: Optional[Persistor] = None,\n-        fixed_model_name: Text = None,\n-        persist_nlu_training_data: bool = False,\n-    ) -> Text:\n-        \"\"\"Persist all components of the pipeline to the passed path.\n-\n-        Returns the directory of the persisted model.\"\"\"\n-\n-        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n-        metadata = {\"language\": self.config[\"language\"], \"pipeline\": []}\n-\n-        if fixed_model_name:\n-            model_name = fixed_model_name\n-        else:\n-            model_name = NLU_MODEL_NAME_PREFIX + timestamp\n-\n-        path = os.path.abspath(path)\n-        dir_name = os.path.join(path, model_name)\n-\n-        rasa.shared.utils.io.create_directory(dir_name)\n-\n-        if self.training_data and persist_nlu_training_data:\n-            metadata.update(self.training_data.persist(dir_name))\n-\n-        for i, component in enumerate(self.pipeline):\n-            file_name = self._file_name(i, component.name)\n-            update = component.persist(file_name, dir_name)\n-            component_meta = component.component_config\n-            if update:\n-                component_meta.update(update)\n-            component_meta[\n-                \"class\"\n-            ] = rasa.shared.utils.common.module_path_from_instance(component)\n-\n-            metadata[\"pipeline\"].append(component_meta)\n-\n-        Metadata(metadata).persist(dir_name)\n-\n-        if persistor is not None:\n-            persistor.persist(dir_name, model_name)\n-        logger.info(\n-            \"Successfully saved model into '{}'\".format(os.path.abspath(dir_name))\n-        )\n-        return dir_name\n-\n-\n-class Interpreter:\n-    \"\"\"Use a trained pipeline of components to parse text messages.\"\"\"\n-\n-    # Defines all attributes (& default values)\n-    # that will be returned by `parse`\n-    @staticmethod\n-    def default_output_attributes() -> Dict[Text, Any]:\n-        return {\n-            TEXT: \"\",\n-            INTENT: {INTENT_NAME_KEY: None, PREDICTED_CONFIDENCE_KEY: 0.0},\n-            ENTITIES: [],\n-        }\n-\n-    @staticmethod\n-    def ensure_model_compatibility(\n-        metadata: Metadata, version_to_check: Optional[Text] = None\n-    ) -> None:\n-        from packaging import version\n-\n-        if version_to_check is None:\n-            version_to_check = MINIMUM_COMPATIBLE_VERSION\n-\n-        model_version = metadata.get(\"rasa_version\", \"0.0.0\")\n-        if version.parse(model_version) < version.parse(version_to_check):\n-            raise UnsupportedModelError(\n-                f\"The model version is trained using Rasa Open Source {model_version} \"\n-                f\"and is not compatible with your current installation \"\n-                f\"({rasa.__version__}). \"\n-                f\"This means that you either need to retrain your model \"\n-                f\"or revert back to the Rasa version that trained the model \"\n-                f\"to ensure that the versions match up again.\"\n-            )\n-\n-    @staticmethod\n-    def load(\n-        model_dir: Text,\n-        component_builder: Optional[ComponentBuilder] = None,\n-        skip_validation: bool = False,\n-        new_config: Optional[Dict] = None,\n-        finetuning_epoch_fraction: float = 1.0,\n-    ) -> \"Interpreter\":\n-        \"\"\"Create an interpreter based on a persisted model.\n-\n-        Args:\n-            skip_validation: If set to `True`, does not check that all\n-                required packages for the components are installed\n-                before loading them.\n-            model_dir: The path of the model to load\n-            component_builder: The\n-                :class:`rasa.nlu.components.ComponentBuilder` to use.\n-            new_config: Optional new config to use for the new epochs.\n-            finetuning_epoch_fraction: Value to multiply all epochs by.\n-\n-        Returns:\n-            An interpreter that uses the loaded model.\n-        \"\"\"\n-        model_metadata = Metadata.load(model_dir)\n-\n-        if new_config:\n-            Interpreter._update_metadata_epochs(\n-                model_metadata, new_config, finetuning_epoch_fraction\n-            )\n-\n-        Interpreter.ensure_model_compatibility(model_metadata)\n-        return Interpreter.create(\n-            model_dir,\n-            model_metadata,\n-            component_builder,\n-            skip_validation,\n-            should_finetune=new_config is not None,\n-        )\n-\n-    @staticmethod\n-    def _get_default_value_for_component(name: Text, key: Text) -> Any:\n-        from rasa.nlu.registry import get_component_class\n-\n-        return get_component_class(name).defaults[key]\n-\n-    @staticmethod\n-    def _update_metadata_epochs(\n-        model_metadata: Metadata,\n-        new_config: Optional[Dict] = None,\n-        finetuning_epoch_fraction: float = 1.0,\n-    ) -> Metadata:\n-        new_config = new_config or {}\n-        for old_component_config, new_component_config in zip(\n-            model_metadata.metadata[\"pipeline\"], new_config[\"pipeline\"]\n-        ):\n-            if EPOCHS in old_component_config:\n-                new_epochs = new_component_config.get(\n-                    EPOCHS,\n-                    Interpreter._get_default_value_for_component(\n-                        old_component_config[\"class\"], EPOCHS\n-                    ),\n-                )\n-                old_component_config[EPOCHS] = ceil(\n-                    new_epochs * finetuning_epoch_fraction\n-                )\n-        return model_metadata\n-\n-    @staticmethod\n-    def create(\n-        model_dir: Text,\n-        model_metadata: Metadata,\n-        component_builder: Optional[ComponentBuilder] = None,\n-        skip_validation: bool = False,\n-        should_finetune: bool = False,\n-    ) -> \"Interpreter\":\n-        \"\"\"Create model and components defined by the provided metadata.\n-\n-        Args:\n-            model_dir: The directory containing the model.\n-            model_metadata: The metadata describing each component.\n-            component_builder: The\n-                :class:`rasa.nlu.components.ComponentBuilder` to use.\n-            skip_validation: If set to `True`, does not check that all\n-                required packages for the components are installed\n-                before loading them.\n-            should_finetune: Indicates if the model components will be fine-tuned.\n-\n-        Returns:\n-            An interpreter that uses the created model.\n-        \"\"\"\n-        context: Dict[Text, Any] = {\"should_finetune\": should_finetune}\n-\n-        if component_builder is None:\n-            # If no builder is passed, every interpreter creation will result\n-            # in a new builder. hence, no components are reused.\n-            component_builder = components.ComponentBuilder()\n-\n-        pipeline = []\n-\n-        # Before instantiating the component classes,\n-        # lets check if all required packages are available\n-        if not skip_validation:\n-            components.validate_requirements(model_metadata.component_classes)\n-\n-        for i in range(model_metadata.number_of_components):\n-            component_meta = model_metadata.for_component(i)\n-            component = component_builder.load_component(\n-                component_meta, model_dir, model_metadata, **context\n-            )\n-            try:\n-                updates = component.provide_context()\n-                if updates:\n-                    context.update(updates)\n-                pipeline.append(component)\n-            except components.MissingArgumentError as e:\n-                raise Exception(\n-                    \"Failed to initialize component '{}'. \"\n-                    \"{}\".format(component.name, e)\n-                )\n-\n-        return Interpreter(pipeline, context, model_metadata)\n-\n-    def __init__(\n-        self,\n-        pipeline: List[Component],\n-        context: Optional[Dict[Text, Any]],\n-        model_metadata: Optional[Metadata] = None,\n-    ) -> None:\n-\n-        self.pipeline = pipeline\n-        self.context = context if context is not None else {}\n-        self.model_metadata = model_metadata\n-        self.has_already_warned_of_overlapping_entities = False\n-\n-    def parse(\n-        self,\n-        text: Text,\n-        time: Optional[datetime.datetime] = None,\n-        only_output_properties: bool = True,\n-    ) -> Dict[Text, Any]:\n-        \"\"\"Parse the input text, classify it and return pipeline result.\n-\n-        The pipeline result usually contains intent and entities.\"\"\"\n-\n-        if not text:\n-            # Not all components are able to handle empty strings. So we need\n-            # to prevent that... This default return will not contain all\n-            # output attributes of all components, but in the end, no one\n-            # should pass an empty string in the first place.\n-            output = self.default_output_attributes()\n-            output[\"text\"] = \"\"\n-            return output\n-\n-        timestamp = int(time.timestamp()) if time else None\n-        data = self.default_output_attributes()\n-        data[TEXT] = text\n-\n-        message = Message(data=data, time=timestamp, output_properties={TEXT_TOKENS})\n-\n-        for component in self.pipeline:\n-            component.process(message, **self.context)\n-\n-        if not self.has_already_warned_of_overlapping_entities:\n-            self.warn_of_overlapping_entities(message)\n-\n-        output = self.default_output_attributes()\n-        output.update(message.as_dict(only_output_properties=only_output_properties))\n-        return output\n-\n-    def featurize_message(self, message: Message) -> Message:\n-        \"\"\"\n-        Tokenize and featurize the input message\n-        Args:\n-            message: message storing text to process;\n-        Returns:\n-            message: it contains the tokens and features which are the output of the\n-            NLU pipeline;\n-        \"\"\"\n-\n-        for component in self.pipeline:\n-            if not isinstance(component, (EntityExtractor, IntentClassifier)):\n-                component.process(message, **self.context)\n-        return message\n-\n-    def warn_of_overlapping_entities(self, message: Message) -> None:\n-        \"\"\"Issues a warning when there are overlapping entity annotations.\n-\n-        This warning is only issued once per Interpreter life time.\n-\n-        Args:\n-            message: user message with all processing metadata such as entities\n-        \"\"\"\n-        overlapping_entity_pairs = message.find_overlapping_entities()\n-        if len(overlapping_entity_pairs) > 0:\n-            message_text = message.get(\"text\")\n-            first_pair = overlapping_entity_pairs[0]\n-            entity_1 = first_pair[0]\n-            entity_2 = first_pair[1]\n-            rasa.shared.utils.io.raise_warning(\n-                f\"Parsing of message: '{message_text}' lead to overlapping \"\n-                f\"entities: {entity_1['value']} of type \"\n-                f\"{entity_1['entity']} extracted by \"\n-                f\"{entity_1['extractor']} overlaps with \"\n-                f\"{entity_2['value']} of type {entity_2['entity']} extracted by \"\n-                f\"{entity_2['extractor']}. This can lead to unintended filling of \"\n-                f\"slots. Please refer to the documentation section on entity \"\n-                f\"extractors and entities getting extracted multiple times:\"\n-                f\"{DOCS_URL_COMPONENTS}#entity-extractors\"\n-            )\n-            self.has_already_warned_of_overlapping_entities = True\n",
        "source_code_with_indent": "        write_json_to_file(filename, metadata, indent=4)\n\n\n<DED><DED>class Trainer:\n    <IND>\"\"\"Trainer will load the data and train all components.\n\n    Requires a pipeline specification and configuration to use for\n    the training.\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: RasaNLUModelConfig,\n        component_builder: Optional[ComponentBuilder] = None,\n        skip_validation: bool = False,\n        model_to_finetune: Optional[\"Interpreter\"] = None,\n    ) -> None:\n\n        <IND>self.config = cfg\n        self.skip_validation = skip_validation\n        self.training_data = None  # type: Optional[TrainingData]\n\n        if component_builder is None:\n            # If no builder is passed, every interpreter creation will result in\n            # a new builder. hence, no components are reused.\n            <IND>component_builder = components.ComponentBuilder()\n\n        # Before instantiating the component classes, lets check if all\n        # required packages are available\n        <DED>if not self.skip_validation:\n            <IND>components.validate_requirements(cfg.component_names)\n\n        <DED>if model_to_finetune:\n            <IND>self.pipeline = model_to_finetune.pipeline\n        <DED>else:\n            <IND>self.pipeline = self._build_pipeline(cfg, component_builder)\n\n    <DED><DED>def _build_pipeline(\n        self, cfg: RasaNLUModelConfig, component_builder: ComponentBuilder\n    ) -> List[Component]:\n        <IND>\"\"\"Transform the passed names of the pipeline components into classes.\"\"\"\n        pipeline = []\n\n        # Transform the passed names of the pipeline components into classes\n        for index, pipeline_component in enumerate(cfg.pipeline):\n            <IND>component_cfg = cfg.for_component(index)\n            component = component_builder.create_component(component_cfg, cfg)\n            components.validate_component_keys(component, pipeline_component)\n            pipeline.append(component)\n\n        <DED>if not self.skip_validation:\n            <IND>components.validate_pipeline(pipeline)\n\n        <DED>return pipeline\n\n    <DED>def train(self, data: TrainingData, **kwargs: Any) -> \"Interpreter\":\n        <IND>\"\"\"Trains the underlying pipeline using the provided training data.\"\"\"\n\n        self.training_data = data\n\n        self.training_data.validate()\n\n        context = kwargs\n\n        for component in self.pipeline:\n            <IND>updates = component.provide_context()\n            if updates:\n                <IND>context.update(updates)\n\n        # Before the training starts: check that all arguments are provided\n        <DED><DED>if not self.skip_validation:\n            <IND>components.validate_required_components_from_data(\n                self.pipeline, self.training_data\n            )\n\n        # Warn if there is an obvious case of competing entity extractors\n        <DED>components.warn_of_competing_extractors(self.pipeline)\n        components.warn_of_competition_with_regex_extractor(\n            self.pipeline, self.training_data\n        )\n\n        # data gets modified internally during the training - hence the copy\n        working_data: TrainingData = copy.deepcopy(data)\n\n        for i, component in enumerate(self.pipeline):\n            <IND>logger.info(f\"Starting to train component {component.name}\")\n            component.prepare_partial_processing(self.pipeline[:i], context)\n            component.train(working_data, self.config, **context)\n            logger.info(\"Finished training component.\")\n\n        <DED>return Interpreter(self.pipeline, context)\n\n    <DED>@staticmethod\n    def _file_name(index: int, name: Text) -> Text:\n        <IND>return f\"component_{index}_{name}\"\n\n    <DED>def persist(\n        self,\n        path: Text,\n        persistor: Optional[Persistor] = None,\n        fixed_model_name: Text = None,\n        persist_nlu_training_data: bool = False,\n    ) -> Text:\n        <IND>\"\"\"Persist all components of the pipeline to the passed path.\n\n        Returns the directory of the persisted model.\"\"\"\n\n        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n        metadata = {\"language\": self.config[\"language\"], \"pipeline\": []}\n\n        if fixed_model_name:\n            <IND>model_name = fixed_model_name\n        <DED>else:\n            <IND>model_name = NLU_MODEL_NAME_PREFIX + timestamp\n\n        <DED>path = os.path.abspath(path)\n        dir_name = os.path.join(path, model_name)\n\n        rasa.shared.utils.io.create_directory(dir_name)\n\n        if self.training_data and persist_nlu_training_data:\n            <IND>metadata.update(self.training_data.persist(dir_name))\n\n        <DED>for i, component in enumerate(self.pipeline):\n            <IND>file_name = self._file_name(i, component.name)\n            update = component.persist(file_name, dir_name)\n            component_meta = component.component_config\n            if update:\n                <IND>component_meta.update(update)\n            <DED>component_meta[\n                \"class\"\n            ] = rasa.shared.utils.common.module_path_from_instance(component)\n\n            metadata[\"pipeline\"].append(component_meta)\n\n        <DED>Metadata(metadata).persist(dir_name)\n\n        if persistor is not None:\n            <IND>persistor.persist(dir_name, model_name)\n        <DED>logger.info(\n            \"Successfully saved model into '{}'\".format(os.path.abspath(dir_name))\n        )\n        return dir_name\n\n\n<DED><DED>class Interpreter:\n    <IND>\"\"\"Use a trained pipeline of components to parse text messages.\"\"\"\n\n    # Defines all attributes (& default values)\n    # that will be returned by `parse`\n    @staticmethod\n    def default_output_attributes() -> Dict[Text, Any]:\n        <IND>return {\n            TEXT: \"\",\n            INTENT: {INTENT_NAME_KEY: None, PREDICTED_CONFIDENCE_KEY: 0.0},\n            ENTITIES: [],\n        }\n\n    <DED>@staticmethod\n    def ensure_model_compatibility(\n        metadata: Metadata, version_to_check: Optional[Text] = None\n    ) -> None:\n        <IND>from packaging import version\n\n        if version_to_check is None:\n            <IND>version_to_check = MINIMUM_COMPATIBLE_VERSION\n\n        <DED>model_version = metadata.get(\"rasa_version\", \"0.0.0\")\n        if version.parse(model_version) < version.parse(version_to_check):\n            <IND>raise UnsupportedModelError(\n                f\"The model version is trained using Rasa Open Source {model_version} \"\n                f\"and is not compatible with your current installation \"\n                f\"({rasa.__version__}). \"\n                f\"This means that you either need to retrain your model \"\n                f\"or revert back to the Rasa version that trained the model \"\n                f\"to ensure that the versions match up again.\"\n            )\n\n    <DED><DED>@staticmethod\n    def load(\n        model_dir: Text,\n        component_builder: Optional[ComponentBuilder] = None,\n        skip_validation: bool = False,\n        new_config: Optional[Dict] = None,\n        finetuning_epoch_fraction: float = 1.0,\n    ) -> \"Interpreter\":\n        <IND>\"\"\"Create an interpreter based on a persisted model.\n\n        Args:\n            skip_validation: If set to `True`, does not check that all\n                required packages for the components are installed\n                before loading them.\n            model_dir: The path of the model to load\n            component_builder: The\n                :class:`rasa.nlu.components.ComponentBuilder` to use.\n            new_config: Optional new config to use for the new epochs.\n            finetuning_epoch_fraction: Value to multiply all epochs by.\n\n        Returns:\n            An interpreter that uses the loaded model.\n        \"\"\"\n        model_metadata = Metadata.load(model_dir)\n\n        if new_config:\n            <IND>Interpreter._update_metadata_epochs(\n                model_metadata, new_config, finetuning_epoch_fraction\n            )\n\n        <DED>Interpreter.ensure_model_compatibility(model_metadata)\n        return Interpreter.create(\n            model_dir,\n            model_metadata,\n            component_builder,\n            skip_validation,\n            should_finetune=new_config is not None,\n        )\n\n    <DED>@staticmethod\n    def _get_default_value_for_component(name: Text, key: Text) -> Any:\n        <IND>from rasa.nlu.registry import get_component_class\n\n        return get_component_class(name).defaults[key]\n\n    <DED>@staticmethod\n    def _update_metadata_epochs(\n        model_metadata: Metadata,\n        new_config: Optional[Dict] = None,\n        finetuning_epoch_fraction: float = 1.0,\n    ) -> Metadata:\n        <IND>new_config = new_config or {}\n        for old_component_config, new_component_config in zip(\n            model_metadata.metadata[\"pipeline\"], new_config[\"pipeline\"]\n        ):\n            <IND>if EPOCHS in old_component_config:\n                <IND>new_epochs = new_component_config.get(\n                    EPOCHS,\n                    Interpreter._get_default_value_for_component(\n                        old_component_config[\"class\"], EPOCHS\n                    ),\n                )\n                old_component_config[EPOCHS] = ceil(\n                    new_epochs * finetuning_epoch_fraction\n                )\n        <DED><DED>return model_metadata\n\n    <DED>@staticmethod\n    def create(\n        model_dir: Text,\n        model_metadata: Metadata,\n        component_builder: Optional[ComponentBuilder] = None,\n        skip_validation: bool = False,\n        should_finetune: bool = False,\n    ) -> \"Interpreter\":\n        <IND>\"\"\"Create model and components defined by the provided metadata.\n\n        Args:\n            model_dir: The directory containing the model.\n            model_metadata: The metadata describing each component.\n            component_builder: The\n                :class:`rasa.nlu.components.ComponentBuilder` to use.\n            skip_validation: If set to `True`, does not check that all\n                required packages for the components are installed\n                before loading them.\n            should_finetune: Indicates if the model components will be fine-tuned.\n\n        Returns:\n            An interpreter that uses the created model.\n        \"\"\"\n        context: Dict[Text, Any] = {\"should_finetune\": should_finetune}\n\n        if component_builder is None:\n            # If no builder is passed, every interpreter creation will result\n            # in a new builder. hence, no components are reused.\n            <IND>component_builder = components.ComponentBuilder()\n\n        <DED>pipeline = []\n\n        # Before instantiating the component classes,\n        # lets check if all required packages are available\n        if not skip_validation:\n            <IND>components.validate_requirements(model_metadata.component_classes)\n\n        <DED>for i in range(model_metadata.number_of_components):\n            <IND>component_meta = model_metadata.for_component(i)\n            component = component_builder.load_component(\n                component_meta, model_dir, model_metadata, **context\n            )\n            try:\n                <IND>updates = component.provide_context()\n                if updates:\n                    <IND>context.update(updates)\n                <DED>pipeline.append(component)\n            <DED>except components.MissingArgumentError as e:\n                <IND>raise Exception(\n                    \"Failed to initialize component '{}'. \"\n                    \"{}\".format(component.name, e)\n                )\n\n        <DED><DED>return Interpreter(pipeline, context, model_metadata)\n\n    <DED>def __init__(\n        self,\n        pipeline: List[Component],\n        context: Optional[Dict[Text, Any]],\n        model_metadata: Optional[Metadata] = None,\n    ) -> None:\n\n        <IND>self.pipeline = pipeline\n        self.context = context if context is not None else {}\n        self.model_metadata = model_metadata\n        self.has_already_warned_of_overlapping_entities = False\n\n    <DED>def parse(\n        self,\n        text: Text,\n        time: Optional[datetime.datetime] = None,\n        only_output_properties: bool = True,\n    ) -> Dict[Text, Any]:\n        <IND>\"\"\"Parse the input text, classify it and return pipeline result.\n\n        The pipeline result usually contains intent and entities.\"\"\"\n\n        if not text:\n            # Not all components are able to handle empty strings. So we need\n            # to prevent that... This default return will not contain all\n            # output attributes of all components, but in the end, no one\n            # should pass an empty string in the first place.\n            <IND>output = self.default_output_attributes()\n            output[\"text\"] = \"\"\n            return output\n\n        <DED>timestamp = int(time.timestamp()) if time else None\n        data = self.default_output_attributes()\n        data[TEXT] = text\n\n        message = Message(data=data, time=timestamp, output_properties={TEXT_TOKENS})\n\n        for component in self.pipeline:\n            <IND>component.process(message, **self.context)\n\n        <DED>if not self.has_already_warned_of_overlapping_entities:\n            <IND>self.warn_of_overlapping_entities(message)\n\n        <DED>output = self.default_output_attributes()\n        output.update(message.as_dict(only_output_properties=only_output_properties))\n        return output\n\n    <DED>def featurize_message(self, message: Message) -> Message:\n        <IND>\"\"\"\n        Tokenize and featurize the input message\n        Args:\n            message: message storing text to process;\n        Returns:\n            message: it contains the tokens and features which are the output of the\n            NLU pipeline;\n        \"\"\"\n\n        for component in self.pipeline:\n            <IND>if not isinstance(component, (EntityExtractor, IntentClassifier)):\n                <IND>component.process(message, **self.context)\n        <DED><DED>return message\n\n    <DED>def warn_of_overlapping_entities(self, message: Message) -> None:\n        <IND>\"\"\"Issues a warning when there are overlapping entity annotations.\n\n        This warning is only issued once per Interpreter life time.\n\n        Args:\n            message: user message with all processing metadata such as entities\n        \"\"\"\n        overlapping_entity_pairs = message.find_overlapping_entities()\n        if len(overlapping_entity_pairs) > 0:\n            <IND>message_text = message.get(\"text\")\n            first_pair = overlapping_entity_pairs[0]\n            entity_1 = first_pair[0]\n            entity_2 = first_pair[1]\n            rasa.shared.utils.io.raise_warning(\n                f\"Parsing of message: '{message_text}' lead to overlapping \"\n                f\"entities: {entity_1['value']} of type \"\n                f\"{entity_1['entity']} extracted by \"\n                f\"{entity_1['extractor']} overlaps with \"\n                f\"{entity_2['value']} of type {entity_2['entity']} extracted by \"\n                f\"{entity_2['extractor']}. This can lead to unintended filling of \"\n                f\"slots. Please refer to the documentation section on entity \"\n                f\"extractors and entities getting extracted multiple times:\"\n                f\"{DOCS_URL_COMPONENTS}#entity-extractors\"\n            )\n            self.has_already_warned_of_overlapping_entities = True\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        write_json_to_file(filename, metadata, indent=4)\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "rasa/nlu/train.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/RasaHQ-rasa/rasa/nlu/train.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "rasa/nlu/train.py:119:29 Incompatible parameter type [6]: Expected `str` for 3rd positional only parameter to call `Trainer.persist` but got `Optional[str]`.",
    "message": " Expected `str` for 3rd positional only parameter to call `Trainer.persist` but got `Optional[str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 119,
    "warning_line": "            path, persistor, fixed_model_name, persist_nlu_training_data",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        return None\n\n\nasync def train(\n    nlu_config: Union[Text, Dict, RasaNLUModelConfig],\n    data: Union[Text, \"TrainingDataImporter\"],\n    path: Optional[Text] = None,\n    fixed_model_name: Optional[Text] = None,\n    storage: Optional[Text] = None,\n    component_builder: Optional[ComponentBuilder] = None,\n    training_data_endpoint: Optional[EndpointConfig] = None,\n    persist_nlu_training_data: bool = False,\n    model_to_finetune: Optional[Interpreter] = None,\n    **kwargs: Any,\n) -> Tuple[Trainer, Interpreter, Optional[Text]]:\n    \"\"\"Loads the trainer and the data and runs the training of the model.\"\"\"\n    from rasa.shared.importers.importer import TrainingDataImporter\n\n    if not isinstance(nlu_config, RasaNLUModelConfig):\n        nlu_config = config.load(nlu_config)\n\n    # Ensure we are training a model that we can save in the end\n    # WARN: there is still a race condition if a model with the same name is\n    # trained in another subprocess\n    trainer = Trainer(\n        nlu_config, component_builder, model_to_finetune=model_to_finetune\n    )\n    persistor = create_persistor(storage)\n    if training_data_endpoint is not None:\n        training_data = await load_data_from_endpoint(\n            training_data_endpoint, nlu_config.language\n        )\n    elif isinstance(data, TrainingDataImporter):\n        training_data = data.get_nlu_data(nlu_config.language)\n    else:\n        training_data = load_data(data, nlu_config.language)\n\n    training_data.print_stats()\n\n    interpreter = trainer.train(training_data, **kwargs)\n\n    if path:\n        persisted_path = trainer.persist(\n            path, persistor, fixed_model_name, persist_nlu_training_data\n        )\n    else:\n        persisted_path = None\n\n    return trainer, interpreter, persisted_path\n",
        "source_code_len": 1778,
        "target_code": "        return None\n",
        "target_code_len": 20,
        "diff_format": "@@ -76,49 +72,1 @@\n         return None\n-\n-\n-async def train(\n-    nlu_config: Union[Text, Dict, RasaNLUModelConfig],\n-    data: Union[Text, \"TrainingDataImporter\"],\n-    path: Optional[Text] = None,\n-    fixed_model_name: Optional[Text] = None,\n-    storage: Optional[Text] = None,\n-    component_builder: Optional[ComponentBuilder] = None,\n-    training_data_endpoint: Optional[EndpointConfig] = None,\n-    persist_nlu_training_data: bool = False,\n-    model_to_finetune: Optional[Interpreter] = None,\n-    **kwargs: Any,\n-) -> Tuple[Trainer, Interpreter, Optional[Text]]:\n-    \"\"\"Loads the trainer and the data and runs the training of the model.\"\"\"\n-    from rasa.shared.importers.importer import TrainingDataImporter\n-\n-    if not isinstance(nlu_config, RasaNLUModelConfig):\n-        nlu_config = config.load(nlu_config)\n-\n-    # Ensure we are training a model that we can save in the end\n-    # WARN: there is still a race condition if a model with the same name is\n-    # trained in another subprocess\n-    trainer = Trainer(\n-        nlu_config, component_builder, model_to_finetune=model_to_finetune\n-    )\n-    persistor = create_persistor(storage)\n-    if training_data_endpoint is not None:\n-        training_data = await load_data_from_endpoint(\n-            training_data_endpoint, nlu_config.language\n-        )\n-    elif isinstance(data, TrainingDataImporter):\n-        training_data = data.get_nlu_data(nlu_config.language)\n-    else:\n-        training_data = load_data(data, nlu_config.language)\n-\n-    training_data.print_stats()\n-\n-    interpreter = trainer.train(training_data, **kwargs)\n-\n-    if path:\n-        persisted_path = trainer.persist(\n-            path, persistor, fixed_model_name, persist_nlu_training_data\n-        )\n-    else:\n-        persisted_path = None\n-\n-    return trainer, interpreter, persisted_path\n",
        "source_code_with_indent": "        <IND>return None\n\n\n<DED><DED>async def train(\n    nlu_config: Union[Text, Dict, RasaNLUModelConfig],\n    data: Union[Text, \"TrainingDataImporter\"],\n    path: Optional[Text] = None,\n    fixed_model_name: Optional[Text] = None,\n    storage: Optional[Text] = None,\n    component_builder: Optional[ComponentBuilder] = None,\n    training_data_endpoint: Optional[EndpointConfig] = None,\n    persist_nlu_training_data: bool = False,\n    model_to_finetune: Optional[Interpreter] = None,\n    **kwargs: Any,\n) -> Tuple[Trainer, Interpreter, Optional[Text]]:\n    <IND>\"\"\"Loads the trainer and the data and runs the training of the model.\"\"\"\n    from rasa.shared.importers.importer import TrainingDataImporter\n\n    if not isinstance(nlu_config, RasaNLUModelConfig):\n        <IND>nlu_config = config.load(nlu_config)\n\n    # Ensure we are training a model that we can save in the end\n    # WARN: there is still a race condition if a model with the same name is\n    # trained in another subprocess\n    <DED>trainer = Trainer(\n        nlu_config, component_builder, model_to_finetune=model_to_finetune\n    )\n    persistor = create_persistor(storage)\n    if training_data_endpoint is not None:\n        <IND>training_data = await load_data_from_endpoint(\n            training_data_endpoint, nlu_config.language\n        )\n    <DED>elif isinstance(data, TrainingDataImporter):\n        <IND>training_data = data.get_nlu_data(nlu_config.language)\n    <DED>else:\n        <IND>training_data = load_data(data, nlu_config.language)\n\n    <DED>training_data.print_stats()\n\n    interpreter = trainer.train(training_data, **kwargs)\n\n    if path:\n        <IND>persisted_path = trainer.persist(\n            path, persistor, fixed_model_name, persist_nlu_training_data\n        )\n    <DED>else:\n        <IND>persisted_path = None\n\n    <DED>return trainer, interpreter, persisted_path\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <IND>return None\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "rasa/server.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/RasaHQ-rasa/rasa/server.py",
    "file_hunks_size": 22,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "rasa/server.py:500:12 Incompatible parameter type [6]: Expected `Union[EndpointConfig, rasa.core.nlg.generator.NaturalLanguageGenerator]` for 4th parameter `generator` to call `rasa.core.agent.load_agent` but got `Optional[EndpointConfig]`.",
    "message": " Expected `Union[EndpointConfig, rasa.core.nlg.generator.NaturalLanguageGenerator]` for 4th parameter `generator` to call `rasa.core.agent.load_agent` but got `Optional[EndpointConfig]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 500,
    "warning_line": "            generator=generator,"
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "rasa/shared/core/training_data/visualization.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/RasaHQ-rasa/rasa/shared/core/training_data/visualization.py",
    "file_hunks_size": 4,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "rasa/shared/core/training_data/visualization.py:451:54 Incompatible parameter type [6]: Expected `str` for 1st positional only parameter to call `NaturalLanguageInterpreter.parse` but got `Optional[str]`.",
    "message": " Expected `str` for 1st positional only parameter to call `NaturalLanguageInterpreter.parse` but got `Optional[str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 451,
    "warning_line": "                    message = await interpreter.parse(el.text)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    max_history: int = 2,\n    interpreter: NaturalLanguageInterpreter = RegexInterpreter(),\n    nlu_training_data: Optional[\"TrainingData\"] = None,\n",
        "source_code_len": 148,
        "target_code": "    max_history: int = 2,\n    interpreter=None,\n    nlu_training_data: Optional[\"TrainingData\"] = None,\n",
        "target_code_len": 104,
        "diff_format": "@@ -419,3 +418,3 @@\n     max_history: int = 2,\n-    interpreter: NaturalLanguageInterpreter = RegexInterpreter(),\n+    interpreter=None,\n     nlu_training_data: Optional[\"TrainingData\"] = None,\n",
        "source_code_with_indent": "    max_history: int = 2,\n    interpreter: NaturalLanguageInterpreter = RegexInterpreter(),\n    nlu_training_data: Optional[\"TrainingData\"] = None,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    max_history: int = 2,\n    interpreter=None,\n    nlu_training_data: Optional[\"TrainingData\"] = None,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "rasa/shared/core/training_data/visualization.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/RasaHQ-rasa/rasa/shared/core/training_data/visualization.py",
    "file_hunks_size": 4,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "rasa/shared/core/training_data/visualization.py:466:27 Incompatible parameter type [6]: Expected `Optional[Dict[str, typing.Any]]` for 2nd positional only parameter to call `_add_message_edge` but got `typing.Union[None, Dict[str, typing.Any], rasa.shared.core.events.NLUPredictionData]`.",
    "message": " Expected `Optional[Dict[str, typing.Any]]` for 2nd positional only parameter to call `_add_message_edge` but got `typing.Union[None, Dict[str, typing.Any], rasa.shared.core.events.NLUPredictionData]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 466,
    "warning_line": "                    graph, message, current_node, next_node_idx, is_current",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    max_history: int = 2,\n    interpreter: NaturalLanguageInterpreter = RegexInterpreter(),\n    nlu_training_data: Optional[\"TrainingData\"] = None,\n",
        "source_code_len": 148,
        "target_code": "    max_history: int = 2,\n    interpreter=None,\n    nlu_training_data: Optional[\"TrainingData\"] = None,\n",
        "target_code_len": 104,
        "diff_format": "@@ -419,3 +418,3 @@\n     max_history: int = 2,\n-    interpreter: NaturalLanguageInterpreter = RegexInterpreter(),\n+    interpreter=None,\n     nlu_training_data: Optional[\"TrainingData\"] = None,\n",
        "source_code_with_indent": "    max_history: int = 2,\n    interpreter: NaturalLanguageInterpreter = RegexInterpreter(),\n    nlu_training_data: Optional[\"TrainingData\"] = None,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    max_history: int = 2,\n    interpreter=None,\n    nlu_training_data: Optional[\"TrainingData\"] = None,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "rasa/shared/core/training_data/visualization.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/RasaHQ-rasa/rasa/shared/core/training_data/visualization.py",
    "file_hunks_size": 4,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "rasa/shared/core/training_data/visualization.py:507:33 Incompatible parameter type [6]: Expected `Optional[Dict[str, typing.Any]]` for 2nd positional only parameter to call `_add_message_edge` but got `typing.Union[None, Dict[str, typing.Any], rasa.shared.core.events.NLUPredictionData]`.",
    "message": " Expected `Optional[Dict[str, typing.Any]]` for 2nd positional only parameter to call `_add_message_edge` but got `typing.Union[None, Dict[str, typing.Any], rasa.shared.core.events.NLUPredictionData]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 507,
    "warning_line": "        _add_message_edge(graph, message, current_node, target, is_current)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    max_history: int = 2,\n    interpreter: NaturalLanguageInterpreter = RegexInterpreter(),\n    nlu_training_data: Optional[\"TrainingData\"] = None,\n",
        "source_code_len": 148,
        "target_code": "    max_history: int = 2,\n    interpreter=None,\n    nlu_training_data: Optional[\"TrainingData\"] = None,\n",
        "target_code_len": 104,
        "diff_format": "@@ -419,3 +418,3 @@\n     max_history: int = 2,\n-    interpreter: NaturalLanguageInterpreter = RegexInterpreter(),\n+    interpreter=None,\n     nlu_training_data: Optional[\"TrainingData\"] = None,\n",
        "source_code_with_indent": "    max_history: int = 2,\n    interpreter: NaturalLanguageInterpreter = RegexInterpreter(),\n    nlu_training_data: Optional[\"TrainingData\"] = None,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    max_history: int = 2,\n    interpreter=None,\n    nlu_training_data: Optional[\"TrainingData\"] = None,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/featurizers/test_tracker_featurizer.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/featurizers/test_tracker_featurizer.py:47:53 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `SingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/featurizers/test_tracker_featurizer.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/featurizers/test_tracker_featurizer.py:757:8 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `SingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/featurizers/test_tracker_featurizer.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/featurizers/test_tracker_featurizer.py:878:8 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `SingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/featurizers/test_tracker_featurizer.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/featurizers/test_tracker_featurizer.py:944:8 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `SingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/featurizers/test_tracker_featurizer.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/featurizers/test_tracker_featurizer.py:1010:8 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `SingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/featurizers/test_tracker_featurizer.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/featurizers/test_tracker_featurizer.py:1125:8 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `SingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/featurizers/test_tracker_featurizer.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/featurizers/test_tracker_featurizer.py:1188:8 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `SingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/featurizers/test_tracker_featurizer.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/featurizers/test_tracker_featurizer.py:1252:8 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `SingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/featurizers/test_tracker_featurizer.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/featurizers/test_tracker_featurizer.py:1299:8 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `SingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/featurizers/test_tracker_featurizer.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/featurizers/test_tracker_featurizer.py:1351:8 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `SingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/featurizers/test_tracker_featurizer.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/featurizers/test_tracker_featurizer.py:1440:8 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `SingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/featurizers/test_tracker_featurizer.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/featurizers/test_tracker_featurizer.py:1506:8 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `SingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/featurizers/test_tracker_featurizer.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/featurizers/test_tracker_featurizer.py:1585:8 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `IntentTokenizerSingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/featurizers/test_tracker_featurizer.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/featurizers/test_tracker_featurizer.py:1663:8 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `IntentTokenizerSingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/featurizers/test_tracker_featurizer.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/featurizers/test_tracker_featurizer.py:1730:8 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `IntentTokenizerSingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/featurizers/test_tracker_featurizer.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/featurizers/test_tracker_featurizer.py:1788:8 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `IntentTokenizerSingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/featurizers/test_tracker_featurizer.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/featurizers/test_tracker_featurizer.py:1869:8 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `IntentTokenizerSingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/featurizers/test_tracker_featurizer.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/featurizers/test_tracker_featurizer.py:1934:8 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `IntentTokenizerSingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/featurizers/test_tracker_featurizer.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/featurizers/test_tracker_featurizer.py:2001:8 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `IntentTokenizerSingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/featurizers/test_tracker_featurizer.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/featurizers/test_tracker_featurizer.py:2052:8 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `IntentTokenizerSingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/featurizers/test_tracker_featurizer.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/featurizers/test_tracker_featurizer.py:2099:8 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `IntentTokenizerSingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/featurizers/test_tracker_featurizer.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/featurizers/test_tracker_featurizer.py:2159:8 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `IntentTokenizerSingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/featurizers/test_tracker_featurizer.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/featurizers/test_tracker_featurizer.py:2221:8 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `IntentTokenizerSingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/featurizers/test_tracker_featurizer.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/featurizers/test_tracker_featurizer.py:2289:8 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `IntentTokenizerSingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/policies/test_unexpected_intent_policy.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/policies/test_unexpected_intent_policy.py:69:12 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `rasa.core.featurizers.tracker_featurizers.MaxHistoryTrackerFeaturizer2.__init__` but got `IntentTokenizerSingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/policies/test_unexpected_intent_policy.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/policies/test_unexpected_intent_policy.py:1149:12 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `rasa.core.featurizers.tracker_featurizers.MaxHistoryTrackerFeaturizer2.__init__` but got `IntentTokenizerSingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/core/test_policies.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/core/test_policies.py:122:12 Incompatible parameter type [6]: Expected `Optional[rasa.core.featurizers._single_state_featurizer.SingleStateFeaturizer]` for 1st positional only parameter to call `MaxHistoryTrackerFeaturizer.__init__` but got `SingleStateFeaturizer`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/nlu/test_components.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/nlu/test_components.py:107:30 Incompatible parameter type [6]: Expected `str` for 1st positional only parameter to call `Interpreter.load` but got `Optional[str]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/RasaHQ-rasa/tests/nlu/test_components.py'",
    "dd_fail": true
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/nlu/test_train.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/RasaHQ-rasa/tests/nlu/test_train.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "tests/nlu/test_train.py:162:11 Unbound name [10]: Name `trained` is used but not defined in the current scope.",
    "message": " Name `trained` is used but not defined in the current scope.",
    "rule_id": "Unbound name [10]",
    "warning_line_no": 162,
    "warning_line": "    assert trained.pipeline"
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/nlu/test_train.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/RasaHQ-rasa/tests/nlu/test_train.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "tests/nlu/test_train.py:164:30 Incompatible parameter type [6]: Expected `str` for 1st positional only parameter to call `Interpreter.load` but got `Optional[str]`.",
    "message": " Expected `str` for 1st positional only parameter to call `Interpreter.load` but got `Optional[str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 164,
    "warning_line": "    loaded = Interpreter.load(persisted_path, component_builder)"
  },
  {
    "project": "RasaHQ/rasa",
    "commit": "2e7b6ee19155244d573d6a22b4c9c9da1e6c1be9",
    "filename": "tests/nlu/test_train.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/RasaHQ-rasa/tests/nlu/test_train.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "tests/nlu/test_train.py:164:46 Unbound name [10]: Name `component_builder` is used but not defined in the current scope.",
    "message": " Name `component_builder` is used but not defined in the current scope.",
    "rule_id": "Unbound name [10]",
    "warning_line_no": 164,
    "warning_line": "    loaded = Interpreter.load(persisted_path, component_builder)"
  }
]