[
  {
    "project": "allenai/allennlp",
    "commit": "8e3c612815cb03dfaf4f17f9ce79fcfe03088e30",
    "filename": "allennlp/nn/initializers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/nn/initializers.py",
    "file_hunks_size": 8,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/nn/initializers.py:180:37 Incompatible parameter type [6]: Expected `Dict[str, Tuple[Type[typing.Any], str]]` for 2nd positional only parameter to call `dict.__setitem__` but got `Dict[str, Tuple[Type[Initializer], None]]`.",
    "message": " Expected `Dict[str, Tuple[Type[typing.Any], str]]` for 2nd positional only parameter to call `dict.__setitem__` but got `Dict[str, Tuple[Type[Initializer], None]]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 180,
    "warning_line": "Registrable._registry[Initializer] = {",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\ndef _initializer_wrapper(init_function: Callable[..., None]) -> Type[Initializer]:\n    class Init(Initializer):\n        _initializer_wrapper = True\n\n        def __init__(self, **kwargs):\n            self._init_function = init_function\n            self._kwargs = kwargs\n\n        def __call__(self, tensor: torch.Tensor, **kwargs) -> None:\n            self._init_function(tensor, **self._kwargs)\n\n        def __repr__(self):\n            return \"Init: %s, with params: %s\" % (self._init_function, self._kwargs)\n\n        @classmethod\n        def from_params(cls, params: Params, **extras):  # type: ignore\n            return cls(**params.as_dict())\n\n    return Init\n\n\n# There are no classes to decorate, so we hack these into Registrable._registry\nRegistrable._registry[Initializer] = {\n    \"normal\": (_initializer_wrapper(torch.nn.init.normal_), None),\n    \"uniform\": (_initializer_wrapper(torch.nn.init.uniform_), None),\n    \"orthogonal\": (_initializer_wrapper(torch.nn.init.orthogonal_), None),\n    \"constant\": (_initializer_wrapper(torch.nn.init.constant_), None),\n    \"dirac\": (_initializer_wrapper(torch.nn.init.dirac_), None),\n    \"xavier_normal\": (_initializer_wrapper(torch.nn.init.xavier_normal_), None),\n    \"xavier_uniform\": (_initializer_wrapper(torch.nn.init.xavier_uniform_), None),\n    \"kaiming_normal\": (_initializer_wrapper(torch.nn.init.kaiming_normal_), None),\n    \"kaiming_uniform\": (_initializer_wrapper(torch.nn.init.kaiming_uniform_), None),\n    \"sparse\": (_initializer_wrapper(torch.nn.init.sparse_), None),\n    \"eye\": (_initializer_wrapper(torch.nn.init.eye_), None),\n    \"block_orthogonal\": (_initializer_wrapper(block_orthogonal), None),\n    \"uniform_unit_scaling\": (_initializer_wrapper(uniform_unit_scaling), None),\n    \"zero\": (_initializer_wrapper(zero), None),\n    \"lstm_hidden_bias\": (_initializer_wrapper(lstm_hidden_bias), None),\n}\n\n",
        "source_code_len": 1866,
        "target_code": "\nclass _InitializerWrapper(Initializer):\n    def __init__(self, init_function: Callable[..., None], **kwargs):\n        self._init_function = init_function\n        self._kwargs = kwargs\n\n    def __call__(self, tensor: torch.Tensor, **kwargs) -> None:\n        self._init_function(tensor, **self._kwargs)\n\n    def __repr__(self):\n        return \"Init: %s, with params: %s\" % (self._init_function, self._kwargs)\n\n\n@Initializer.register(\"normal\")\nclass NormalInitializer(_InitializerWrapper):\n    def __init__(self, mean: float = 0.0, std: float = 0.1):\n        super().__init__(init_function=torch.nn.init.normal_, mean=mean, std=std)\n\n\n@Initializer.register(\"orthogonal\")\nclass OrthogonalInitializer(_InitializerWrapper):\n    def __init__(self, gain: float = 1.0):\n        super().__init__(init_function=torch.nn.init.orthogonal_, gain=gain)\n\n\n@Initializer.register(\"uniform\")\nclass UniformInitializer(_InitializerWrapper):\n    def __init__(self, a: float = 0.0, b: float = 1.0):\n        super().__init__(init_function=torch.nn.init.uniform_, a=a, b=b)\n\n\n@Initializer.register(\"constant\")\nclass ConstantInitializer(_InitializerWrapper):\n    def __init__(self, val: float):\n        super().__init__(init_function=torch.nn.init.constant_, val=val)\n\n\n@Initializer.register(\"dirac\")\nclass DiracInitializer(_InitializerWrapper):\n    def __init__(self):\n        super().__init__(init_function=torch.nn.init.dirac_)\n\n\n@Initializer.register(\"xavier_uniform\")\nclass XavierUniformInitializer(_InitializerWrapper):\n    def __init__(self, gain: float = 1.0):\n        super().__init__(init_function=torch.nn.init.xavier_uniform_, gain=gain)\n\n\n@Initializer.register(\"xavier_normal\")\nclass XavierNormalInitializer(_InitializerWrapper):\n    def __init__(self, gain: float = 1.0):\n        super().__init__(init_function=torch.nn.init.xavier_normal_, gain=gain)\n\n\n@Initializer.register(\"kaiming_uniform\")\nclass KaimingUniformInitializer(_InitializerWrapper):\n    def __init__(self, a: float = 0.0, mode: str = \"fan_in\", nonlinearity: str = \"leaky_relu\"):\n        super().__init__(\n            init_function=torch.nn.init.kaiming_uniform_, a=a, mode=mode, nonlinearity=nonlinearity\n        )\n\n\n@Initializer.register(\"kaiming_normal\")\nclass KaimingNormalInitializer(_InitializerWrapper):\n    def __init__(self, a: float = 0.0, mode: str = \"fan_in\", nonlinearity: str = \"leaky_relu\"):\n        super().__init__(\n            init_function=torch.nn.init.kaiming_normal_, a=a, mode=mode, nonlinearity=nonlinearity\n        )\n\n\n@Initializer.register(\"sparse\")\nclass SparseInitializer(_InitializerWrapper):\n    def __init__(self, sparsity: float, std: float = 0.01):\n        super().__init__(init_function=torch.nn.init.sparse_, sparsity=sparsity, std=std)\n\n\n@Initializer.register(\"eye\")\nclass EyeInitializer(_InitializerWrapper):\n    def __init__(self):\n        super().__init__(init_function=torch.nn.init.eye_)\n\n\n@Initializer.register(\"block_orthogonal\")\nclass BlockOrthogonalInitializer(_InitializerWrapper):\n    def __init__(self, split_sizes: List[int], gain: float = 1.0):\n        super().__init__(init_function=block_orthogonal, split_sizes=split_sizes, gain=gain)\n\n\n@Initializer.register(\"uniform_unit_scaling\")\nclass UniformUnitScalingInitializer(_InitializerWrapper):\n    def __init__(self, nonlinearity: str = \"linear\"):\n        super().__init__(init_function=uniform_unit_scaling, nonlinearity=nonlinearity)\n\n\n@Initializer.register(\"zero\")\nclass ZeroInitializer(_InitializerWrapper):\n    def __init__(self):\n        super().__init__(init_function=zero)\n\n\n@Initializer.register(\"lstm_hidden_bias\")\nclass LstmHiddenBiasInitializer(_InitializerWrapper):\n    def __init__(self):\n        super().__init__(init_function=lstm_hidden_bias)\n\n",
        "target_code_len": 3715,
        "diff_format": "@@ -157,41 +156,106 @@\n \n-def _initializer_wrapper(init_function: Callable[..., None]) -> Type[Initializer]:\n-    class Init(Initializer):\n-        _initializer_wrapper = True\n-\n-        def __init__(self, **kwargs):\n-            self._init_function = init_function\n-            self._kwargs = kwargs\n-\n-        def __call__(self, tensor: torch.Tensor, **kwargs) -> None:\n-            self._init_function(tensor, **self._kwargs)\n-\n-        def __repr__(self):\n-            return \"Init: %s, with params: %s\" % (self._init_function, self._kwargs)\n-\n-        @classmethod\n-        def from_params(cls, params: Params, **extras):  # type: ignore\n-            return cls(**params.as_dict())\n-\n-    return Init\n-\n-\n-# There are no classes to decorate, so we hack these into Registrable._registry\n-Registrable._registry[Initializer] = {\n-    \"normal\": (_initializer_wrapper(torch.nn.init.normal_), None),\n-    \"uniform\": (_initializer_wrapper(torch.nn.init.uniform_), None),\n-    \"orthogonal\": (_initializer_wrapper(torch.nn.init.orthogonal_), None),\n-    \"constant\": (_initializer_wrapper(torch.nn.init.constant_), None),\n-    \"dirac\": (_initializer_wrapper(torch.nn.init.dirac_), None),\n-    \"xavier_normal\": (_initializer_wrapper(torch.nn.init.xavier_normal_), None),\n-    \"xavier_uniform\": (_initializer_wrapper(torch.nn.init.xavier_uniform_), None),\n-    \"kaiming_normal\": (_initializer_wrapper(torch.nn.init.kaiming_normal_), None),\n-    \"kaiming_uniform\": (_initializer_wrapper(torch.nn.init.kaiming_uniform_), None),\n-    \"sparse\": (_initializer_wrapper(torch.nn.init.sparse_), None),\n-    \"eye\": (_initializer_wrapper(torch.nn.init.eye_), None),\n-    \"block_orthogonal\": (_initializer_wrapper(block_orthogonal), None),\n-    \"uniform_unit_scaling\": (_initializer_wrapper(uniform_unit_scaling), None),\n-    \"zero\": (_initializer_wrapper(zero), None),\n-    \"lstm_hidden_bias\": (_initializer_wrapper(lstm_hidden_bias), None),\n-}\n+class _InitializerWrapper(Initializer):\n+    def __init__(self, init_function: Callable[..., None], **kwargs):\n+        self._init_function = init_function\n+        self._kwargs = kwargs\n+\n+    def __call__(self, tensor: torch.Tensor, **kwargs) -> None:\n+        self._init_function(tensor, **self._kwargs)\n+\n+    def __repr__(self):\n+        return \"Init: %s, with params: %s\" % (self._init_function, self._kwargs)\n+\n+\n+@Initializer.register(\"normal\")\n+class NormalInitializer(_InitializerWrapper):\n+    def __init__(self, mean: float = 0.0, std: float = 0.1):\n+        super().__init__(init_function=torch.nn.init.normal_, mean=mean, std=std)\n+\n+\n+@Initializer.register(\"orthogonal\")\n+class OrthogonalInitializer(_InitializerWrapper):\n+    def __init__(self, gain: float = 1.0):\n+        super().__init__(init_function=torch.nn.init.orthogonal_, gain=gain)\n+\n+\n+@Initializer.register(\"uniform\")\n+class UniformInitializer(_InitializerWrapper):\n+    def __init__(self, a: float = 0.0, b: float = 1.0):\n+        super().__init__(init_function=torch.nn.init.uniform_, a=a, b=b)\n+\n+\n+@Initializer.register(\"constant\")\n+class ConstantInitializer(_InitializerWrapper):\n+    def __init__(self, val: float):\n+        super().__init__(init_function=torch.nn.init.constant_, val=val)\n+\n+\n+@Initializer.register(\"dirac\")\n+class DiracInitializer(_InitializerWrapper):\n+    def __init__(self):\n+        super().__init__(init_function=torch.nn.init.dirac_)\n+\n+\n+@Initializer.register(\"xavier_uniform\")\n+class XavierUniformInitializer(_InitializerWrapper):\n+    def __init__(self, gain: float = 1.0):\n+        super().__init__(init_function=torch.nn.init.xavier_uniform_, gain=gain)\n+\n+\n+@Initializer.register(\"xavier_normal\")\n+class XavierNormalInitializer(_InitializerWrapper):\n+    def __init__(self, gain: float = 1.0):\n+        super().__init__(init_function=torch.nn.init.xavier_normal_, gain=gain)\n+\n+\n+@Initializer.register(\"kaiming_uniform\")\n+class KaimingUniformInitializer(_InitializerWrapper):\n+    def __init__(self, a: float = 0.0, mode: str = \"fan_in\", nonlinearity: str = \"leaky_relu\"):\n+        super().__init__(\n+            init_function=torch.nn.init.kaiming_uniform_, a=a, mode=mode, nonlinearity=nonlinearity\n+        )\n+\n+\n+@Initializer.register(\"kaiming_normal\")\n+class KaimingNormalInitializer(_InitializerWrapper):\n+    def __init__(self, a: float = 0.0, mode: str = \"fan_in\", nonlinearity: str = \"leaky_relu\"):\n+        super().__init__(\n+            init_function=torch.nn.init.kaiming_normal_, a=a, mode=mode, nonlinearity=nonlinearity\n+        )\n+\n+\n+@Initializer.register(\"sparse\")\n+class SparseInitializer(_InitializerWrapper):\n+    def __init__(self, sparsity: float, std: float = 0.01):\n+        super().__init__(init_function=torch.nn.init.sparse_, sparsity=sparsity, std=std)\n+\n+\n+@Initializer.register(\"eye\")\n+class EyeInitializer(_InitializerWrapper):\n+    def __init__(self):\n+        super().__init__(init_function=torch.nn.init.eye_)\n+\n+\n+@Initializer.register(\"block_orthogonal\")\n+class BlockOrthogonalInitializer(_InitializerWrapper):\n+    def __init__(self, split_sizes: List[int], gain: float = 1.0):\n+        super().__init__(init_function=block_orthogonal, split_sizes=split_sizes, gain=gain)\n+\n+\n+@Initializer.register(\"uniform_unit_scaling\")\n+class UniformUnitScalingInitializer(_InitializerWrapper):\n+    def __init__(self, nonlinearity: str = \"linear\"):\n+        super().__init__(init_function=uniform_unit_scaling, nonlinearity=nonlinearity)\n+\n+\n+@Initializer.register(\"zero\")\n+class ZeroInitializer(_InitializerWrapper):\n+    def __init__(self):\n+        super().__init__(init_function=zero)\n+\n+\n+@Initializer.register(\"lstm_hidden_bias\")\n+class LstmHiddenBiasInitializer(_InitializerWrapper):\n+    def __init__(self):\n+        super().__init__(init_function=lstm_hidden_bias)\n \n",
        "source_code_with_indent": "\n<DED>def _initializer_wrapper(init_function: Callable[..., None]) -> Type[Initializer]:\n    <IND>class Init(Initializer):\n        <IND>_initializer_wrapper = True\n\n        def __init__(self, **kwargs):\n            <IND>self._init_function = init_function\n            self._kwargs = kwargs\n\n        <DED>def __call__(self, tensor: torch.Tensor, **kwargs) -> None:\n            <IND>self._init_function(tensor, **self._kwargs)\n\n        <DED>def __repr__(self):\n            <IND>return \"Init: %s, with params: %s\" % (self._init_function, self._kwargs)\n\n        <DED>@classmethod\n        def from_params(cls, params: Params, **extras):  # type: ignore\n            <IND>return cls(**params.as_dict())\n\n    <DED><DED>return Init\n\n\n# There are no classes to decorate, so we hack these into Registrable._registry\n<DED>Registrable._registry[Initializer] = {\n    \"normal\": (_initializer_wrapper(torch.nn.init.normal_), None),\n    \"uniform\": (_initializer_wrapper(torch.nn.init.uniform_), None),\n    \"orthogonal\": (_initializer_wrapper(torch.nn.init.orthogonal_), None),\n    \"constant\": (_initializer_wrapper(torch.nn.init.constant_), None),\n    \"dirac\": (_initializer_wrapper(torch.nn.init.dirac_), None),\n    \"xavier_normal\": (_initializer_wrapper(torch.nn.init.xavier_normal_), None),\n    \"xavier_uniform\": (_initializer_wrapper(torch.nn.init.xavier_uniform_), None),\n    \"kaiming_normal\": (_initializer_wrapper(torch.nn.init.kaiming_normal_), None),\n    \"kaiming_uniform\": (_initializer_wrapper(torch.nn.init.kaiming_uniform_), None),\n    \"sparse\": (_initializer_wrapper(torch.nn.init.sparse_), None),\n    \"eye\": (_initializer_wrapper(torch.nn.init.eye_), None),\n    \"block_orthogonal\": (_initializer_wrapper(block_orthogonal), None),\n    \"uniform_unit_scaling\": (_initializer_wrapper(uniform_unit_scaling), None),\n    \"zero\": (_initializer_wrapper(zero), None),\n    \"lstm_hidden_bias\": (_initializer_wrapper(lstm_hidden_bias), None),\n}\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n<DED>class _InitializerWrapper(Initializer):\n    <IND>def __init__(self, init_function: Callable[..., None], **kwargs):\n        <IND>self._init_function = init_function\n        self._kwargs = kwargs\n\n    <DED>def __call__(self, tensor: torch.Tensor, **kwargs) -> None:\n        <IND>self._init_function(tensor, **self._kwargs)\n\n    <DED>def __repr__(self):\n        <IND>return \"Init: %s, with params: %s\" % (self._init_function, self._kwargs)\n\n\n<DED><DED>@Initializer.register(\"normal\")\nclass NormalInitializer(_InitializerWrapper):\n    <IND>def __init__(self, mean: float = 0.0, std: float = 0.1):\n        <IND>super().__init__(init_function=torch.nn.init.normal_, mean=mean, std=std)\n\n\n<DED><DED>@Initializer.register(\"orthogonal\")\nclass OrthogonalInitializer(_InitializerWrapper):\n    <IND>def __init__(self, gain: float = 1.0):\n        <IND>super().__init__(init_function=torch.nn.init.orthogonal_, gain=gain)\n\n\n<DED><DED>@Initializer.register(\"uniform\")\nclass UniformInitializer(_InitializerWrapper):\n    <IND>def __init__(self, a: float = 0.0, b: float = 1.0):\n        <IND>super().__init__(init_function=torch.nn.init.uniform_, a=a, b=b)\n\n\n<DED><DED>@Initializer.register(\"constant\")\nclass ConstantInitializer(_InitializerWrapper):\n    <IND>def __init__(self, val: float):\n        <IND>super().__init__(init_function=torch.nn.init.constant_, val=val)\n\n\n<DED><DED>@Initializer.register(\"dirac\")\nclass DiracInitializer(_InitializerWrapper):\n    <IND>def __init__(self):\n        <IND>super().__init__(init_function=torch.nn.init.dirac_)\n\n\n<DED><DED>@Initializer.register(\"xavier_uniform\")\nclass XavierUniformInitializer(_InitializerWrapper):\n    <IND>def __init__(self, gain: float = 1.0):\n        <IND>super().__init__(init_function=torch.nn.init.xavier_uniform_, gain=gain)\n\n\n<DED><DED>@Initializer.register(\"xavier_normal\")\nclass XavierNormalInitializer(_InitializerWrapper):\n    <IND>def __init__(self, gain: float = 1.0):\n        <IND>super().__init__(init_function=torch.nn.init.xavier_normal_, gain=gain)\n\n\n<DED><DED>@Initializer.register(\"kaiming_uniform\")\nclass KaimingUniformInitializer(_InitializerWrapper):\n    <IND>def __init__(self, a: float = 0.0, mode: str = \"fan_in\", nonlinearity: str = \"leaky_relu\"):\n        <IND>super().__init__(\n            init_function=torch.nn.init.kaiming_uniform_, a=a, mode=mode, nonlinearity=nonlinearity\n        )\n\n\n<DED><DED>@Initializer.register(\"kaiming_normal\")\nclass KaimingNormalInitializer(_InitializerWrapper):\n    <IND>def __init__(self, a: float = 0.0, mode: str = \"fan_in\", nonlinearity: str = \"leaky_relu\"):\n        <IND>super().__init__(\n            init_function=torch.nn.init.kaiming_normal_, a=a, mode=mode, nonlinearity=nonlinearity\n        )\n\n\n<DED><DED>@Initializer.register(\"sparse\")\nclass SparseInitializer(_InitializerWrapper):\n    <IND>def __init__(self, sparsity: float, std: float = 0.01):\n        <IND>super().__init__(init_function=torch.nn.init.sparse_, sparsity=sparsity, std=std)\n\n\n<DED><DED>@Initializer.register(\"eye\")\nclass EyeInitializer(_InitializerWrapper):\n    <IND>def __init__(self):\n        <IND>super().__init__(init_function=torch.nn.init.eye_)\n\n\n<DED><DED>@Initializer.register(\"block_orthogonal\")\nclass BlockOrthogonalInitializer(_InitializerWrapper):\n    <IND>def __init__(self, split_sizes: List[int], gain: float = 1.0):\n        <IND>super().__init__(init_function=block_orthogonal, split_sizes=split_sizes, gain=gain)\n\n\n<DED><DED>@Initializer.register(\"uniform_unit_scaling\")\nclass UniformUnitScalingInitializer(_InitializerWrapper):\n    <IND>def __init__(self, nonlinearity: str = \"linear\"):\n        <IND>super().__init__(init_function=uniform_unit_scaling, nonlinearity=nonlinearity)\n\n\n<DED><DED>@Initializer.register(\"zero\")\nclass ZeroInitializer(_InitializerWrapper):\n    <IND>def __init__(self):\n        <IND>super().__init__(init_function=zero)\n\n\n<DED><DED>@Initializer.register(\"lstm_hidden_bias\")\nclass LstmHiddenBiasInitializer(_InitializerWrapper):\n    <IND>def __init__(self):\n        <IND>super().__init__(init_function=lstm_hidden_bias)\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "8e3c612815cb03dfaf4f17f9ce79fcfe03088e30",
    "filename": "allennlp/nn/initializers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/nn/initializers.py",
    "file_hunks_size": 8,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/nn/initializers.py:291:14 Incompatible variable type [9]: initializers is declared to have type `List[Tuple[str, Initializer]]` but is used as type `None`.",
    "message": " initializers is declared to have type `List[Tuple[str, Initializer]]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 291,
    "warning_line": "        self, initializers: List[Tuple[str, Initializer]] = None, prevent_regexes: List[str] = None"
  },
  {
    "project": "allenai/allennlp",
    "commit": "8e3c612815cb03dfaf4f17f9ce79fcfe03088e30",
    "filename": "allennlp/nn/initializers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/nn/initializers.py",
    "file_hunks_size": 8,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/nn/initializers.py:345:25 Incompatible variable type [9]: params is declared to have type `List[Tuple[str, Params]]` but is used as type `None`.",
    "message": " params is declared to have type `List[Tuple[str, Params]]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 345,
    "warning_line": "    def from_params(cls, params: List[Tuple[str, Params]] = None) -> \"InitializerApplicator\":",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "            logger.info(\"   %s\", name)\n\n    @classmethod\n    def from_params(cls, params: List[Tuple[str, Params]] = None) -> \"InitializerApplicator\":\n        \"\"\"\n        Converts a Params object into an InitializerApplicator. The json should\n        be formatted as follows:\n\n        ```\n            [\n                [\"parameter_regex_match1\",\n                    {\n                        \"type\": \"normal\"\n                        \"mean\": 0.01\n                        \"std\": 0.1\n                    }\n                ],\n                [\"parameter_regex_match2\", \"uniform\"]\n                [\"prevent_init_regex\", \"prevent\"]\n            ]\n        ```\n\n        where the first item in each tuple is the regex that matches to parameters, and the second\n        item is a set of parameters that will be passed to `Initialzer.from_params()`.  These\n        values can either be strings, in which case they correspond to the names of initializers,\n        or dictionaries, in which case they must contain the \"type\" key, corresponding to the name\n        of an initializer.  In addition, they may contain auxiliary named parameters which will be\n        fed to the initializer itself. To determine valid auxiliary parameters, please refer to the\n        torch.nn.init documentation. Only \"prevent\" is a special type which does not have corresponding\n        initializer. Any parameter matching its corresponding regex will be overridden to NOT initialize.\n\n        # Returns\n\n        An InitializerApplicator containing the specified initializers.\n        \"\"\"\n\n        params = params or []\n\n        def is_prevent(item):\n            return item in (\"prevent\", {\"type\": \"prevent\"})\n\n        prevent_regexes = [param[0] for param in params if is_prevent(param[1])]\n        params = [param for param in params if param[1] if not is_prevent(param[1])]\n        initializers = [\n            (name, Initializer.from_params(init_params)) for name, init_params in params\n        ]\n        return InitializerApplicator(initializers, prevent_regexes)\n",
        "source_code_len": 2037,
        "target_code": "            logger.info(\"   %s\", name)\n",
        "target_code_len": 39,
        "diff_format": "@@ -342,47 +435,1 @@\n             logger.info(\"   %s\", name)\n-\n-    @classmethod\n-    def from_params(cls, params: List[Tuple[str, Params]] = None) -> \"InitializerApplicator\":\n-        \"\"\"\n-        Converts a Params object into an InitializerApplicator. The json should\n-        be formatted as follows:\n-\n-        ```\n-            [\n-                [\"parameter_regex_match1\",\n-                    {\n-                        \"type\": \"normal\"\n-                        \"mean\": 0.01\n-                        \"std\": 0.1\n-                    }\n-                ],\n-                [\"parameter_regex_match2\", \"uniform\"]\n-                [\"prevent_init_regex\", \"prevent\"]\n-            ]\n-        ```\n-\n-        where the first item in each tuple is the regex that matches to parameters, and the second\n-        item is a set of parameters that will be passed to `Initialzer.from_params()`.  These\n-        values can either be strings, in which case they correspond to the names of initializers,\n-        or dictionaries, in which case they must contain the \"type\" key, corresponding to the name\n-        of an initializer.  In addition, they may contain auxiliary named parameters which will be\n-        fed to the initializer itself. To determine valid auxiliary parameters, please refer to the\n-        torch.nn.init documentation. Only \"prevent\" is a special type which does not have corresponding\n-        initializer. Any parameter matching its corresponding regex will be overridden to NOT initialize.\n-\n-        # Returns\n-\n-        An InitializerApplicator containing the specified initializers.\n-        \"\"\"\n-\n-        params = params or []\n-\n-        def is_prevent(item):\n-            return item in (\"prevent\", {\"type\": \"prevent\"})\n-\n-        prevent_regexes = [param[0] for param in params if is_prevent(param[1])]\n-        params = [param for param in params if param[1] if not is_prevent(param[1])]\n-        initializers = [\n-            (name, Initializer.from_params(init_params)) for name, init_params in params\n-        ]\n-        return InitializerApplicator(initializers, prevent_regexes)\n",
        "source_code_with_indent": "            <IND>logger.info(\"   %s\", name)\n\n    <DED><DED>@classmethod\n    def from_params(cls, params: List[Tuple[str, Params]] = None) -> \"InitializerApplicator\":\n        <IND>\"\"\"\n        Converts a Params object into an InitializerApplicator. The json should\n        be formatted as follows:\n\n        ```\n            [\n                [\"parameter_regex_match1\",\n                    {\n                        \"type\": \"normal\"\n                        \"mean\": 0.01\n                        \"std\": 0.1\n                    }\n                ],\n                [\"parameter_regex_match2\", \"uniform\"]\n                [\"prevent_init_regex\", \"prevent\"]\n            ]\n        ```\n\n        where the first item in each tuple is the regex that matches to parameters, and the second\n        item is a set of parameters that will be passed to `Initialzer.from_params()`.  These\n        values can either be strings, in which case they correspond to the names of initializers,\n        or dictionaries, in which case they must contain the \"type\" key, corresponding to the name\n        of an initializer.  In addition, they may contain auxiliary named parameters which will be\n        fed to the initializer itself. To determine valid auxiliary parameters, please refer to the\n        torch.nn.init documentation. Only \"prevent\" is a special type which does not have corresponding\n        initializer. Any parameter matching its corresponding regex will be overridden to NOT initialize.\n\n        # Returns\n\n        An InitializerApplicator containing the specified initializers.\n        \"\"\"\n\n        params = params or []\n\n        def is_prevent(item):\n            <IND>return item in (\"prevent\", {\"type\": \"prevent\"})\n\n        <DED>prevent_regexes = [param[0] for param in params if is_prevent(param[1])]\n        params = [param for param in params if param[1] if not is_prevent(param[1])]\n        initializers = [\n            (name, Initializer.from_params(init_params)) for name, init_params in params\n        ]\n        return InitializerApplicator(initializers, prevent_regexes)\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "            <IND>logger.info(\"   %s\", name)\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  }
]