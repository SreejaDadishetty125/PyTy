[
  {
    "project": "pyjanitor-devs/pyjanitor",
    "commit": "d7842613b4e4a7532a88f673fd54e94c3ba5a96b",
    "filename": "janitor/utils.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/pyjanitor-devs-pyjanitor/janitor/utils.py",
    "file_hunks_size": 22,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "janitor/utils.py:827:53 Incompatible parameter type [6]: Expected `List[Union[Dict[typing.Any, typing.Any], List[typing.Any], str]]` for 2nd positional only parameter to call `_create_indexer_for_complete` but got `List[Union[Dict[typing.Any, typing.Any], List[typing.Any], str, typing.Tuple[typing.Any, ...]]]`.",
    "message": " Expected `List[Union[Dict[typing.Any, typing.Any], List[typing.Any], str]]` for 2nd positional only parameter to call `_create_indexer_for_complete` but got `List[Union[Dict[typing.Any, typing.Any], List[typing.Any], str, typing.Tuple[typing.Any, ...]]]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 827,
    "warning_line": "    indexer = _create_indexer_for_complete(df_index, columns)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    columns: List[Union[List, Tuple, Dict, str]],\n    by: Optional[Union[list, str]] = None,\n",
        "source_code_len": 93,
        "target_code": "    columns: List[Union[List, Tuple, Dict, str]],\n    sort: Optional[bool] = False,\n    by: Optional[Union[list, str]] = None,\n",
        "target_code_len": 127,
        "diff_format": "@@ -678,2 +596,3 @@\n     columns: List[Union[List, Tuple, Dict, str]],\n+    sort: Optional[bool] = False,\n     by: Optional[Union[list, str]] = None,\n",
        "source_code_with_indent": "    columns: List[Union[List, Tuple, Dict, str]],\n    by: Optional[Union[list, str]] = None,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    columns: List[Union[List, Tuple, Dict, str]],\n    sort: Optional[bool] = False,\n    by: Optional[Union[list, str]] = None,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    For some cases, the `stack/unstack` combination is preferred; it is more\n    efficient than `reindex`, as the size of the data grows. It is only\n    applicable if all the entries in `columns` are strings, there are\n    no nulls(stacking implicitly removes nulls in columns),\n    the length of `columns` is greater than 1, and the index\n    has no duplicates.\n\n    If there is a dictionary in `columns`, it is possible that all the values\n    of a key, or keys, may not be in the existing column with the same key(s);\n    as such, a union of the current index and the generated index is executed,\n    to ensure that all combinations are in the final dataframe.\n\n    A dataframe, with rows of missing values, if any, is returned.\n    \"\"\"\n\n    df, columns, column_checker, by = _data_checks_complete(df, columns, by)\n\n    dict_present = any((isinstance(entry, dict) for entry in columns))\n    all_strings = all(isinstance(column, str) for column in columns)\n\n    df = df.set_index(column_checker)\n\n    df_index = df.index\n    df_names = df_index.names\n\n    any_nulls = any(\n        df_index.get_level_values(name).hasnans for name in df_names\n    )\n\n    if not by:\n\n        df = _base_complete(df, columns, all_strings, any_nulls, dict_present)\n\n    # a better (and faster) way would be to create a dataframe\n    # from the groupby ...\n    # solution here got me thinking\n    # https://stackoverflow.com/a/66667034/7175713\n    # still thinking on how to improve speed of groupby apply\n    else:\n        df = df.groupby(by).apply(\n            _base_complete,\n            columns,\n            all_strings,\n            any_nulls,\n            dict_present,\n        )\n        df = df.drop(columns=by)\n\n    df = df.reset_index()\n\n    return df\n\n\ndef _base_complete(\n    df: pd.DataFrame,\n    columns: List[Union[List, Tuple, Dict, str]],\n    all_strings: bool,\n    any_nulls: bool,\n    dict_present: bool,\n) -> pd.DataFrame:\n\n    df_empty = df.empty\n    df_index = df.index\n    unique_index = df_index.is_unique\n    columns_to_stack = None\n\n    # stack...unstack implemented here if conditions are met\n    # usually faster than reindex\n    if all_strings and (not any_nulls) and (len(columns) > 1) and unique_index:\n        if df_empty:\n            df[\"dummy\"] = 1\n\n        columns_to_stack = columns[1:]\n        df = df.unstack(columns_to_stack)  # noqa: PD010\n        df = df.stack(columns_to_stack, dropna=False)  # noqa: PD013\n        if df_empty:\n            df = df.drop(columns=\"dummy\")\n        columns_to_stack = None\n        return df\n\n    indexer = _create_indexer_for_complete(df_index, columns)\n\n    if unique_index:\n        if dict_present:\n            indexer = df_index.union(indexer, sort=None)\n        df = df.reindex(indexer)\n\n    else:  # reindex not possible on duplicate indices\n        df = df.join(pd.DataFrame([], index=indexer), how=\"outer\")\n\n    return df\n\n\ndef _create_indexer_for_complete(\n    df_index: pd.Index,\n    columns: List[Union[List, Dict, str]],\n) -> pd.DataFrame:\n    \"\"\"\n    This creates the index that will be used\n    to expand the dataframe in the `complete` function.\n\n    A pandas Index is returned.\n    \"\"\"\n\n    complete_columns = (\n        _complete_column(column, df_index) for column in columns\n    )\n\n    complete_columns = (\n        (entry,) if not isinstance(entry, list) else entry\n        for entry in complete_columns\n    )\n    complete_columns = chain.from_iterable(complete_columns)\n    indexer = [*complete_columns]\n\n    if len(indexer) > 1:\n        indexer = _complete_indexer_expand_grid(indexer)\n\n    else:\n        indexer = indexer[0]\n\n    return indexer\n\n\ndef _complete_indexer_expand_grid(indexer):\n    \"\"\"\n    Generate indices to expose explicitly missing values,\n    using the `expand_grid` function.\n\n    Returns a pandas Index.\n    \"\"\"\n    indexers = []\n    mgrid_values = [slice(len(value)) for value in indexer]\n    mgrid_values = np.mgrid[mgrid_values]\n    mgrid_values = map(np.ravel, mgrid_values)\n\n    indexer = zip(indexer, mgrid_values)\n    indexer = (\n        _expand_grid(value, None, mgrid_values, mode=None)\n        for value, mgrid_values in indexer\n    )\n\n    for entry in indexer:\n        if isinstance(entry, pd.MultiIndex):\n            names = entry.names\n            val = (entry.get_level_values(name) for name in names)\n            indexers.extend(val)\n        else:\n            indexers.append(entry)\n    indexer = pd.MultiIndex.from_arrays(indexers)\n    indexers = None\n    return indexer\n\n",
        "source_code_len": 4477,
        "target_code": "\n    A DataFrame, with rows of missing values, if any, is returned.\n    \"\"\"\n\n    columns, column_checker, sort, by = _data_checks_complete(\n        df, columns, sort, by\n    )\n\n    all_strings = True\n    for column in columns:\n        if not isinstance(column, str):\n            all_strings = False\n            break\n\n    # nothing to 'complete' here\n    if all_strings and len(columns) == 1:\n        return df\n\n    # under the right conditions, stack/unstack can be faster\n    # plus it always returns a sorted DataFrame\n    # which does help in viewing the missing rows\n    # however, using a merge keeps things simple\n    # with a stack/unstack,\n    # the relevant columns combination should be unique\n    # and there should be no nulls\n    # trade-off for the simplicity of merge is not so bad\n    # of course there could be a better way ...\n    if by is None:\n        uniques = _generic_complete(df, columns, all_strings)\n        return df.merge(uniques, how=\"outer\", on=column_checker, sort=sort)\n\n    uniques = df.groupby(by)\n    uniques = uniques.apply(_generic_complete, columns, all_strings)\n    uniques = uniques.droplevel(-1)\n    return df.merge(uniques, how=\"outer\", on=by + column_checker, sort=sort)\n\n\ndef _generic_complete(\n    df: pd.DataFrame, columns: list, all_strings: bool = True\n):\n    \"\"\"\n    Generate cartesian product for `_computations_complete`.\n\n    Returns a Series or DataFrame, with no duplicates.\n    \"\"\"\n    if all_strings:\n        uniques = {col: df[col].unique() for col in columns}\n        uniques = _computations_expand_grid(uniques)\n        uniques = uniques.droplevel(level=-1, axis=\"columns\")\n        return uniques\n\n    uniques = {}\n    for index, column in enumerate(columns):\n        if isinstance(column, dict):\n            column = _complete_column(column, df)\n            uniques = {**uniques, **column}\n        else:\n            uniques[index] = _complete_column(column, df)\n\n    if len(uniques) == 1:\n        _, uniques = uniques.popitem()\n        return uniques.to_frame()\n\n    uniques = _computations_expand_grid(uniques)\n    return uniques.droplevel(level=0, axis=\"columns\")\n\n",
        "target_code_len": 2128,
        "diff_format": "@@ -746,154 +669,66 @@\n \n-    For some cases, the `stack/unstack` combination is preferred; it is more\n-    efficient than `reindex`, as the size of the data grows. It is only\n-    applicable if all the entries in `columns` are strings, there are\n-    no nulls(stacking implicitly removes nulls in columns),\n-    the length of `columns` is greater than 1, and the index\n-    has no duplicates.\n-\n-    If there is a dictionary in `columns`, it is possible that all the values\n-    of a key, or keys, may not be in the existing column with the same key(s);\n-    as such, a union of the current index and the generated index is executed,\n-    to ensure that all combinations are in the final dataframe.\n-\n-    A dataframe, with rows of missing values, if any, is returned.\n-    \"\"\"\n-\n-    df, columns, column_checker, by = _data_checks_complete(df, columns, by)\n-\n-    dict_present = any((isinstance(entry, dict) for entry in columns))\n-    all_strings = all(isinstance(column, str) for column in columns)\n-\n-    df = df.set_index(column_checker)\n-\n-    df_index = df.index\n-    df_names = df_index.names\n-\n-    any_nulls = any(\n-        df_index.get_level_values(name).hasnans for name in df_names\n+    A DataFrame, with rows of missing values, if any, is returned.\n+    \"\"\"\n+\n+    columns, column_checker, sort, by = _data_checks_complete(\n+        df, columns, sort, by\n     )\n \n-    if not by:\n-\n-        df = _base_complete(df, columns, all_strings, any_nulls, dict_present)\n-\n-    # a better (and faster) way would be to create a dataframe\n-    # from the groupby ...\n-    # solution here got me thinking\n-    # https://stackoverflow.com/a/66667034/7175713\n-    # still thinking on how to improve speed of groupby apply\n-    else:\n-        df = df.groupby(by).apply(\n-            _base_complete,\n-            columns,\n-            all_strings,\n-            any_nulls,\n-            dict_present,\n-        )\n-        df = df.drop(columns=by)\n-\n-    df = df.reset_index()\n-\n-    return df\n-\n-\n-def _base_complete(\n-    df: pd.DataFrame,\n-    columns: List[Union[List, Tuple, Dict, str]],\n-    all_strings: bool,\n-    any_nulls: bool,\n-    dict_present: bool,\n-) -> pd.DataFrame:\n-\n-    df_empty = df.empty\n-    df_index = df.index\n-    unique_index = df_index.is_unique\n-    columns_to_stack = None\n-\n-    # stack...unstack implemented here if conditions are met\n-    # usually faster than reindex\n-    if all_strings and (not any_nulls) and (len(columns) > 1) and unique_index:\n-        if df_empty:\n-            df[\"dummy\"] = 1\n-\n-        columns_to_stack = columns[1:]\n-        df = df.unstack(columns_to_stack)  # noqa: PD010\n-        df = df.stack(columns_to_stack, dropna=False)  # noqa: PD013\n-        if df_empty:\n-            df = df.drop(columns=\"dummy\")\n-        columns_to_stack = None\n+    all_strings = True\n+    for column in columns:\n+        if not isinstance(column, str):\n+            all_strings = False\n+            break\n+\n+    # nothing to 'complete' here\n+    if all_strings and len(columns) == 1:\n         return df\n \n-    indexer = _create_indexer_for_complete(df_index, columns)\n-\n-    if unique_index:\n-        if dict_present:\n-            indexer = df_index.union(indexer, sort=None)\n-        df = df.reindex(indexer)\n-\n-    else:  # reindex not possible on duplicate indices\n-        df = df.join(pd.DataFrame([], index=indexer), how=\"outer\")\n-\n-    return df\n-\n-\n-def _create_indexer_for_complete(\n-    df_index: pd.Index,\n-    columns: List[Union[List, Dict, str]],\n-) -> pd.DataFrame:\n-    \"\"\"\n-    This creates the index that will be used\n-    to expand the dataframe in the `complete` function.\n-\n-    A pandas Index is returned.\n-    \"\"\"\n-\n-    complete_columns = (\n-        _complete_column(column, df_index) for column in columns\n-    )\n-\n-    complete_columns = (\n-        (entry,) if not isinstance(entry, list) else entry\n-        for entry in complete_columns\n-    )\n-    complete_columns = chain.from_iterable(complete_columns)\n-    indexer = [*complete_columns]\n-\n-    if len(indexer) > 1:\n-        indexer = _complete_indexer_expand_grid(indexer)\n-\n-    else:\n-        indexer = indexer[0]\n-\n-    return indexer\n-\n-\n-def _complete_indexer_expand_grid(indexer):\n-    \"\"\"\n-    Generate indices to expose explicitly missing values,\n-    using the `expand_grid` function.\n-\n-    Returns a pandas Index.\n-    \"\"\"\n-    indexers = []\n-    mgrid_values = [slice(len(value)) for value in indexer]\n-    mgrid_values = np.mgrid[mgrid_values]\n-    mgrid_values = map(np.ravel, mgrid_values)\n-\n-    indexer = zip(indexer, mgrid_values)\n-    indexer = (\n-        _expand_grid(value, None, mgrid_values, mode=None)\n-        for value, mgrid_values in indexer\n-    )\n-\n-    for entry in indexer:\n-        if isinstance(entry, pd.MultiIndex):\n-            names = entry.names\n-            val = (entry.get_level_values(name) for name in names)\n-            indexers.extend(val)\n+    # under the right conditions, stack/unstack can be faster\n+    # plus it always returns a sorted DataFrame\n+    # which does help in viewing the missing rows\n+    # however, using a merge keeps things simple\n+    # with a stack/unstack,\n+    # the relevant columns combination should be unique\n+    # and there should be no nulls\n+    # trade-off for the simplicity of merge is not so bad\n+    # of course there could be a better way ...\n+    if by is None:\n+        uniques = _generic_complete(df, columns, all_strings)\n+        return df.merge(uniques, how=\"outer\", on=column_checker, sort=sort)\n+\n+    uniques = df.groupby(by)\n+    uniques = uniques.apply(_generic_complete, columns, all_strings)\n+    uniques = uniques.droplevel(-1)\n+    return df.merge(uniques, how=\"outer\", on=by + column_checker, sort=sort)\n+\n+\n+def _generic_complete(\n+    df: pd.DataFrame, columns: list, all_strings: bool = True\n+):\n+    \"\"\"\n+    Generate cartesian product for `_computations_complete`.\n+\n+    Returns a Series or DataFrame, with no duplicates.\n+    \"\"\"\n+    if all_strings:\n+        uniques = {col: df[col].unique() for col in columns}\n+        uniques = _computations_expand_grid(uniques)\n+        uniques = uniques.droplevel(level=-1, axis=\"columns\")\n+        return uniques\n+\n+    uniques = {}\n+    for index, column in enumerate(columns):\n+        if isinstance(column, dict):\n+            column = _complete_column(column, df)\n+            uniques = {**uniques, **column}\n         else:\n-            indexers.append(entry)\n-    indexer = pd.MultiIndex.from_arrays(indexers)\n-    indexers = None\n-    return indexer\n+            uniques[index] = _complete_column(column, df)\n+\n+    if len(uniques) == 1:\n+        _, uniques = uniques.popitem()\n+        return uniques.to_frame()\n+\n+    uniques = _computations_expand_grid(uniques)\n+    return uniques.droplevel(level=0, axis=\"columns\")\n \n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n\n    df, columns, column_checker, by = _data_checks_complete(df, columns, by)\n\n    dict_present = any((isinstance(entry, dict) for entry in columns))\n    all_strings = all(isinstance(column, str) for column in columns)\n\n    df = df.set_index(column_checker)\n\n    df_index = df.index\n    df_names = df_index.names\n\n    any_nulls = any(\n        df_index.get_level_values(name).hasnans for name in df_names\n    )\n\n    if not by:\n\n        <IND>df = _base_complete(df, columns, all_strings, any_nulls, dict_present)\n\n    # a better (and faster) way would be to create a dataframe\n    # from the groupby ...\n    # solution here got me thinking\n    # https://stackoverflow.com/a/66667034/7175713\n    # still thinking on how to improve speed of groupby apply\n    <DED>else:\n        <IND>df = df.groupby(by).apply(\n            _base_complete,\n            columns,\n            all_strings,\n            any_nulls,\n            dict_present,\n        )\n        df = df.drop(columns=by)\n\n    <DED>df = df.reset_index()\n\n    return df\n\n\n<DED>def _base_complete(\n    df: pd.DataFrame,\n    columns: List[Union[List, Tuple, Dict, str]],\n    all_strings: bool,\n    any_nulls: bool,\n    dict_present: bool,\n) -> pd.DataFrame:\n\n    <IND>df_empty = df.empty\n    df_index = df.index\n    unique_index = df_index.is_unique\n    columns_to_stack = None\n\n    # stack...unstack implemented here if conditions are met\n    # usually faster than reindex\n    if all_strings and (not any_nulls) and (len(columns) > 1) and unique_index:\n        <IND>if df_empty:\n            <IND>df[\"dummy\"] = 1\n\n        <DED>columns_to_stack = columns[1:]\n        df = df.unstack(columns_to_stack)  # noqa: PD010\n        df = df.stack(columns_to_stack, dropna=False)  # noqa: PD013\n        if df_empty:\n            <IND>df = df.drop(columns=\"dummy\")\n        <DED>columns_to_stack = None\n        return df\n\n    <DED>indexer = _create_indexer_for_complete(df_index, columns)\n\n    if unique_index:\n        <IND>if dict_present:\n            <IND>indexer = df_index.union(indexer, sort=None)\n        <DED>df = df.reindex(indexer)\n\n    <DED>else:  # reindex not possible on duplicate indices\n        <IND>df = df.join(pd.DataFrame([], index=indexer), how=\"outer\")\n\n    <DED>return df\n\n\n<DED>def _create_indexer_for_complete(\n    df_index: pd.Index,\n    columns: List[Union[List, Dict, str]],\n) -> pd.DataFrame:\n    <IND>\"\"\"\n    This creates the index that will be used\n    to expand the dataframe in the `complete` function.\n\n    A pandas Index is returned.\n    \"\"\"\n\n    complete_columns = (\n        _complete_column(column, df_index) for column in columns\n    )\n\n    complete_columns = (\n        (entry,) if not isinstance(entry, list) else entry\n        for entry in complete_columns\n    )\n    complete_columns = chain.from_iterable(complete_columns)\n    indexer = [*complete_columns]\n\n    if len(indexer) > 1:\n        <IND>indexer = _complete_indexer_expand_grid(indexer)\n\n    <DED>else:\n        <IND>indexer = indexer[0]\n\n    <DED>return indexer\n\n\n<DED>def _complete_indexer_expand_grid(indexer):\n    <IND>\"\"\"\n    Generate indices to expose explicitly missing values,\n    using the `expand_grid` function.\n\n    Returns a pandas Index.\n    \"\"\"\n    indexers = []\n    mgrid_values = [slice(len(value)) for value in indexer]\n    mgrid_values = np.mgrid[mgrid_values]\n    mgrid_values = map(np.ravel, mgrid_values)\n\n    indexer = zip(indexer, mgrid_values)\n    indexer = (\n        _expand_grid(value, None, mgrid_values, mode=None)\n        for value, mgrid_values in indexer\n    )\n\n    for entry in indexer:\n        <IND>if isinstance(entry, pd.MultiIndex):\n            <IND>names = entry.names\n            val = (entry.get_level_values(name) for name in names)\n            indexers.extend(val)\n        <DED>else:\n            <IND>indexers.append(entry)\n    <DED><DED>indexer = pd.MultiIndex.from_arrays(indexers)\n    indexers = None\n    return indexer\n\n",
        "target_code_with_indent": "\n\n    columns, column_checker, sort, by = _data_checks_complete(\n        df, columns, sort, by\n    )\n\n    all_strings = True\n    for column in columns:\n        <IND>if not isinstance(column, str):\n            <IND>all_strings = False\n            break\n\n    # nothing to 'complete' here\n    <DED><DED>if all_strings and len(columns) == 1:\n        <IND>return df\n\n    # under the right conditions, stack/unstack can be faster\n    # plus it always returns a sorted DataFrame\n    # which does help in viewing the missing rows\n    # however, using a merge keeps things simple\n    # with a stack/unstack,\n    # the relevant columns combination should be unique\n    # and there should be no nulls\n    # trade-off for the simplicity of merge is not so bad\n    # of course there could be a better way ...\n    <DED>if by is None:\n        <IND>uniques = _generic_complete(df, columns, all_strings)\n        return df.merge(uniques, how=\"outer\", on=column_checker, sort=sort)\n\n    <DED>uniques = df.groupby(by)\n    uniques = uniques.apply(_generic_complete, columns, all_strings)\n    uniques = uniques.droplevel(-1)\n    return df.merge(uniques, how=\"outer\", on=by + column_checker, sort=sort)\n\n\n<DED>def _generic_complete(\n    df: pd.DataFrame, columns: list, all_strings: bool = True\n):\n    <IND>\"\"\"\n    Generate cartesian product for `_computations_complete`.\n\n    Returns a Series or DataFrame, with no duplicates.\n    \"\"\"\n    if all_strings:\n        <IND>uniques = {col: df[col].unique() for col in columns}\n        uniques = _computations_expand_grid(uniques)\n        uniques = uniques.droplevel(level=-1, axis=\"columns\")\n        return uniques\n\n    <DED>uniques = {}\n    for index, column in enumerate(columns):\n        <IND>if isinstance(column, dict):\n            <IND>column = _complete_column(column, df)\n            uniques = {**uniques, **column}\n        <DED>else:\n            <IND>uniques[index] = _complete_column(column, df)\n\n    <DED><DED>if len(uniques) == 1:\n        <IND>_, uniques = uniques.popitem()\n        return uniques.to_frame()\n\n    <DED>uniques = _computations_expand_grid(uniques)\n    return uniques.droplevel(level=0, axis=\"columns\")\n\n"
      }
    ]
  }
]