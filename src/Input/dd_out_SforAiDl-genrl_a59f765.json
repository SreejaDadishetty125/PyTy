[
  {
    "project": "SforAiDl/genrl",
    "commit": "a59f765bca4f083f6af66f33ad9c875c8ba23ac2",
    "filename": "genrl/deep/agents/base.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/SforAiDl-genrl/genrl/deep/agents/base/base.py",
    "file_hunks_size": 8,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "genrl/deep/agents/base.py:230:33 Incompatible variable type [9]: beta is declared to have type `float` but is used as type `None`.",
    "message": " beta is declared to have type `float` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 230,
    "warning_line": "    def sample_from_buffer(self, beta: float = None):",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n\nclass OnPolicyAgent(BaseAgent):\n    def __init__(self, *args, rollout_size: int = 2048, **kwargs):\n        super(OnPolicyAgent, self).__init__(*args, **kwargs)\n        self.rollout_size = rollout_size\n\n    def collect_rewards(self, dones, timestep):\n        for i, done in enumerate(dones):\n            if done or timestep == self.rollout_size - 1:\n                self.rewards.append(self.env.episode_reward[i])\n                self.env.episode_reward[i] = 0\n\n    def collect_rollouts(self, state):\n        for i in range(self.rollout_size):\n            action, values, old_log_probs = self.select_action(state)\n\n            next_state, reward, dones, _ = self.env.step(np.array(action))\n\n            if self.render:\n                self.env.render()\n\n            self.rollout.add(\n                state,\n                action.reshape(self.env.n_envs, 1),\n                reward,\n                dones,\n                values.detach(),\n                old_log_probs.detach(),\n            )\n\n            state = next_state\n\n            self.collect_rewards(dones, i)\n\n        return values, dones\n\n\nclass OffPolicyAgent(BaseAgent):\n    \"\"\"Off Policy Agent Base Class\n\n    Attributes:\n        network (str): The network type of the Q-value function.\n            Supported types: [\"cnn\", \"mlp\"]\n        env (Environment): The environment that the agent is supposed to act on\n        create_model (bool): Whether the model of the algo should be created when initialised\n        batch_size (int): Mini batch size for loading experiences\n        gamma (float): The discount factor for rewards\n        layers (:obj:`tuple` of :obj:`int`): Layers in the Neural Network\n            of the Q-value function\n        lr_policy (float): Learning rate for the policy/actor\n        lr_value (float): Learning rate for the Q-value function\n        replay_size (int): Capacity of the Replay Buffer\n        buffer_type (str): Choose the type of Buffer: [\"push\", \"prioritized\"]\n        seed (int): Seed for randomness\n        render (bool): Should the env be rendered during training?\n        device (str): Hardware being used for training. Options:\n            [\"cuda\" -> GPU, \"cpu\" -> CPU]\n    \"\"\"\n\n    def __init__(\n        self, *args, replay_size: int = 1000, buffer_type: str = \"push\", **kwargs\n    ):\n        super(OffPolicyAgent, self).__init__(*args, **kwargs)\n        self.replay_size = replay_size\n        self.buffer_type = buffer_type\n\n        if buffer_type == \"push\":\n            self.buffer_class = PushReplayBuffer\n        elif buffer_type == \"prioritized\":\n            self.buffer_class = PrioritizedBuffer\n        else:\n            raise NotImplementedError\n\n    def update_params_before_select_action(self, timestep: int) -> None:\n        \"\"\"Update any parameters before selecting action like epsilon for decaying epsilon greedy\n\n        Args:\n            timestep (int): Timestep in the training process\n        \"\"\"\n        pass\n\n    def update_target_model(self) -> None:\n        \"\"\"Function to update the target Q model\n\n        Updates the target model with the training model's weights when called\n        \"\"\"\n        raise NotImplementedError\n\n    def _reshape_batch(self, batch: List):\n        \"\"\"Function to reshape experiences\n\n        Can be modified for individual algorithm usage\n\n        Args:\n            batch (:obj:`list`): List of experiences that are being replayed\n\n        Returns:\n            batch (:obj:`list`): Reshaped experiences for replay\n        \"\"\"\n        return [*batch]\n\n    def sample_from_buffer(self, beta: float = None):\n        \"\"\"Samples experiences from the buffer and converts them into usable formats\n\n        Args:\n            beta (float): Importance-Sampling beta for prioritized replay\n\n        Returns:\n            batch (:obj:`list`): Replay experiences sampled from the buffer\n        \"\"\"\n        # Samples from the buffer\n        if beta is not None:\n            batch = self.replay_buffer.sample(self.batch_size, beta=beta)\n        else:\n            batch = self.replay_buffer.sample(self.batch_size)\n\n        states, actions, rewards, next_states, dones = self._reshape_batch(batch)\n\n        # Convert every experience to a Named Tuple. Either Replay or Prioritized Replay samples.\n        if self.buffer_type == \"push\":\n            batch = ReplayBufferSamples(*[states, actions, rewards, next_states, dones])\n        elif self.buffer_type == \"prioritized\":\n            indices, weights = batch[5], batch[6]\n            batch = PrioritizedReplayBufferSamples(\n                *[states, actions, rewards, next_states, dones, indices, weights]\n            )\n        return batch\n\n    def get_q_loss(self, batch: collections.namedtuple) -> torch.Tensor:\n        \"\"\"Normal Function to calculate the loss of the Q-function or critic\n\n        Args:\n            batch (:obj:`collections.namedtuple` of :obj:`torch.Tensor`): Batch of experiences\n\n        Returns:\n            loss (:obj:`torch.Tensor`): Calculated loss of the Q-function\n        \"\"\"\n        q_values = self.get_q_values(batch.states, batch.actions)\n        target_q_values = self.get_target_q_values(\n            batch.next_states, batch.rewards, batch.dones\n        )\n        loss = F.mse_loss(q_values, target_q_values)\n        return loss\n\n\nclass OffPolicyAgentAC(OffPolicyAgent):\n    \"\"\"Off Policy Agent Base Class\n\n    Attributes:\n        network (str): The network type of the Q-value function.\n            Supported types: [\"cnn\", \"mlp\"]\n        env (Environment): The environment that the agent is supposed to act on\n        create_model (bool): Whether the model of the algo should be created when initialised\n        batch_size (int): Mini batch size for loading experiences\n        gamma (float): The discount factor for rewards\n        layers (:obj:`tuple` of :obj:`int`): Layers in the Neural Network\n            of the Q-value function\n        lr_policy (float): Learning rate for the policy/actor\n        lr_value (float): Learning rate for the Q-value function\n        replay_size (int): Capacity of the Replay Buffer\n        buffer_type (str): Choose the type of Buffer: [\"push\", \"prioritized\"]\n        seed (int): Seed for randomness\n        render (bool): Should the env be rendered during training?\n        device (str): Hardware being used for training. Options:\n            [\"cuda\" -> GPU, \"cpu\" -> CPU]\n    \"\"\"\n\n    def __init__(self, *args, polyak=0.995, **kwargs):\n        super(OffPolicyAgentAC, self).__init__(*args, **kwargs)\n        self.polyak = polyak\n\n    def select_action(\n        self, state: np.ndarray, deterministic: bool = True\n    ) -> np.ndarray:\n        \"\"\"Select action given state\n\n        Deterministic Action Selection with Noise\n\n        Args:\n            state (:obj:`np.ndarray`): Current state of the environment\n            deterministic (bool): Should the policy be deterministic or stochastic\n\n        Returns:\n            action (:obj:`np.ndarray`): Action taken by the agent\n        \"\"\"\n        state = torch.as_tensor(state).float()\n        action, _ = self.ac.get_action(state, deterministic)\n        action = action.detach().cpu().numpy()\n\n        # add noise to output from policy network\n        if self.noise is not None:\n            action += self.noise()\n\n        return np.clip(\n            action, self.env.action_space.low[0], self.env.action_space.high[0]\n        )\n\n    def update_target_model(self) -> None:\n        \"\"\"Function to update the target Q model\n\n        Updates the target model with the training model's weights when called\n        \"\"\"\n        for param, param_target in zip(\n            self.ac.parameters(), self.ac_target.parameters()\n        ):\n            param_target.data.mul_(self.polyak)\n            param_target.data.add_((1 - self.polyak) * param.data)\n\n    def get_p_loss(self, states: torch.Tensor) -> torch.Tensor:\n        \"\"\"Function to get the Policy loss\n\n        Args:\n            states (:obj:`torch.Tensor`): States for which Q-values need to be found\n\n        Returns:\n            loss (:obj:`torch.Tensor`): Calculated policy loss\n        \"\"\"\n        next_best_actions = self.ac.get_action(states, True)[0]\n        q_values = self.ac.get_value(torch.cat([states, next_best_actions], dim=-1))\n        policy_loss = -torch.mean(q_values)\n        return policy_loss\n\n    def load_weights(self, weights) -> None:\n        \"\"\"\n        Load weights for the agent from pretrained model\n        \"\"\"\n        self.ac.load_state_dict(weights[\"weights\"])\n",
        "source_code_len": 8462,
        "target_code": "\n    def empty_logs(self):\n        \"\"\"Empties logs\n        \"\"\"\n        raise NotImplementedError\n",
        "target_code_len": 97,
        "diff_format": "@@ -128,228 +112,5 @@\n \n-\n-class OnPolicyAgent(BaseAgent):\n-    def __init__(self, *args, rollout_size: int = 2048, **kwargs):\n-        super(OnPolicyAgent, self).__init__(*args, **kwargs)\n-        self.rollout_size = rollout_size\n-\n-    def collect_rewards(self, dones, timestep):\n-        for i, done in enumerate(dones):\n-            if done or timestep == self.rollout_size - 1:\n-                self.rewards.append(self.env.episode_reward[i])\n-                self.env.episode_reward[i] = 0\n-\n-    def collect_rollouts(self, state):\n-        for i in range(self.rollout_size):\n-            action, values, old_log_probs = self.select_action(state)\n-\n-            next_state, reward, dones, _ = self.env.step(np.array(action))\n-\n-            if self.render:\n-                self.env.render()\n-\n-            self.rollout.add(\n-                state,\n-                action.reshape(self.env.n_envs, 1),\n-                reward,\n-                dones,\n-                values.detach(),\n-                old_log_probs.detach(),\n-            )\n-\n-            state = next_state\n-\n-            self.collect_rewards(dones, i)\n-\n-        return values, dones\n-\n-\n-class OffPolicyAgent(BaseAgent):\n-    \"\"\"Off Policy Agent Base Class\n-\n-    Attributes:\n-        network (str): The network type of the Q-value function.\n-            Supported types: [\"cnn\", \"mlp\"]\n-        env (Environment): The environment that the agent is supposed to act on\n-        create_model (bool): Whether the model of the algo should be created when initialised\n-        batch_size (int): Mini batch size for loading experiences\n-        gamma (float): The discount factor for rewards\n-        layers (:obj:`tuple` of :obj:`int`): Layers in the Neural Network\n-            of the Q-value function\n-        lr_policy (float): Learning rate for the policy/actor\n-        lr_value (float): Learning rate for the Q-value function\n-        replay_size (int): Capacity of the Replay Buffer\n-        buffer_type (str): Choose the type of Buffer: [\"push\", \"prioritized\"]\n-        seed (int): Seed for randomness\n-        render (bool): Should the env be rendered during training?\n-        device (str): Hardware being used for training. Options:\n-            [\"cuda\" -> GPU, \"cpu\" -> CPU]\n-    \"\"\"\n-\n-    def __init__(\n-        self, *args, replay_size: int = 1000, buffer_type: str = \"push\", **kwargs\n-    ):\n-        super(OffPolicyAgent, self).__init__(*args, **kwargs)\n-        self.replay_size = replay_size\n-        self.buffer_type = buffer_type\n-\n-        if buffer_type == \"push\":\n-            self.buffer_class = PushReplayBuffer\n-        elif buffer_type == \"prioritized\":\n-            self.buffer_class = PrioritizedBuffer\n-        else:\n-            raise NotImplementedError\n-\n-    def update_params_before_select_action(self, timestep: int) -> None:\n-        \"\"\"Update any parameters before selecting action like epsilon for decaying epsilon greedy\n-\n-        Args:\n-            timestep (int): Timestep in the training process\n-        \"\"\"\n-        pass\n-\n-    def update_target_model(self) -> None:\n-        \"\"\"Function to update the target Q model\n-\n-        Updates the target model with the training model's weights when called\n+    def empty_logs(self):\n+        \"\"\"Empties logs\n         \"\"\"\n         raise NotImplementedError\n-\n-    def _reshape_batch(self, batch: List):\n-        \"\"\"Function to reshape experiences\n-\n-        Can be modified for individual algorithm usage\n-\n-        Args:\n-            batch (:obj:`list`): List of experiences that are being replayed\n-\n-        Returns:\n-            batch (:obj:`list`): Reshaped experiences for replay\n-        \"\"\"\n-        return [*batch]\n-\n-    def sample_from_buffer(self, beta: float = None):\n-        \"\"\"Samples experiences from the buffer and converts them into usable formats\n-\n-        Args:\n-            beta (float): Importance-Sampling beta for prioritized replay\n-\n-        Returns:\n-            batch (:obj:`list`): Replay experiences sampled from the buffer\n-        \"\"\"\n-        # Samples from the buffer\n-        if beta is not None:\n-            batch = self.replay_buffer.sample(self.batch_size, beta=beta)\n-        else:\n-            batch = self.replay_buffer.sample(self.batch_size)\n-\n-        states, actions, rewards, next_states, dones = self._reshape_batch(batch)\n-\n-        # Convert every experience to a Named Tuple. Either Replay or Prioritized Replay samples.\n-        if self.buffer_type == \"push\":\n-            batch = ReplayBufferSamples(*[states, actions, rewards, next_states, dones])\n-        elif self.buffer_type == \"prioritized\":\n-            indices, weights = batch[5], batch[6]\n-            batch = PrioritizedReplayBufferSamples(\n-                *[states, actions, rewards, next_states, dones, indices, weights]\n-            )\n-        return batch\n-\n-    def get_q_loss(self, batch: collections.namedtuple) -> torch.Tensor:\n-        \"\"\"Normal Function to calculate the loss of the Q-function or critic\n-\n-        Args:\n-            batch (:obj:`collections.namedtuple` of :obj:`torch.Tensor`): Batch of experiences\n-\n-        Returns:\n-            loss (:obj:`torch.Tensor`): Calculated loss of the Q-function\n-        \"\"\"\n-        q_values = self.get_q_values(batch.states, batch.actions)\n-        target_q_values = self.get_target_q_values(\n-            batch.next_states, batch.rewards, batch.dones\n-        )\n-        loss = F.mse_loss(q_values, target_q_values)\n-        return loss\n-\n-\n-class OffPolicyAgentAC(OffPolicyAgent):\n-    \"\"\"Off Policy Agent Base Class\n-\n-    Attributes:\n-        network (str): The network type of the Q-value function.\n-            Supported types: [\"cnn\", \"mlp\"]\n-        env (Environment): The environment that the agent is supposed to act on\n-        create_model (bool): Whether the model of the algo should be created when initialised\n-        batch_size (int): Mini batch size for loading experiences\n-        gamma (float): The discount factor for rewards\n-        layers (:obj:`tuple` of :obj:`int`): Layers in the Neural Network\n-            of the Q-value function\n-        lr_policy (float): Learning rate for the policy/actor\n-        lr_value (float): Learning rate for the Q-value function\n-        replay_size (int): Capacity of the Replay Buffer\n-        buffer_type (str): Choose the type of Buffer: [\"push\", \"prioritized\"]\n-        seed (int): Seed for randomness\n-        render (bool): Should the env be rendered during training?\n-        device (str): Hardware being used for training. Options:\n-            [\"cuda\" -> GPU, \"cpu\" -> CPU]\n-    \"\"\"\n-\n-    def __init__(self, *args, polyak=0.995, **kwargs):\n-        super(OffPolicyAgentAC, self).__init__(*args, **kwargs)\n-        self.polyak = polyak\n-\n-    def select_action(\n-        self, state: np.ndarray, deterministic: bool = True\n-    ) -> np.ndarray:\n-        \"\"\"Select action given state\n-\n-        Deterministic Action Selection with Noise\n-\n-        Args:\n-            state (:obj:`np.ndarray`): Current state of the environment\n-            deterministic (bool): Should the policy be deterministic or stochastic\n-\n-        Returns:\n-            action (:obj:`np.ndarray`): Action taken by the agent\n-        \"\"\"\n-        state = torch.as_tensor(state).float()\n-        action, _ = self.ac.get_action(state, deterministic)\n-        action = action.detach().cpu().numpy()\n-\n-        # add noise to output from policy network\n-        if self.noise is not None:\n-            action += self.noise()\n-\n-        return np.clip(\n-            action, self.env.action_space.low[0], self.env.action_space.high[0]\n-        )\n-\n-    def update_target_model(self) -> None:\n-        \"\"\"Function to update the target Q model\n-\n-        Updates the target model with the training model's weights when called\n-        \"\"\"\n-        for param, param_target in zip(\n-            self.ac.parameters(), self.ac_target.parameters()\n-        ):\n-            param_target.data.mul_(self.polyak)\n-            param_target.data.add_((1 - self.polyak) * param.data)\n-\n-    def get_p_loss(self, states: torch.Tensor) -> torch.Tensor:\n-        \"\"\"Function to get the Policy loss\n-\n-        Args:\n-            states (:obj:`torch.Tensor`): States for which Q-values need to be found\n-\n-        Returns:\n-            loss (:obj:`torch.Tensor`): Calculated policy loss\n-        \"\"\"\n-        next_best_actions = self.ac.get_action(states, True)[0]\n-        q_values = self.ac.get_value(torch.cat([states, next_best_actions], dim=-1))\n-        policy_loss = -torch.mean(q_values)\n-        return policy_loss\n-\n-    def load_weights(self, weights) -> None:\n-        \"\"\"\n-        Load weights for the agent from pretrained model\n-        \"\"\"\n-        self.ac.load_state_dict(weights[\"weights\"])\n",
        "source_code_with_indent": "\n\n<DED><DED>class OnPolicyAgent(BaseAgent):\n    <IND>def __init__(self, *args, rollout_size: int = 2048, **kwargs):\n        <IND>super(OnPolicyAgent, self).__init__(*args, **kwargs)\n        self.rollout_size = rollout_size\n\n    <DED>def collect_rewards(self, dones, timestep):\n        <IND>for i, done in enumerate(dones):\n            <IND>if done or timestep == self.rollout_size - 1:\n                <IND>self.rewards.append(self.env.episode_reward[i])\n                self.env.episode_reward[i] = 0\n\n    <DED><DED><DED>def collect_rollouts(self, state):\n        <IND>for i in range(self.rollout_size):\n            <IND>action, values, old_log_probs = self.select_action(state)\n\n            next_state, reward, dones, _ = self.env.step(np.array(action))\n\n            if self.render:\n                <IND>self.env.render()\n\n            <DED>self.rollout.add(\n                state,\n                action.reshape(self.env.n_envs, 1),\n                reward,\n                dones,\n                values.detach(),\n                old_log_probs.detach(),\n            )\n\n            state = next_state\n\n            self.collect_rewards(dones, i)\n\n        <DED>return values, dones\n\n\n<DED><DED>class OffPolicyAgent(BaseAgent):\n    <IND>\"\"\"Off Policy Agent Base Class\n\n    Attributes:\n        network (str): The network type of the Q-value function.\n            Supported types: [\"cnn\", \"mlp\"]\n        env (Environment): The environment that the agent is supposed to act on\n        create_model (bool): Whether the model of the algo should be created when initialised\n        batch_size (int): Mini batch size for loading experiences\n        gamma (float): The discount factor for rewards\n        layers (:obj:`tuple` of :obj:`int`): Layers in the Neural Network\n            of the Q-value function\n        lr_policy (float): Learning rate for the policy/actor\n        lr_value (float): Learning rate for the Q-value function\n        replay_size (int): Capacity of the Replay Buffer\n        buffer_type (str): Choose the type of Buffer: [\"push\", \"prioritized\"]\n        seed (int): Seed for randomness\n        render (bool): Should the env be rendered during training?\n        device (str): Hardware being used for training. Options:\n            [\"cuda\" -> GPU, \"cpu\" -> CPU]\n    \"\"\"\n\n    def __init__(\n        self, *args, replay_size: int = 1000, buffer_type: str = \"push\", **kwargs\n    ):\n        <IND>super(OffPolicyAgent, self).__init__(*args, **kwargs)\n        self.replay_size = replay_size\n        self.buffer_type = buffer_type\n\n        if buffer_type == \"push\":\n            <IND>self.buffer_class = PushReplayBuffer\n        <DED>elif buffer_type == \"prioritized\":\n            <IND>self.buffer_class = PrioritizedBuffer\n        <DED>else:\n            <IND>raise NotImplementedError\n\n    <DED><DED>def update_params_before_select_action(self, timestep: int) -> None:\n        <IND>\"\"\"Update any parameters before selecting action like epsilon for decaying epsilon greedy\n\n        Args:\n            timestep (int): Timestep in the training process\n        \"\"\"\n        pass\n\n    <DED>def update_target_model(self) -> None:\n        <IND>\"\"\"Function to update the target Q model\n\n        Updates the target model with the training model's weights when called\n        \"\"\"\n        raise NotImplementedError\n\n    <DED>def _reshape_batch(self, batch: List):\n        <IND>\"\"\"Function to reshape experiences\n\n        Can be modified for individual algorithm usage\n\n        Args:\n            batch (:obj:`list`): List of experiences that are being replayed\n\n        Returns:\n            batch (:obj:`list`): Reshaped experiences for replay\n        \"\"\"\n        return [*batch]\n\n    <DED>def sample_from_buffer(self, beta: float = None):\n        <IND>\"\"\"Samples experiences from the buffer and converts them into usable formats\n\n        Args:\n            beta (float): Importance-Sampling beta for prioritized replay\n\n        Returns:\n            batch (:obj:`list`): Replay experiences sampled from the buffer\n        \"\"\"\n        # Samples from the buffer\n        if beta is not None:\n            <IND>batch = self.replay_buffer.sample(self.batch_size, beta=beta)\n        <DED>else:\n            <IND>batch = self.replay_buffer.sample(self.batch_size)\n\n        <DED>states, actions, rewards, next_states, dones = self._reshape_batch(batch)\n\n        # Convert every experience to a Named Tuple. Either Replay or Prioritized Replay samples.\n        if self.buffer_type == \"push\":\n            <IND>batch = ReplayBufferSamples(*[states, actions, rewards, next_states, dones])\n        <DED>elif self.buffer_type == \"prioritized\":\n            <IND>indices, weights = batch[5], batch[6]\n            batch = PrioritizedReplayBufferSamples(\n                *[states, actions, rewards, next_states, dones, indices, weights]\n            )\n        <DED>return batch\n\n    <DED>def get_q_loss(self, batch: collections.namedtuple) -> torch.Tensor:\n        <IND>\"\"\"Normal Function to calculate the loss of the Q-function or critic\n\n        Args:\n            batch (:obj:`collections.namedtuple` of :obj:`torch.Tensor`): Batch of experiences\n\n        Returns:\n            loss (:obj:`torch.Tensor`): Calculated loss of the Q-function\n        \"\"\"\n        q_values = self.get_q_values(batch.states, batch.actions)\n        target_q_values = self.get_target_q_values(\n            batch.next_states, batch.rewards, batch.dones\n        )\n        loss = F.mse_loss(q_values, target_q_values)\n        return loss\n\n\n<DED><DED>class OffPolicyAgentAC(OffPolicyAgent):\n    <IND>\"\"\"Off Policy Agent Base Class\n\n    Attributes:\n        network (str): The network type of the Q-value function.\n            Supported types: [\"cnn\", \"mlp\"]\n        env (Environment): The environment that the agent is supposed to act on\n        create_model (bool): Whether the model of the algo should be created when initialised\n        batch_size (int): Mini batch size for loading experiences\n        gamma (float): The discount factor for rewards\n        layers (:obj:`tuple` of :obj:`int`): Layers in the Neural Network\n            of the Q-value function\n        lr_policy (float): Learning rate for the policy/actor\n        lr_value (float): Learning rate for the Q-value function\n        replay_size (int): Capacity of the Replay Buffer\n        buffer_type (str): Choose the type of Buffer: [\"push\", \"prioritized\"]\n        seed (int): Seed for randomness\n        render (bool): Should the env be rendered during training?\n        device (str): Hardware being used for training. Options:\n            [\"cuda\" -> GPU, \"cpu\" -> CPU]\n    \"\"\"\n\n    def __init__(self, *args, polyak=0.995, **kwargs):\n        <IND>super(OffPolicyAgentAC, self).__init__(*args, **kwargs)\n        self.polyak = polyak\n\n    <DED>def select_action(\n        self, state: np.ndarray, deterministic: bool = True\n    ) -> np.ndarray:\n        <IND>\"\"\"Select action given state\n\n        Deterministic Action Selection with Noise\n\n        Args:\n            state (:obj:`np.ndarray`): Current state of the environment\n            deterministic (bool): Should the policy be deterministic or stochastic\n\n        Returns:\n            action (:obj:`np.ndarray`): Action taken by the agent\n        \"\"\"\n        state = torch.as_tensor(state).float()\n        action, _ = self.ac.get_action(state, deterministic)\n        action = action.detach().cpu().numpy()\n\n        # add noise to output from policy network\n        if self.noise is not None:\n            <IND>action += self.noise()\n\n        <DED>return np.clip(\n            action, self.env.action_space.low[0], self.env.action_space.high[0]\n        )\n\n    <DED>def update_target_model(self) -> None:\n        <IND>\"\"\"Function to update the target Q model\n\n        Updates the target model with the training model's weights when called\n        \"\"\"\n        for param, param_target in zip(\n            self.ac.parameters(), self.ac_target.parameters()\n        ):\n            <IND>param_target.data.mul_(self.polyak)\n            param_target.data.add_((1 - self.polyak) * param.data)\n\n    <DED><DED>def get_p_loss(self, states: torch.Tensor) -> torch.Tensor:\n        <IND>\"\"\"Function to get the Policy loss\n\n        Args:\n            states (:obj:`torch.Tensor`): States for which Q-values need to be found\n\n        Returns:\n            loss (:obj:`torch.Tensor`): Calculated policy loss\n        \"\"\"\n        next_best_actions = self.ac.get_action(states, True)[0]\n        q_values = self.ac.get_value(torch.cat([states, next_best_actions], dim=-1))\n        policy_loss = -torch.mean(q_values)\n        return policy_loss\n\n    <DED>def load_weights(self, weights) -> None:\n        <IND>\"\"\"\n        Load weights for the agent from pretrained model\n        \"\"\"\n        self.ac.load_state_dict(weights[\"weights\"])\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def empty_logs(self):\n        <IND>\"\"\"Empties logs\n        \"\"\"\n        raise NotImplementedError\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "SforAiDl/genrl",
    "commit": "a59f765bca4f083f6af66f33ad9c875c8ba23ac2",
    "filename": "genrl/deep/agents/ddpg/ddpg.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/SforAiDl-genrl/genrl/deep/agents/ddpg/ddpg.py",
    "file_hunks_size": 4,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "genrl/deep/agents/ddpg/ddpg.py:56:74 Incompatible parameter type [6]: Expected `str` for 2nd positional only parameter to call `get_env_properties` but got `typing.Union[genrl.deep.common.base.BaseActorCritic, str]`.",
    "message": " Expected `str` for 2nd positional only parameter to call `get_env_properties` but got `typing.Union[genrl.deep.common.base.BaseActorCritic, str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 56,
    "warning_line": "        input_dim, action_dim, discrete, _ = get_env_properties(self.env, self.network)"
  },
  {
    "project": "SforAiDl/genrl",
    "commit": "a59f765bca4f083f6af66f33ad9c875c8ba23ac2",
    "filename": "genrl/deep/agents/ddpg/ddpg.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/SforAiDl-genrl/genrl/deep/agents/ddpg/ddpg.py",
    "file_hunks_size": 4,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "genrl/deep/agents/ddpg/ddpg.py:115:26 Unsupported operand [58]: `+` is not supported for operand types `List[float]` and `float`.",
    "message": " `+` is not supported for operand types `List[float]` and `float`.",
    "rule_id": "Unsupported operand [58]",
    "warning_line_no": 115,
    "warning_line": "        target_q_values = rewards + self.gamma * (1 - dones) * next_q_target_values",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\n        self.replay_buffer = self.buffer_class(self.replay_size)\n        self.optimizer_policy = opt.Adam(self.ac.actor.parameters(), lr=self.lr_policy)\n        self.optimizer_value = opt.Adam(self.ac.critic.parameters(), lr=self.lr_value)\n\n    def get_q_values(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n        \"\"\"Get Q values corresponding to specific states and actions\n\n        Args:\n            states (:obj:`torch.Tensor`): States for which Q-values need to be found\n            actions (:obj:`torch.Tensor`): Actions taken at respective states\n\n        Returns:\n            q_values (:obj:`torch.Tensor`): Q values for the given states and actions\n        \"\"\"\n        q_values = self.ac.critic.get_value(torch.cat([states, actions], dim=-1))\n        return q_values\n\n    def get_target_q_values(\n        self, next_states: torch.Tensor, rewards: List[float], dones: List[bool]\n    ) -> torch.Tensor:\n        \"\"\"Get target Q values for the DDPG\n\n        Args:\n            next_states (:obj:`torch.Tensor`): Next states for which target Q-values\n                need to be found\n            rewards (:obj:`list`): Rewards at each timestep for each environment\n            dones (:obj:`list`): Game over status for each environment\n\n        Returns:\n            target_q_values (:obj:`torch.Tensor`): Target Q values for the DDPG\n        \"\"\"\n        next_target_actions = self.ac_target.get_action(next_states, True)[0]\n        next_q_target_values = self.ac_target.get_value(\n            torch.cat([next_states, next_target_actions], dim=-1)\n        )\n        target_q_values = rewards + self.gamma * (1 - dones) * next_q_target_values\n        return target_q_values\n\n    def get_p_loss(self, states: np.ndarray) -> torch.Tensor:\n        \"\"\"Get policy loss for DDPG\n\n        Args:\n            states (:obj:`np.ndarray`): State at which the loss needs to be found\n\n        Returns:\n            policy_loss (:obj:`torch.Tensor`): Policy loss at the state\n        \"\"\"\n        next_best_actions = self.ac.get_action(states, True)[0]\n        q_values = self.ac.get_value(torch.cat([states, next_best_actions], dim=-1))\n        policy_loss = -torch.mean(q_values)\n        return policy_loss\n\n",
        "source_code_len": 2218,
        "target_code": "\n        self.optimizer_policy = opt.Adam(self.ac.actor.parameters(), lr=self.lr_policy)\n        self.optimizer_value = opt.Adam(self.ac.critic.parameters(), lr=self.lr_value)\n\n",
        "target_code_len": 177,
        "diff_format": "@@ -79,53 +78,4 @@\n \n-        self.replay_buffer = self.buffer_class(self.replay_size)\n         self.optimizer_policy = opt.Adam(self.ac.actor.parameters(), lr=self.lr_policy)\n         self.optimizer_value = opt.Adam(self.ac.critic.parameters(), lr=self.lr_value)\n-\n-    def get_q_values(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n-        \"\"\"Get Q values corresponding to specific states and actions\n-\n-        Args:\n-            states (:obj:`torch.Tensor`): States for which Q-values need to be found\n-            actions (:obj:`torch.Tensor`): Actions taken at respective states\n-\n-        Returns:\n-            q_values (:obj:`torch.Tensor`): Q values for the given states and actions\n-        \"\"\"\n-        q_values = self.ac.critic.get_value(torch.cat([states, actions], dim=-1))\n-        return q_values\n-\n-    def get_target_q_values(\n-        self, next_states: torch.Tensor, rewards: List[float], dones: List[bool]\n-    ) -> torch.Tensor:\n-        \"\"\"Get target Q values for the DDPG\n-\n-        Args:\n-            next_states (:obj:`torch.Tensor`): Next states for which target Q-values\n-                need to be found\n-            rewards (:obj:`list`): Rewards at each timestep for each environment\n-            dones (:obj:`list`): Game over status for each environment\n-\n-        Returns:\n-            target_q_values (:obj:`torch.Tensor`): Target Q values for the DDPG\n-        \"\"\"\n-        next_target_actions = self.ac_target.get_action(next_states, True)[0]\n-        next_q_target_values = self.ac_target.get_value(\n-            torch.cat([next_states, next_target_actions], dim=-1)\n-        )\n-        target_q_values = rewards + self.gamma * (1 - dones) * next_q_target_values\n-        return target_q_values\n-\n-    def get_p_loss(self, states: np.ndarray) -> torch.Tensor:\n-        \"\"\"Get policy loss for DDPG\n-\n-        Args:\n-            states (:obj:`np.ndarray`): State at which the loss needs to be found\n-\n-        Returns:\n-            policy_loss (:obj:`torch.Tensor`): Policy loss at the state\n-        \"\"\"\n-        next_best_actions = self.ac.get_action(states, True)[0]\n-        q_values = self.ac.get_value(torch.cat([states, next_best_actions], dim=-1))\n-        policy_loss = -torch.mean(q_values)\n-        return policy_loss\n \n",
        "source_code_with_indent": "\n        self.replay_buffer = self.buffer_class(self.replay_size)\n        self.optimizer_policy = opt.Adam(self.ac.actor.parameters(), lr=self.lr_policy)\n        self.optimizer_value = opt.Adam(self.ac.critic.parameters(), lr=self.lr_value)\n\n    <DED>def get_q_values(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n        <IND>\"\"\"Get Q values corresponding to specific states and actions\n\n        Args:\n            states (:obj:`torch.Tensor`): States for which Q-values need to be found\n            actions (:obj:`torch.Tensor`): Actions taken at respective states\n\n        Returns:\n            q_values (:obj:`torch.Tensor`): Q values for the given states and actions\n        \"\"\"\n        q_values = self.ac.critic.get_value(torch.cat([states, actions], dim=-1))\n        return q_values\n\n    <DED>def get_target_q_values(\n        self, next_states: torch.Tensor, rewards: List[float], dones: List[bool]\n    ) -> torch.Tensor:\n        <IND>\"\"\"Get target Q values for the DDPG\n\n        Args:\n            next_states (:obj:`torch.Tensor`): Next states for which target Q-values\n                need to be found\n            rewards (:obj:`list`): Rewards at each timestep for each environment\n            dones (:obj:`list`): Game over status for each environment\n\n        Returns:\n            target_q_values (:obj:`torch.Tensor`): Target Q values for the DDPG\n        \"\"\"\n        next_target_actions = self.ac_target.get_action(next_states, True)[0]\n        next_q_target_values = self.ac_target.get_value(\n            torch.cat([next_states, next_target_actions], dim=-1)\n        )\n        target_q_values = rewards + self.gamma * (1 - dones) * next_q_target_values\n        return target_q_values\n\n    <DED>def get_p_loss(self, states: np.ndarray) -> torch.Tensor:\n        <IND>\"\"\"Get policy loss for DDPG\n\n        Args:\n            states (:obj:`np.ndarray`): State at which the loss needs to be found\n\n        Returns:\n            policy_loss (:obj:`torch.Tensor`): Policy loss at the state\n        \"\"\"\n        next_best_actions = self.ac.get_action(states, True)[0]\n        q_values = self.ac.get_value(torch.cat([states, next_best_actions], dim=-1))\n        policy_loss = -torch.mean(q_values)\n        return policy_loss\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        self.optimizer_policy = opt.Adam(self.ac.actor.parameters(), lr=self.lr_policy)\n        self.optimizer_value = opt.Adam(self.ac.critic.parameters(), lr=self.lr_value)\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "SforAiDl/genrl",
    "commit": "a59f765bca4f083f6af66f33ad9c875c8ba23ac2",
    "filename": "genrl/deep/agents/ddpg/ddpg.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/SforAiDl-genrl/genrl/deep/agents/ddpg/ddpg.py",
    "file_hunks_size": 4,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "genrl/deep/agents/ddpg/ddpg.py:115:54 Unsupported operand [58]: `-` is not supported for operand types `int` and `List[bool]`.",
    "message": " `-` is not supported for operand types `int` and `List[bool]`.",
    "rule_id": "Unsupported operand [58]",
    "warning_line_no": 115,
    "warning_line": "        target_q_values = rewards + self.gamma * (1 - dones) * next_q_target_values",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\n        self.replay_buffer = self.buffer_class(self.replay_size)\n        self.optimizer_policy = opt.Adam(self.ac.actor.parameters(), lr=self.lr_policy)\n        self.optimizer_value = opt.Adam(self.ac.critic.parameters(), lr=self.lr_value)\n\n    def get_q_values(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n        \"\"\"Get Q values corresponding to specific states and actions\n\n        Args:\n            states (:obj:`torch.Tensor`): States for which Q-values need to be found\n            actions (:obj:`torch.Tensor`): Actions taken at respective states\n\n        Returns:\n            q_values (:obj:`torch.Tensor`): Q values for the given states and actions\n        \"\"\"\n        q_values = self.ac.critic.get_value(torch.cat([states, actions], dim=-1))\n        return q_values\n\n    def get_target_q_values(\n        self, next_states: torch.Tensor, rewards: List[float], dones: List[bool]\n    ) -> torch.Tensor:\n        \"\"\"Get target Q values for the DDPG\n\n        Args:\n            next_states (:obj:`torch.Tensor`): Next states for which target Q-values\n                need to be found\n            rewards (:obj:`list`): Rewards at each timestep for each environment\n            dones (:obj:`list`): Game over status for each environment\n\n        Returns:\n            target_q_values (:obj:`torch.Tensor`): Target Q values for the DDPG\n        \"\"\"\n        next_target_actions = self.ac_target.get_action(next_states, True)[0]\n        next_q_target_values = self.ac_target.get_value(\n            torch.cat([next_states, next_target_actions], dim=-1)\n        )\n        target_q_values = rewards + self.gamma * (1 - dones) * next_q_target_values\n        return target_q_values\n\n    def get_p_loss(self, states: np.ndarray) -> torch.Tensor:\n        \"\"\"Get policy loss for DDPG\n\n        Args:\n            states (:obj:`np.ndarray`): State at which the loss needs to be found\n\n        Returns:\n            policy_loss (:obj:`torch.Tensor`): Policy loss at the state\n        \"\"\"\n        next_best_actions = self.ac.get_action(states, True)[0]\n        q_values = self.ac.get_value(torch.cat([states, next_best_actions], dim=-1))\n        policy_loss = -torch.mean(q_values)\n        return policy_loss\n\n",
        "source_code_len": 2218,
        "target_code": "\n        self.optimizer_policy = opt.Adam(self.ac.actor.parameters(), lr=self.lr_policy)\n        self.optimizer_value = opt.Adam(self.ac.critic.parameters(), lr=self.lr_value)\n\n",
        "target_code_len": 177,
        "diff_format": "@@ -79,53 +78,4 @@\n \n-        self.replay_buffer = self.buffer_class(self.replay_size)\n         self.optimizer_policy = opt.Adam(self.ac.actor.parameters(), lr=self.lr_policy)\n         self.optimizer_value = opt.Adam(self.ac.critic.parameters(), lr=self.lr_value)\n-\n-    def get_q_values(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n-        \"\"\"Get Q values corresponding to specific states and actions\n-\n-        Args:\n-            states (:obj:`torch.Tensor`): States for which Q-values need to be found\n-            actions (:obj:`torch.Tensor`): Actions taken at respective states\n-\n-        Returns:\n-            q_values (:obj:`torch.Tensor`): Q values for the given states and actions\n-        \"\"\"\n-        q_values = self.ac.critic.get_value(torch.cat([states, actions], dim=-1))\n-        return q_values\n-\n-    def get_target_q_values(\n-        self, next_states: torch.Tensor, rewards: List[float], dones: List[bool]\n-    ) -> torch.Tensor:\n-        \"\"\"Get target Q values for the DDPG\n-\n-        Args:\n-            next_states (:obj:`torch.Tensor`): Next states for which target Q-values\n-                need to be found\n-            rewards (:obj:`list`): Rewards at each timestep for each environment\n-            dones (:obj:`list`): Game over status for each environment\n-\n-        Returns:\n-            target_q_values (:obj:`torch.Tensor`): Target Q values for the DDPG\n-        \"\"\"\n-        next_target_actions = self.ac_target.get_action(next_states, True)[0]\n-        next_q_target_values = self.ac_target.get_value(\n-            torch.cat([next_states, next_target_actions], dim=-1)\n-        )\n-        target_q_values = rewards + self.gamma * (1 - dones) * next_q_target_values\n-        return target_q_values\n-\n-    def get_p_loss(self, states: np.ndarray) -> torch.Tensor:\n-        \"\"\"Get policy loss for DDPG\n-\n-        Args:\n-            states (:obj:`np.ndarray`): State at which the loss needs to be found\n-\n-        Returns:\n-            policy_loss (:obj:`torch.Tensor`): Policy loss at the state\n-        \"\"\"\n-        next_best_actions = self.ac.get_action(states, True)[0]\n-        q_values = self.ac.get_value(torch.cat([states, next_best_actions], dim=-1))\n-        policy_loss = -torch.mean(q_values)\n-        return policy_loss\n \n",
        "source_code_with_indent": "\n        self.replay_buffer = self.buffer_class(self.replay_size)\n        self.optimizer_policy = opt.Adam(self.ac.actor.parameters(), lr=self.lr_policy)\n        self.optimizer_value = opt.Adam(self.ac.critic.parameters(), lr=self.lr_value)\n\n    <DED>def get_q_values(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n        <IND>\"\"\"Get Q values corresponding to specific states and actions\n\n        Args:\n            states (:obj:`torch.Tensor`): States for which Q-values need to be found\n            actions (:obj:`torch.Tensor`): Actions taken at respective states\n\n        Returns:\n            q_values (:obj:`torch.Tensor`): Q values for the given states and actions\n        \"\"\"\n        q_values = self.ac.critic.get_value(torch.cat([states, actions], dim=-1))\n        return q_values\n\n    <DED>def get_target_q_values(\n        self, next_states: torch.Tensor, rewards: List[float], dones: List[bool]\n    ) -> torch.Tensor:\n        <IND>\"\"\"Get target Q values for the DDPG\n\n        Args:\n            next_states (:obj:`torch.Tensor`): Next states for which target Q-values\n                need to be found\n            rewards (:obj:`list`): Rewards at each timestep for each environment\n            dones (:obj:`list`): Game over status for each environment\n\n        Returns:\n            target_q_values (:obj:`torch.Tensor`): Target Q values for the DDPG\n        \"\"\"\n        next_target_actions = self.ac_target.get_action(next_states, True)[0]\n        next_q_target_values = self.ac_target.get_value(\n            torch.cat([next_states, next_target_actions], dim=-1)\n        )\n        target_q_values = rewards + self.gamma * (1 - dones) * next_q_target_values\n        return target_q_values\n\n    <DED>def get_p_loss(self, states: np.ndarray) -> torch.Tensor:\n        <IND>\"\"\"Get policy loss for DDPG\n\n        Args:\n            states (:obj:`np.ndarray`): State at which the loss needs to be found\n\n        Returns:\n            policy_loss (:obj:`torch.Tensor`): Policy loss at the state\n        \"\"\"\n        next_best_actions = self.ac.get_action(states, True)[0]\n        q_values = self.ac.get_value(torch.cat([states, next_best_actions], dim=-1))\n        policy_loss = -torch.mean(q_values)\n        return policy_loss\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        self.optimizer_policy = opt.Adam(self.ac.actor.parameters(), lr=self.lr_policy)\n        self.optimizer_value = opt.Adam(self.ac.critic.parameters(), lr=self.lr_value)\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "SforAiDl/genrl",
    "commit": "a59f765bca4f083f6af66f33ad9c875c8ba23ac2",
    "filename": "genrl/deep/agents/dqn/base.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/SforAiDl-genrl/genrl/deep/agents/dqn/base.py",
    "file_hunks_size": 7,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "genrl/deep/agents/dqn/base.py:63:74 Incompatible parameter type [6]: Expected `str` for 2nd positional only parameter to call `get_env_properties` but got `typing.Union[genrl.deep.common.base.BaseActorCritic, str]`.",
    "message": " Expected `str` for 2nd positional only parameter to call `get_env_properties` but got `typing.Union[genrl.deep.common.base.BaseActorCritic, str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 63,
    "warning_line": "        input_dim, action_dim, discrete, _ = get_env_properties(self.env, self.network)"
  },
  {
    "project": "SforAiDl/genrl",
    "commit": "a59f765bca4f083f6af66f33ad9c875c8ba23ac2",
    "filename": "genrl/deep/agents/sac/sac.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/SforAiDl-genrl/genrl/deep/agents/sac/sac.py",
    "file_hunks_size": 11,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "genrl/deep/agents/sac/sac.py:137:16 Call error [29]: `Union` is not a function.",
    "message": " `Union` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 137,
    "warning_line": "                get_model(\"v\", self.network)("
  },
  {
    "project": "SforAiDl/genrl",
    "commit": "a59f765bca4f083f6af66f33ad9c875c8ba23ac2",
    "filename": "genrl/deep/agents/sac/sac.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/SforAiDl-genrl/genrl/deep/agents/sac/sac.py",
    "file_hunks_size": 11,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "genrl/deep/agents/sac/sac.py:137:31 Incompatible parameter type [6]: Expected `str` for 2nd positional only parameter to call `get_model` but got `Union[BaseActorCritic, str]`.",
    "message": " Expected `str` for 2nd positional only parameter to call `get_model` but got `Union[BaseActorCritic, str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 137,
    "warning_line": "                get_model(\"v\", self.network)("
  },
  {
    "project": "SforAiDl/genrl",
    "commit": "a59f765bca4f083f6af66f33ad9c875c8ba23ac2",
    "filename": "genrl/deep/agents/sac/sac.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/SforAiDl-genrl/genrl/deep/agents/sac/sac.py",
    "file_hunks_size": 11,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "genrl/deep/agents/sac/sac.py:145:16 Call error [29]: `Union` is not a function.",
    "message": " `Union` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 145,
    "warning_line": "                get_model(\"v\", self.network)("
  },
  {
    "project": "SforAiDl/genrl",
    "commit": "a59f765bca4f083f6af66f33ad9c875c8ba23ac2",
    "filename": "genrl/deep/agents/sac/sac.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/SforAiDl-genrl/genrl/deep/agents/sac/sac.py",
    "file_hunks_size": 11,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "genrl/deep/agents/sac/sac.py:145:31 Incompatible parameter type [6]: Expected `str` for 2nd positional only parameter to call `get_model` but got `Union[BaseActorCritic, str]`.",
    "message": " Expected `str` for 2nd positional only parameter to call `get_model` but got `Union[BaseActorCritic, str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 145,
    "warning_line": "                get_model(\"v\", self.network)("
  },
  {
    "project": "SforAiDl/genrl",
    "commit": "a59f765bca4f083f6af66f33ad9c875c8ba23ac2",
    "filename": "genrl/deep/agents/sac/sac.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/SforAiDl-genrl/genrl/deep/agents/sac/sac.py",
    "file_hunks_size": 11,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "genrl/deep/agents/sac/sac.py:153:16 Call error [29]: `Union` is not a function.",
    "message": " `Union` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 153,
    "warning_line": "                get_model(\"p\", self.network)("
  },
  {
    "project": "SforAiDl/genrl",
    "commit": "a59f765bca4f083f6af66f33ad9c875c8ba23ac2",
    "filename": "genrl/deep/agents/sac/sac.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/SforAiDl-genrl/genrl/deep/agents/sac/sac.py",
    "file_hunks_size": 11,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "genrl/deep/agents/sac/sac.py:153:31 Incompatible parameter type [6]: Expected `str` for 2nd positional only parameter to call `get_model` but got `Union[BaseActorCritic, str]`.",
    "message": " Expected `str` for 2nd positional only parameter to call `get_model` but got `Union[BaseActorCritic, str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 153,
    "warning_line": "                get_model(\"p\", self.network)("
  },
  {
    "project": "SforAiDl/genrl",
    "commit": "a59f765bca4f083f6af66f33ad9c875c8ba23ac2",
    "filename": "genrl/deep/agents/sac/sac.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/SforAiDl-genrl/genrl/deep/agents/sac/sac.py",
    "file_hunks_size": 11,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "genrl/deep/agents/sac/sac.py:360:8 Incompatible return type [7]: Expected `Tuple[float]` but got implicit return value of `None`.",
    "message": " Expected `Tuple[float]` but got implicit return value of `None`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 360,
    "warning_line": "        self.logs[\"alpha_loss\"].append(alpha_loss.item())"
  },
  {
    "project": "SforAiDl/genrl",
    "commit": "a59f765bca4f083f6af66f33ad9c875c8ba23ac2",
    "filename": "genrl/deep/agents/td3/td3.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/SforAiDl-genrl/genrl/deep/agents/td3/td3.py",
    "file_hunks_size": 13,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "genrl/deep/agents/td3/td3.py:64:74 Incompatible parameter type [6]: Expected `str` for 2nd positional only parameter to call `get_env_properties` but got `typing.Union[genrl.deep.common.base.BaseActorCritic, str]`.",
    "message": " Expected `str` for 2nd positional only parameter to call `get_env_properties` but got `typing.Union[genrl.deep.common.base.BaseActorCritic, str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 64,
    "warning_line": "        input_dim, action_dim, discrete, _ = get_env_properties(self.env, self.network)"
  },
  {
    "project": "SforAiDl/genrl",
    "commit": "a59f765bca4f083f6af66f33ad9c875c8ba23ac2",
    "filename": "genrl/deep/agents/td3/td3.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/SforAiDl-genrl/genrl/deep/agents/td3/td3.py",
    "file_hunks_size": 13,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "genrl/deep/agents/td3/td3.py:131:26 Unsupported operand [58]: `+` is not supported for operand types `List[float]` and `float`.",
    "message": " `+` is not supported for operand types `List[float]` and `float`.",
    "rule_id": "Unsupported operand [58]",
    "warning_line_no": 131,
    "warning_line": "        target_q_values = rewards + self.gamma * (1 - dones) * next_q_target_values",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        )\n\n    def get_q_values(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n        \"\"\"Get Q values corresponding to specific states and actions\n\n        Args:\n            states (:obj:`torch.Tensor`): States for which Q-values need to be found\n            actions (:obj:`torch.Tensor`): Actions taken at respective states\n\n        Returns:\n            q_values (:obj:`torch.Tensor`): Q values for the given states and actions\n        \"\"\"\n        q_values = self.ac.get_value(torch.cat([states, actions], dim=-1), mode=\"both\")\n        return q_values\n\n    def get_target_q_values(\n        self, next_states: torch.Tensor, rewards: List[float], dones: List[bool]\n    ) -> torch.Tensor:\n        \"\"\"Get target Q values for the TD3\n\n        Args:\n            next_states (:obj:`torch.Tensor`): Next states for which target Q-values\n                need to be found\n            rewards (:obj:`list`): Rewards at each timestep for each environment\n            dones (:obj:`list`): Game over status for each environment\n\n        Returns:\n            target_q_values (:obj:`torch.Tensor`): Target Q values for the TD3\n        \"\"\"\n        next_target_actions = self.ac_target.get_action(next_states, True)[0]\n        next_q_target_values = self.ac_target.get_value(\n            torch.cat([next_states, next_target_actions], dim=-1), mode=\"min\"\n        )\n        target_q_values = rewards + self.gamma * (1 - dones) * next_q_target_values\n        return target_q_values\n\n    def get_q_loss(self, batch: NamedTuple) -> torch.Tensor:\n        \"\"\"TD3 Function to calculate the loss of the critic\n\n        Args:\n            batch (:obj:`collections.namedtuple` of :obj:`torch.Tensor`): Batch of experiences\n\n        Returns:\n            loss (:obj:`torch.Tensor`): Calculated loss of the Q-function\n        \"\"\"\n        q_values = self.get_q_values(batch.states, batch.actions)\n        target_q_values = self.get_target_q_values(\n            batch.next_states, batch.rewards, batch.dones\n        )\n        loss = F.mse_loss(q_values[0], target_q_values) + F.mse_loss(\n            q_values[1], target_q_values\n        )\n        return loss\n\n",
        "source_code_len": 2148,
        "target_code": "        )\n\n",
        "target_code_len": 11,
        "diff_format": "@@ -98,54 +96,2 @@\n         )\n-\n-    def get_q_values(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n-        \"\"\"Get Q values corresponding to specific states and actions\n-\n-        Args:\n-            states (:obj:`torch.Tensor`): States for which Q-values need to be found\n-            actions (:obj:`torch.Tensor`): Actions taken at respective states\n-\n-        Returns:\n-            q_values (:obj:`torch.Tensor`): Q values for the given states and actions\n-        \"\"\"\n-        q_values = self.ac.get_value(torch.cat([states, actions], dim=-1), mode=\"both\")\n-        return q_values\n-\n-    def get_target_q_values(\n-        self, next_states: torch.Tensor, rewards: List[float], dones: List[bool]\n-    ) -> torch.Tensor:\n-        \"\"\"Get target Q values for the TD3\n-\n-        Args:\n-            next_states (:obj:`torch.Tensor`): Next states for which target Q-values\n-                need to be found\n-            rewards (:obj:`list`): Rewards at each timestep for each environment\n-            dones (:obj:`list`): Game over status for each environment\n-\n-        Returns:\n-            target_q_values (:obj:`torch.Tensor`): Target Q values for the TD3\n-        \"\"\"\n-        next_target_actions = self.ac_target.get_action(next_states, True)[0]\n-        next_q_target_values = self.ac_target.get_value(\n-            torch.cat([next_states, next_target_actions], dim=-1), mode=\"min\"\n-        )\n-        target_q_values = rewards + self.gamma * (1 - dones) * next_q_target_values\n-        return target_q_values\n-\n-    def get_q_loss(self, batch: NamedTuple) -> torch.Tensor:\n-        \"\"\"TD3 Function to calculate the loss of the critic\n-\n-        Args:\n-            batch (:obj:`collections.namedtuple` of :obj:`torch.Tensor`): Batch of experiences\n-\n-        Returns:\n-            loss (:obj:`torch.Tensor`): Calculated loss of the Q-function\n-        \"\"\"\n-        q_values = self.get_q_values(batch.states, batch.actions)\n-        target_q_values = self.get_target_q_values(\n-            batch.next_states, batch.rewards, batch.dones\n-        )\n-        loss = F.mse_loss(q_values[0], target_q_values) + F.mse_loss(\n-            q_values[1], target_q_values\n-        )\n-        return loss\n \n",
        "source_code_with_indent": "        )\n\n    <DED>def get_q_values(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n        <IND>\"\"\"Get Q values corresponding to specific states and actions\n\n        Args:\n            states (:obj:`torch.Tensor`): States for which Q-values need to be found\n            actions (:obj:`torch.Tensor`): Actions taken at respective states\n\n        Returns:\n            q_values (:obj:`torch.Tensor`): Q values for the given states and actions\n        \"\"\"\n        q_values = self.ac.get_value(torch.cat([states, actions], dim=-1), mode=\"both\")\n        return q_values\n\n    <DED>def get_target_q_values(\n        self, next_states: torch.Tensor, rewards: List[float], dones: List[bool]\n    ) -> torch.Tensor:\n        <IND>\"\"\"Get target Q values for the TD3\n\n        Args:\n            next_states (:obj:`torch.Tensor`): Next states for which target Q-values\n                need to be found\n            rewards (:obj:`list`): Rewards at each timestep for each environment\n            dones (:obj:`list`): Game over status for each environment\n\n        Returns:\n            target_q_values (:obj:`torch.Tensor`): Target Q values for the TD3\n        \"\"\"\n        next_target_actions = self.ac_target.get_action(next_states, True)[0]\n        next_q_target_values = self.ac_target.get_value(\n            torch.cat([next_states, next_target_actions], dim=-1), mode=\"min\"\n        )\n        target_q_values = rewards + self.gamma * (1 - dones) * next_q_target_values\n        return target_q_values\n\n    <DED>def get_q_loss(self, batch: NamedTuple) -> torch.Tensor:\n        <IND>\"\"\"TD3 Function to calculate the loss of the critic\n\n        Args:\n            batch (:obj:`collections.namedtuple` of :obj:`torch.Tensor`): Batch of experiences\n\n        Returns:\n            loss (:obj:`torch.Tensor`): Calculated loss of the Q-function\n        \"\"\"\n        q_values = self.get_q_values(batch.states, batch.actions)\n        target_q_values = self.get_target_q_values(\n            batch.next_states, batch.rewards, batch.dones\n        )\n        loss = F.mse_loss(q_values[0], target_q_values) + F.mse_loss(\n            q_values[1], target_q_values\n        )\n        return loss\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        )\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "SforAiDl/genrl",
    "commit": "a59f765bca4f083f6af66f33ad9c875c8ba23ac2",
    "filename": "genrl/deep/agents/td3/td3.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/SforAiDl-genrl/genrl/deep/agents/td3/td3.py",
    "file_hunks_size": 13,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "genrl/deep/agents/td3/td3.py:131:54 Unsupported operand [58]: `-` is not supported for operand types `int` and `List[bool]`.",
    "message": " `-` is not supported for operand types `int` and `List[bool]`.",
    "rule_id": "Unsupported operand [58]",
    "warning_line_no": 131,
    "warning_line": "        target_q_values = rewards + self.gamma * (1 - dones) * next_q_target_values",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        )\n\n    def get_q_values(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n        \"\"\"Get Q values corresponding to specific states and actions\n\n        Args:\n            states (:obj:`torch.Tensor`): States for which Q-values need to be found\n            actions (:obj:`torch.Tensor`): Actions taken at respective states\n\n        Returns:\n            q_values (:obj:`torch.Tensor`): Q values for the given states and actions\n        \"\"\"\n        q_values = self.ac.get_value(torch.cat([states, actions], dim=-1), mode=\"both\")\n        return q_values\n\n    def get_target_q_values(\n        self, next_states: torch.Tensor, rewards: List[float], dones: List[bool]\n    ) -> torch.Tensor:\n        \"\"\"Get target Q values for the TD3\n\n        Args:\n            next_states (:obj:`torch.Tensor`): Next states for which target Q-values\n                need to be found\n            rewards (:obj:`list`): Rewards at each timestep for each environment\n            dones (:obj:`list`): Game over status for each environment\n\n        Returns:\n            target_q_values (:obj:`torch.Tensor`): Target Q values for the TD3\n        \"\"\"\n        next_target_actions = self.ac_target.get_action(next_states, True)[0]\n        next_q_target_values = self.ac_target.get_value(\n            torch.cat([next_states, next_target_actions], dim=-1), mode=\"min\"\n        )\n        target_q_values = rewards + self.gamma * (1 - dones) * next_q_target_values\n        return target_q_values\n\n    def get_q_loss(self, batch: NamedTuple) -> torch.Tensor:\n        \"\"\"TD3 Function to calculate the loss of the critic\n\n        Args:\n            batch (:obj:`collections.namedtuple` of :obj:`torch.Tensor`): Batch of experiences\n\n        Returns:\n            loss (:obj:`torch.Tensor`): Calculated loss of the Q-function\n        \"\"\"\n        q_values = self.get_q_values(batch.states, batch.actions)\n        target_q_values = self.get_target_q_values(\n            batch.next_states, batch.rewards, batch.dones\n        )\n        loss = F.mse_loss(q_values[0], target_q_values) + F.mse_loss(\n            q_values[1], target_q_values\n        )\n        return loss\n\n",
        "source_code_len": 2148,
        "target_code": "        )\n\n",
        "target_code_len": 11,
        "diff_format": "@@ -98,54 +96,2 @@\n         )\n-\n-    def get_q_values(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n-        \"\"\"Get Q values corresponding to specific states and actions\n-\n-        Args:\n-            states (:obj:`torch.Tensor`): States for which Q-values need to be found\n-            actions (:obj:`torch.Tensor`): Actions taken at respective states\n-\n-        Returns:\n-            q_values (:obj:`torch.Tensor`): Q values for the given states and actions\n-        \"\"\"\n-        q_values = self.ac.get_value(torch.cat([states, actions], dim=-1), mode=\"both\")\n-        return q_values\n-\n-    def get_target_q_values(\n-        self, next_states: torch.Tensor, rewards: List[float], dones: List[bool]\n-    ) -> torch.Tensor:\n-        \"\"\"Get target Q values for the TD3\n-\n-        Args:\n-            next_states (:obj:`torch.Tensor`): Next states for which target Q-values\n-                need to be found\n-            rewards (:obj:`list`): Rewards at each timestep for each environment\n-            dones (:obj:`list`): Game over status for each environment\n-\n-        Returns:\n-            target_q_values (:obj:`torch.Tensor`): Target Q values for the TD3\n-        \"\"\"\n-        next_target_actions = self.ac_target.get_action(next_states, True)[0]\n-        next_q_target_values = self.ac_target.get_value(\n-            torch.cat([next_states, next_target_actions], dim=-1), mode=\"min\"\n-        )\n-        target_q_values = rewards + self.gamma * (1 - dones) * next_q_target_values\n-        return target_q_values\n-\n-    def get_q_loss(self, batch: NamedTuple) -> torch.Tensor:\n-        \"\"\"TD3 Function to calculate the loss of the critic\n-\n-        Args:\n-            batch (:obj:`collections.namedtuple` of :obj:`torch.Tensor`): Batch of experiences\n-\n-        Returns:\n-            loss (:obj:`torch.Tensor`): Calculated loss of the Q-function\n-        \"\"\"\n-        q_values = self.get_q_values(batch.states, batch.actions)\n-        target_q_values = self.get_target_q_values(\n-            batch.next_states, batch.rewards, batch.dones\n-        )\n-        loss = F.mse_loss(q_values[0], target_q_values) + F.mse_loss(\n-            q_values[1], target_q_values\n-        )\n-        return loss\n \n",
        "source_code_with_indent": "        )\n\n    <DED>def get_q_values(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n        <IND>\"\"\"Get Q values corresponding to specific states and actions\n\n        Args:\n            states (:obj:`torch.Tensor`): States for which Q-values need to be found\n            actions (:obj:`torch.Tensor`): Actions taken at respective states\n\n        Returns:\n            q_values (:obj:`torch.Tensor`): Q values for the given states and actions\n        \"\"\"\n        q_values = self.ac.get_value(torch.cat([states, actions], dim=-1), mode=\"both\")\n        return q_values\n\n    <DED>def get_target_q_values(\n        self, next_states: torch.Tensor, rewards: List[float], dones: List[bool]\n    ) -> torch.Tensor:\n        <IND>\"\"\"Get target Q values for the TD3\n\n        Args:\n            next_states (:obj:`torch.Tensor`): Next states for which target Q-values\n                need to be found\n            rewards (:obj:`list`): Rewards at each timestep for each environment\n            dones (:obj:`list`): Game over status for each environment\n\n        Returns:\n            target_q_values (:obj:`torch.Tensor`): Target Q values for the TD3\n        \"\"\"\n        next_target_actions = self.ac_target.get_action(next_states, True)[0]\n        next_q_target_values = self.ac_target.get_value(\n            torch.cat([next_states, next_target_actions], dim=-1), mode=\"min\"\n        )\n        target_q_values = rewards + self.gamma * (1 - dones) * next_q_target_values\n        return target_q_values\n\n    <DED>def get_q_loss(self, batch: NamedTuple) -> torch.Tensor:\n        <IND>\"\"\"TD3 Function to calculate the loss of the critic\n\n        Args:\n            batch (:obj:`collections.namedtuple` of :obj:`torch.Tensor`): Batch of experiences\n\n        Returns:\n            loss (:obj:`torch.Tensor`): Calculated loss of the Q-function\n        \"\"\"\n        q_values = self.get_q_values(batch.states, batch.actions)\n        target_q_values = self.get_target_q_values(\n            batch.next_states, batch.rewards, batch.dones\n        )\n        loss = F.mse_loss(q_values[0], target_q_values) + F.mse_loss(\n            q_values[1], target_q_values\n        )\n        return loss\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        )\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "SforAiDl/genrl",
    "commit": "a59f765bca4f083f6af66f33ad9c875c8ba23ac2",
    "filename": "genrl/deep/common/trainer.py",
    "min_patch_found": false,
    "full_warning_msg": "genrl/deep/common/trainer.py:62:8 Incompatible variable type [9]: buffer is declared to have type `Union[Type[PrioritizedBuffer], Type[ReplayBuffer]]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/SforAiDl-genrl/genrl/deep/common/trainer.py'",
    "dd_fail": true
  },
  {
    "project": "SforAiDl/genrl",
    "commit": "a59f765bca4f083f6af66f33ad9c875c8ba23ac2",
    "filename": "genrl/deep/common/trainer.py",
    "min_patch_found": false,
    "full_warning_msg": "genrl/deep/common/trainer.py:66:8 Incompatible variable type [9]: run_num is declared to have type `int` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/SforAiDl-genrl/genrl/deep/common/trainer.py'",
    "dd_fail": true
  },
  {
    "project": "SforAiDl/genrl",
    "commit": "a59f765bca4f083f6af66f33ad9c875c8ba23ac2",
    "filename": "genrl/deep/common/trainer.py",
    "min_patch_found": false,
    "full_warning_msg": "genrl/deep/common/trainer.py:67:8 Incompatible variable type [9]: load_model is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/SforAiDl-genrl/genrl/deep/common/trainer.py'",
    "dd_fail": true
  },
  {
    "project": "SforAiDl/genrl",
    "commit": "a59f765bca4f083f6af66f33ad9c875c8ba23ac2",
    "filename": "genrl/deep/common/trainer.py",
    "min_patch_found": false,
    "full_warning_msg": "genrl/deep/common/trainer.py:271:8 Incompatible variable type [9]: buffer is declared to have type `Union[Type[PrioritizedBuffer], Type[ReplayBuffer]]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/SforAiDl-genrl/genrl/deep/common/trainer.py'",
    "dd_fail": true
  },
  {
    "project": "SforAiDl/genrl",
    "commit": "a59f765bca4f083f6af66f33ad9c875c8ba23ac2",
    "filename": "genrl/deep/common/trainer.py",
    "min_patch_found": false,
    "full_warning_msg": "genrl/deep/common/trainer.py:275:8 Incompatible variable type [9]: run_num is declared to have type `int` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/SforAiDl-genrl/genrl/deep/common/trainer.py'",
    "dd_fail": true
  },
  {
    "project": "SforAiDl/genrl",
    "commit": "a59f765bca4f083f6af66f33ad9c875c8ba23ac2",
    "filename": "genrl/deep/common/trainer.py",
    "min_patch_found": false,
    "full_warning_msg": "genrl/deep/common/trainer.py:276:8 Incompatible variable type [9]: load_model is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/SforAiDl-genrl/genrl/deep/common/trainer.py'",
    "dd_fail": true
  },
  {
    "project": "SforAiDl/genrl",
    "commit": "a59f765bca4f083f6af66f33ad9c875c8ba23ac2",
    "filename": "genrl/deep/common/trainer.py",
    "min_patch_found": false,
    "full_warning_msg": "genrl/deep/common/trainer.py:455:8 Incompatible variable type [9]: run_num is declared to have type `int` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/SforAiDl-genrl/genrl/deep/common/trainer.py'",
    "dd_fail": true
  },
  {
    "project": "SforAiDl/genrl",
    "commit": "a59f765bca4f083f6af66f33ad9c875c8ba23ac2",
    "filename": "genrl/deep/common/trainer.py",
    "min_patch_found": false,
    "full_warning_msg": "genrl/deep/common/trainer.py:456:8 Incompatible variable type [9]: load_model is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/SforAiDl-genrl/genrl/deep/common/trainer.py'",
    "dd_fail": true
  },
  {
    "project": "SforAiDl/genrl",
    "commit": "a59f765bca4f083f6af66f33ad9c875c8ba23ac2",
    "filename": "genrl/deep/common/trainer.py",
    "min_patch_found": false,
    "full_warning_msg": "genrl/deep/common/trainer.py:475:12 Incompatible parameter type [6]: Expected `Union[Type[PrioritizedBuffer], Type[ReplayBuffer]]` for 5th parameter `buffer` to call `Trainer.__init__` but got `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/SforAiDl-genrl/genrl/deep/common/trainer.py'",
    "dd_fail": true
  }
]