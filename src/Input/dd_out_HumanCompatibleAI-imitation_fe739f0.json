[
  {
    "project": "HumanCompatibleAI/imitation",
    "commit": "fe739f018cda818184d877bce7025e84d6ad7518",
    "filename": "src/imitation/scripts/analyze.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/HumanCompatibleAI-imitation/src/imitation/scripts/analyze.py",
    "file_hunks_size": 9,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "src/imitation/scripts/analyze.py:39:49 Incompatible parameter type [6]: Expected `str` for 2nd positional only parameter to call `_get_sacred_dicts` but got `Optional[str]`.",
    "message": " Expected `str` for 2nd positional only parameter to call `_get_sacred_dicts` but got `Optional[str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 39,
    "warning_line": "    sacred_dicts = _get_sacred_dicts(source_dir, run_name, env_name, skip_failed_runs)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "import logging\n",
        "source_code_len": 15,
        "target_code": "import collections\nimport itertools\nimport json\nimport logging\n",
        "target_code_len": 63,
        "diff_format": "@@ -1,1 +1,4 @@\n+import collections\n+import itertools\n+import json\n import logging\n",
        "source_code_with_indent": "import logging\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "import collections\nimport itertools\nimport json\nimport logging\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "import tempfile\nfrom collections import OrderedDict\nfrom typing import List, Optional\n\n",
        "source_code_len": 87,
        "target_code": "import tempfile\nimport warnings\nfrom collections import OrderedDict\nfrom typing import Any, Callable, List, Mapping, Optional, Sequence, Set\n\n",
        "target_code_len": 142,
        "diff_format": "@@ -4,4 +7,5 @@\n import tempfile\n+import warnings\n from collections import OrderedDict\n-from typing import List, Optional\n+from typing import Any, Callable, List, Mapping, Optional, Sequence, Set\n \n",
        "source_code_with_indent": "import tempfile\nfrom collections import OrderedDict\nfrom typing import List, Optional\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "import tempfile\nimport warnings\nfrom collections import OrderedDict\nfrom typing import Any, Callable, List, Mapping, Optional, Sequence, Set\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n@analysis_ex.command\ndef gather_tb_directories(\n    source_dir: str,\n    run_name: Optional[str],\n    env_name: Optional[str],\n    skip_failed_runs: bool,\n) -> dict:\n    \"\"\"Gather Tensorboard directories from a `parallel_ex` run.\n",
        "source_code_len": 231,
        "target_code": "\n@analysis_ex.capture\ndef _gather_sacred_dicts(\n    source_dirs: Sequence[str], run_name: str, env_name: str, skip_failed_runs: bool\n) -> List[sacred_util.SacredDicts]:\n    \"\"\"Helper function for parsing and selecting Sacred experiment JSON files.\n\n    Args:\n        source_dirs: A directory containing Sacred FileObserver subdirectories\n            associated with the `train_adversarial` Sacred script. Behavior is\n            undefined if there are Sacred subdirectories associated with other\n            scripts. (Captured argument)\n        run_name: If provided, then only analyze results from Sacred directories\n            associated with this run name. `run_name` is compared against the\n            \"experiment.name\" key in `run.json`. (Captured argument)\n        skip_failed_runs: If True, then filter out runs where the status is FAILED.\n            (Captured argument)\n\n    Returns:\n        A list of `SacredDicts` corresponding to the selected Sacred directories.\n    \"\"\"\n    # e.g. chain.from_iterable([[\"pathone\", \"pathtwo\"], [], [\"paththree\"]]) =>\n    # (\"pathone\", \"pathtwo\", \"paththree\")\n    sacred_dirs = itertools.chain.from_iterable(\n        sacred_util.filter_subdirs(source_dir) for source_dir in source_dirs\n    )\n    sacred_dicts = []\n\n    for sacred_dir in sacred_dirs:\n        try:\n            sacred_dicts.append(sacred_util.SacredDicts.load_from_dir(sacred_dir))\n        except json.JSONDecodeError:\n            warnings.warn(f\"Invalid JSON file in {sacred_dir}\", RuntimeWarning)\n\n    if run_name is not None:\n        sacred_dicts = filter(\n            lambda sd: get(sd.run, \"experiment.name\") == run_name, sacred_dicts\n        )\n\n    if env_name is not None:\n        sacred_dicts = filter(\n            lambda sd: get(sd.config, \"env_name\") == env_name, sacred_dicts\n        )\n\n    if skip_failed_runs:\n        sacred_dicts = filter(\n            lambda sd: get(sd.run, \"status\") != \"FAILED\", sacred_dicts\n        )\n\n    return list(sacred_dicts)\n\n\n@analysis_ex.command\ndef gather_tb_directories() -> dict:\n    \"\"\"Gather Tensorboard directories from a `parallel_ex` run.\n",
        "target_code_len": 2100,
        "diff_format": "@@ -15,9 +19,55 @@\n \n+@analysis_ex.capture\n+def _gather_sacred_dicts(\n+    source_dirs: Sequence[str], run_name: str, env_name: str, skip_failed_runs: bool\n+) -> List[sacred_util.SacredDicts]:\n+    \"\"\"Helper function for parsing and selecting Sacred experiment JSON files.\n+\n+    Args:\n+        source_dirs: A directory containing Sacred FileObserver subdirectories\n+            associated with the `train_adversarial` Sacred script. Behavior is\n+            undefined if there are Sacred subdirectories associated with other\n+            scripts. (Captured argument)\n+        run_name: If provided, then only analyze results from Sacred directories\n+            associated with this run name. `run_name` is compared against the\n+            \"experiment.name\" key in `run.json`. (Captured argument)\n+        skip_failed_runs: If True, then filter out runs where the status is FAILED.\n+            (Captured argument)\n+\n+    Returns:\n+        A list of `SacredDicts` corresponding to the selected Sacred directories.\n+    \"\"\"\n+    # e.g. chain.from_iterable([[\"pathone\", \"pathtwo\"], [], [\"paththree\"]]) =>\n+    # (\"pathone\", \"pathtwo\", \"paththree\")\n+    sacred_dirs = itertools.chain.from_iterable(\n+        sacred_util.filter_subdirs(source_dir) for source_dir in source_dirs\n+    )\n+    sacred_dicts = []\n+\n+    for sacred_dir in sacred_dirs:\n+        try:\n+            sacred_dicts.append(sacred_util.SacredDicts.load_from_dir(sacred_dir))\n+        except json.JSONDecodeError:\n+            warnings.warn(f\"Invalid JSON file in {sacred_dir}\", RuntimeWarning)\n+\n+    if run_name is not None:\n+        sacred_dicts = filter(\n+            lambda sd: get(sd.run, \"experiment.name\") == run_name, sacred_dicts\n+        )\n+\n+    if env_name is not None:\n+        sacred_dicts = filter(\n+            lambda sd: get(sd.config, \"env_name\") == env_name, sacred_dicts\n+        )\n+\n+    if skip_failed_runs:\n+        sacred_dicts = filter(\n+            lambda sd: get(sd.run, \"status\") != \"FAILED\", sacred_dicts\n+        )\n+\n+    return list(sacred_dicts)\n+\n+\n @analysis_ex.command\n-def gather_tb_directories(\n-    source_dir: str,\n-    run_name: Optional[str],\n-    env_name: Optional[str],\n-    skip_failed_runs: bool,\n-) -> dict:\n+def gather_tb_directories() -> dict:\n     \"\"\"Gather Tensorboard directories from a `parallel_ex` run.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n@analysis_ex.command\ndef gather_tb_directories(\n    source_dir: str,\n    run_name: Optional[str],\n    env_name: Optional[str],\n    skip_failed_runs: bool,\n) -> dict:\n    <IND>",
        "target_code_with_indent": "\n@analysis_ex.capture\ndef _gather_sacred_dicts(\n    source_dirs: Sequence[str], run_name: str, env_name: str, skip_failed_runs: bool\n) -> List[sacred_util.SacredDicts]:\n    <IND>\"\"\"Helper function for parsing and selecting Sacred experiment JSON files.\n\n    Args:\n        source_dirs: A directory containing Sacred FileObserver subdirectories\n            associated with the `train_adversarial` Sacred script. Behavior is\n            undefined if there are Sacred subdirectories associated with other\n            scripts. (Captured argument)\n        run_name: If provided, then only analyze results from Sacred directories\n            associated with this run name. `run_name` is compared against the\n            \"experiment.name\" key in `run.json`. (Captured argument)\n        skip_failed_runs: If True, then filter out runs where the status is FAILED.\n            (Captured argument)\n\n    Returns:\n        A list of `SacredDicts` corresponding to the selected Sacred directories.\n    \"\"\"\n    # e.g. chain.from_iterable([[\"pathone\", \"pathtwo\"], [], [\"paththree\"]]) =>\n    # (\"pathone\", \"pathtwo\", \"paththree\")\n    sacred_dirs = itertools.chain.from_iterable(\n        sacred_util.filter_subdirs(source_dir) for source_dir in source_dirs\n    )\n    sacred_dicts = []\n\n    for sacred_dir in sacred_dirs:\n        <IND>try:\n            <IND>sacred_dicts.append(sacred_util.SacredDicts.load_from_dir(sacred_dir))\n        <DED>except json.JSONDecodeError:\n            <IND>warnings.warn(f\"Invalid JSON file in {sacred_dir}\", RuntimeWarning)\n\n    <DED><DED>if run_name is not None:\n        <IND>sacred_dicts = filter(\n            lambda sd: get(sd.run, \"experiment.name\") == run_name, sacred_dicts\n        )\n\n    <DED>if env_name is not None:\n        <IND>sacred_dicts = filter(\n            lambda sd: get(sd.config, \"env_name\") == env_name, sacred_dicts\n        )\n\n    <DED>if skip_failed_runs:\n        <IND>sacred_dicts = filter(\n            lambda sd: get(sd.run, \"status\") != \"FAILED\", sacred_dicts\n        )\n\n    <DED>return list(sacred_dicts)\n\n\n<DED>@analysis_ex.command\ndef gather_tb_directories() -> dict:\n    <IND>"
      },
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    \"\"\"\n    sacred_dicts = _get_sacred_dicts(source_dir, run_name, env_name, skip_failed_runs)\n    os.makedirs(\"/tmp/analysis_tb\", exist_ok=True)\n",
        "source_code_len": 146,
        "target_code": "    \"\"\"\n\n    os.makedirs(\"/tmp/analysis_tb\", exist_ok=True)\n",
        "target_code_len": 60,
        "diff_format": "@@ -38,3 +87,3 @@\n     \"\"\"\n-    sacred_dicts = _get_sacred_dicts(source_dir, run_name, env_name, skip_failed_runs)\n+\n     os.makedirs(\"/tmp/analysis_tb\", exist_ok=True)\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n    sacred_dicts = _get_sacred_dicts(source_dir, run_name, env_name, skip_failed_runs)\n    os.makedirs(\"/tmp/analysis_tb\", exist_ok=True)\n",
        "target_code_with_indent": "\n\n    os.makedirs(\"/tmp/analysis_tb\", exist_ok=True)\n"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    tb_dirs_count = 0\n    for sd in sacred_dicts:\n        # Expecting a path like \"~/ray_results/{run_name}/sacred/1\".\n",
        "source_code_len": 119,
        "target_code": "    tb_dirs_count = 0\n    for sd in _gather_sacred_dicts():\n        # Expecting a path like \"~/ray_results/{run_name}/sacred/1\".\n",
        "target_code_len": 129,
        "diff_format": "@@ -43,3 +92,3 @@\n     tb_dirs_count = 0\n-    for sd in sacred_dicts:\n+    for sd in _gather_sacred_dicts():\n         # Expecting a path like \"~/ray_results/{run_name}/sacred/1\".\n",
        "source_code_with_indent": "    tb_dirs_count = 0\n    for sd in sacred_dicts:\n        # Expecting a path like \"~/ray_results/{run_name}/sacred/1\".\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    tb_dirs_count = 0\n    for sd in _gather_sacred_dicts():\n        # Expecting a path like \"~/ray_results/{run_name}/sacred/1\".\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "HumanCompatibleAI/imitation",
    "commit": "fe739f018cda818184d877bce7025e84d6ad7518",
    "filename": "src/imitation/scripts/analyze.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/HumanCompatibleAI-imitation/src/imitation/scripts/analyze.py",
    "file_hunks_size": 9,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "src/imitation/scripts/analyze.py:39:59 Incompatible parameter type [6]: Expected `str` for 3rd positional only parameter to call `_get_sacred_dicts` but got `Optional[str]`.",
    "message": " Expected `str` for 3rd positional only parameter to call `_get_sacred_dicts` but got `Optional[str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 39,
    "warning_line": "    sacred_dicts = _get_sacred_dicts(source_dir, run_name, env_name, skip_failed_runs)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "import logging\n",
        "source_code_len": 15,
        "target_code": "import collections\nimport itertools\nimport json\nimport logging\n",
        "target_code_len": 63,
        "diff_format": "@@ -1,1 +1,4 @@\n+import collections\n+import itertools\n+import json\n import logging\n",
        "source_code_with_indent": "import logging\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "import collections\nimport itertools\nimport json\nimport logging\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "import tempfile\nfrom collections import OrderedDict\nfrom typing import List, Optional\n\n",
        "source_code_len": 87,
        "target_code": "import tempfile\nimport warnings\nfrom collections import OrderedDict\nfrom typing import Any, Callable, List, Mapping, Optional, Sequence, Set\n\n",
        "target_code_len": 142,
        "diff_format": "@@ -4,4 +7,5 @@\n import tempfile\n+import warnings\n from collections import OrderedDict\n-from typing import List, Optional\n+from typing import Any, Callable, List, Mapping, Optional, Sequence, Set\n \n",
        "source_code_with_indent": "import tempfile\nfrom collections import OrderedDict\nfrom typing import List, Optional\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "import tempfile\nimport warnings\nfrom collections import OrderedDict\nfrom typing import Any, Callable, List, Mapping, Optional, Sequence, Set\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n@analysis_ex.command\ndef gather_tb_directories(\n    source_dir: str,\n    run_name: Optional[str],\n    env_name: Optional[str],\n    skip_failed_runs: bool,\n) -> dict:\n    \"\"\"Gather Tensorboard directories from a `parallel_ex` run.\n",
        "source_code_len": 231,
        "target_code": "\n@analysis_ex.capture\ndef _gather_sacred_dicts(\n    source_dirs: Sequence[str], run_name: str, env_name: str, skip_failed_runs: bool\n) -> List[sacred_util.SacredDicts]:\n    \"\"\"Helper function for parsing and selecting Sacred experiment JSON files.\n\n    Args:\n        source_dirs: A directory containing Sacred FileObserver subdirectories\n            associated with the `train_adversarial` Sacred script. Behavior is\n            undefined if there are Sacred subdirectories associated with other\n            scripts. (Captured argument)\n        run_name: If provided, then only analyze results from Sacred directories\n            associated with this run name. `run_name` is compared against the\n            \"experiment.name\" key in `run.json`. (Captured argument)\n        skip_failed_runs: If True, then filter out runs where the status is FAILED.\n            (Captured argument)\n\n    Returns:\n        A list of `SacredDicts` corresponding to the selected Sacred directories.\n    \"\"\"\n    # e.g. chain.from_iterable([[\"pathone\", \"pathtwo\"], [], [\"paththree\"]]) =>\n    # (\"pathone\", \"pathtwo\", \"paththree\")\n    sacred_dirs = itertools.chain.from_iterable(\n        sacred_util.filter_subdirs(source_dir) for source_dir in source_dirs\n    )\n    sacred_dicts = []\n\n    for sacred_dir in sacred_dirs:\n        try:\n            sacred_dicts.append(sacred_util.SacredDicts.load_from_dir(sacred_dir))\n        except json.JSONDecodeError:\n            warnings.warn(f\"Invalid JSON file in {sacred_dir}\", RuntimeWarning)\n\n    if run_name is not None:\n        sacred_dicts = filter(\n            lambda sd: get(sd.run, \"experiment.name\") == run_name, sacred_dicts\n        )\n\n    if env_name is not None:\n        sacred_dicts = filter(\n            lambda sd: get(sd.config, \"env_name\") == env_name, sacred_dicts\n        )\n\n    if skip_failed_runs:\n        sacred_dicts = filter(\n            lambda sd: get(sd.run, \"status\") != \"FAILED\", sacred_dicts\n        )\n\n    return list(sacred_dicts)\n\n\n@analysis_ex.command\ndef gather_tb_directories() -> dict:\n    \"\"\"Gather Tensorboard directories from a `parallel_ex` run.\n",
        "target_code_len": 2100,
        "diff_format": "@@ -15,9 +19,55 @@\n \n+@analysis_ex.capture\n+def _gather_sacred_dicts(\n+    source_dirs: Sequence[str], run_name: str, env_name: str, skip_failed_runs: bool\n+) -> List[sacred_util.SacredDicts]:\n+    \"\"\"Helper function for parsing and selecting Sacred experiment JSON files.\n+\n+    Args:\n+        source_dirs: A directory containing Sacred FileObserver subdirectories\n+            associated with the `train_adversarial` Sacred script. Behavior is\n+            undefined if there are Sacred subdirectories associated with other\n+            scripts. (Captured argument)\n+        run_name: If provided, then only analyze results from Sacred directories\n+            associated with this run name. `run_name` is compared against the\n+            \"experiment.name\" key in `run.json`. (Captured argument)\n+        skip_failed_runs: If True, then filter out runs where the status is FAILED.\n+            (Captured argument)\n+\n+    Returns:\n+        A list of `SacredDicts` corresponding to the selected Sacred directories.\n+    \"\"\"\n+    # e.g. chain.from_iterable([[\"pathone\", \"pathtwo\"], [], [\"paththree\"]]) =>\n+    # (\"pathone\", \"pathtwo\", \"paththree\")\n+    sacred_dirs = itertools.chain.from_iterable(\n+        sacred_util.filter_subdirs(source_dir) for source_dir in source_dirs\n+    )\n+    sacred_dicts = []\n+\n+    for sacred_dir in sacred_dirs:\n+        try:\n+            sacred_dicts.append(sacred_util.SacredDicts.load_from_dir(sacred_dir))\n+        except json.JSONDecodeError:\n+            warnings.warn(f\"Invalid JSON file in {sacred_dir}\", RuntimeWarning)\n+\n+    if run_name is not None:\n+        sacred_dicts = filter(\n+            lambda sd: get(sd.run, \"experiment.name\") == run_name, sacred_dicts\n+        )\n+\n+    if env_name is not None:\n+        sacred_dicts = filter(\n+            lambda sd: get(sd.config, \"env_name\") == env_name, sacred_dicts\n+        )\n+\n+    if skip_failed_runs:\n+        sacred_dicts = filter(\n+            lambda sd: get(sd.run, \"status\") != \"FAILED\", sacred_dicts\n+        )\n+\n+    return list(sacred_dicts)\n+\n+\n @analysis_ex.command\n-def gather_tb_directories(\n-    source_dir: str,\n-    run_name: Optional[str],\n-    env_name: Optional[str],\n-    skip_failed_runs: bool,\n-) -> dict:\n+def gather_tb_directories() -> dict:\n     \"\"\"Gather Tensorboard directories from a `parallel_ex` run.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n@analysis_ex.command\ndef gather_tb_directories(\n    source_dir: str,\n    run_name: Optional[str],\n    env_name: Optional[str],\n    skip_failed_runs: bool,\n) -> dict:\n    <IND>",
        "target_code_with_indent": "\n@analysis_ex.capture\ndef _gather_sacred_dicts(\n    source_dirs: Sequence[str], run_name: str, env_name: str, skip_failed_runs: bool\n) -> List[sacred_util.SacredDicts]:\n    <IND>\"\"\"Helper function for parsing and selecting Sacred experiment JSON files.\n\n    Args:\n        source_dirs: A directory containing Sacred FileObserver subdirectories\n            associated with the `train_adversarial` Sacred script. Behavior is\n            undefined if there are Sacred subdirectories associated with other\n            scripts. (Captured argument)\n        run_name: If provided, then only analyze results from Sacred directories\n            associated with this run name. `run_name` is compared against the\n            \"experiment.name\" key in `run.json`. (Captured argument)\n        skip_failed_runs: If True, then filter out runs where the status is FAILED.\n            (Captured argument)\n\n    Returns:\n        A list of `SacredDicts` corresponding to the selected Sacred directories.\n    \"\"\"\n    # e.g. chain.from_iterable([[\"pathone\", \"pathtwo\"], [], [\"paththree\"]]) =>\n    # (\"pathone\", \"pathtwo\", \"paththree\")\n    sacred_dirs = itertools.chain.from_iterable(\n        sacred_util.filter_subdirs(source_dir) for source_dir in source_dirs\n    )\n    sacred_dicts = []\n\n    for sacred_dir in sacred_dirs:\n        <IND>try:\n            <IND>sacred_dicts.append(sacred_util.SacredDicts.load_from_dir(sacred_dir))\n        <DED>except json.JSONDecodeError:\n            <IND>warnings.warn(f\"Invalid JSON file in {sacred_dir}\", RuntimeWarning)\n\n    <DED><DED>if run_name is not None:\n        <IND>sacred_dicts = filter(\n            lambda sd: get(sd.run, \"experiment.name\") == run_name, sacred_dicts\n        )\n\n    <DED>if env_name is not None:\n        <IND>sacred_dicts = filter(\n            lambda sd: get(sd.config, \"env_name\") == env_name, sacred_dicts\n        )\n\n    <DED>if skip_failed_runs:\n        <IND>sacred_dicts = filter(\n            lambda sd: get(sd.run, \"status\") != \"FAILED\", sacred_dicts\n        )\n\n    <DED>return list(sacred_dicts)\n\n\n<DED>@analysis_ex.command\ndef gather_tb_directories() -> dict:\n    <IND>"
      },
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    \"\"\"\n    sacred_dicts = _get_sacred_dicts(source_dir, run_name, env_name, skip_failed_runs)\n    os.makedirs(\"/tmp/analysis_tb\", exist_ok=True)\n",
        "source_code_len": 146,
        "target_code": "    \"\"\"\n\n    os.makedirs(\"/tmp/analysis_tb\", exist_ok=True)\n",
        "target_code_len": 60,
        "diff_format": "@@ -38,3 +87,3 @@\n     \"\"\"\n-    sacred_dicts = _get_sacred_dicts(source_dir, run_name, env_name, skip_failed_runs)\n+\n     os.makedirs(\"/tmp/analysis_tb\", exist_ok=True)\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n    sacred_dicts = _get_sacred_dicts(source_dir, run_name, env_name, skip_failed_runs)\n    os.makedirs(\"/tmp/analysis_tb\", exist_ok=True)\n",
        "target_code_with_indent": "\n\n    os.makedirs(\"/tmp/analysis_tb\", exist_ok=True)\n"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    tb_dirs_count = 0\n    for sd in sacred_dicts:\n        # Expecting a path like \"~/ray_results/{run_name}/sacred/1\".\n",
        "source_code_len": 119,
        "target_code": "    tb_dirs_count = 0\n    for sd in _gather_sacred_dicts():\n        # Expecting a path like \"~/ray_results/{run_name}/sacred/1\".\n",
        "target_code_len": 129,
        "diff_format": "@@ -43,3 +92,3 @@\n     tb_dirs_count = 0\n-    for sd in sacred_dicts:\n+    for sd in _gather_sacred_dicts():\n         # Expecting a path like \"~/ray_results/{run_name}/sacred/1\".\n",
        "source_code_with_indent": "    tb_dirs_count = 0\n    for sd in sacred_dicts:\n        # Expecting a path like \"~/ray_results/{run_name}/sacred/1\".\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    tb_dirs_count = 0\n    for sd in _gather_sacred_dicts():\n        # Expecting a path like \"~/ray_results/{run_name}/sacred/1\".\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "HumanCompatibleAI/imitation",
    "commit": "fe739f018cda818184d877bce7025e84d6ad7518",
    "filename": "src/imitation/scripts/analyze.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/HumanCompatibleAI-imitation/src/imitation/scripts/analyze.py",
    "file_hunks_size": 9,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "src/imitation/scripts/analyze.py:103:49 Incompatible parameter type [6]: Expected `str` for 2nd positional only parameter to call `_get_sacred_dicts` but got `Optional[str]`.",
    "message": " Expected `str` for 2nd positional only parameter to call `_get_sacred_dicts` but got `Optional[str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 103,
    "warning_line": "    sacred_dicts = _get_sacred_dicts(source_dir, run_name, env_name, skip_failed_runs)"
  },
  {
    "project": "HumanCompatibleAI/imitation",
    "commit": "fe739f018cda818184d877bce7025e84d6ad7518",
    "filename": "src/imitation/scripts/analyze.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/HumanCompatibleAI-imitation/src/imitation/scripts/analyze.py",
    "file_hunks_size": 9,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "src/imitation/scripts/analyze.py:103:59 Incompatible parameter type [6]: Expected `str` for 3rd positional only parameter to call `_get_sacred_dicts` but got `Optional[str]`.",
    "message": " Expected `str` for 3rd positional only parameter to call `_get_sacred_dicts` but got `Optional[str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 103,
    "warning_line": "    sacred_dicts = _get_sacred_dicts(source_dir, run_name, env_name, skip_failed_runs)"
  }
]