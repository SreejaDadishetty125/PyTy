[
  {
    "project": "asappresearch/flambe",
    "commit": "8a3944f5f11fec50499fed7839f5771dc4ad790d",
    "filename": "flambe/learn/distillation.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asappresearch-flambe/flambe/learn/distillation.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "flambe/learn/distillation.py:114:25 Incompatible parameter type [6]: Expected `Optional[str]` for 10th positional only parameter to call `flambe.learn.train.Trainer.__init__` but got `int`.",
    "message": " Expected `Optional[str]` for 10th positional only parameter to call `flambe.learn.train.Trainer.__init__` but got `int`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 114,
    "warning_line": "                         max_steps,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "                 scheduler: Optional[_LRScheduler] = None,\n                 device: Optional[str] = None,\n",
        "source_code_len": 106,
        "target_code": "                 scheduler: Optional[_LRScheduler] = None,\n                 iter_scheduler: Optional[_LRScheduler] = None,\n                 device: Optional[str] = None,\n",
        "target_code_len": 170,
        "diff_format": "@@ -34,2 +34,3 @@\n                  scheduler: Optional[_LRScheduler] = None,\n+                 iter_scheduler: Optional[_LRScheduler] = None,\n                  device: Optional[str] = None,\n",
        "source_code_with_indent": "                 scheduler: Optional[_LRScheduler] = None,\n                 device: Optional[str] = None,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                 scheduler: Optional[_LRScheduler] = None,\n                 iter_scheduler: Optional[_LRScheduler] = None,\n                 device: Optional[str] = None,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "                         scheduler,\n                         device,\n",
        "source_code_len": 69,
        "target_code": "                         scheduler,\n                         iter_scheduler,\n                         device,\n",
        "target_code_len": 110,
        "diff_format": "@@ -112,2 +116,3 @@\n                          scheduler,\n+                         iter_scheduler,\n                          device,\n",
        "source_code_with_indent": "                         scheduler,\n                         device,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                         scheduler,\n                         iter_scheduler,\n                         device,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "asappresearch/flambe",
    "commit": "8a3944f5f11fec50499fed7839f5771dc4ad790d",
    "filename": "flambe/learn/distillation.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asappresearch-flambe/flambe/learn/distillation.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "flambe/learn/distillation.py:115:25 Incompatible parameter type [6]: Expected `int` for 11th positional only parameter to call `flambe.learn.train.Trainer.__init__` but got `float`.",
    "message": " Expected `int` for 11th positional only parameter to call `flambe.learn.train.Trainer.__init__` but got `float`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 115,
    "warning_line": "                         epoch_per_step,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "                 scheduler: Optional[_LRScheduler] = None,\n                 device: Optional[str] = None,\n",
        "source_code_len": 106,
        "target_code": "                 scheduler: Optional[_LRScheduler] = None,\n                 iter_scheduler: Optional[_LRScheduler] = None,\n                 device: Optional[str] = None,\n",
        "target_code_len": 170,
        "diff_format": "@@ -34,2 +34,3 @@\n                  scheduler: Optional[_LRScheduler] = None,\n+                 iter_scheduler: Optional[_LRScheduler] = None,\n                  device: Optional[str] = None,\n",
        "source_code_with_indent": "                 scheduler: Optional[_LRScheduler] = None,\n                 device: Optional[str] = None,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                 scheduler: Optional[_LRScheduler] = None,\n                 iter_scheduler: Optional[_LRScheduler] = None,\n                 device: Optional[str] = None,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "                         scheduler,\n                         device,\n",
        "source_code_len": 69,
        "target_code": "                         scheduler,\n                         iter_scheduler,\n                         device,\n",
        "target_code_len": 110,
        "diff_format": "@@ -112,2 +116,3 @@\n                          scheduler,\n+                         iter_scheduler,\n                          device,\n",
        "source_code_with_indent": "                         scheduler,\n                         device,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                         scheduler,\n                         iter_scheduler,\n                         device,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "asappresearch/flambe",
    "commit": "8a3944f5f11fec50499fed7839f5771dc4ad790d",
    "filename": "flambe/learn/distillation.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asappresearch-flambe/flambe/learn/distillation.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "flambe/learn/distillation.py:116:25 Incompatible parameter type [6]: Expected `float` for 12th positional only parameter to call `flambe.learn.train.Trainer.__init__` but got `Optional[int]`.",
    "message": " Expected `float` for 12th positional only parameter to call `flambe.learn.train.Trainer.__init__` but got `Optional[int]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 116,
    "warning_line": "                         iter_per_step,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "                 scheduler: Optional[_LRScheduler] = None,\n                 device: Optional[str] = None,\n",
        "source_code_len": 106,
        "target_code": "                 scheduler: Optional[_LRScheduler] = None,\n                 iter_scheduler: Optional[_LRScheduler] = None,\n                 device: Optional[str] = None,\n",
        "target_code_len": 170,
        "diff_format": "@@ -34,2 +34,3 @@\n                  scheduler: Optional[_LRScheduler] = None,\n+                 iter_scheduler: Optional[_LRScheduler] = None,\n                  device: Optional[str] = None,\n",
        "source_code_with_indent": "                 scheduler: Optional[_LRScheduler] = None,\n                 device: Optional[str] = None,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                 scheduler: Optional[_LRScheduler] = None,\n                 iter_scheduler: Optional[_LRScheduler] = None,\n                 device: Optional[str] = None,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "                         scheduler,\n                         device,\n",
        "source_code_len": 69,
        "target_code": "                         scheduler,\n                         iter_scheduler,\n                         device,\n",
        "target_code_len": 110,
        "diff_format": "@@ -112,2 +116,3 @@\n                          scheduler,\n+                         iter_scheduler,\n                          device,\n",
        "source_code_with_indent": "                         scheduler,\n                         device,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                         scheduler,\n                         iter_scheduler,\n                         device,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "asappresearch/flambe",
    "commit": "8a3944f5f11fec50499fed7839f5771dc4ad790d",
    "filename": "flambe/learn/distillation.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asappresearch-flambe/flambe/learn/distillation.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "flambe/learn/distillation.py:119:25 Incompatible parameter type [6]: Expected `bool` for 15th positional only parameter to call `flambe.learn.train.Trainer.__init__` but got `Optional[float]`.",
    "message": " Expected `bool` for 15th positional only parameter to call `flambe.learn.train.Trainer.__init__` but got `Optional[float]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 119,
    "warning_line": "                         max_grad_norm,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "                 scheduler: Optional[_LRScheduler] = None,\n                 device: Optional[str] = None,\n",
        "source_code_len": 106,
        "target_code": "                 scheduler: Optional[_LRScheduler] = None,\n                 iter_scheduler: Optional[_LRScheduler] = None,\n                 device: Optional[str] = None,\n",
        "target_code_len": 170,
        "diff_format": "@@ -34,2 +34,3 @@\n                  scheduler: Optional[_LRScheduler] = None,\n+                 iter_scheduler: Optional[_LRScheduler] = None,\n                  device: Optional[str] = None,\n",
        "source_code_with_indent": "                 scheduler: Optional[_LRScheduler] = None,\n                 device: Optional[str] = None,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                 scheduler: Optional[_LRScheduler] = None,\n                 iter_scheduler: Optional[_LRScheduler] = None,\n                 device: Optional[str] = None,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "                         scheduler,\n                         device,\n",
        "source_code_len": 69,
        "target_code": "                         scheduler,\n                         iter_scheduler,\n                         device,\n",
        "target_code_len": 110,
        "diff_format": "@@ -112,2 +116,3 @@\n                          scheduler,\n+                         iter_scheduler,\n                          device,\n",
        "source_code_with_indent": "                         scheduler,\n                         device,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                         scheduler,\n                         iter_scheduler,\n                         device,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "asappresearch/flambe",
    "commit": "8a3944f5f11fec50499fed7839f5771dc4ad790d",
    "filename": "flambe/learn/distillation.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asappresearch-flambe/flambe/learn/distillation.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "flambe/learn/distillation.py:121:25 Incompatible parameter type [6]: Expected `Optional[float]` for 17th positional only parameter to call `flambe.learn.train.Trainer.__init__` but got `Optional[List[flambe.metric.metric.Metric]]`.",
    "message": " Expected `Optional[float]` for 17th positional only parameter to call `flambe.learn.train.Trainer.__init__` but got `Optional[List[flambe.metric.metric.Metric]]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 121,
    "warning_line": "                         extra_validation_metrics)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "                 scheduler: Optional[_LRScheduler] = None,\n                 device: Optional[str] = None,\n",
        "source_code_len": 106,
        "target_code": "                 scheduler: Optional[_LRScheduler] = None,\n                 iter_scheduler: Optional[_LRScheduler] = None,\n                 device: Optional[str] = None,\n",
        "target_code_len": 170,
        "diff_format": "@@ -34,2 +34,3 @@\n                  scheduler: Optional[_LRScheduler] = None,\n+                 iter_scheduler: Optional[_LRScheduler] = None,\n                  device: Optional[str] = None,\n",
        "source_code_with_indent": "                 scheduler: Optional[_LRScheduler] = None,\n                 device: Optional[str] = None,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                 scheduler: Optional[_LRScheduler] = None,\n                 iter_scheduler: Optional[_LRScheduler] = None,\n                 device: Optional[str] = None,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "                         scheduler,\n                         device,\n",
        "source_code_len": 69,
        "target_code": "                         scheduler,\n                         iter_scheduler,\n                         device,\n",
        "target_code_len": 110,
        "diff_format": "@@ -112,2 +116,3 @@\n                          scheduler,\n+                         iter_scheduler,\n                          device,\n",
        "source_code_with_indent": "                         scheduler,\n                         device,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                         scheduler,\n                         iter_scheduler,\n                         device,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "asappresearch/flambe",
    "commit": "8a3944f5f11fec50499fed7839f5771dc4ad790d",
    "filename": "flambe/learn/train.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asappresearch-flambe/flambe/learn/train.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": true,
    "full_warning_msg": "flambe/learn/train.py:224:49 Unbound name [10]: Name `val_loss` is used but not defined in the current scope.",
    "message": " Name `val_loss` is used but not defined in the current scope.",
    "rule_id": "Unbound name [10]",
    "warning_line_no": 224,
    "warning_line": "                        self.iter_scheduler.step(val_loss)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": true,
        "source_code": "                if self.iter_scheduler is not None:\n                    log(f'{tb_prefix}Training/LR', self.iter_scheduler.get_lr()[0], global_step)\n                    if isinstance(self.iter_scheduler, ReduceLROnPlateau):\n                        self.iter_scheduler.step(val_loss)\n                    else:\n                        # torch's _LRScheduler.step DOES have a default value\n                        # so passing in no args is fine; it will automatically\n                        # compute the current epoch\n                        self.iter_scheduler.step()  # type: ignore\n\n            # Zero the gradients when exiting a train step\n            self.optimizer.zero_grad()\n\n",
        "source_code_len": 685,
        "target_code": "                if self.iter_scheduler is not None:\n                    learning_rate = self.iter_scheduler.get_lr()[0]  # type: ignore\n                    log(f'{tb_prefix}Training/LR', learning_rate, global_step)\n                    self.iter_scheduler.step()  # type: ignore\n\n            # Zero the gradients when exiting a train step\n                self.optimizer.zero_grad()\n\n",
        "target_code_len": 382,
        "diff_format": "@@ -221,13 +222,8 @@\n                 if self.iter_scheduler is not None:\n-                    log(f'{tb_prefix}Training/LR', self.iter_scheduler.get_lr()[0], global_step)\n-                    if isinstance(self.iter_scheduler, ReduceLROnPlateau):\n-                        self.iter_scheduler.step(val_loss)\n-                    else:\n-                        # torch's _LRScheduler.step DOES have a default value\n-                        # so passing in no args is fine; it will automatically\n-                        # compute the current epoch\n-                        self.iter_scheduler.step()  # type: ignore\n+                    learning_rate = self.iter_scheduler.get_lr()[0]  # type: ignore\n+                    log(f'{tb_prefix}Training/LR', learning_rate, global_step)\n+                    self.iter_scheduler.step()  # type: ignore\n \n             # Zero the gradients when exiting a train step\n-            self.optimizer.zero_grad()\n+                self.optimizer.zero_grad()\n \n",
        "source_code_with_indent": "                if self.iter_scheduler is not None:\n                    <IND>log(f'{tb_prefix}Training/LR', self.iter_scheduler.get_lr()[0], global_step)\n                    if isinstance(self.iter_scheduler, ReduceLROnPlateau):\n                        <IND>self.iter_scheduler.step(val_loss)\n                    <DED>else:\n                        # torch's _LRScheduler.step DOES have a default value\n                        # so passing in no args is fine; it will automatically\n                        # compute the current epoch\n                        <IND>self.iter_scheduler.step()  # type: ignore\n\n            # Zero the gradients when exiting a train step\n            <DED><DED><DED>self.optimizer.zero_grad()\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                if self.iter_scheduler is not None:\n                    <IND>learning_rate = self.iter_scheduler.get_lr()[0]  # type: ignore\n                    log(f'{tb_prefix}Training/LR', learning_rate, global_step)\n                    self.iter_scheduler.step()  # type: ignore\n\n            # Zero the gradients when exiting a train step\n                <DED>self.optimizer.zero_grad()\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "asappresearch/flambe",
    "commit": "8a3944f5f11fec50499fed7839f5771dc4ad790d",
    "filename": "flambe/nn/mlp.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asappresearch-flambe/flambe/nn/mlp.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "flambe/nn/mlp.py:62:15 Unsupported operand [58]: `>` is not supported for operand types `Optional[float]` and `int`.",
    "message": " `>` is not supported for operand types `Optional[float]` and `int`.",
    "rule_id": "Unsupported operand [58]",
    "warning_line_no": 62,
    "warning_line": "            if dropout > 0:",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "                 n_layers: int = 1,\n                 dropout: Optional[float] = 0.,\n                 output_activation: Optional[nn.Module] = None,\n",
        "source_code_len": 148,
        "target_code": "                 n_layers: int = 1,\n                 dropout: float = 0.,\n                 output_activation: Optional[nn.Module] = None,\n",
        "target_code_len": 138,
        "diff_format": "@@ -24,3 +24,3 @@\n                  n_layers: int = 1,\n-                 dropout: Optional[float] = 0.,\n+                 dropout: float = 0.,\n                  output_activation: Optional[nn.Module] = None,\n",
        "source_code_with_indent": "                 n_layers: int = 1,\n                 dropout: Optional[float] = 0.,\n                 output_activation: Optional[nn.Module] = None,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                 n_layers: int = 1,\n                 dropout: float = 0.,\n                 output_activation: Optional[nn.Module] = None,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "asappresearch/flambe",
    "commit": "8a3944f5f11fec50499fed7839f5771dc4ad790d",
    "filename": "flambe/nn/mlp.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asappresearch-flambe/flambe/nn/mlp.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "flambe/nn/mlp.py:69:19 Unsupported operand [58]: `>` is not supported for operand types `Optional[float]` and `int`.",
    "message": " `>` is not supported for operand types `Optional[float]` and `int`.",
    "rule_id": "Unsupported operand [58]",
    "warning_line_no": 69,
    "warning_line": "                if dropout > 0:",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "                 n_layers: int = 1,\n                 dropout: Optional[float] = 0.,\n                 output_activation: Optional[nn.Module] = None,\n",
        "source_code_len": 148,
        "target_code": "                 n_layers: int = 1,\n                 dropout: float = 0.,\n                 output_activation: Optional[nn.Module] = None,\n",
        "target_code_len": 138,
        "diff_format": "@@ -24,3 +24,3 @@\n                  n_layers: int = 1,\n-                 dropout: Optional[float] = 0.,\n+                 dropout: float = 0.,\n                  output_activation: Optional[nn.Module] = None,\n",
        "source_code_with_indent": "                 n_layers: int = 1,\n                 dropout: Optional[float] = 0.,\n                 output_activation: Optional[nn.Module] = None,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                 n_layers: int = 1,\n                 dropout: float = 0.,\n                 output_activation: Optional[nn.Module] = None,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "asappresearch/flambe",
    "commit": "8a3944f5f11fec50499fed7839f5771dc4ad790d",
    "filename": "flambe/nn/mlp.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asappresearch-flambe/flambe/nn/mlp.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "flambe/nn/mlp.py:75:11 Unsupported operand [58]: `>` is not supported for operand types `Optional[float]` and `int`.",
    "message": " `>` is not supported for operand types `Optional[float]` and `int`.",
    "rule_id": "Unsupported operand [58]",
    "warning_line_no": 75,
    "warning_line": "        if dropout > 0:",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "                 n_layers: int = 1,\n                 dropout: Optional[float] = 0.,\n                 output_activation: Optional[nn.Module] = None,\n",
        "source_code_len": 148,
        "target_code": "                 n_layers: int = 1,\n                 dropout: float = 0.,\n                 output_activation: Optional[nn.Module] = None,\n",
        "target_code_len": 138,
        "diff_format": "@@ -24,3 +24,3 @@\n                  n_layers: int = 1,\n-                 dropout: Optional[float] = 0.,\n+                 dropout: float = 0.,\n                  output_activation: Optional[nn.Module] = None,\n",
        "source_code_with_indent": "                 n_layers: int = 1,\n                 dropout: Optional[float] = 0.,\n                 output_activation: Optional[nn.Module] = None,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                 n_layers: int = 1,\n                 dropout: float = 0.,\n                 output_activation: Optional[nn.Module] = None,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "asappresearch/flambe",
    "commit": "8a3944f5f11fec50499fed7839f5771dc4ad790d",
    "filename": "flambe/nn/softmax.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asappresearch-flambe/flambe/nn/softmax.py",
    "file_hunks_size": 4,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "flambe/nn/softmax.py:64:27 Call error [29]: `MLPEncoder` is not a function.",
    "message": " `MLPEncoder` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 64,
    "warning_line": "        out = self.softmax(self.mlp(data))",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        self.mlp = MLPEncoder(input_size=input_size, output_size=output_size,\n                              n_layers=mlp_layers, dropout=mlp_dropout,\n                              hidden_activation=mlp_hidden_activation)\n        self.softmax = nn.LogSoftmax(dim=-1) if take_log else nn.Softmax()\n\n    def forward(self, data: Tensor) -> Tensor:\n        \"\"\"Performs a forward pass through the network.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            The input data, as a float tensor\n\n        Returns\n        -------\n        torch.Tensor\n            The encoded output, as a float tensor\n\n        \"\"\"\n        out = self.softmax(self.mlp(data))\n        return out\n",
        "source_code_len": 693,
        "target_code": "\n        softmax = nn.LogSoftmax(dim=-1) if take_log else nn.Softmax()\n        self.mlp = MLPEncoder(input_size=input_size, output_size=output_size,\n                              n_layers=mlp_layers, dropout=mlp_dropout,\n                              hidden_activation=mlp_hidden_activation,\n                              output_activation=softmax)\n",
        "target_code_len": 349,
        "diff_format": "@@ -44,22 +46,6 @@\n \n+        softmax = nn.LogSoftmax(dim=-1) if take_log else nn.Softmax()\n         self.mlp = MLPEncoder(input_size=input_size, output_size=output_size,\n                               n_layers=mlp_layers, dropout=mlp_dropout,\n-                              hidden_activation=mlp_hidden_activation)\n-        self.softmax = nn.LogSoftmax(dim=-1) if take_log else nn.Softmax()\n-\n-    def forward(self, data: Tensor) -> Tensor:\n-        \"\"\"Performs a forward pass through the network.\n-\n-        Parameters\n-        ----------\n-        data : torch.Tensor\n-            The input data, as a float tensor\n-\n-        Returns\n-        -------\n-        torch.Tensor\n-            The encoded output, as a float tensor\n-\n-        \"\"\"\n-        out = self.softmax(self.mlp(data))\n-        return out\n+                              hidden_activation=mlp_hidden_activation,\n+                              output_activation=softmax)\n",
        "source_code_with_indent": "\n        self.mlp = MLPEncoder(input_size=input_size, output_size=output_size,\n                              n_layers=mlp_layers, dropout=mlp_dropout,\n                              hidden_activation=mlp_hidden_activation)\n        self.softmax = nn.LogSoftmax(dim=-1) if take_log else nn.Softmax()\n\n    <DED>def forward(self, data: Tensor) -> Tensor:\n        <IND>\"\"\"Performs a forward pass through the network.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            The input data, as a float tensor\n\n        Returns\n        -------\n        torch.Tensor\n            The encoded output, as a float tensor\n\n        \"\"\"\n        out = self.softmax(self.mlp(data))\n        return out\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        softmax = nn.LogSoftmax(dim=-1) if take_log else nn.Softmax()\n        self.mlp = MLPEncoder(input_size=input_size, output_size=output_size,\n                              n_layers=mlp_layers, dropout=mlp_dropout,\n                              hidden_activation=mlp_hidden_activation,\n                              output_activation=softmax)\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  }
]