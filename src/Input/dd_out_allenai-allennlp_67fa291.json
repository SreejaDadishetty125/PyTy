[
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/commands/train.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/commands/train.py",
    "file_hunks_size": 29,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/commands/train.py:516:8 Incompatible variable type [9]: evaluation_data_loader is declared to have type `allennlp.data.dataloader.DataLoader` but is used as type `None`.",
    "message": " evaluation_data_loader is declared to have type `allennlp.data.dataloader.DataLoader` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 516,
    "warning_line": "        evaluation_data_loader: DataLoader = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/commands/train.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/commands/train.py",
    "file_hunks_size": 29,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/commands/train.py:564:8 Incompatible variable type [9]: validation_data_path is declared to have type `str` but is used as type `None`.",
    "message": " validation_data_path is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 564,
    "warning_line": "        validation_data_path: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        validation_dataset_reader: DatasetReader = None,\n        validation_data_path: str = None,\n        validation_data_loader: Lazy[DataLoader] = None,\n        test_data_path: str = None,\n        evaluate_on_test: bool = False,\n",
        "source_code_len": 232,
        "target_code": "        validation_dataset_reader: DatasetReader = None,\n        validation_data_path: Any = None,\n        validation_data_loader: Lazy[DataLoader] = None,\n        test_data_path: Any = None,\n        evaluate_on_test: bool = False,\n",
        "target_code_len": 232,
        "diff_format": "@@ -563,5 +563,5 @@\n         validation_dataset_reader: DatasetReader = None,\n-        validation_data_path: str = None,\n+        validation_data_path: Any = None,\n         validation_data_loader: Lazy[DataLoader] = None,\n-        test_data_path: str = None,\n+        test_data_path: Any = None,\n         evaluate_on_test: bool = False,\n",
        "source_code_with_indent": "        validation_dataset_reader: DatasetReader = None,\n        validation_data_path: str = None,\n        validation_data_loader: Lazy[DataLoader] = None,\n        test_data_path: str = None,\n        evaluate_on_test: bool = False,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        validation_dataset_reader: DatasetReader = None,\n        validation_data_path: Any = None,\n        validation_data_loader: Lazy[DataLoader] = None,\n        test_data_path: Any = None,\n        evaluate_on_test: bool = False,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/commands/train.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/commands/train.py",
    "file_hunks_size": 29,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/commands/train.py:565:8 Incompatible variable type [9]: validation_data_loader is declared to have type `allennlp.common.lazy.Lazy[allennlp.data.dataloader.DataLoader]` but is used as type `None`.",
    "message": " validation_data_loader is declared to have type `allennlp.common.lazy.Lazy[allennlp.data.dataloader.DataLoader]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 565,
    "warning_line": "        validation_data_loader: Lazy[DataLoader] = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/commands/train.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/commands/train.py",
    "file_hunks_size": 29,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/commands/train.py:566:8 Incompatible variable type [9]: test_data_path is declared to have type `str` but is used as type `None`.",
    "message": " test_data_path is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 566,
    "warning_line": "        test_data_path: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        validation_dataset_reader: DatasetReader = None,\n        validation_data_path: str = None,\n        validation_data_loader: Lazy[DataLoader] = None,\n        test_data_path: str = None,\n        evaluate_on_test: bool = False,\n",
        "source_code_len": 232,
        "target_code": "        validation_dataset_reader: DatasetReader = None,\n        validation_data_path: Any = None,\n        validation_data_loader: Lazy[DataLoader] = None,\n        test_data_path: Any = None,\n        evaluate_on_test: bool = False,\n",
        "target_code_len": 232,
        "diff_format": "@@ -563,5 +563,5 @@\n         validation_dataset_reader: DatasetReader = None,\n-        validation_data_path: str = None,\n+        validation_data_path: Any = None,\n         validation_data_loader: Lazy[DataLoader] = None,\n-        test_data_path: str = None,\n+        test_data_path: Any = None,\n         evaluate_on_test: bool = False,\n",
        "source_code_with_indent": "        validation_dataset_reader: DatasetReader = None,\n        validation_data_path: str = None,\n        validation_data_loader: Lazy[DataLoader] = None,\n        test_data_path: str = None,\n        evaluate_on_test: bool = False,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        validation_dataset_reader: DatasetReader = None,\n        validation_data_path: Any = None,\n        validation_data_loader: Lazy[DataLoader] = None,\n        test_data_path: Any = None,\n        evaluate_on_test: bool = False,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/commands/train.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/commands/train.py",
    "file_hunks_size": 29,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/commands/train.py:725:12 Incompatible parameter type [6]: Expected `allennlp.data.dataloader.DataLoader` for 4th parameter `evaluation_data_loader` to call `TrainModel.__init__` but got `Optional[allennlp.data.dataloader.DataLoader]`.",
    "message": " Expected `allennlp.data.dataloader.DataLoader` for 4th parameter `evaluation_data_loader` to call `TrainModel.__init__` but got `Optional[allennlp.data.dataloader.DataLoader]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 725,
    "warning_line": "            evaluation_data_loader=test_data_loader,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/data/dataloader.py",
    "min_patch_found": false,
    "full_warning_msg": "allennlp/data/dataloader.py:74:8 Incompatible variable type [9]: sampler is declared to have type `allennlp.data.samplers.samplers.Sampler` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/dataloader.py'",
    "dd_fail": true
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/data/dataloader.py",
    "min_patch_found": false,
    "full_warning_msg": "allennlp/data/dataloader.py:75:8 Incompatible variable type [9]: batch_sampler is declared to have type `allennlp.data.samplers.samplers.BatchSampler` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/dataloader.py'",
    "dd_fail": true
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/data/dataloader.py",
    "min_patch_found": false,
    "full_warning_msg": "allennlp/data/dataloader.py:85:8 Incompatible variable type [9]: multiprocessing_context is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/dataloader.py'",
    "dd_fail": true
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/data/dataloader.py",
    "min_patch_found": false,
    "full_warning_msg": "allennlp/data/dataloader.py:86:8 Incompatible variable type [9]: batches_per_epoch is declared to have type `int` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/dataloader.py'",
    "dd_fail": true
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/data/dataloader.py",
    "min_patch_found": false,
    "full_warning_msg": "allennlp/data/dataloader.py:130:8 Incompatible variable type [9]: sampler is declared to have type `Lazy[allennlp.data.samplers.samplers.Sampler]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/dataloader.py'",
    "dd_fail": true
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/data/dataloader.py",
    "min_patch_found": false,
    "full_warning_msg": "allennlp/data/dataloader.py:131:8 Incompatible variable type [9]: batch_sampler is declared to have type `Lazy[allennlp.data.samplers.samplers.BatchSampler]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/dataloader.py'",
    "dd_fail": true
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/data/dataloader.py",
    "min_patch_found": false,
    "full_warning_msg": "allennlp/data/dataloader.py:137:8 Incompatible variable type [9]: multiprocessing_context is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/dataloader.py'",
    "dd_fail": true
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/data/dataloader.py",
    "min_patch_found": false,
    "full_warning_msg": "allennlp/data/dataloader.py:138:8 Incompatible variable type [9]: batches_per_epoch is declared to have type `int` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/dataloader.py'",
    "dd_fail": true
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/data/dataloader.py",
    "min_patch_found": false,
    "full_warning_msg": "allennlp/data/dataloader.py:149:12 Incompatible parameter type [6]: Expected `allennlp.data.samplers.samplers.Sampler` for 4th parameter `sampler` to call `PyTorchDataLoader.__init__` but got `typing.Optional[allennlp.data.samplers.samplers.Sampler]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/dataloader.py'",
    "dd_fail": true
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/data/dataloader.py",
    "min_patch_found": false,
    "full_warning_msg": "allennlp/data/dataloader.py:150:12 Incompatible parameter type [6]: Expected `allennlp.data.samplers.samplers.BatchSampler` for 5th parameter `batch_sampler` to call `PyTorchDataLoader.__init__` but got `typing.Optional[allennlp.data.samplers.samplers.BatchSampler]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/dataloader.py'",
    "dd_fail": true
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/data/dataset_readers/dataset_reader.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/dataset_readers/dataset_reader.py",
    "file_hunks_size": 10,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/data/dataset_readers/dataset_reader.py:29:50 Incompatible variable type [9]: vocab is declared to have type `Vocabulary` but is used as type `None`.",
    "message": " vocab is declared to have type `Vocabulary` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 29,
    "warning_line": "    def __init__(self, instances: List[Instance], vocab: Vocabulary = None):"
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/data/dataset_readers/dataset_reader.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/dataset_readers/dataset_reader.py",
    "file_hunks_size": 10,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/data/dataset_readers/dataset_reader.py:72:8 Incompatible variable type [9]: vocab is declared to have type `Vocabulary` but is used as type `None`.",
    "message": " vocab is declared to have type `Vocabulary` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 72,
    "warning_line": "        vocab: Vocabulary = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/data/samplers/samplers.py",
    "min_patch_found": false,
    "full_warning_msg": "allennlp/data/samplers/samplers.py:77:68 Incompatible variable type [9]: num_samples is declared to have type `int` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/samplers/samplers.py'",
    "dd_fail": true
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/training/metric_tracker.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/metric_tracker.py",
    "file_hunks_size": 11,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/metric_tracker.py:33:46 Incompatible variable type [9]: metric_name is declared to have type `str` but is used as type `None`.",
    "message": " metric_name is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 33,
    "warning_line": "        self, patience: Optional[int] = None, metric_name: str = None, should_decrease: bool = None",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "from typing import Optional, Iterable, Dict, Any\n\n",
        "source_code_len": 50,
        "target_code": "from typing import Optional, Dict, Any, List, Union\n\n",
        "target_code_len": 53,
        "diff_format": "@@ -1,2 +1,2 @@\n-from typing import Optional, Iterable, Dict, Any\n+from typing import Optional, Dict, Any, List, Union\n \n",
        "source_code_with_indent": "from typing import Optional, Iterable, Dict, Any\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "from typing import Optional, Dict, Any, List, Union\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    def __init__(\n        self, patience: Optional[int] = None, metric_name: str = None, should_decrease: bool = None\n    ) -> None:\n        self._best_so_far: Optional[float] = None\n        self._patience = patience\n        self._epochs_with_no_improvement = 0\n        self._is_best_so_far = True\n        self.best_epoch_metrics: Dict[str, float] = {}\n        self._epoch_number = 0\n        self.best_epoch: Optional[int] = None\n\n        # If the metric name starts with \"+\", we want it to increase.\n        # If the metric name starts with \"-\", we want it to decrease.\n        # We also allow you to not specify a metric name and just set `should_decrease` directly.\n        if should_decrease is not None and metric_name is not None:\n            raise ConfigurationError(\n                \"must specify either `should_decrease` or `metric_name` (but not both)\"\n            )\n        elif metric_name is not None:\n            if metric_name[0] == \"-\":\n                self._should_decrease = True\n            elif metric_name[0] == \"+\":\n                self._should_decrease = False\n            else:\n                raise ConfigurationError(\"metric_name must start with + or -\")\n        elif should_decrease is not None:\n            self._should_decrease = should_decrease\n        else:\n            raise ConfigurationError(\n                \"must specify either `should_decrease` or `metric_name` (but not both)\"\n            )\n\n",
        "source_code_len": 1430,
        "target_code": "    def __init__(\n        self,\n        metric_name: Union[str, List[str]],\n        patience: Optional[int] = None,\n    ) -> None:\n        self._patience = patience\n        self._best_so_far: Optional[float] = None\n        self._epochs_with_no_improvement = 0\n        self._is_best_so_far = True\n        self._epoch_number = 0\n        self.best_epoch: Optional[int] = None\n        self.best_epoch_metrics: Dict[str, float] = {}\n\n        if isinstance(metric_name, str):\n            metric_name = [metric_name]\n        self.tracked_metrics = []\n        for name in metric_name:\n            if name.startswith(\"+\"):\n                self.tracked_metrics.append((1.0, name[1:]))\n            elif name.startswith(\"-\"):\n                self.tracked_metrics.append((-1.0, name[1:]))\n            else:\n                raise ConfigurationError(\"metric_name must start with + or -\")\n\n",
        "target_code_len": 874,
        "diff_format": "@@ -32,32 +29,24 @@\n     def __init__(\n-        self, patience: Optional[int] = None, metric_name: str = None, should_decrease: bool = None\n+        self,\n+        metric_name: Union[str, List[str]],\n+        patience: Optional[int] = None,\n     ) -> None:\n+        self._patience = patience\n         self._best_so_far: Optional[float] = None\n-        self._patience = patience\n         self._epochs_with_no_improvement = 0\n         self._is_best_so_far = True\n-        self.best_epoch_metrics: Dict[str, float] = {}\n         self._epoch_number = 0\n         self.best_epoch: Optional[int] = None\n+        self.best_epoch_metrics: Dict[str, float] = {}\n \n-        # If the metric name starts with \"+\", we want it to increase.\n-        # If the metric name starts with \"-\", we want it to decrease.\n-        # We also allow you to not specify a metric name and just set `should_decrease` directly.\n-        if should_decrease is not None and metric_name is not None:\n-            raise ConfigurationError(\n-                \"must specify either `should_decrease` or `metric_name` (but not both)\"\n-            )\n-        elif metric_name is not None:\n-            if metric_name[0] == \"-\":\n-                self._should_decrease = True\n-            elif metric_name[0] == \"+\":\n-                self._should_decrease = False\n+        if isinstance(metric_name, str):\n+            metric_name = [metric_name]\n+        self.tracked_metrics = []\n+        for name in metric_name:\n+            if name.startswith(\"+\"):\n+                self.tracked_metrics.append((1.0, name[1:]))\n+            elif name.startswith(\"-\"):\n+                self.tracked_metrics.append((-1.0, name[1:]))\n             else:\n                 raise ConfigurationError(\"metric_name must start with + or -\")\n-        elif should_decrease is not None:\n-            self._should_decrease = should_decrease\n-        else:\n-            raise ConfigurationError(\n-                \"must specify either `should_decrease` or `metric_name` (but not both)\"\n-            )\n \n",
        "source_code_with_indent": "    def __init__(\n        self, patience: Optional[int] = None, metric_name: str = None, should_decrease: bool = None\n    ) -> None:\n        <IND>self._best_so_far: Optional[float] = None\n        self._patience = patience\n        self._epochs_with_no_improvement = 0\n        self._is_best_so_far = True\n        self.best_epoch_metrics: Dict[str, float] = {}\n        self._epoch_number = 0\n        self.best_epoch: Optional[int] = None\n\n        # If the metric name starts with \"+\", we want it to increase.\n        # If the metric name starts with \"-\", we want it to decrease.\n        # We also allow you to not specify a metric name and just set `should_decrease` directly.\n        if should_decrease is not None and metric_name is not None:\n            <IND>raise ConfigurationError(\n                \"must specify either `should_decrease` or `metric_name` (but not both)\"\n            )\n        <DED>elif metric_name is not None:\n            <IND>if metric_name[0] == \"-\":\n                <IND>self._should_decrease = True\n            <DED>elif metric_name[0] == \"+\":\n                <IND>self._should_decrease = False\n            <DED>else:\n                <IND>raise ConfigurationError(\"metric_name must start with + or -\")\n        <DED><DED>elif should_decrease is not None:\n            <IND>self._should_decrease = should_decrease\n        <DED>else:\n            <IND>raise ConfigurationError(\n                \"must specify either `should_decrease` or `metric_name` (but not both)\"\n            )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    def __init__(\n        self,\n        metric_name: Union[str, List[str]],\n        patience: Optional[int] = None,\n    ) -> None:\n        <IND>self._patience = patience\n        self._best_so_far: Optional[float] = None\n        self._epochs_with_no_improvement = 0\n        self._is_best_so_far = True\n        self._epoch_number = 0\n        self.best_epoch: Optional[int] = None\n        self.best_epoch_metrics: Dict[str, float] = {}\n\n        if isinstance(metric_name, str):\n            <IND>metric_name = [metric_name]\n        <DED>self.tracked_metrics = []\n        for name in metric_name:\n            <IND>if name.startswith(\"+\"):\n                <IND>self.tracked_metrics.append((1.0, name[1:]))\n            <DED>elif name.startswith(\"-\"):\n                <IND>self.tracked_metrics.append((-1.0, name[1:]))\n            <DED>else:\n                <IND>raise ConfigurationError(\"metric_name must start with + or -\")\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "            \"best_so_far\": self._best_so_far,\n            \"patience\": self._patience,\n            \"epochs_with_no_improvement\": self._epochs_with_no_improvement,\n            \"is_best_so_far\": self._is_best_so_far,\n            \"should_decrease\": self._should_decrease,\n            \"best_epoch_metrics\": self.best_epoch_metrics,\n            \"epoch_number\": self._epoch_number,\n            \"best_epoch\": self.best_epoch,\n        }\n",
        "source_code_len": 428,
        "target_code": "            \"best_so_far\": self._best_so_far,\n            \"epochs_with_no_improvement\": self._epochs_with_no_improvement,\n            \"is_best_so_far\": self._is_best_so_far,\n            \"epoch_number\": self._epoch_number,\n            \"best_epoch\": self.best_epoch,\n            \"best_epoch_metrics\": self.best_epoch_metrics,\n        }\n",
        "target_code_len": 334,
        "diff_format": "@@ -79,9 +69,7 @@\n             \"best_so_far\": self._best_so_far,\n-            \"patience\": self._patience,\n             \"epochs_with_no_improvement\": self._epochs_with_no_improvement,\n             \"is_best_so_far\": self._is_best_so_far,\n-            \"should_decrease\": self._should_decrease,\n-            \"best_epoch_metrics\": self.best_epoch_metrics,\n             \"epoch_number\": self._epoch_number,\n             \"best_epoch\": self.best_epoch,\n+            \"best_epoch_metrics\": self.best_epoch_metrics,\n         }\n",
        "source_code_with_indent": "            \"best_so_far\": self._best_so_far,\n            \"patience\": self._patience,\n            \"epochs_with_no_improvement\": self._epochs_with_no_improvement,\n            \"is_best_so_far\": self._is_best_so_far,\n            \"should_decrease\": self._should_decrease,\n            \"best_epoch_metrics\": self.best_epoch_metrics,\n            \"epoch_number\": self._epoch_number,\n            \"best_epoch\": self.best_epoch,\n        }\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "            \"best_so_far\": self._best_so_far,\n            \"epochs_with_no_improvement\": self._epochs_with_no_improvement,\n            \"is_best_so_far\": self._is_best_so_far,\n            \"epoch_number\": self._epoch_number,\n            \"best_epoch\": self.best_epoch,\n            \"best_epoch_metrics\": self.best_epoch_metrics,\n        }\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        self._best_so_far = state_dict[\"best_so_far\"]\n        self._patience = state_dict[\"patience\"]\n        self._epochs_with_no_improvement = state_dict[\"epochs_with_no_improvement\"]\n        self._is_best_so_far = state_dict[\"is_best_so_far\"]\n        self._should_decrease = state_dict[\"should_decrease\"]\n        self.best_epoch_metrics = state_dict[\"best_epoch_metrics\"]\n        self._epoch_number = state_dict[\"epoch_number\"]\n",
        "source_code_len": 431,
        "target_code": "        self._best_so_far = state_dict[\"best_so_far\"]\n        self._epochs_with_no_improvement = state_dict[\"epochs_with_no_improvement\"]\n        self._is_best_so_far = state_dict[\"is_best_so_far\"]\n        self._epoch_number = state_dict[\"epoch_number\"]\n",
        "target_code_len": 254,
        "diff_format": "@@ -93,7 +81,4 @@\n         self._best_so_far = state_dict[\"best_so_far\"]\n-        self._patience = state_dict[\"patience\"]\n         self._epochs_with_no_improvement = state_dict[\"epochs_with_no_improvement\"]\n         self._is_best_so_far = state_dict[\"is_best_so_far\"]\n-        self._should_decrease = state_dict[\"should_decrease\"]\n-        self.best_epoch_metrics = state_dict[\"best_epoch_metrics\"]\n         self._epoch_number = state_dict[\"epoch_number\"]\n",
        "source_code_with_indent": "        self._best_so_far = state_dict[\"best_so_far\"]\n        self._patience = state_dict[\"patience\"]\n        self._epochs_with_no_improvement = state_dict[\"epochs_with_no_improvement\"]\n        self._is_best_so_far = state_dict[\"is_best_so_far\"]\n        self._should_decrease = state_dict[\"should_decrease\"]\n        self.best_epoch_metrics = state_dict[\"best_epoch_metrics\"]\n        self._epoch_number = state_dict[\"epoch_number\"]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        self._best_so_far = state_dict[\"best_so_far\"]\n        self._epochs_with_no_improvement = state_dict[\"epochs_with_no_improvement\"]\n        self._is_best_so_far = state_dict[\"is_best_so_far\"]\n        self._epoch_number = state_dict[\"epoch_number\"]\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    def add_metric(self, metric: float) -> None:\n        \"\"\"\n",
        "source_code_len": 62,
        "target_code": "\n        # Even though we don't promise backwards compatibility for the --recover flag,\n        # it's particularly easy and harmless to provide it here, so we do it.\n        self.best_epoch_metrics = state_dict.get(\"best_epoch_metrics\", {})\n\n    def add_metrics(self, metrics: Dict[str, float]) -> None:\n        \"\"\"\n",
        "target_code_len": 317,
        "diff_format": "@@ -101,3 +86,7 @@\n \n-    def add_metric(self, metric: float) -> None:\n+        # Even though we don't promise backwards compatibility for the --recover flag,\n+        # it's particularly easy and harmless to provide it here, so we do it.\n+        self.best_epoch_metrics = state_dict.get(\"best_epoch_metrics\", {})\n+\n+    def add_metrics(self, metrics: Dict[str, float]) -> None:\n         \"\"\"\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n    <DED>def add_metric(self, metric: float) -> None:\n        <IND>",
        "target_code_with_indent": "\n        # Even though we don't promise backwards compatibility for the --recover flag,\n        # it's particularly easy and harmless to provide it here, so we do it.\n        self.best_epoch_metrics = state_dict.get(\"best_epoch_metrics\", {})\n\n    <DED>def add_metrics(self, metrics: Dict[str, float]) -> None:\n        <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        \"\"\"\n        new_best = (\n            (self._best_so_far is None)\n            or (self._should_decrease and metric < self._best_so_far)\n            or (not self._should_decrease and metric > self._best_so_far)\n        )\n\n        if new_best:\n            self.best_epoch = self._epoch_number\n            self._is_best_so_far = True\n            self._best_so_far = metric\n            self._epochs_with_no_improvement = 0\n        else:\n            self._is_best_so_far = False\n            self._epochs_with_no_improvement += 1\n        self._epoch_number += 1\n\n    def add_metrics(self, metrics: Iterable[float]) -> None:\n        \"\"\"\n        Helper to add multiple metrics at once.\n        \"\"\"\n        for metric in metrics:\n            self.add_metric(metric)\n\n",
        "source_code_len": 765,
        "target_code": "        \"\"\"\n        try:\n            combined_score = sum(\n                factor * metrics[metric_name] for factor, metric_name in self.tracked_metrics\n            )\n        except KeyError as e:\n            raise ConfigurationError(\n                f\"You configured the trainer to use the {e.args[0]}\"\n                \"metric for early stopping, but the model did not produce that metric.\"\n            )\n\n        new_best = (self._best_so_far is None) or (combined_score > self._best_so_far)\n\n        if new_best:\n            self._best_so_far = combined_score\n            self._epochs_with_no_improvement = 0\n            self._is_best_so_far = True\n            self.best_epoch = self._epoch_number\n        else:\n            self._epochs_with_no_improvement += 1\n            self._is_best_so_far = False\n        self._epoch_number += 1\n\n",
        "target_code_len": 839,
        "diff_format": "@@ -105,24 +94,23 @@\n         \"\"\"\n-        new_best = (\n-            (self._best_so_far is None)\n-            or (self._should_decrease and metric < self._best_so_far)\n-            or (not self._should_decrease and metric > self._best_so_far)\n-        )\n+        try:\n+            combined_score = sum(\n+                factor * metrics[metric_name] for factor, metric_name in self.tracked_metrics\n+            )\n+        except KeyError as e:\n+            raise ConfigurationError(\n+                f\"You configured the trainer to use the {e.args[0]}\"\n+                \"metric for early stopping, but the model did not produce that metric.\"\n+            )\n+\n+        new_best = (self._best_so_far is None) or (combined_score > self._best_so_far)\n \n         if new_best:\n+            self._best_so_far = combined_score\n+            self._epochs_with_no_improvement = 0\n+            self._is_best_so_far = True\n             self.best_epoch = self._epoch_number\n-            self._is_best_so_far = True\n-            self._best_so_far = metric\n-            self._epochs_with_no_improvement = 0\n         else:\n+            self._epochs_with_no_improvement += 1\n             self._is_best_so_far = False\n-            self._epochs_with_no_improvement += 1\n         self._epoch_number += 1\n-\n-    def add_metrics(self, metrics: Iterable[float]) -> None:\n-        \"\"\"\n-        Helper to add multiple metrics at once.\n-        \"\"\"\n-        for metric in metrics:\n-            self.add_metric(metric)\n \n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n        new_best = (\n            (self._best_so_far is None)\n            or (self._should_decrease and metric < self._best_so_far)\n            or (not self._should_decrease and metric > self._best_so_far)\n        )\n\n        if new_best:\n            <IND>self.best_epoch = self._epoch_number\n            self._is_best_so_far = True\n            self._best_so_far = metric\n            self._epochs_with_no_improvement = 0\n        <DED>else:\n            <IND>self._is_best_so_far = False\n            self._epochs_with_no_improvement += 1\n        <DED>self._epoch_number += 1\n\n    <DED>def add_metrics(self, metrics: Iterable[float]) -> None:\n        <IND>\"\"\"\n        Helper to add multiple metrics at once.\n        \"\"\"\n        for metric in metrics:\n            <IND>self.add_metric(metric)\n\n",
        "target_code_with_indent": "\n        try:\n            <IND>combined_score = sum(\n                factor * metrics[metric_name] for factor, metric_name in self.tracked_metrics\n            )\n        <DED>except KeyError as e:\n            <IND>raise ConfigurationError(\n                f\"You configured the trainer to use the {e.args[0]}\"\n                \"metric for early stopping, but the model did not produce that metric.\"\n            )\n\n        <DED>new_best = (self._best_so_far is None) or (combined_score > self._best_so_far)\n\n        if new_best:\n            <IND>self._best_so_far = combined_score\n            self._epochs_with_no_improvement = 0\n            self._is_best_so_far = True\n            self.best_epoch = self._epoch_number\n        <DED>else:\n            <IND>self._epochs_with_no_improvement += 1\n            self._is_best_so_far = False\n        <DED>self._epoch_number += 1\n\n"
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/training/metric_tracker.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/metric_tracker.py",
    "file_hunks_size": 11,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/metric_tracker.py:33:71 Incompatible variable type [9]: should_decrease is declared to have type `bool` but is used as type `None`.",
    "message": " should_decrease is declared to have type `bool` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 33,
    "warning_line": "        self, patience: Optional[int] = None, metric_name: str = None, should_decrease: bool = None",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "from typing import Optional, Iterable, Dict, Any\n\n",
        "source_code_len": 50,
        "target_code": "from typing import Optional, Dict, Any, List, Union\n\n",
        "target_code_len": 53,
        "diff_format": "@@ -1,2 +1,2 @@\n-from typing import Optional, Iterable, Dict, Any\n+from typing import Optional, Dict, Any, List, Union\n \n",
        "source_code_with_indent": "from typing import Optional, Iterable, Dict, Any\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "from typing import Optional, Dict, Any, List, Union\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    def __init__(\n        self, patience: Optional[int] = None, metric_name: str = None, should_decrease: bool = None\n    ) -> None:\n        self._best_so_far: Optional[float] = None\n        self._patience = patience\n        self._epochs_with_no_improvement = 0\n        self._is_best_so_far = True\n        self.best_epoch_metrics: Dict[str, float] = {}\n        self._epoch_number = 0\n        self.best_epoch: Optional[int] = None\n\n        # If the metric name starts with \"+\", we want it to increase.\n        # If the metric name starts with \"-\", we want it to decrease.\n        # We also allow you to not specify a metric name and just set `should_decrease` directly.\n        if should_decrease is not None and metric_name is not None:\n            raise ConfigurationError(\n                \"must specify either `should_decrease` or `metric_name` (but not both)\"\n            )\n        elif metric_name is not None:\n            if metric_name[0] == \"-\":\n                self._should_decrease = True\n            elif metric_name[0] == \"+\":\n                self._should_decrease = False\n            else:\n                raise ConfigurationError(\"metric_name must start with + or -\")\n        elif should_decrease is not None:\n            self._should_decrease = should_decrease\n        else:\n            raise ConfigurationError(\n                \"must specify either `should_decrease` or `metric_name` (but not both)\"\n            )\n\n",
        "source_code_len": 1430,
        "target_code": "    def __init__(\n        self,\n        metric_name: Union[str, List[str]],\n        patience: Optional[int] = None,\n    ) -> None:\n        self._patience = patience\n        self._best_so_far: Optional[float] = None\n        self._epochs_with_no_improvement = 0\n        self._is_best_so_far = True\n        self._epoch_number = 0\n        self.best_epoch: Optional[int] = None\n        self.best_epoch_metrics: Dict[str, float] = {}\n\n        if isinstance(metric_name, str):\n            metric_name = [metric_name]\n        self.tracked_metrics = []\n        for name in metric_name:\n            if name.startswith(\"+\"):\n                self.tracked_metrics.append((1.0, name[1:]))\n            elif name.startswith(\"-\"):\n                self.tracked_metrics.append((-1.0, name[1:]))\n            else:\n                raise ConfigurationError(\"metric_name must start with + or -\")\n\n",
        "target_code_len": 874,
        "diff_format": "@@ -32,32 +29,24 @@\n     def __init__(\n-        self, patience: Optional[int] = None, metric_name: str = None, should_decrease: bool = None\n+        self,\n+        metric_name: Union[str, List[str]],\n+        patience: Optional[int] = None,\n     ) -> None:\n+        self._patience = patience\n         self._best_so_far: Optional[float] = None\n-        self._patience = patience\n         self._epochs_with_no_improvement = 0\n         self._is_best_so_far = True\n-        self.best_epoch_metrics: Dict[str, float] = {}\n         self._epoch_number = 0\n         self.best_epoch: Optional[int] = None\n+        self.best_epoch_metrics: Dict[str, float] = {}\n \n-        # If the metric name starts with \"+\", we want it to increase.\n-        # If the metric name starts with \"-\", we want it to decrease.\n-        # We also allow you to not specify a metric name and just set `should_decrease` directly.\n-        if should_decrease is not None and metric_name is not None:\n-            raise ConfigurationError(\n-                \"must specify either `should_decrease` or `metric_name` (but not both)\"\n-            )\n-        elif metric_name is not None:\n-            if metric_name[0] == \"-\":\n-                self._should_decrease = True\n-            elif metric_name[0] == \"+\":\n-                self._should_decrease = False\n+        if isinstance(metric_name, str):\n+            metric_name = [metric_name]\n+        self.tracked_metrics = []\n+        for name in metric_name:\n+            if name.startswith(\"+\"):\n+                self.tracked_metrics.append((1.0, name[1:]))\n+            elif name.startswith(\"-\"):\n+                self.tracked_metrics.append((-1.0, name[1:]))\n             else:\n                 raise ConfigurationError(\"metric_name must start with + or -\")\n-        elif should_decrease is not None:\n-            self._should_decrease = should_decrease\n-        else:\n-            raise ConfigurationError(\n-                \"must specify either `should_decrease` or `metric_name` (but not both)\"\n-            )\n \n",
        "source_code_with_indent": "    def __init__(\n        self, patience: Optional[int] = None, metric_name: str = None, should_decrease: bool = None\n    ) -> None:\n        <IND>self._best_so_far: Optional[float] = None\n        self._patience = patience\n        self._epochs_with_no_improvement = 0\n        self._is_best_so_far = True\n        self.best_epoch_metrics: Dict[str, float] = {}\n        self._epoch_number = 0\n        self.best_epoch: Optional[int] = None\n\n        # If the metric name starts with \"+\", we want it to increase.\n        # If the metric name starts with \"-\", we want it to decrease.\n        # We also allow you to not specify a metric name and just set `should_decrease` directly.\n        if should_decrease is not None and metric_name is not None:\n            <IND>raise ConfigurationError(\n                \"must specify either `should_decrease` or `metric_name` (but not both)\"\n            )\n        <DED>elif metric_name is not None:\n            <IND>if metric_name[0] == \"-\":\n                <IND>self._should_decrease = True\n            <DED>elif metric_name[0] == \"+\":\n                <IND>self._should_decrease = False\n            <DED>else:\n                <IND>raise ConfigurationError(\"metric_name must start with + or -\")\n        <DED><DED>elif should_decrease is not None:\n            <IND>self._should_decrease = should_decrease\n        <DED>else:\n            <IND>raise ConfigurationError(\n                \"must specify either `should_decrease` or `metric_name` (but not both)\"\n            )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    def __init__(\n        self,\n        metric_name: Union[str, List[str]],\n        patience: Optional[int] = None,\n    ) -> None:\n        <IND>self._patience = patience\n        self._best_so_far: Optional[float] = None\n        self._epochs_with_no_improvement = 0\n        self._is_best_so_far = True\n        self._epoch_number = 0\n        self.best_epoch: Optional[int] = None\n        self.best_epoch_metrics: Dict[str, float] = {}\n\n        if isinstance(metric_name, str):\n            <IND>metric_name = [metric_name]\n        <DED>self.tracked_metrics = []\n        for name in metric_name:\n            <IND>if name.startswith(\"+\"):\n                <IND>self.tracked_metrics.append((1.0, name[1:]))\n            <DED>elif name.startswith(\"-\"):\n                <IND>self.tracked_metrics.append((-1.0, name[1:]))\n            <DED>else:\n                <IND>raise ConfigurationError(\"metric_name must start with + or -\")\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "            \"best_so_far\": self._best_so_far,\n            \"patience\": self._patience,\n            \"epochs_with_no_improvement\": self._epochs_with_no_improvement,\n            \"is_best_so_far\": self._is_best_so_far,\n            \"should_decrease\": self._should_decrease,\n            \"best_epoch_metrics\": self.best_epoch_metrics,\n            \"epoch_number\": self._epoch_number,\n            \"best_epoch\": self.best_epoch,\n        }\n",
        "source_code_len": 428,
        "target_code": "            \"best_so_far\": self._best_so_far,\n            \"epochs_with_no_improvement\": self._epochs_with_no_improvement,\n            \"is_best_so_far\": self._is_best_so_far,\n            \"epoch_number\": self._epoch_number,\n            \"best_epoch\": self.best_epoch,\n            \"best_epoch_metrics\": self.best_epoch_metrics,\n        }\n",
        "target_code_len": 334,
        "diff_format": "@@ -79,9 +69,7 @@\n             \"best_so_far\": self._best_so_far,\n-            \"patience\": self._patience,\n             \"epochs_with_no_improvement\": self._epochs_with_no_improvement,\n             \"is_best_so_far\": self._is_best_so_far,\n-            \"should_decrease\": self._should_decrease,\n-            \"best_epoch_metrics\": self.best_epoch_metrics,\n             \"epoch_number\": self._epoch_number,\n             \"best_epoch\": self.best_epoch,\n+            \"best_epoch_metrics\": self.best_epoch_metrics,\n         }\n",
        "source_code_with_indent": "            \"best_so_far\": self._best_so_far,\n            \"patience\": self._patience,\n            \"epochs_with_no_improvement\": self._epochs_with_no_improvement,\n            \"is_best_so_far\": self._is_best_so_far,\n            \"should_decrease\": self._should_decrease,\n            \"best_epoch_metrics\": self.best_epoch_metrics,\n            \"epoch_number\": self._epoch_number,\n            \"best_epoch\": self.best_epoch,\n        }\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "            \"best_so_far\": self._best_so_far,\n            \"epochs_with_no_improvement\": self._epochs_with_no_improvement,\n            \"is_best_so_far\": self._is_best_so_far,\n            \"epoch_number\": self._epoch_number,\n            \"best_epoch\": self.best_epoch,\n            \"best_epoch_metrics\": self.best_epoch_metrics,\n        }\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        self._best_so_far = state_dict[\"best_so_far\"]\n        self._patience = state_dict[\"patience\"]\n        self._epochs_with_no_improvement = state_dict[\"epochs_with_no_improvement\"]\n        self._is_best_so_far = state_dict[\"is_best_so_far\"]\n        self._should_decrease = state_dict[\"should_decrease\"]\n        self.best_epoch_metrics = state_dict[\"best_epoch_metrics\"]\n        self._epoch_number = state_dict[\"epoch_number\"]\n",
        "source_code_len": 431,
        "target_code": "        self._best_so_far = state_dict[\"best_so_far\"]\n        self._epochs_with_no_improvement = state_dict[\"epochs_with_no_improvement\"]\n        self._is_best_so_far = state_dict[\"is_best_so_far\"]\n        self._epoch_number = state_dict[\"epoch_number\"]\n",
        "target_code_len": 254,
        "diff_format": "@@ -93,7 +81,4 @@\n         self._best_so_far = state_dict[\"best_so_far\"]\n-        self._patience = state_dict[\"patience\"]\n         self._epochs_with_no_improvement = state_dict[\"epochs_with_no_improvement\"]\n         self._is_best_so_far = state_dict[\"is_best_so_far\"]\n-        self._should_decrease = state_dict[\"should_decrease\"]\n-        self.best_epoch_metrics = state_dict[\"best_epoch_metrics\"]\n         self._epoch_number = state_dict[\"epoch_number\"]\n",
        "source_code_with_indent": "        self._best_so_far = state_dict[\"best_so_far\"]\n        self._patience = state_dict[\"patience\"]\n        self._epochs_with_no_improvement = state_dict[\"epochs_with_no_improvement\"]\n        self._is_best_so_far = state_dict[\"is_best_so_far\"]\n        self._should_decrease = state_dict[\"should_decrease\"]\n        self.best_epoch_metrics = state_dict[\"best_epoch_metrics\"]\n        self._epoch_number = state_dict[\"epoch_number\"]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        self._best_so_far = state_dict[\"best_so_far\"]\n        self._epochs_with_no_improvement = state_dict[\"epochs_with_no_improvement\"]\n        self._is_best_so_far = state_dict[\"is_best_so_far\"]\n        self._epoch_number = state_dict[\"epoch_number\"]\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    def add_metric(self, metric: float) -> None:\n        \"\"\"\n",
        "source_code_len": 62,
        "target_code": "\n        # Even though we don't promise backwards compatibility for the --recover flag,\n        # it's particularly easy and harmless to provide it here, so we do it.\n        self.best_epoch_metrics = state_dict.get(\"best_epoch_metrics\", {})\n\n    def add_metrics(self, metrics: Dict[str, float]) -> None:\n        \"\"\"\n",
        "target_code_len": 317,
        "diff_format": "@@ -101,3 +86,7 @@\n \n-    def add_metric(self, metric: float) -> None:\n+        # Even though we don't promise backwards compatibility for the --recover flag,\n+        # it's particularly easy and harmless to provide it here, so we do it.\n+        self.best_epoch_metrics = state_dict.get(\"best_epoch_metrics\", {})\n+\n+    def add_metrics(self, metrics: Dict[str, float]) -> None:\n         \"\"\"\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n    <DED>def add_metric(self, metric: float) -> None:\n        <IND>",
        "target_code_with_indent": "\n        # Even though we don't promise backwards compatibility for the --recover flag,\n        # it's particularly easy and harmless to provide it here, so we do it.\n        self.best_epoch_metrics = state_dict.get(\"best_epoch_metrics\", {})\n\n    <DED>def add_metrics(self, metrics: Dict[str, float]) -> None:\n        <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        \"\"\"\n        new_best = (\n            (self._best_so_far is None)\n            or (self._should_decrease and metric < self._best_so_far)\n            or (not self._should_decrease and metric > self._best_so_far)\n        )\n\n        if new_best:\n            self.best_epoch = self._epoch_number\n            self._is_best_so_far = True\n            self._best_so_far = metric\n            self._epochs_with_no_improvement = 0\n        else:\n            self._is_best_so_far = False\n            self._epochs_with_no_improvement += 1\n        self._epoch_number += 1\n\n    def add_metrics(self, metrics: Iterable[float]) -> None:\n        \"\"\"\n        Helper to add multiple metrics at once.\n        \"\"\"\n        for metric in metrics:\n            self.add_metric(metric)\n\n",
        "source_code_len": 765,
        "target_code": "        \"\"\"\n        try:\n            combined_score = sum(\n                factor * metrics[metric_name] for factor, metric_name in self.tracked_metrics\n            )\n        except KeyError as e:\n            raise ConfigurationError(\n                f\"You configured the trainer to use the {e.args[0]}\"\n                \"metric for early stopping, but the model did not produce that metric.\"\n            )\n\n        new_best = (self._best_so_far is None) or (combined_score > self._best_so_far)\n\n        if new_best:\n            self._best_so_far = combined_score\n            self._epochs_with_no_improvement = 0\n            self._is_best_so_far = True\n            self.best_epoch = self._epoch_number\n        else:\n            self._epochs_with_no_improvement += 1\n            self._is_best_so_far = False\n        self._epoch_number += 1\n\n",
        "target_code_len": 839,
        "diff_format": "@@ -105,24 +94,23 @@\n         \"\"\"\n-        new_best = (\n-            (self._best_so_far is None)\n-            or (self._should_decrease and metric < self._best_so_far)\n-            or (not self._should_decrease and metric > self._best_so_far)\n-        )\n+        try:\n+            combined_score = sum(\n+                factor * metrics[metric_name] for factor, metric_name in self.tracked_metrics\n+            )\n+        except KeyError as e:\n+            raise ConfigurationError(\n+                f\"You configured the trainer to use the {e.args[0]}\"\n+                \"metric for early stopping, but the model did not produce that metric.\"\n+            )\n+\n+        new_best = (self._best_so_far is None) or (combined_score > self._best_so_far)\n \n         if new_best:\n+            self._best_so_far = combined_score\n+            self._epochs_with_no_improvement = 0\n+            self._is_best_so_far = True\n             self.best_epoch = self._epoch_number\n-            self._is_best_so_far = True\n-            self._best_so_far = metric\n-            self._epochs_with_no_improvement = 0\n         else:\n+            self._epochs_with_no_improvement += 1\n             self._is_best_so_far = False\n-            self._epochs_with_no_improvement += 1\n         self._epoch_number += 1\n-\n-    def add_metrics(self, metrics: Iterable[float]) -> None:\n-        \"\"\"\n-        Helper to add multiple metrics at once.\n-        \"\"\"\n-        for metric in metrics:\n-            self.add_metric(metric)\n \n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n        new_best = (\n            (self._best_so_far is None)\n            or (self._should_decrease and metric < self._best_so_far)\n            or (not self._should_decrease and metric > self._best_so_far)\n        )\n\n        if new_best:\n            <IND>self.best_epoch = self._epoch_number\n            self._is_best_so_far = True\n            self._best_so_far = metric\n            self._epochs_with_no_improvement = 0\n        <DED>else:\n            <IND>self._is_best_so_far = False\n            self._epochs_with_no_improvement += 1\n        <DED>self._epoch_number += 1\n\n    <DED>def add_metrics(self, metrics: Iterable[float]) -> None:\n        <IND>\"\"\"\n        Helper to add multiple metrics at once.\n        \"\"\"\n        for metric in metrics:\n            <IND>self.add_metric(metric)\n\n",
        "target_code_with_indent": "\n        try:\n            <IND>combined_score = sum(\n                factor * metrics[metric_name] for factor, metric_name in self.tracked_metrics\n            )\n        <DED>except KeyError as e:\n            <IND>raise ConfigurationError(\n                f\"You configured the trainer to use the {e.args[0]}\"\n                \"metric for early stopping, but the model did not produce that metric.\"\n            )\n\n        <DED>new_best = (self._best_so_far is None) or (combined_score > self._best_so_far)\n\n        if new_best:\n            <IND>self._best_so_far = combined_score\n            self._epochs_with_no_improvement = 0\n            self._is_best_so_far = True\n            self.best_epoch = self._epoch_number\n        <DED>else:\n            <IND>self._epochs_with_no_improvement += 1\n            self._is_best_so_far = False\n        <DED>self._epoch_number += 1\n\n"
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/training/metric_tracker.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/metric_tracker.py",
    "file_hunks_size": 11,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/metric_tracker.py:109:55 Unsupported operand [58]: `>` is not supported for operand types `float` and `Optional[float]`.",
    "message": " `>` is not supported for operand types `float` and `Optional[float]`.",
    "rule_id": "Unsupported operand [58]",
    "warning_line_no": 109,
    "warning_line": "            or (not self._should_decrease and metric > self._best_so_far)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "from typing import Optional, Iterable, Dict, Any\n\n",
        "source_code_len": 50,
        "target_code": "from typing import Optional, Dict, Any, List, Union\n\n",
        "target_code_len": 53,
        "diff_format": "@@ -1,2 +1,2 @@\n-from typing import Optional, Iterable, Dict, Any\n+from typing import Optional, Dict, Any, List, Union\n \n",
        "source_code_with_indent": "from typing import Optional, Iterable, Dict, Any\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "from typing import Optional, Dict, Any, List, Union\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    def __init__(\n        self, patience: Optional[int] = None, metric_name: str = None, should_decrease: bool = None\n    ) -> None:\n        self._best_so_far: Optional[float] = None\n        self._patience = patience\n        self._epochs_with_no_improvement = 0\n        self._is_best_so_far = True\n        self.best_epoch_metrics: Dict[str, float] = {}\n        self._epoch_number = 0\n        self.best_epoch: Optional[int] = None\n\n        # If the metric name starts with \"+\", we want it to increase.\n        # If the metric name starts with \"-\", we want it to decrease.\n        # We also allow you to not specify a metric name and just set `should_decrease` directly.\n        if should_decrease is not None and metric_name is not None:\n            raise ConfigurationError(\n                \"must specify either `should_decrease` or `metric_name` (but not both)\"\n            )\n        elif metric_name is not None:\n            if metric_name[0] == \"-\":\n                self._should_decrease = True\n            elif metric_name[0] == \"+\":\n                self._should_decrease = False\n            else:\n                raise ConfigurationError(\"metric_name must start with + or -\")\n        elif should_decrease is not None:\n            self._should_decrease = should_decrease\n        else:\n            raise ConfigurationError(\n                \"must specify either `should_decrease` or `metric_name` (but not both)\"\n            )\n\n",
        "source_code_len": 1430,
        "target_code": "    def __init__(\n        self,\n        metric_name: Union[str, List[str]],\n        patience: Optional[int] = None,\n    ) -> None:\n        self._patience = patience\n        self._best_so_far: Optional[float] = None\n        self._epochs_with_no_improvement = 0\n        self._is_best_so_far = True\n        self._epoch_number = 0\n        self.best_epoch: Optional[int] = None\n        self.best_epoch_metrics: Dict[str, float] = {}\n\n        if isinstance(metric_name, str):\n            metric_name = [metric_name]\n        self.tracked_metrics = []\n        for name in metric_name:\n            if name.startswith(\"+\"):\n                self.tracked_metrics.append((1.0, name[1:]))\n            elif name.startswith(\"-\"):\n                self.tracked_metrics.append((-1.0, name[1:]))\n            else:\n                raise ConfigurationError(\"metric_name must start with + or -\")\n\n",
        "target_code_len": 874,
        "diff_format": "@@ -32,32 +29,24 @@\n     def __init__(\n-        self, patience: Optional[int] = None, metric_name: str = None, should_decrease: bool = None\n+        self,\n+        metric_name: Union[str, List[str]],\n+        patience: Optional[int] = None,\n     ) -> None:\n+        self._patience = patience\n         self._best_so_far: Optional[float] = None\n-        self._patience = patience\n         self._epochs_with_no_improvement = 0\n         self._is_best_so_far = True\n-        self.best_epoch_metrics: Dict[str, float] = {}\n         self._epoch_number = 0\n         self.best_epoch: Optional[int] = None\n+        self.best_epoch_metrics: Dict[str, float] = {}\n \n-        # If the metric name starts with \"+\", we want it to increase.\n-        # If the metric name starts with \"-\", we want it to decrease.\n-        # We also allow you to not specify a metric name and just set `should_decrease` directly.\n-        if should_decrease is not None and metric_name is not None:\n-            raise ConfigurationError(\n-                \"must specify either `should_decrease` or `metric_name` (but not both)\"\n-            )\n-        elif metric_name is not None:\n-            if metric_name[0] == \"-\":\n-                self._should_decrease = True\n-            elif metric_name[0] == \"+\":\n-                self._should_decrease = False\n+        if isinstance(metric_name, str):\n+            metric_name = [metric_name]\n+        self.tracked_metrics = []\n+        for name in metric_name:\n+            if name.startswith(\"+\"):\n+                self.tracked_metrics.append((1.0, name[1:]))\n+            elif name.startswith(\"-\"):\n+                self.tracked_metrics.append((-1.0, name[1:]))\n             else:\n                 raise ConfigurationError(\"metric_name must start with + or -\")\n-        elif should_decrease is not None:\n-            self._should_decrease = should_decrease\n-        else:\n-            raise ConfigurationError(\n-                \"must specify either `should_decrease` or `metric_name` (but not both)\"\n-            )\n \n",
        "source_code_with_indent": "    def __init__(\n        self, patience: Optional[int] = None, metric_name: str = None, should_decrease: bool = None\n    ) -> None:\n        <IND>self._best_so_far: Optional[float] = None\n        self._patience = patience\n        self._epochs_with_no_improvement = 0\n        self._is_best_so_far = True\n        self.best_epoch_metrics: Dict[str, float] = {}\n        self._epoch_number = 0\n        self.best_epoch: Optional[int] = None\n\n        # If the metric name starts with \"+\", we want it to increase.\n        # If the metric name starts with \"-\", we want it to decrease.\n        # We also allow you to not specify a metric name and just set `should_decrease` directly.\n        if should_decrease is not None and metric_name is not None:\n            <IND>raise ConfigurationError(\n                \"must specify either `should_decrease` or `metric_name` (but not both)\"\n            )\n        <DED>elif metric_name is not None:\n            <IND>if metric_name[0] == \"-\":\n                <IND>self._should_decrease = True\n            <DED>elif metric_name[0] == \"+\":\n                <IND>self._should_decrease = False\n            <DED>else:\n                <IND>raise ConfigurationError(\"metric_name must start with + or -\")\n        <DED><DED>elif should_decrease is not None:\n            <IND>self._should_decrease = should_decrease\n        <DED>else:\n            <IND>raise ConfigurationError(\n                \"must specify either `should_decrease` or `metric_name` (but not both)\"\n            )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    def __init__(\n        self,\n        metric_name: Union[str, List[str]],\n        patience: Optional[int] = None,\n    ) -> None:\n        <IND>self._patience = patience\n        self._best_so_far: Optional[float] = None\n        self._epochs_with_no_improvement = 0\n        self._is_best_so_far = True\n        self._epoch_number = 0\n        self.best_epoch: Optional[int] = None\n        self.best_epoch_metrics: Dict[str, float] = {}\n\n        if isinstance(metric_name, str):\n            <IND>metric_name = [metric_name]\n        <DED>self.tracked_metrics = []\n        for name in metric_name:\n            <IND>if name.startswith(\"+\"):\n                <IND>self.tracked_metrics.append((1.0, name[1:]))\n            <DED>elif name.startswith(\"-\"):\n                <IND>self.tracked_metrics.append((-1.0, name[1:]))\n            <DED>else:\n                <IND>raise ConfigurationError(\"metric_name must start with + or -\")\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "            \"best_so_far\": self._best_so_far,\n            \"patience\": self._patience,\n            \"epochs_with_no_improvement\": self._epochs_with_no_improvement,\n            \"is_best_so_far\": self._is_best_so_far,\n            \"should_decrease\": self._should_decrease,\n            \"best_epoch_metrics\": self.best_epoch_metrics,\n            \"epoch_number\": self._epoch_number,\n            \"best_epoch\": self.best_epoch,\n        }\n",
        "source_code_len": 428,
        "target_code": "            \"best_so_far\": self._best_so_far,\n            \"epochs_with_no_improvement\": self._epochs_with_no_improvement,\n            \"is_best_so_far\": self._is_best_so_far,\n            \"epoch_number\": self._epoch_number,\n            \"best_epoch\": self.best_epoch,\n            \"best_epoch_metrics\": self.best_epoch_metrics,\n        }\n",
        "target_code_len": 334,
        "diff_format": "@@ -79,9 +69,7 @@\n             \"best_so_far\": self._best_so_far,\n-            \"patience\": self._patience,\n             \"epochs_with_no_improvement\": self._epochs_with_no_improvement,\n             \"is_best_so_far\": self._is_best_so_far,\n-            \"should_decrease\": self._should_decrease,\n-            \"best_epoch_metrics\": self.best_epoch_metrics,\n             \"epoch_number\": self._epoch_number,\n             \"best_epoch\": self.best_epoch,\n+            \"best_epoch_metrics\": self.best_epoch_metrics,\n         }\n",
        "source_code_with_indent": "            \"best_so_far\": self._best_so_far,\n            \"patience\": self._patience,\n            \"epochs_with_no_improvement\": self._epochs_with_no_improvement,\n            \"is_best_so_far\": self._is_best_so_far,\n            \"should_decrease\": self._should_decrease,\n            \"best_epoch_metrics\": self.best_epoch_metrics,\n            \"epoch_number\": self._epoch_number,\n            \"best_epoch\": self.best_epoch,\n        }\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "            \"best_so_far\": self._best_so_far,\n            \"epochs_with_no_improvement\": self._epochs_with_no_improvement,\n            \"is_best_so_far\": self._is_best_so_far,\n            \"epoch_number\": self._epoch_number,\n            \"best_epoch\": self.best_epoch,\n            \"best_epoch_metrics\": self.best_epoch_metrics,\n        }\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        self._best_so_far = state_dict[\"best_so_far\"]\n        self._patience = state_dict[\"patience\"]\n        self._epochs_with_no_improvement = state_dict[\"epochs_with_no_improvement\"]\n        self._is_best_so_far = state_dict[\"is_best_so_far\"]\n        self._should_decrease = state_dict[\"should_decrease\"]\n        self.best_epoch_metrics = state_dict[\"best_epoch_metrics\"]\n        self._epoch_number = state_dict[\"epoch_number\"]\n",
        "source_code_len": 431,
        "target_code": "        self._best_so_far = state_dict[\"best_so_far\"]\n        self._epochs_with_no_improvement = state_dict[\"epochs_with_no_improvement\"]\n        self._is_best_so_far = state_dict[\"is_best_so_far\"]\n        self._epoch_number = state_dict[\"epoch_number\"]\n",
        "target_code_len": 254,
        "diff_format": "@@ -93,7 +81,4 @@\n         self._best_so_far = state_dict[\"best_so_far\"]\n-        self._patience = state_dict[\"patience\"]\n         self._epochs_with_no_improvement = state_dict[\"epochs_with_no_improvement\"]\n         self._is_best_so_far = state_dict[\"is_best_so_far\"]\n-        self._should_decrease = state_dict[\"should_decrease\"]\n-        self.best_epoch_metrics = state_dict[\"best_epoch_metrics\"]\n         self._epoch_number = state_dict[\"epoch_number\"]\n",
        "source_code_with_indent": "        self._best_so_far = state_dict[\"best_so_far\"]\n        self._patience = state_dict[\"patience\"]\n        self._epochs_with_no_improvement = state_dict[\"epochs_with_no_improvement\"]\n        self._is_best_so_far = state_dict[\"is_best_so_far\"]\n        self._should_decrease = state_dict[\"should_decrease\"]\n        self.best_epoch_metrics = state_dict[\"best_epoch_metrics\"]\n        self._epoch_number = state_dict[\"epoch_number\"]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        self._best_so_far = state_dict[\"best_so_far\"]\n        self._epochs_with_no_improvement = state_dict[\"epochs_with_no_improvement\"]\n        self._is_best_so_far = state_dict[\"is_best_so_far\"]\n        self._epoch_number = state_dict[\"epoch_number\"]\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    def add_metric(self, metric: float) -> None:\n        \"\"\"\n",
        "source_code_len": 62,
        "target_code": "\n        # Even though we don't promise backwards compatibility for the --recover flag,\n        # it's particularly easy and harmless to provide it here, so we do it.\n        self.best_epoch_metrics = state_dict.get(\"best_epoch_metrics\", {})\n\n    def add_metrics(self, metrics: Dict[str, float]) -> None:\n        \"\"\"\n",
        "target_code_len": 317,
        "diff_format": "@@ -101,3 +86,7 @@\n \n-    def add_metric(self, metric: float) -> None:\n+        # Even though we don't promise backwards compatibility for the --recover flag,\n+        # it's particularly easy and harmless to provide it here, so we do it.\n+        self.best_epoch_metrics = state_dict.get(\"best_epoch_metrics\", {})\n+\n+    def add_metrics(self, metrics: Dict[str, float]) -> None:\n         \"\"\"\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n    <DED>def add_metric(self, metric: float) -> None:\n        <IND>",
        "target_code_with_indent": "\n        # Even though we don't promise backwards compatibility for the --recover flag,\n        # it's particularly easy and harmless to provide it here, so we do it.\n        self.best_epoch_metrics = state_dict.get(\"best_epoch_metrics\", {})\n\n    <DED>def add_metrics(self, metrics: Dict[str, float]) -> None:\n        <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        \"\"\"\n        new_best = (\n            (self._best_so_far is None)\n            or (self._should_decrease and metric < self._best_so_far)\n            or (not self._should_decrease and metric > self._best_so_far)\n        )\n\n        if new_best:\n            self.best_epoch = self._epoch_number\n            self._is_best_so_far = True\n            self._best_so_far = metric\n            self._epochs_with_no_improvement = 0\n        else:\n            self._is_best_so_far = False\n            self._epochs_with_no_improvement += 1\n        self._epoch_number += 1\n\n    def add_metrics(self, metrics: Iterable[float]) -> None:\n        \"\"\"\n        Helper to add multiple metrics at once.\n        \"\"\"\n        for metric in metrics:\n            self.add_metric(metric)\n\n",
        "source_code_len": 765,
        "target_code": "        \"\"\"\n        try:\n            combined_score = sum(\n                factor * metrics[metric_name] for factor, metric_name in self.tracked_metrics\n            )\n        except KeyError as e:\n            raise ConfigurationError(\n                f\"You configured the trainer to use the {e.args[0]}\"\n                \"metric for early stopping, but the model did not produce that metric.\"\n            )\n\n        new_best = (self._best_so_far is None) or (combined_score > self._best_so_far)\n\n        if new_best:\n            self._best_so_far = combined_score\n            self._epochs_with_no_improvement = 0\n            self._is_best_so_far = True\n            self.best_epoch = self._epoch_number\n        else:\n            self._epochs_with_no_improvement += 1\n            self._is_best_so_far = False\n        self._epoch_number += 1\n\n",
        "target_code_len": 839,
        "diff_format": "@@ -105,24 +94,23 @@\n         \"\"\"\n-        new_best = (\n-            (self._best_so_far is None)\n-            or (self._should_decrease and metric < self._best_so_far)\n-            or (not self._should_decrease and metric > self._best_so_far)\n-        )\n+        try:\n+            combined_score = sum(\n+                factor * metrics[metric_name] for factor, metric_name in self.tracked_metrics\n+            )\n+        except KeyError as e:\n+            raise ConfigurationError(\n+                f\"You configured the trainer to use the {e.args[0]}\"\n+                \"metric for early stopping, but the model did not produce that metric.\"\n+            )\n+\n+        new_best = (self._best_so_far is None) or (combined_score > self._best_so_far)\n \n         if new_best:\n+            self._best_so_far = combined_score\n+            self._epochs_with_no_improvement = 0\n+            self._is_best_so_far = True\n             self.best_epoch = self._epoch_number\n-            self._is_best_so_far = True\n-            self._best_so_far = metric\n-            self._epochs_with_no_improvement = 0\n         else:\n+            self._epochs_with_no_improvement += 1\n             self._is_best_so_far = False\n-            self._epochs_with_no_improvement += 1\n         self._epoch_number += 1\n-\n-    def add_metrics(self, metrics: Iterable[float]) -> None:\n-        \"\"\"\n-        Helper to add multiple metrics at once.\n-        \"\"\"\n-        for metric in metrics:\n-            self.add_metric(metric)\n \n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n        new_best = (\n            (self._best_so_far is None)\n            or (self._should_decrease and metric < self._best_so_far)\n            or (not self._should_decrease and metric > self._best_so_far)\n        )\n\n        if new_best:\n            <IND>self.best_epoch = self._epoch_number\n            self._is_best_so_far = True\n            self._best_so_far = metric\n            self._epochs_with_no_improvement = 0\n        <DED>else:\n            <IND>self._is_best_so_far = False\n            self._epochs_with_no_improvement += 1\n        <DED>self._epoch_number += 1\n\n    <DED>def add_metrics(self, metrics: Iterable[float]) -> None:\n        <IND>\"\"\"\n        Helper to add multiple metrics at once.\n        \"\"\"\n        for metric in metrics:\n            <IND>self.add_metric(metric)\n\n",
        "target_code_with_indent": "\n        try:\n            <IND>combined_score = sum(\n                factor * metrics[metric_name] for factor, metric_name in self.tracked_metrics\n            )\n        <DED>except KeyError as e:\n            <IND>raise ConfigurationError(\n                f\"You configured the trainer to use the {e.args[0]}\"\n                \"metric for early stopping, but the model did not produce that metric.\"\n            )\n\n        <DED>new_best = (self._best_so_far is None) or (combined_score > self._best_so_far)\n\n        if new_best:\n            <IND>self._best_so_far = combined_score\n            self._epochs_with_no_improvement = 0\n            self._is_best_so_far = True\n            self.best_epoch = self._epoch_number\n        <DED>else:\n            <IND>self._epochs_with_no_improvement += 1\n            self._is_best_so_far = False\n        <DED>self._epoch_number += 1\n\n"
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 42,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:231:8 Incompatible return type [7]: Expected `Type[Union[BatchCallback, EpochCallback]]` but got `Type[_TrainerCallbackMeta._make_callback_type._Wrapper]`.",
    "message": " Expected `Type[Union[BatchCallback, EpochCallback]]` but got `Type[_TrainerCallbackMeta._make_callback_type._Wrapper]`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 231,
    "warning_line": "        return _Wrapper"
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 42,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:475:8 Incompatible variable type [9]: validation_data_loader is declared to have type `allennlp.data.dataloader.DataLoader` but is used as type `None`.",
    "message": " validation_data_loader is declared to have type `allennlp.data.dataloader.DataLoader` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 475,
    "warning_line": "        validation_data_loader: DataLoader = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 42,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:486:8 Incompatible variable type [9]: batch_callbacks is declared to have type `List[BatchCallback]` but is used as type `None`.",
    "message": " batch_callbacks is declared to have type `List[BatchCallback]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 486,
    "warning_line": "        batch_callbacks: List[BatchCallback] = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 42,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:487:8 Incompatible variable type [9]: epoch_callbacks is declared to have type `List[EpochCallback]` but is used as type `None`.",
    "message": " epoch_callbacks is declared to have type `List[EpochCallback]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 487,
    "warning_line": "        epoch_callbacks: List[EpochCallback] = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 42,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:488:8 Incompatible variable type [9]: end_callbacks is declared to have type `List[EpochCallback]` but is used as type `None`.",
    "message": " end_callbacks is declared to have type `List[EpochCallback]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 488,
    "warning_line": "        end_callbacks: List[EpochCallback] = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 42,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:489:8 Incompatible variable type [9]: trainer_callbacks is declared to have type `List[TrainerCallback]` but is used as type `None`.",
    "message": " trainer_callbacks is declared to have type `List[TrainerCallback]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 489,
    "warning_line": "        trainer_callbacks: List[TrainerCallback] = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 42,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:649:31 Incompatible parameter type [6]: Expected `typing.Iterable[Variable[_T]]` for 1st positional only parameter to call `iter` but got `allennlp.data.dataloader.DataLoader`.",
    "message": " Expected `typing.Iterable[Variable[_T]]` for 1st positional only parameter to call `iter` but got `allennlp.data.dataloader.DataLoader`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 649,
    "warning_line": "        batch_generator = iter(self.data_loader)"
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 42,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:1219:8 Incompatible variable type [9]: validation_data_loader is declared to have type `allennlp.data.dataloader.DataLoader` but is used as type `None`.",
    "message": " validation_data_loader is declared to have type `allennlp.data.dataloader.DataLoader` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 1219,
    "warning_line": "        validation_data_loader: DataLoader = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 42,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:1238:8 Incompatible variable type [9]: batch_callbacks is declared to have type `List[BatchCallback]` but is used as type `None`.",
    "message": " batch_callbacks is declared to have type `List[BatchCallback]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 1238,
    "warning_line": "        batch_callbacks: List[BatchCallback] = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 42,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:1239:8 Incompatible variable type [9]: epoch_callbacks is declared to have type `List[EpochCallback]` but is used as type `None`.",
    "message": " epoch_callbacks is declared to have type `List[EpochCallback]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 1239,
    "warning_line": "        epoch_callbacks: List[EpochCallback] = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 42,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:1240:8 Incompatible variable type [9]: end_callbacks is declared to have type `List[EpochCallback]` but is used as type `None`.",
    "message": " end_callbacks is declared to have type `List[EpochCallback]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 1240,
    "warning_line": "        end_callbacks: List[EpochCallback] = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 42,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:1241:8 Incompatible variable type [9]: trainer_callbacks is declared to have type `List[TrainerCallback]` but is used as type `None`.",
    "message": " trainer_callbacks is declared to have type `List[TrainerCallback]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 1241,
    "warning_line": "        trainer_callbacks: List[TrainerCallback] = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/training/util.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/util.py",
    "file_hunks_size": 18,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/util.py:96:4 Incompatible variable type [9]: validation_dataset_reader is declared to have type `allennlp.data.dataset_readers.dataset_reader.DatasetReader` but is used as type `None`.",
    "message": " validation_dataset_reader is declared to have type `allennlp.data.dataset_readers.dataset_reader.DatasetReader` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 96,
    "warning_line": "    validation_dataset_reader: DatasetReader = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/training/util.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/util.py",
    "file_hunks_size": 18,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/util.py:97:4 Incompatible variable type [9]: validation_data_path is declared to have type `str` but is used as type `None`.",
    "message": " validation_data_path is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 97,
    "warning_line": "    validation_data_path: str = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "allennlp/training/util.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/util.py",
    "file_hunks_size": 18,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/util.py:98:4 Incompatible variable type [9]: test_data_path is declared to have type `str` but is used as type `None`.",
    "message": " test_data_path is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 98,
    "warning_line": "    test_data_path: str = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "tests/data/dataset_readers/dataset_reader_test.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/tests/data/dataset_readers/dataset_reader_test.py",
    "file_hunks_size": 6,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "tests/data/dataset_readers/dataset_reader_test.py:124:48 Incompatible parameter type [6]: Expected `typing.Sized` for 1st positional only parameter to call `len` but got `typing.Union[AllennlpLazyDataset, dataset_reader.AllennlpDataset]`.",
    "message": " Expected `typing.Sized` for 1st positional only parameter to call `len` but got `typing.Union[AllennlpLazyDataset, dataset_reader.AllennlpDataset]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 124,
    "warning_line": "        assert len(first_pass_instances) == len(cached_instances)"
  },
  {
    "project": "allenai/allennlp",
    "commit": "67fa291cb72ce92a1e88a56974ba54168c1a8b15",
    "filename": "tests/data/dataset_readers/lazy_dataset_reader_test.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/data/dataset_readers/lazy_dataset_reader_test.py:19:4 Inconsistent override [14]: `tests.data.dataset_readers.lazy_dataset_reader_test.LazyDatasetReader._read` overrides method defined in `allennlp.data.dataset_readers.dataset_reader.DatasetReader` inconsistently. Could not find parameter `file_path` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/tests/data/dataset_readers/lazy_dataset_reader_test.py'",
    "dd_fail": true
  }
]