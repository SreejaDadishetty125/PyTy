[
  {
    "project": "PyTorchLightning/pytorch-lightning",
    "commit": "08573d0f7ea5426c3efe3eebe3ecf39bfeb085b8",
    "filename": "pytorch_lightning/trainer/distrib_parts.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-pytorch-lightning/pytorch_lightning/trainer/distrib_parts.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pytorch_lightning/trainer/distrib_parts.py:407:28 Incompatible parameter type [6]: Expected `List[int]` for 1st positional only parameter to call `sanitize_gpu_ids` but got `Optional[List[int]]`.",
    "message": " Expected `List[int]` for 1st positional only parameter to call `sanitize_gpu_ids` but got `Optional[List[int]]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 407,
    "warning_line": "    gpus = sanitize_gpu_ids(gpus)",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    gpus = normalize_parse_gpu_input_to_list(gpus)\n    gpus = sanitize_gpu_ids(gpus)\n\n    if not gpus:\n        raise MisconfigurationException(\"GPUs requested but none are available.\")\n    return gpus\n",
        "source_code_len": 201,
        "target_code": "    gpus = normalize_parse_gpu_input_to_list(gpus)\n    if not gpus:\n        raise MisconfigurationException(\"GPUs requested but none are available.\")\n    gpus = sanitize_gpu_ids(gpus)\n\n    return gpus\n",
        "target_code_len": 201,
        "diff_format": "@@ -406,6 +415,6 @@\n     gpus = normalize_parse_gpu_input_to_list(gpus)\n-    gpus = sanitize_gpu_ids(gpus)\n-\n     if not gpus:\n         raise MisconfigurationException(\"GPUs requested but none are available.\")\n+    gpus = sanitize_gpu_ids(gpus)\n+\n     return gpus\n",
        "source_code_with_indent": "    gpus = normalize_parse_gpu_input_to_list(gpus)\n    gpus = sanitize_gpu_ids(gpus)\n\n    if not gpus:\n        <IND>raise MisconfigurationException(\"GPUs requested but none are available.\")\n    <DED>return gpus\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    gpus = normalize_parse_gpu_input_to_list(gpus)\n    if not gpus:\n        <IND>raise MisconfigurationException(\"GPUs requested but none are available.\")\n    <DED>gpus = sanitize_gpu_ids(gpus)\n\n    return gpus\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "PyTorchLightning/pytorch-lightning",
    "commit": "08573d0f7ea5426c3efe3eebe3ecf39bfeb085b8",
    "filename": "pytorch_lightning/trainer/seed.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-pytorch-lightning/pytorch_lightning/trainer/seed.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pytorch_lightning/trainer/seed.py:21:19 Incompatible parameter type [6]: Expected `typing.Union[_SupportsTrunc, bytes, str, typing.SupportsInt, typing_extensions.SupportsIndex]` for 1st positional only parameter to call `int.__new__` but got `Optional[int]`.",
    "message": " Expected `typing.Union[_SupportsTrunc, bytes, str, typing.SupportsInt, typing_extensions.SupportsIndex]` for 1st positional only parameter to call `int.__new__` but got `Optional[int]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 21,
    "warning_line": "        seed = int(seed)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    try:\n        seed = int(seed)\n    except (TypeError, ValueError):\n",
        "source_code_len": 70,
        "target_code": "    try:\n        if seed is None:\n            seed = _select_seed_randomly(min_seed_value, max_seed_value)\n        else:\n            seed = int(seed)\n    except (TypeError, ValueError):\n",
        "target_code_len": 186,
        "diff_format": "@@ -20,3 +20,6 @@\n     try:\n-        seed = int(seed)\n+        if seed is None:\n+            seed = _select_seed_randomly(min_seed_value, max_seed_value)\n+        else:\n+            seed = int(seed)\n     except (TypeError, ValueError):\n",
        "source_code_with_indent": "    try:\n        <IND>seed = int(seed)\n    <DED>except (TypeError, ValueError):\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    try:\n        <IND>if seed is None:\n            <IND>seed = _select_seed_randomly(min_seed_value, max_seed_value)\n        <DED>else:\n            <IND>seed = int(seed)\n    <DED><DED>except (TypeError, ValueError):\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "PyTorchLightning/pytorch-lightning",
    "commit": "08573d0f7ea5426c3efe3eebe3ecf39bfeb085b8",
    "filename": "pytorch_lightning/trainer/supporters.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-pytorch-lightning/pytorch_lightning/trainer/supporters.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pytorch_lightning/trainer/supporters.py:28:8 Incompatible attribute type [8]: Attribute `last_idx` declared in class `TensorRunningAccum` has type `int` but is used as type `None`.",
    "message": " Attribute `last_idx` declared in class `TensorRunningAccum` has type `int` but is used as type `None`.",
    "rule_id": "Incompatible attribute type [8]",
    "warning_line_no": 28,
    "warning_line": "        self.last_idx: int = None",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "import torch\n",
        "source_code_len": 13,
        "target_code": "from typing import Optional\n\nimport torch\n",
        "target_code_len": 42,
        "diff_format": "@@ -1,1 +1,3 @@\n+from typing import Optional\n+\n import torch\n",
        "source_code_with_indent": "import torch\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "from typing import Optional\n\nimport torch\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        self.current_idx: int = 0\n        self.last_idx: int = None\n        self.rotated: bool = False\n",
        "source_code_len": 103,
        "target_code": "        self.current_idx: int = 0\n        self.last_idx: Optional[int] = None\n        self.rotated: bool = False\n",
        "target_code_len": 113,
        "diff_format": "@@ -27,3 +29,3 @@\n         self.current_idx: int = 0\n-        self.last_idx: int = None\n+        self.last_idx: Optional[int] = None\n         self.rotated: bool = False\n",
        "source_code_with_indent": "        self.current_idx: int = 0\n        self.last_idx: int = None\n        self.rotated: bool = False\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        self.current_idx: int = 0\n        self.last_idx: Optional[int] = None\n        self.rotated: bool = False\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "PyTorchLightning/pytorch-lightning",
    "commit": "08573d0f7ea5426c3efe3eebe3ecf39bfeb085b8",
    "filename": "pytorch_lightning/trainer/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-pytorch-lightning/pytorch_lightning/trainer/trainer.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pytorch_lightning/trainer/trainer.py:561:8 Incompatible return type [7]: Expected `int` but got `Optional[int]`.",
    "message": " Expected `int` but got `Optional[int]`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 561,
    "warning_line": "        return job_id",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    @property\n    def slurm_job_id(self) -> int:\n        try:\n",
        "source_code_len": 62,
        "target_code": "    @property\n    def slurm_job_id(self) -> Optional[int]:\n        try:\n",
        "target_code_len": 72,
        "diff_format": "@@ -548,3 +548,3 @@\n     @property\n-    def slurm_job_id(self) -> int:\n+    def slurm_job_id(self) -> Optional[int]:\n         try:\n",
        "source_code_with_indent": "    <DED>@property\n    def slurm_job_id(self) -> int:\n        <IND>try:\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    <DED>@property\n    def slurm_job_id(self) -> Optional[int]:\n        <IND>try:\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  }
]