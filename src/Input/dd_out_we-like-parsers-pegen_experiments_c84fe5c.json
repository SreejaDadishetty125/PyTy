[
  {
    "project": "we-like-parsers/pegen_experiments",
    "commit": "c84fe5cd389657f3744dcc34e08c001bc64edef2",
    "filename": "pegen/__main__.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/we-like-parsers-pegen_experiments/pegen/__main__.py",
    "file_hunks_size": 2,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pegen/__main__.py:24:9 Unbound name [10]: Name `Final` is used but not defined in the current scope.",
    "message": " Name `Final` is used but not defined in the current scope.",
    "rule_id": "Unbound name [10]",
    "warning_line_no": 24,
    "warning_line": "    MiB: Final = 2 ** 20"
  },
  {
    "project": "we-like-parsers/pegen_experiments",
    "commit": "c84fe5cd389657f3744dcc34e08c001bc64edef2",
    "filename": "pegen/__main__.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/we-like-parsers-pegen_experiments/pegen/__main__.py",
    "file_hunks_size": 2,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pegen/__main__.py:76:38 Incompatible parameter type [6]: Expected `typing.Optional[typing.Type[BaseException]]` for 1st positional only parameter to call `traceback.print_exception` but got `typing.Type[typing.NoReturn]`.",
    "message": " Expected `typing.Optional[typing.Type[BaseException]]` for 1st positional only parameter to call `traceback.print_exception` but got `typing.Type[typing.NoReturn]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 76,
    "warning_line": "            traceback.print_exception(err.__class__, err, None)"
  },
  {
    "project": "we-like-parsers/pegen_experiments",
    "commit": "c84fe5cd389657f3744dcc34e08c001bc64edef2",
    "filename": "pegen/__main__.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/we-like-parsers-pegen_experiments/pegen/__main__.py",
    "file_hunks_size": 2,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pegen/__main__.py:76:53 Incompatible parameter type [6]: Expected `typing.Optional[BaseException]` for 2nd positional only parameter to call `traceback.print_exception` but got `typing.NoReturn`.",
    "message": " Expected `typing.Optional[BaseException]` for 2nd positional only parameter to call `traceback.print_exception` but got `typing.NoReturn`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 76,
    "warning_line": "            traceback.print_exception(err.__class__, err, None)"
  },
  {
    "project": "we-like-parsers/pegen_experiments",
    "commit": "c84fe5cd389657f3744dcc34e08c001bc64edef2",
    "filename": "pegen/parser.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/we-like-parsers-pegen_experiments/pegen/parser.py",
    "file_hunks_size": 9,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pegen/parser.py:164:37 Incompatible parameter type [6]: Expected `bool` for 2nd positional only parameter to call `dict.__setitem__` but got `Tuple[Optional[tokenize.TokenInfo], typing.Any]`.",
    "message": " Expected `bool` for 2nd positional only parameter to call `dict.__setitem__` but got `Tuple[Optional[tokenize.TokenInfo], typing.Any]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 164,
    "warning_line": "            self._token_cache[key] = res, endmark",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        self._level = 0\n        self._symbol_cache: Dict[Tuple[Mark,\n                                       Callable[[Parser], Optional[T]]],\n                                 Tuple[Optional[T], Mark]] = {}\n        self._token_cache: Dict[Tuple[Mark, str], bool] = {}\n        # Pass through common tokeniser methods.\n",
        "source_code_len": 316,
        "target_code": "        self._level = 0\n        self._symbol_cache: Dict[Tuple[Mark, str], Tuple[Optional[T], Mark]] = {}\n        self._token_cache: Dict[Tuple[Mark, str], Tuple[Optional[T], Mark]] = {}\n        # Pass through common tokeniser methods.\n",
        "target_code_len": 236,
        "diff_format": "@@ -180,6 +180,4 @@\n         self._level = 0\n-        self._symbol_cache: Dict[Tuple[Mark,\n-                                       Callable[[Parser], Optional[T]]],\n-                                 Tuple[Optional[T], Mark]] = {}\n-        self._token_cache: Dict[Tuple[Mark, str], bool] = {}\n+        self._symbol_cache: Dict[Tuple[Mark, str], Tuple[Optional[T], Mark]] = {}\n+        self._token_cache: Dict[Tuple[Mark, str], Tuple[Optional[T], Mark]] = {}\n         # Pass through common tokeniser methods.\n",
        "source_code_with_indent": "        self._level = 0\n        self._symbol_cache: Dict[Tuple[Mark,\n                                       Callable[[Parser], Optional[T]]],\n                                 Tuple[Optional[T], Mark]] = {}\n        self._token_cache: Dict[Tuple[Mark, str], bool] = {}\n        # Pass through common tokeniser methods.\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        self._level = 0\n        self._symbol_cache: Dict[Tuple[Mark, str], Tuple[Optional[T], Mark]] = {}\n        self._token_cache: Dict[Tuple[Mark, str], Tuple[Optional[T], Mark]] = {}\n        # Pass through common tokeniser methods.\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "we-like-parsers/pegen_experiments",
    "commit": "c84fe5cd389657f3744dcc34e08c001bc64edef2",
    "filename": "pegen/parser.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/we-like-parsers-pegen_experiments/pegen/parser.py",
    "file_hunks_size": 9,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pegen/parser.py:171:4 Incompatible return type [7]: Expected `bool` but got `typing.Callable[[Named(self, Parser[typing.Any]), Named(type, str)], Optional[tokenize.TokenInfo]]`.",
    "message": " Expected `bool` but got `typing.Callable[[Named(self, Parser[typing.Any]), Named(type, str)], Optional[tokenize.TokenInfo]]`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 171,
    "warning_line": "    return expect_wrapper",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\ndef memoize_expect(method: Callable[[Parser], Optional[tokenize.TokenInfo]]) -> bool:\n    \"\"\"Memoize the expect() method.\"\"\"\n\n    def expect_wrapper(self: Parser, type: str) -> Optional[tokenize.TokenInfo]:\n        mark = self.mark()\n",
        "source_code_len": 235,
        "target_code": "\ndef memoize_expect(method: Callable[[Parser, str], T]) -> Callable[[Parser, str], T]:\n    \"\"\"Memoize the expect() method.\"\"\"\n\n    def expect_wrapper(self: Parser, type: str) -> T:\n        mark = self.mark()\n",
        "target_code_len": 208,
        "diff_format": "@@ -140,6 +140,6 @@\n \n-def memoize_expect(method: Callable[[Parser], Optional[tokenize.TokenInfo]]) -> bool:\n+def memoize_expect(method: Callable[[Parser, str], T]) -> Callable[[Parser, str], T]:\n     \"\"\"Memoize the expect() method.\"\"\"\n \n-    def expect_wrapper(self: Parser, type: str) -> Optional[tokenize.TokenInfo]:\n+    def expect_wrapper(self: Parser, type: str) -> T:\n         mark = self.mark()\n",
        "source_code_with_indent": "\n<DED>def memoize_expect(method: Callable[[Parser], Optional[tokenize.TokenInfo]]) -> bool:\n    <IND>\"\"\"Memoize the expect() method.\"\"\"\n\n    def expect_wrapper(self: Parser, type: str) -> Optional[tokenize.TokenInfo]:\n        <IND>mark = self.mark()\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n<DED>def memoize_expect(method: Callable[[Parser, str], T]) -> Callable[[Parser, str], T]:\n    <IND>\"\"\"Memoize the expect() method.\"\"\"\n\n    def expect_wrapper(self: Parser, type: str) -> T:\n        <IND>mark = self.mark()\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        self._level = 0\n        self._symbol_cache: Dict[Tuple[Mark,\n                                       Callable[[Parser], Optional[T]]],\n                                 Tuple[Optional[T], Mark]] = {}\n        self._token_cache: Dict[Tuple[Mark, str], bool] = {}\n        # Pass through common tokeniser methods.\n",
        "source_code_len": 316,
        "target_code": "        self._level = 0\n        self._symbol_cache: Dict[Tuple[Mark, str], Tuple[Optional[T], Mark]] = {}\n        self._token_cache: Dict[Tuple[Mark, str], Tuple[Optional[T], Mark]] = {}\n        # Pass through common tokeniser methods.\n",
        "target_code_len": 236,
        "diff_format": "@@ -180,6 +180,4 @@\n         self._level = 0\n-        self._symbol_cache: Dict[Tuple[Mark,\n-                                       Callable[[Parser], Optional[T]]],\n-                                 Tuple[Optional[T], Mark]] = {}\n-        self._token_cache: Dict[Tuple[Mark, str], bool] = {}\n+        self._symbol_cache: Dict[Tuple[Mark, str], Tuple[Optional[T], Mark]] = {}\n+        self._token_cache: Dict[Tuple[Mark, str], Tuple[Optional[T], Mark]] = {}\n         # Pass through common tokeniser methods.\n",
        "source_code_with_indent": "        self._level = 0\n        self._symbol_cache: Dict[Tuple[Mark,\n                                       Callable[[Parser], Optional[T]]],\n                                 Tuple[Optional[T], Mark]] = {}\n        self._token_cache: Dict[Tuple[Mark, str], bool] = {}\n        # Pass through common tokeniser methods.\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        self._level = 0\n        self._symbol_cache: Dict[Tuple[Mark, str], Tuple[Optional[T], Mark]] = {}\n        self._token_cache: Dict[Tuple[Mark, str], Tuple[Optional[T], Mark]] = {}\n        # Pass through common tokeniser methods.\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "we-like-parsers/pegen_experiments",
    "commit": "c84fe5cd389657f3744dcc34e08c001bc64edef2",
    "filename": "pegen/parser.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/we-like-parsers-pegen_experiments/pegen/parser.py",
    "file_hunks_size": 9,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pegen/parser.py:257:8 Incompatible return type [7]: Expected `NoReturn` but got `SyntaxError`.",
    "message": " Expected `NoReturn` but got `SyntaxError`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 257,
    "warning_line": "        return SyntaxError(\"pegen parse failure\", (filename, tok.start[0], 1 + tok.start[1], tok.line))",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    def make_syntax_error(self, filename=\"<unknown>\") -> NoReturn:\n        tok = self._tokenizer.diagnose()\n",
        "source_code_len": 109,
        "target_code": "\n    def make_syntax_error(self, filename=\"<unknown>\") -> SyntaxError:\n        tok = self._tokenizer.diagnose()\n",
        "target_code_len": 112,
        "diff_format": "@@ -254,3 +252,3 @@\n \n-    def make_syntax_error(self, filename=\"<unknown>\") -> NoReturn:\n+    def make_syntax_error(self, filename=\"<unknown>\") -> SyntaxError:\n         tok = self._tokenizer.diagnose()\n",
        "source_code_with_indent": "\n    <DED>def make_syntax_error(self, filename=\"<unknown>\") -> NoReturn:\n        <IND>tok = self._tokenizer.diagnose()\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def make_syntax_error(self, filename=\"<unknown>\") -> SyntaxError:\n        <IND>tok = self._tokenizer.diagnose()\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "we-like-parsers/pegen_experiments",
    "commit": "c84fe5cd389657f3744dcc34e08c001bc64edef2",
    "filename": "pegen/parser_generator.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/we-like-parsers-pegen_experiments/pegen/parser_generator.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pegen/parser_generator.py:89:12 Incompatible return type [7]: Expected `None` but got `typing.Generator[None, None, None]`.",
    "message": " Expected `None` but got `typing.Generator[None, None, None]`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 89,
    "warning_line": "            yield",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    @contextlib.contextmanager\n    def indent(self) -> None:\n        self.level += 1\n",
        "source_code_len": 85,
        "target_code": "    @contextlib.contextmanager\n    def indent(self) -> Iterator[None]:\n        self.level += 1\n",
        "target_code_len": 95,
        "diff_format": "@@ -85,3 +86,3 @@\n     @contextlib.contextmanager\n-    def indent(self) -> None:\n+    def indent(self) -> Iterator[None]:\n         self.level += 1\n",
        "source_code_with_indent": "    <DED>@contextlib.contextmanager\n    def indent(self) -> None:\n        <IND>self.level += 1\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    <DED>@contextlib.contextmanager\n    def indent(self) -> Iterator[None]:\n        <IND>self.level += 1\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "we-like-parsers/pegen_experiments",
    "commit": "c84fe5cd389657f3744dcc34e08c001bc64edef2",
    "filename": "pegen/parser_generator.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/we-like-parsers-pegen_experiments/pegen/parser_generator.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pegen/parser_generator.py:152:37 Incompatible parameter type [6]: Expected `str` for 2nd positional only parameter to call `Rule.__init__` but got `None`.",
    "message": " Expected `str` for 2nd positional only parameter to call `Rule.__init__` but got `None`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 152,
    "warning_line": "        self.todo[name] = Rule(name, None, rhs)"
  },
  {
    "project": "we-like-parsers/pegen_experiments",
    "commit": "c84fe5cd389657f3744dcc34e08c001bc64edef2",
    "filename": "pegen/parser_generator.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/we-like-parsers-pegen_experiments/pegen/parser_generator.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pegen/parser_generator.py:155:30 Unbound name [10]: Name `Plain` is used but not defined in the current scope.",
    "message": " Name `Plain` is used but not defined in the current scope.",
    "rule_id": "Unbound name [10]",
    "warning_line_no": 155,
    "warning_line": "    def name_loop(self, node: Plain, is_repeat1: bool) -> str:",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "from pegen.grammar import NamedItem\n\n",
        "source_code_len": 37,
        "target_code": "from pegen.grammar import NamedItem\nfrom pegen.grammar import Plain\n\n",
        "target_code_len": 69,
        "diff_format": "@@ -10,2 +10,3 @@\n from pegen.grammar import NamedItem\n+from pegen.grammar import Plain\n \n",
        "source_code_with_indent": "from pegen.grammar import NamedItem\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "from pegen.grammar import NamedItem\nfrom pegen.grammar import Plain\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "we-like-parsers/pegen_experiments",
    "commit": "c84fe5cd389657f3744dcc34e08c001bc64edef2",
    "filename": "pegen/parser_generator.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/we-like-parsers-pegen_experiments/pegen/parser_generator.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pegen/parser_generator.py:162:37 Incompatible parameter type [6]: Expected `str` for 2nd positional only parameter to call `Rule.__init__` but got `None`.",
    "message": " Expected `str` for 2nd positional only parameter to call `Rule.__init__` but got `None`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 162,
    "warning_line": "        self.todo[name] = Rule(name, None, Rhs([Alt([NamedItem(None, node)])]))"
  },
  {
    "project": "we-like-parsers/pegen_experiments",
    "commit": "c84fe5cd389657f3744dcc34e08c001bc64edef2",
    "filename": "pegen/parser_generator.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/we-like-parsers-pegen_experiments/pegen/parser_generator.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pegen/parser_generator.py:177:69 Incompatible parameter type [6]: Expected `typing.Dict[str, typing.Iterable[str]]` for 2nd positional only parameter to call `sccutils.strongly_connected_components` but got `typing.Dict[str, str]`.",
    "message": " Expected `typing.Dict[str, typing.Iterable[str]]` for 2nd positional only parameter to call `sccutils.strongly_connected_components` but got `typing.Dict[str, str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 177,
    "warning_line": "    sccs = list(sccutils.strongly_connected_components(graph.keys(), graph))"
  },
  {
    "project": "we-like-parsers/pegen_experiments",
    "commit": "c84fe5cd389657f3744dcc34e08c001bc64edef2",
    "filename": "pegen/parser_generator.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/we-like-parsers-pegen_experiments/pegen/parser_generator.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pegen/parser_generator.py:185:57 Incompatible parameter type [6]: Expected `typing.Dict[str, typing.Set[str]]` for 1st positional only parameter to call `sccutils.find_cycles_in_scc` but got `typing.Dict[str, str]`.",
    "message": " Expected `typing.Dict[str, typing.Set[str]]` for 1st positional only parameter to call `sccutils.find_cycles_in_scc` but got `typing.Dict[str, str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 185,
    "warning_line": "                for cycle in sccutils.find_cycles_in_scc(graph, scc, start):"
  },
  {
    "project": "we-like-parsers/pegen_experiments",
    "commit": "c84fe5cd389657f3744dcc34e08c001bc64edef2",
    "filename": "pegen/parser_generator.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/we-like-parsers-pegen_experiments/pegen/parser_generator.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pegen/parser_generator.py:199:4 Incompatible return type [7]: Expected `typing.Tuple[typing.Dict[str, typing.Set[str]], typing.List[typing.Set[str]]]` but got `typing.Tuple[typing.Dict[str, str], typing.List[typing.Set[str]]]`.",
    "message": " Expected `typing.Tuple[typing.Dict[str, typing.Set[str]], typing.List[typing.Set[str]]]` but got `typing.Tuple[typing.Dict[str, str], typing.List[typing.Set[str]]]`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 199,
    "warning_line": "    return graph, sccs"
  },
  {
    "project": "we-like-parsers/pegen_experiments",
    "commit": "c84fe5cd389657f3744dcc34e08c001bc64edef2",
    "filename": "pegen/tokenizer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/we-like-parsers-pegen_experiments/pegen/tokenizer.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pegen/tokenizer.py:16:18 Unbound name [10]: Name `tokenizer` is used but not defined in the current scope.",
    "message": " Name `tokenizer` is used but not defined in the current scope.",
    "rule_id": "Unbound name [10]",
    "warning_line_no": 16,
    "warning_line": "def shorttok(tok: tokenizer.TokenInfo) -> str:",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\ndef shorttok(tok: tokenizer.TokenInfo) -> str:\n    return \"%-25.25s\" % f\"{tok.start[0]}.{tok.start[1]}: {token.tok_name[tok.type]}:{tok.string!r}\"\n",
        "source_code_len": 148,
        "target_code": "\ndef shorttok(tok: tokenize.TokenInfo) -> str:\n    return \"%-25.25s\" % f\"{tok.start[0]}.{tok.start[1]}: {token.tok_name[tok.type]}:{tok.string!r}\"\n",
        "target_code_len": 147,
        "diff_format": "@@ -15,3 +15,3 @@\n \n-def shorttok(tok: tokenizer.TokenInfo) -> str:\n+def shorttok(tok: tokenize.TokenInfo) -> str:\n     return \"%-25.25s\" % f\"{tok.start[0]}.{tok.start[1]}: {token.tok_name[tok.type]}:{tok.string!r}\"\n",
        "source_code_with_indent": "\ndef shorttok(tok: tokenizer.TokenInfo) -> str:\n    <IND>return \"%-25.25s\" % f\"{tok.start[0]}.{tok.start[1]}: {token.tok_name[tok.type]}:{tok.string!r}\"\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\ndef shorttok(tok: tokenize.TokenInfo) -> str:\n    <IND>return \"%-25.25s\" % f\"{tok.start[0]}.{tok.start[1]}: {token.tok_name[tok.type]}:{tok.string!r}\"\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "we-like-parsers/pegen_experiments",
    "commit": "c84fe5cd389657f3744dcc34e08c001bc64edef2",
    "filename": "pegen/tokenizer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/we-like-parsers-pegen_experiments/pegen/tokenizer.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pegen/tokenizer.py:28:42 Unbound name [10]: Name `TokenInfo` is used but not defined in the current scope.",
    "message": " Name `TokenInfo` is used but not defined in the current scope.",
    "rule_id": "Unbound name [10]",
    "warning_line_no": 28,
    "warning_line": "    def __init__(self, tokengen: Iterable[TokenInfo], *, verbose=False):",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "import tokenize\nfrom typing import List, Iterable\n\n",
        "source_code_len": 51,
        "target_code": "import tokenize\nfrom typing import List, Iterator\n\n",
        "target_code_len": 51,
        "diff_format": "@@ -4,3 +4,3 @@\n import tokenize\n-from typing import List, Iterable\n+from typing import List, Iterator\n \n",
        "source_code_with_indent": "import tokenize\nfrom typing import List, Iterable\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "import tokenize\nfrom typing import List, Iterator\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    def __init__(self, tokengen: Iterable[TokenInfo], *, verbose=False):\n        self._tokengen = tokengen\n",
        "source_code_len": 108,
        "target_code": "\n    def __init__(self, tokengen: Iterator[tokenize.TokenInfo], *, verbose=False):\n        self._tokengen = tokengen\n",
        "target_code_len": 117,
        "diff_format": "@@ -27,3 +27,3 @@\n \n-    def __init__(self, tokengen: Iterable[TokenInfo], *, verbose=False):\n+    def __init__(self, tokengen: Iterator[tokenize.TokenInfo], *, verbose=False):\n         self._tokengen = tokengen\n",
        "source_code_with_indent": "\n    def __init__(self, tokengen: Iterable[TokenInfo], *, verbose=False):\n        <IND>self._tokengen = tokengen\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    def __init__(self, tokengen: Iterator[tokenize.TokenInfo], *, verbose=False):\n        <IND>self._tokengen = tokengen\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  }
]