[
  {
    "project": "microsoft/msticpy",
    "commit": "28466d681e261394e18d9f4e063cebfa06ed04b4",
    "filename": "msticpy/common/keyvault_client.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/microsoft-msticpy/msticpy/common/keyvault_client.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "msticpy/common/keyvault_client.py:54:8 Incompatible variable type [9]: name is declared to have type `str` but is used as type `None`.",
    "message": " name is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 54,
    "warning_line": "        name: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "__author__ = \"Matt Richard, Ian Hellen\"\n\n\n# pylint: disable=too-many-instance-attributes\n@export\nclass AuthClient:\n    \"\"\"Authentication class base.\"\"\"\n\n    def __init__(\n        self,\n        tenant_id: str,\n        client_id: str,\n        client_uri: str,\n        name: str = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize base authentication client for credential caching.\n\n        Parameters\n        ----------\n        tenant_id : str\n            Tenant ID of Azure User\n        client_id : str\n            Client ID of application client\n        client_uri : str\n            [description]\n        name : str, optional\n            Name of the secret store, by default None\n        authority : str, optional\n            The AAD authority - one of 'global', 'usgov', 'de' or 'chi'\n        authority_uri : str, optional\n            The AAD authority URI - overrides `authority`\n        debug : bool, optional\n            Output debug information if True, by default False\n\n        Notes\n        -----\n        The parameter values can also be obtained from the\n        KeyVault section of msticpyconfig.yaml.\n\n        \"\"\"\n        self.name = name\n        self.debug = kwargs.pop(\"debug\", False)\n        self.settings: KeyVaultSettings = (\n            kwargs.pop(\"settings\", None) or KeyVaultSettings()\n        )\n        self.tenant_id = tenant_id or self.settings.get(\"tenantid\")\n        if not self.tenant_id:\n            raise MsticpyKeyVaultConfigError(\n                \"Could not get TenantId from function parameters or configuration.\",\n                \"Please add this to the KeyVault section of msticpyconfig.yaml\",\n                title=\"missing tenant ID value.\",\n            )\n        self.authority = kwargs.pop(\n            \"authority\", self.settings.get_tenant_authority_host(tenant_id)\n        )\n        self.client_id = client_id or self.settings.CLIENT_ID\n        self.client_uri = client_uri\n        self.authority_uri = self.settings.get_tenant_authority_uri(\n            authority_uri=kwargs.get(\"authority_uri\"), tenant=self.tenant_id\n        )\n\n        if self.debug:\n            print(\"AuthClient for %s - %s\" % (client_id, client_uri))\n        self._get_creds()\n        if self._expired_creds:\n            if self.debug:\n                print(\"expired creds\")\n            try:\n                self._refresh_creds()\n                return\n            except AdalError:\n                if self.debug:\n                    print(\"Token was no longer valid, forcing a new one.\")\n            self._get_token()\n\n    def _get_token(self):\n        context = AuthenticationContext(self.authority_uri)\n        code = context.acquire_user_code(self.client_uri, self.client_id)\n        _prompt_for_code(code)\n        self.config_data = context.acquire_token_with_device_code(\n            self.client_uri, code, self.client_id\n        )\n        self._cache_creds()\n\n    def _get_creds(self):\n        self._get_token()\n\n    def _cache_creds(self):\n        pass\n\n    def _is_valid_config_data(self):\n        keys = [\"accessToken\", \"refreshToken\", \"expiresOn\"]\n        return (\n            all(key in self.config_data for key in keys)\n            and all(self.config_data.get(key) for key in keys)\n            and all(len(self.config_data.get(key)) > 0 for key in keys)\n        )\n\n    @property\n    def auth_id(self) -> str:\n        \"\"\"Return name or ID of client.\"\"\"\n        return self.name if self.name is not None else self.client_id\n\n    @property\n    def user_oid(self) -> str:\n        \"\"\"\n        Return the user Object ID.\n\n        Returns\n        -------\n        str\n            User OID.\n\n        \"\"\"\n        data = self._get_parsed_token_data()\n        return data.get(\"oid\")\n\n    def _get_parsed_token_data(self) -> Any:\n        tok_data = self.token\n        tok_data = tok_data.split(\".\")[1]\n        tok_data += \"=\" * ((4 - len(tok_data) % 4) % 4)\n        return json.loads(base64.b64decode(tok_data))\n\n    def _refresh_creds(self):\n        context = AuthenticationContext(self.authority_uri)\n        self.config_data = context.acquire_token_with_refresh_token(\n            self.config_data[\"refreshToken\"], self.client_id, self.client_uri\n        )\n        if self.debug:\n            print(f\"got new token expiring {self.config_data['expiresOn']}\")\n            self._cache_creds()\n\n    @property\n    def _expired_creds(self) -> bool:\n        return self._expires_on < datetime.now()\n\n    @property\n    def _expires_on(self) -> datetime:\n        \"\"\"Return token expiry date as string.\"\"\"\n        return datetime.strptime(self.config_data[\"expiresOn\"], \"%Y-%m-%d %H:%M:%S.%f\")\n\n    @property\n    def token(self) -> str:\n        \"\"\"\n        Return the access token.\n\n        Returns\n        -------\n        str\n            Access Token\n\n        \"\"\"\n        if self._expired_creds:\n            try:\n                self._refresh_creds()\n            except AdalError:\n                self._get_token()\n        return self.config_data[\"accessToken\"]\n\n    def _adal_callback(self, server: str, resource: str, scope: str, scheme: str):\n        \"\"\"\n        ADAL Callback for authentication.\n\n        Parameters\n        ----------\n        server : str\n            Not used\n        resource : str\n            Not used\n        scope : str\n            Not used\n        scheme : str\n            Not used\n\n        Returns\n        -------\n        Tuple(str, str)\n            Bearer, Token\n\n        Notes\n        -----\n        None of the parameters are used in this function. However,\n        they are required because of the expected callback signature.\n\n        \"\"\"\n        del (server, resource, scope, scheme)\n        return \"Bearer\", self.token\n\n\n# pylint: enable=too-many-instance-attributes\n\n\n@export\nclass KeyringAuthClient(AuthClient):\n    \"\"\"\n    Key Authentication Client.\n\n    Handles management of authentication and refresh tokens\n    via keyring\n    \"\"\"\n\n    # pylint: disable=too-many-arguments\n    def __init__(\n        self,\n        tenant_id: str,\n        client_id: str,\n        client_url: str,\n        name: str = None,\n        debug: bool = False,\n    ):\n        \"\"\"\n        Initialize KeyringAuthClient.\n\n        Parameters\n        ----------\n        tenant_id : str\n            Tenant ID of Azure User\n        client_id : str\n            Client ID of application client\n        client_url : str\n            [description]\n        name : str, optional\n            Name of the secret store, by default None\n        debug : bool, optional\n            Output debug information if True, by default False\n\n        \"\"\"\n        self.name = name\n        self.keyring = self.auth_id\n        super().__init__(tenant_id, client_id, client_url, name=name, debug=debug)\n\n    # pylint: enable=too-many-arguments\n\n    def _get_creds(self):\n        if self.debug:\n            print(\"Fetching creds from keyring\")\n        try:\n            access_token = (\n                keyring.get_password(self.keyring, \"adal_context_1\")\n                + keyring.get_password(self.keyring, \"adal_context_2\")\n                + keyring.get_password(self.keyring, \"adal_context_3\")\n                + keyring.get_password(self.keyring, \"adal_context_4\")\n            )\n            refresh_token = (\n                keyring.get_password(self.keyring, \"adal_context_5\")\n                + keyring.get_password(self.keyring, \"adal_context_6\")\n                + keyring.get_password(self.keyring, \"adal_context_7\")\n            )\n            expires_on = keyring.get_password(self.keyring, \"adal_context_8\")\n            self.config_data = {\n                \"accessToken\": access_token,\n                \"refreshToken\": refresh_token,\n                \"expiresOn\": expires_on,\n            }\n        except (TypeError, KeyringError):\n            if self.debug:\n                print(\"No valid credentials in keyring %s\" % self.keyring)\n            self._get_token()\n        if not self._is_valid_config_data():\n            if self.debug:\n                print(\"No valid authtoken config found in keyring\")\n            self._get_token()\n\n    def _cache_creds(self):\n        if self.debug:\n            print(\"Saving config data to keyring %s\" % self.keyring)\n        keyring.set_password(\n            self.keyring, \"adal_context_1\", self.config_data[\"accessToken\"][:400]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_2\", self.config_data[\"accessToken\"][400:800]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_3\", self.config_data[\"accessToken\"][800:1200]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_4\", self.config_data[\"accessToken\"][1200:]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_5\", self.config_data[\"refreshToken\"][:400]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_6\", self.config_data[\"refreshToken\"][400:800]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_7\", self.config_data[\"refreshToken\"][800:]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_8\", self.config_data[\"expiresOn\"]\n        )\n\n\n# class KeyVaultAuthClient(AuthClient):\n#     \"\"\"\n#     Keyvault Auth client.\n\n#     Handles management of authentication tokens in keyvault.\n#     \"\"\"\n\n#     def __init__(\n#         self,\n#         tenant_id: str,\n#         client_id: str,\n#         client_url: str,\n#         secret_name: str,\n#         name: str = None,\n#         debug: bool = False,\n#     ):\n#         \"\"\"\n#         Initialize KeyvaultAuthClient.\n\n#         Parameters\n#         ----------\n#         tenant_id : str\n#             Tenant ID of Azure User\n#         client_id : str\n#             Client ID of application client\n#         client_url : str\n#             [description]\n#         name : str, optional\n#             Name of the secret store, by default None\n#         debug : bool, optional\n#             Output debug information if True, by default False\n\n#         \"\"\"\n#         self.secret_name = secret_name\n#         self._get_creds = self._get_keyvault_creds\n#         self._cache_creds = self._cache_creds_keyvault\n#         self.keyvault_client = BHKeyVaultClient(\n#             tenant_id=tenant_id, vault_uri=client_url\n#         )\n#         self.config_data: Any = None\n#         super().__init__(tenant_id, client_id, client_url, name=name, debug=debug)\n\n#     def _get_keyvault_creds(self):\n#         if self.debug:\n#             print(\"getting tokens from keyvault\")\n#         try:\n#             self.config_data = json.loads(\n#                 self.keyvault_client.get_secret(self.secret_name)\n#             )\n#         except KeyVaultMissingSecretException:\n#             if self.debug:\n#                 print(\"missing secret from keyvault, fetching manually\")\n#             self._get_token()\n#         except KeyVaultErrorException as err:\n#             if self.debug:\n#                 print(\"bad creds in keyvault, you gotta getem\")\n#                 print(\"here is what went wrong: %s\" % str(err))\n#             self._get_token()\n\n#     def _cache_creds_keyvault(self):\n#         self.keyvault_client.set_secret(self.secret_name, json.dumps(self.config_data))\n\n",
        "source_code_len": 11202,
        "target_code": "__author__ = \"Matt Richard, Ian Hellen\"\n\n",
        "target_code_len": 41,
        "diff_format": "@@ -41,356 +36,2 @@\n __author__ = \"Matt Richard, Ian Hellen\"\n-\n-\n-# pylint: disable=too-many-instance-attributes\n-@export\n-class AuthClient:\n-    \"\"\"Authentication class base.\"\"\"\n-\n-    def __init__(\n-        self,\n-        tenant_id: str,\n-        client_id: str,\n-        client_uri: str,\n-        name: str = None,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Initialize base authentication client for credential caching.\n-\n-        Parameters\n-        ----------\n-        tenant_id : str\n-            Tenant ID of Azure User\n-        client_id : str\n-            Client ID of application client\n-        client_uri : str\n-            [description]\n-        name : str, optional\n-            Name of the secret store, by default None\n-        authority : str, optional\n-            The AAD authority - one of 'global', 'usgov', 'de' or 'chi'\n-        authority_uri : str, optional\n-            The AAD authority URI - overrides `authority`\n-        debug : bool, optional\n-            Output debug information if True, by default False\n-\n-        Notes\n-        -----\n-        The parameter values can also be obtained from the\n-        KeyVault section of msticpyconfig.yaml.\n-\n-        \"\"\"\n-        self.name = name\n-        self.debug = kwargs.pop(\"debug\", False)\n-        self.settings: KeyVaultSettings = (\n-            kwargs.pop(\"settings\", None) or KeyVaultSettings()\n-        )\n-        self.tenant_id = tenant_id or self.settings.get(\"tenantid\")\n-        if not self.tenant_id:\n-            raise MsticpyKeyVaultConfigError(\n-                \"Could not get TenantId from function parameters or configuration.\",\n-                \"Please add this to the KeyVault section of msticpyconfig.yaml\",\n-                title=\"missing tenant ID value.\",\n-            )\n-        self.authority = kwargs.pop(\n-            \"authority\", self.settings.get_tenant_authority_host(tenant_id)\n-        )\n-        self.client_id = client_id or self.settings.CLIENT_ID\n-        self.client_uri = client_uri\n-        self.authority_uri = self.settings.get_tenant_authority_uri(\n-            authority_uri=kwargs.get(\"authority_uri\"), tenant=self.tenant_id\n-        )\n-\n-        if self.debug:\n-            print(\"AuthClient for %s - %s\" % (client_id, client_uri))\n-        self._get_creds()\n-        if self._expired_creds:\n-            if self.debug:\n-                print(\"expired creds\")\n-            try:\n-                self._refresh_creds()\n-                return\n-            except AdalError:\n-                if self.debug:\n-                    print(\"Token was no longer valid, forcing a new one.\")\n-            self._get_token()\n-\n-    def _get_token(self):\n-        context = AuthenticationContext(self.authority_uri)\n-        code = context.acquire_user_code(self.client_uri, self.client_id)\n-        _prompt_for_code(code)\n-        self.config_data = context.acquire_token_with_device_code(\n-            self.client_uri, code, self.client_id\n-        )\n-        self._cache_creds()\n-\n-    def _get_creds(self):\n-        self._get_token()\n-\n-    def _cache_creds(self):\n-        pass\n-\n-    def _is_valid_config_data(self):\n-        keys = [\"accessToken\", \"refreshToken\", \"expiresOn\"]\n-        return (\n-            all(key in self.config_data for key in keys)\n-            and all(self.config_data.get(key) for key in keys)\n-            and all(len(self.config_data.get(key)) > 0 for key in keys)\n-        )\n-\n-    @property\n-    def auth_id(self) -> str:\n-        \"\"\"Return name or ID of client.\"\"\"\n-        return self.name if self.name is not None else self.client_id\n-\n-    @property\n-    def user_oid(self) -> str:\n-        \"\"\"\n-        Return the user Object ID.\n-\n-        Returns\n-        -------\n-        str\n-            User OID.\n-\n-        \"\"\"\n-        data = self._get_parsed_token_data()\n-        return data.get(\"oid\")\n-\n-    def _get_parsed_token_data(self) -> Any:\n-        tok_data = self.token\n-        tok_data = tok_data.split(\".\")[1]\n-        tok_data += \"=\" * ((4 - len(tok_data) % 4) % 4)\n-        return json.loads(base64.b64decode(tok_data))\n-\n-    def _refresh_creds(self):\n-        context = AuthenticationContext(self.authority_uri)\n-        self.config_data = context.acquire_token_with_refresh_token(\n-            self.config_data[\"refreshToken\"], self.client_id, self.client_uri\n-        )\n-        if self.debug:\n-            print(f\"got new token expiring {self.config_data['expiresOn']}\")\n-            self._cache_creds()\n-\n-    @property\n-    def _expired_creds(self) -> bool:\n-        return self._expires_on < datetime.now()\n-\n-    @property\n-    def _expires_on(self) -> datetime:\n-        \"\"\"Return token expiry date as string.\"\"\"\n-        return datetime.strptime(self.config_data[\"expiresOn\"], \"%Y-%m-%d %H:%M:%S.%f\")\n-\n-    @property\n-    def token(self) -> str:\n-        \"\"\"\n-        Return the access token.\n-\n-        Returns\n-        -------\n-        str\n-            Access Token\n-\n-        \"\"\"\n-        if self._expired_creds:\n-            try:\n-                self._refresh_creds()\n-            except AdalError:\n-                self._get_token()\n-        return self.config_data[\"accessToken\"]\n-\n-    def _adal_callback(self, server: str, resource: str, scope: str, scheme: str):\n-        \"\"\"\n-        ADAL Callback for authentication.\n-\n-        Parameters\n-        ----------\n-        server : str\n-            Not used\n-        resource : str\n-            Not used\n-        scope : str\n-            Not used\n-        scheme : str\n-            Not used\n-\n-        Returns\n-        -------\n-        Tuple(str, str)\n-            Bearer, Token\n-\n-        Notes\n-        -----\n-        None of the parameters are used in this function. However,\n-        they are required because of the expected callback signature.\n-\n-        \"\"\"\n-        del (server, resource, scope, scheme)\n-        return \"Bearer\", self.token\n-\n-\n-# pylint: enable=too-many-instance-attributes\n-\n-\n-@export\n-class KeyringAuthClient(AuthClient):\n-    \"\"\"\n-    Key Authentication Client.\n-\n-    Handles management of authentication and refresh tokens\n-    via keyring\n-    \"\"\"\n-\n-    # pylint: disable=too-many-arguments\n-    def __init__(\n-        self,\n-        tenant_id: str,\n-        client_id: str,\n-        client_url: str,\n-        name: str = None,\n-        debug: bool = False,\n-    ):\n-        \"\"\"\n-        Initialize KeyringAuthClient.\n-\n-        Parameters\n-        ----------\n-        tenant_id : str\n-            Tenant ID of Azure User\n-        client_id : str\n-            Client ID of application client\n-        client_url : str\n-            [description]\n-        name : str, optional\n-            Name of the secret store, by default None\n-        debug : bool, optional\n-            Output debug information if True, by default False\n-\n-        \"\"\"\n-        self.name = name\n-        self.keyring = self.auth_id\n-        super().__init__(tenant_id, client_id, client_url, name=name, debug=debug)\n-\n-    # pylint: enable=too-many-arguments\n-\n-    def _get_creds(self):\n-        if self.debug:\n-            print(\"Fetching creds from keyring\")\n-        try:\n-            access_token = (\n-                keyring.get_password(self.keyring, \"adal_context_1\")\n-                + keyring.get_password(self.keyring, \"adal_context_2\")\n-                + keyring.get_password(self.keyring, \"adal_context_3\")\n-                + keyring.get_password(self.keyring, \"adal_context_4\")\n-            )\n-            refresh_token = (\n-                keyring.get_password(self.keyring, \"adal_context_5\")\n-                + keyring.get_password(self.keyring, \"adal_context_6\")\n-                + keyring.get_password(self.keyring, \"adal_context_7\")\n-            )\n-            expires_on = keyring.get_password(self.keyring, \"adal_context_8\")\n-            self.config_data = {\n-                \"accessToken\": access_token,\n-                \"refreshToken\": refresh_token,\n-                \"expiresOn\": expires_on,\n-            }\n-        except (TypeError, KeyringError):\n-            if self.debug:\n-                print(\"No valid credentials in keyring %s\" % self.keyring)\n-            self._get_token()\n-        if not self._is_valid_config_data():\n-            if self.debug:\n-                print(\"No valid authtoken config found in keyring\")\n-            self._get_token()\n-\n-    def _cache_creds(self):\n-        if self.debug:\n-            print(\"Saving config data to keyring %s\" % self.keyring)\n-        keyring.set_password(\n-            self.keyring, \"adal_context_1\", self.config_data[\"accessToken\"][:400]\n-        )\n-        keyring.set_password(\n-            self.keyring, \"adal_context_2\", self.config_data[\"accessToken\"][400:800]\n-        )\n-        keyring.set_password(\n-            self.keyring, \"adal_context_3\", self.config_data[\"accessToken\"][800:1200]\n-        )\n-        keyring.set_password(\n-            self.keyring, \"adal_context_4\", self.config_data[\"accessToken\"][1200:]\n-        )\n-        keyring.set_password(\n-            self.keyring, \"adal_context_5\", self.config_data[\"refreshToken\"][:400]\n-        )\n-        keyring.set_password(\n-            self.keyring, \"adal_context_6\", self.config_data[\"refreshToken\"][400:800]\n-        )\n-        keyring.set_password(\n-            self.keyring, \"adal_context_7\", self.config_data[\"refreshToken\"][800:]\n-        )\n-        keyring.set_password(\n-            self.keyring, \"adal_context_8\", self.config_data[\"expiresOn\"]\n-        )\n-\n-\n-# class KeyVaultAuthClient(AuthClient):\n-#     \"\"\"\n-#     Keyvault Auth client.\n-\n-#     Handles management of authentication tokens in keyvault.\n-#     \"\"\"\n-\n-#     def __init__(\n-#         self,\n-#         tenant_id: str,\n-#         client_id: str,\n-#         client_url: str,\n-#         secret_name: str,\n-#         name: str = None,\n-#         debug: bool = False,\n-#     ):\n-#         \"\"\"\n-#         Initialize KeyvaultAuthClient.\n-\n-#         Parameters\n-#         ----------\n-#         tenant_id : str\n-#             Tenant ID of Azure User\n-#         client_id : str\n-#             Client ID of application client\n-#         client_url : str\n-#             [description]\n-#         name : str, optional\n-#             Name of the secret store, by default None\n-#         debug : bool, optional\n-#             Output debug information if True, by default False\n-\n-#         \"\"\"\n-#         self.secret_name = secret_name\n-#         self._get_creds = self._get_keyvault_creds\n-#         self._cache_creds = self._cache_creds_keyvault\n-#         self.keyvault_client = BHKeyVaultClient(\n-#             tenant_id=tenant_id, vault_uri=client_url\n-#         )\n-#         self.config_data: Any = None\n-#         super().__init__(tenant_id, client_id, client_url, name=name, debug=debug)\n-\n-#     def _get_keyvault_creds(self):\n-#         if self.debug:\n-#             print(\"getting tokens from keyvault\")\n-#         try:\n-#             self.config_data = json.loads(\n-#                 self.keyvault_client.get_secret(self.secret_name)\n-#             )\n-#         except KeyVaultMissingSecretException:\n-#             if self.debug:\n-#                 print(\"missing secret from keyvault, fetching manually\")\n-#             self._get_token()\n-#         except KeyVaultErrorException as err:\n-#             if self.debug:\n-#                 print(\"bad creds in keyvault, you gotta getem\")\n-#                 print(\"here is what went wrong: %s\" % str(err))\n-#             self._get_token()\n-\n-#     def _cache_creds_keyvault(self):\n-#         self.keyvault_client.set_secret(self.secret_name, json.dumps(self.config_data))\n \n",
        "source_code_with_indent": "__author__ = \"Matt Richard, Ian Hellen\"\n\n\n# pylint: disable=too-many-instance-attributes\n@export\nclass AuthClient:\n    <IND>\"\"\"Authentication class base.\"\"\"\n\n    def __init__(\n        self,\n        tenant_id: str,\n        client_id: str,\n        client_uri: str,\n        name: str = None,\n        **kwargs,\n    ):\n        <IND>\"\"\"\n        Initialize base authentication client for credential caching.\n\n        Parameters\n        ----------\n        tenant_id : str\n            Tenant ID of Azure User\n        client_id : str\n            Client ID of application client\n        client_uri : str\n            [description]\n        name : str, optional\n            Name of the secret store, by default None\n        authority : str, optional\n            The AAD authority - one of 'global', 'usgov', 'de' or 'chi'\n        authority_uri : str, optional\n            The AAD authority URI - overrides `authority`\n        debug : bool, optional\n            Output debug information if True, by default False\n\n        Notes\n        -----\n        The parameter values can also be obtained from the\n        KeyVault section of msticpyconfig.yaml.\n\n        \"\"\"\n        self.name = name\n        self.debug = kwargs.pop(\"debug\", False)\n        self.settings: KeyVaultSettings = (\n            kwargs.pop(\"settings\", None) or KeyVaultSettings()\n        )\n        self.tenant_id = tenant_id or self.settings.get(\"tenantid\")\n        if not self.tenant_id:\n            <IND>raise MsticpyKeyVaultConfigError(\n                \"Could not get TenantId from function parameters or configuration.\",\n                \"Please add this to the KeyVault section of msticpyconfig.yaml\",\n                title=\"missing tenant ID value.\",\n            )\n        <DED>self.authority = kwargs.pop(\n            \"authority\", self.settings.get_tenant_authority_host(tenant_id)\n        )\n        self.client_id = client_id or self.settings.CLIENT_ID\n        self.client_uri = client_uri\n        self.authority_uri = self.settings.get_tenant_authority_uri(\n            authority_uri=kwargs.get(\"authority_uri\"), tenant=self.tenant_id\n        )\n\n        if self.debug:\n            <IND>print(\"AuthClient for %s - %s\" % (client_id, client_uri))\n        <DED>self._get_creds()\n        if self._expired_creds:\n            <IND>if self.debug:\n                <IND>print(\"expired creds\")\n            <DED>try:\n                <IND>self._refresh_creds()\n                return\n            <DED>except AdalError:\n                <IND>if self.debug:\n                    <IND>print(\"Token was no longer valid, forcing a new one.\")\n            <DED><DED>self._get_token()\n\n    <DED><DED>def _get_token(self):\n        <IND>context = AuthenticationContext(self.authority_uri)\n        code = context.acquire_user_code(self.client_uri, self.client_id)\n        _prompt_for_code(code)\n        self.config_data = context.acquire_token_with_device_code(\n            self.client_uri, code, self.client_id\n        )\n        self._cache_creds()\n\n    <DED>def _get_creds(self):\n        <IND>self._get_token()\n\n    <DED>def _cache_creds(self):\n        <IND>pass\n\n    <DED>def _is_valid_config_data(self):\n        <IND>keys = [\"accessToken\", \"refreshToken\", \"expiresOn\"]\n        return (\n            all(key in self.config_data for key in keys)\n            and all(self.config_data.get(key) for key in keys)\n            and all(len(self.config_data.get(key)) > 0 for key in keys)\n        )\n\n    <DED>@property\n    def auth_id(self) -> str:\n        <IND>\"\"\"Return name or ID of client.\"\"\"\n        return self.name if self.name is not None else self.client_id\n\n    <DED>@property\n    def user_oid(self) -> str:\n        <IND>\"\"\"\n        Return the user Object ID.\n\n        Returns\n        -------\n        str\n            User OID.\n\n        \"\"\"\n        data = self._get_parsed_token_data()\n        return data.get(\"oid\")\n\n    <DED>def _get_parsed_token_data(self) -> Any:\n        <IND>tok_data = self.token\n        tok_data = tok_data.split(\".\")[1]\n        tok_data += \"=\" * ((4 - len(tok_data) % 4) % 4)\n        return json.loads(base64.b64decode(tok_data))\n\n    <DED>def _refresh_creds(self):\n        <IND>context = AuthenticationContext(self.authority_uri)\n        self.config_data = context.acquire_token_with_refresh_token(\n            self.config_data[\"refreshToken\"], self.client_id, self.client_uri\n        )\n        if self.debug:\n            <IND>print(f\"got new token expiring {self.config_data['expiresOn']}\")\n            self._cache_creds()\n\n    <DED><DED>@property\n    def _expired_creds(self) -> bool:\n        <IND>return self._expires_on < datetime.now()\n\n    <DED>@property\n    def _expires_on(self) -> datetime:\n        <IND>\"\"\"Return token expiry date as string.\"\"\"\n        return datetime.strptime(self.config_data[\"expiresOn\"], \"%Y-%m-%d %H:%M:%S.%f\")\n\n    <DED>@property\n    def token(self) -> str:\n        <IND>\"\"\"\n        Return the access token.\n\n        Returns\n        -------\n        str\n            Access Token\n\n        \"\"\"\n        if self._expired_creds:\n            <IND>try:\n                <IND>self._refresh_creds()\n            <DED>except AdalError:\n                <IND>self._get_token()\n        <DED><DED>return self.config_data[\"accessToken\"]\n\n    <DED>def _adal_callback(self, server: str, resource: str, scope: str, scheme: str):\n        <IND>\"\"\"\n        ADAL Callback for authentication.\n\n        Parameters\n        ----------\n        server : str\n            Not used\n        resource : str\n            Not used\n        scope : str\n            Not used\n        scheme : str\n            Not used\n\n        Returns\n        -------\n        Tuple(str, str)\n            Bearer, Token\n\n        Notes\n        -----\n        None of the parameters are used in this function. However,\n        they are required because of the expected callback signature.\n\n        \"\"\"\n        del (server, resource, scope, scheme)\n        return \"Bearer\", self.token\n\n\n# pylint: enable=too-many-instance-attributes\n\n\n<DED><DED>@export\nclass KeyringAuthClient(AuthClient):\n    <IND>\"\"\"\n    Key Authentication Client.\n\n    Handles management of authentication and refresh tokens\n    via keyring\n    \"\"\"\n\n    # pylint: disable=too-many-arguments\n    def __init__(\n        self,\n        tenant_id: str,\n        client_id: str,\n        client_url: str,\n        name: str = None,\n        debug: bool = False,\n    ):\n        <IND>\"\"\"\n        Initialize KeyringAuthClient.\n\n        Parameters\n        ----------\n        tenant_id : str\n            Tenant ID of Azure User\n        client_id : str\n            Client ID of application client\n        client_url : str\n            [description]\n        name : str, optional\n            Name of the secret store, by default None\n        debug : bool, optional\n            Output debug information if True, by default False\n\n        \"\"\"\n        self.name = name\n        self.keyring = self.auth_id\n        super().__init__(tenant_id, client_id, client_url, name=name, debug=debug)\n\n    # pylint: enable=too-many-arguments\n\n    <DED>def _get_creds(self):\n        <IND>if self.debug:\n            <IND>print(\"Fetching creds from keyring\")\n        <DED>try:\n            <IND>access_token = (\n                keyring.get_password(self.keyring, \"adal_context_1\")\n                + keyring.get_password(self.keyring, \"adal_context_2\")\n                + keyring.get_password(self.keyring, \"adal_context_3\")\n                + keyring.get_password(self.keyring, \"adal_context_4\")\n            )\n            refresh_token = (\n                keyring.get_password(self.keyring, \"adal_context_5\")\n                + keyring.get_password(self.keyring, \"adal_context_6\")\n                + keyring.get_password(self.keyring, \"adal_context_7\")\n            )\n            expires_on = keyring.get_password(self.keyring, \"adal_context_8\")\n            self.config_data = {\n                \"accessToken\": access_token,\n                \"refreshToken\": refresh_token,\n                \"expiresOn\": expires_on,\n            }\n        <DED>except (TypeError, KeyringError):\n            <IND>if self.debug:\n                <IND>print(\"No valid credentials in keyring %s\" % self.keyring)\n            <DED>self._get_token()\n        <DED>if not self._is_valid_config_data():\n            <IND>if self.debug:\n                <IND>print(\"No valid authtoken config found in keyring\")\n            <DED>self._get_token()\n\n    <DED><DED>def _cache_creds(self):\n        <IND>if self.debug:\n            <IND>print(\"Saving config data to keyring %s\" % self.keyring)\n        <DED>keyring.set_password(\n            self.keyring, \"adal_context_1\", self.config_data[\"accessToken\"][:400]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_2\", self.config_data[\"accessToken\"][400:800]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_3\", self.config_data[\"accessToken\"][800:1200]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_4\", self.config_data[\"accessToken\"][1200:]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_5\", self.config_data[\"refreshToken\"][:400]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_6\", self.config_data[\"refreshToken\"][400:800]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_7\", self.config_data[\"refreshToken\"][800:]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_8\", self.config_data[\"expiresOn\"]\n        )\n\n\n# class KeyVaultAuthClient(AuthClient):\n#     \"\"\"\n#     Keyvault Auth client.\n\n#     Handles management of authentication tokens in keyvault.\n#     \"\"\"\n\n#     def __init__(\n#         self,\n#         tenant_id: str,\n#         client_id: str,\n#         client_url: str,\n#         secret_name: str,\n#         name: str = None,\n#         debug: bool = False,\n#     ):\n#         \"\"\"\n#         Initialize KeyvaultAuthClient.\n\n#         Parameters\n#         ----------\n#         tenant_id : str\n#             Tenant ID of Azure User\n#         client_id : str\n#             Client ID of application client\n#         client_url : str\n#             [description]\n#         name : str, optional\n#             Name of the secret store, by default None\n#         debug : bool, optional\n#             Output debug information if True, by default False\n\n#         \"\"\"\n#         self.secret_name = secret_name\n#         self._get_creds = self._get_keyvault_creds\n#         self._cache_creds = self._cache_creds_keyvault\n#         self.keyvault_client = BHKeyVaultClient(\n#             tenant_id=tenant_id, vault_uri=client_url\n#         )\n#         self.config_data: Any = None\n#         super().__init__(tenant_id, client_id, client_url, name=name, debug=debug)\n\n#     def _get_keyvault_creds(self):\n#         if self.debug:\n#             print(\"getting tokens from keyvault\")\n#         try:\n#             self.config_data = json.loads(\n#                 self.keyvault_client.get_secret(self.secret_name)\n#             )\n#         except KeyVaultMissingSecretException:\n#             if self.debug:\n#                 print(\"missing secret from keyvault, fetching manually\")\n#             self._get_token()\n#         except KeyVaultErrorException as err:\n#             if self.debug:\n#                 print(\"bad creds in keyvault, you gotta getem\")\n#                 print(\"here is what went wrong: %s\" % str(err))\n#             self._get_token()\n\n#     def _cache_creds_keyvault(self):\n#         self.keyvault_client.set_secret(self.secret_name, json.dumps(self.config_data))\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "__author__ = \"Matt Richard, Ian Hellen\"\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "microsoft/msticpy",
    "commit": "28466d681e261394e18d9f4e063cebfa06ed04b4",
    "filename": "msticpy/common/keyvault_client.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/microsoft-msticpy/msticpy/common/keyvault_client.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "msticpy/common/keyvault_client.py:250:8 Incompatible variable type [9]: name is declared to have type `str` but is used as type `None`.",
    "message": " name is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 250,
    "warning_line": "        name: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "__author__ = \"Matt Richard, Ian Hellen\"\n\n\n# pylint: disable=too-many-instance-attributes\n@export\nclass AuthClient:\n    \"\"\"Authentication class base.\"\"\"\n\n    def __init__(\n        self,\n        tenant_id: str,\n        client_id: str,\n        client_uri: str,\n        name: str = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize base authentication client for credential caching.\n\n        Parameters\n        ----------\n        tenant_id : str\n            Tenant ID of Azure User\n        client_id : str\n            Client ID of application client\n        client_uri : str\n            [description]\n        name : str, optional\n            Name of the secret store, by default None\n        authority : str, optional\n            The AAD authority - one of 'global', 'usgov', 'de' or 'chi'\n        authority_uri : str, optional\n            The AAD authority URI - overrides `authority`\n        debug : bool, optional\n            Output debug information if True, by default False\n\n        Notes\n        -----\n        The parameter values can also be obtained from the\n        KeyVault section of msticpyconfig.yaml.\n\n        \"\"\"\n        self.name = name\n        self.debug = kwargs.pop(\"debug\", False)\n        self.settings: KeyVaultSettings = (\n            kwargs.pop(\"settings\", None) or KeyVaultSettings()\n        )\n        self.tenant_id = tenant_id or self.settings.get(\"tenantid\")\n        if not self.tenant_id:\n            raise MsticpyKeyVaultConfigError(\n                \"Could not get TenantId from function parameters or configuration.\",\n                \"Please add this to the KeyVault section of msticpyconfig.yaml\",\n                title=\"missing tenant ID value.\",\n            )\n        self.authority = kwargs.pop(\n            \"authority\", self.settings.get_tenant_authority_host(tenant_id)\n        )\n        self.client_id = client_id or self.settings.CLIENT_ID\n        self.client_uri = client_uri\n        self.authority_uri = self.settings.get_tenant_authority_uri(\n            authority_uri=kwargs.get(\"authority_uri\"), tenant=self.tenant_id\n        )\n\n        if self.debug:\n            print(\"AuthClient for %s - %s\" % (client_id, client_uri))\n        self._get_creds()\n        if self._expired_creds:\n            if self.debug:\n                print(\"expired creds\")\n            try:\n                self._refresh_creds()\n                return\n            except AdalError:\n                if self.debug:\n                    print(\"Token was no longer valid, forcing a new one.\")\n            self._get_token()\n\n    def _get_token(self):\n        context = AuthenticationContext(self.authority_uri)\n        code = context.acquire_user_code(self.client_uri, self.client_id)\n        _prompt_for_code(code)\n        self.config_data = context.acquire_token_with_device_code(\n            self.client_uri, code, self.client_id\n        )\n        self._cache_creds()\n\n    def _get_creds(self):\n        self._get_token()\n\n    def _cache_creds(self):\n        pass\n\n    def _is_valid_config_data(self):\n        keys = [\"accessToken\", \"refreshToken\", \"expiresOn\"]\n        return (\n            all(key in self.config_data for key in keys)\n            and all(self.config_data.get(key) for key in keys)\n            and all(len(self.config_data.get(key)) > 0 for key in keys)\n        )\n\n    @property\n    def auth_id(self) -> str:\n        \"\"\"Return name or ID of client.\"\"\"\n        return self.name if self.name is not None else self.client_id\n\n    @property\n    def user_oid(self) -> str:\n        \"\"\"\n        Return the user Object ID.\n\n        Returns\n        -------\n        str\n            User OID.\n\n        \"\"\"\n        data = self._get_parsed_token_data()\n        return data.get(\"oid\")\n\n    def _get_parsed_token_data(self) -> Any:\n        tok_data = self.token\n        tok_data = tok_data.split(\".\")[1]\n        tok_data += \"=\" * ((4 - len(tok_data) % 4) % 4)\n        return json.loads(base64.b64decode(tok_data))\n\n    def _refresh_creds(self):\n        context = AuthenticationContext(self.authority_uri)\n        self.config_data = context.acquire_token_with_refresh_token(\n            self.config_data[\"refreshToken\"], self.client_id, self.client_uri\n        )\n        if self.debug:\n            print(f\"got new token expiring {self.config_data['expiresOn']}\")\n            self._cache_creds()\n\n    @property\n    def _expired_creds(self) -> bool:\n        return self._expires_on < datetime.now()\n\n    @property\n    def _expires_on(self) -> datetime:\n        \"\"\"Return token expiry date as string.\"\"\"\n        return datetime.strptime(self.config_data[\"expiresOn\"], \"%Y-%m-%d %H:%M:%S.%f\")\n\n    @property\n    def token(self) -> str:\n        \"\"\"\n        Return the access token.\n\n        Returns\n        -------\n        str\n            Access Token\n\n        \"\"\"\n        if self._expired_creds:\n            try:\n                self._refresh_creds()\n            except AdalError:\n                self._get_token()\n        return self.config_data[\"accessToken\"]\n\n    def _adal_callback(self, server: str, resource: str, scope: str, scheme: str):\n        \"\"\"\n        ADAL Callback for authentication.\n\n        Parameters\n        ----------\n        server : str\n            Not used\n        resource : str\n            Not used\n        scope : str\n            Not used\n        scheme : str\n            Not used\n\n        Returns\n        -------\n        Tuple(str, str)\n            Bearer, Token\n\n        Notes\n        -----\n        None of the parameters are used in this function. However,\n        they are required because of the expected callback signature.\n\n        \"\"\"\n        del (server, resource, scope, scheme)\n        return \"Bearer\", self.token\n\n\n# pylint: enable=too-many-instance-attributes\n\n\n@export\nclass KeyringAuthClient(AuthClient):\n    \"\"\"\n    Key Authentication Client.\n\n    Handles management of authentication and refresh tokens\n    via keyring\n    \"\"\"\n\n    # pylint: disable=too-many-arguments\n    def __init__(\n        self,\n        tenant_id: str,\n        client_id: str,\n        client_url: str,\n        name: str = None,\n        debug: bool = False,\n    ):\n        \"\"\"\n        Initialize KeyringAuthClient.\n\n        Parameters\n        ----------\n        tenant_id : str\n            Tenant ID of Azure User\n        client_id : str\n            Client ID of application client\n        client_url : str\n            [description]\n        name : str, optional\n            Name of the secret store, by default None\n        debug : bool, optional\n            Output debug information if True, by default False\n\n        \"\"\"\n        self.name = name\n        self.keyring = self.auth_id\n        super().__init__(tenant_id, client_id, client_url, name=name, debug=debug)\n\n    # pylint: enable=too-many-arguments\n\n    def _get_creds(self):\n        if self.debug:\n            print(\"Fetching creds from keyring\")\n        try:\n            access_token = (\n                keyring.get_password(self.keyring, \"adal_context_1\")\n                + keyring.get_password(self.keyring, \"adal_context_2\")\n                + keyring.get_password(self.keyring, \"adal_context_3\")\n                + keyring.get_password(self.keyring, \"adal_context_4\")\n            )\n            refresh_token = (\n                keyring.get_password(self.keyring, \"adal_context_5\")\n                + keyring.get_password(self.keyring, \"adal_context_6\")\n                + keyring.get_password(self.keyring, \"adal_context_7\")\n            )\n            expires_on = keyring.get_password(self.keyring, \"adal_context_8\")\n            self.config_data = {\n                \"accessToken\": access_token,\n                \"refreshToken\": refresh_token,\n                \"expiresOn\": expires_on,\n            }\n        except (TypeError, KeyringError):\n            if self.debug:\n                print(\"No valid credentials in keyring %s\" % self.keyring)\n            self._get_token()\n        if not self._is_valid_config_data():\n            if self.debug:\n                print(\"No valid authtoken config found in keyring\")\n            self._get_token()\n\n    def _cache_creds(self):\n        if self.debug:\n            print(\"Saving config data to keyring %s\" % self.keyring)\n        keyring.set_password(\n            self.keyring, \"adal_context_1\", self.config_data[\"accessToken\"][:400]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_2\", self.config_data[\"accessToken\"][400:800]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_3\", self.config_data[\"accessToken\"][800:1200]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_4\", self.config_data[\"accessToken\"][1200:]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_5\", self.config_data[\"refreshToken\"][:400]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_6\", self.config_data[\"refreshToken\"][400:800]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_7\", self.config_data[\"refreshToken\"][800:]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_8\", self.config_data[\"expiresOn\"]\n        )\n\n\n# class KeyVaultAuthClient(AuthClient):\n#     \"\"\"\n#     Keyvault Auth client.\n\n#     Handles management of authentication tokens in keyvault.\n#     \"\"\"\n\n#     def __init__(\n#         self,\n#         tenant_id: str,\n#         client_id: str,\n#         client_url: str,\n#         secret_name: str,\n#         name: str = None,\n#         debug: bool = False,\n#     ):\n#         \"\"\"\n#         Initialize KeyvaultAuthClient.\n\n#         Parameters\n#         ----------\n#         tenant_id : str\n#             Tenant ID of Azure User\n#         client_id : str\n#             Client ID of application client\n#         client_url : str\n#             [description]\n#         name : str, optional\n#             Name of the secret store, by default None\n#         debug : bool, optional\n#             Output debug information if True, by default False\n\n#         \"\"\"\n#         self.secret_name = secret_name\n#         self._get_creds = self._get_keyvault_creds\n#         self._cache_creds = self._cache_creds_keyvault\n#         self.keyvault_client = BHKeyVaultClient(\n#             tenant_id=tenant_id, vault_uri=client_url\n#         )\n#         self.config_data: Any = None\n#         super().__init__(tenant_id, client_id, client_url, name=name, debug=debug)\n\n#     def _get_keyvault_creds(self):\n#         if self.debug:\n#             print(\"getting tokens from keyvault\")\n#         try:\n#             self.config_data = json.loads(\n#                 self.keyvault_client.get_secret(self.secret_name)\n#             )\n#         except KeyVaultMissingSecretException:\n#             if self.debug:\n#                 print(\"missing secret from keyvault, fetching manually\")\n#             self._get_token()\n#         except KeyVaultErrorException as err:\n#             if self.debug:\n#                 print(\"bad creds in keyvault, you gotta getem\")\n#                 print(\"here is what went wrong: %s\" % str(err))\n#             self._get_token()\n\n#     def _cache_creds_keyvault(self):\n#         self.keyvault_client.set_secret(self.secret_name, json.dumps(self.config_data))\n\n",
        "source_code_len": 11202,
        "target_code": "__author__ = \"Matt Richard, Ian Hellen\"\n\n",
        "target_code_len": 41,
        "diff_format": "@@ -41,356 +36,2 @@\n __author__ = \"Matt Richard, Ian Hellen\"\n-\n-\n-# pylint: disable=too-many-instance-attributes\n-@export\n-class AuthClient:\n-    \"\"\"Authentication class base.\"\"\"\n-\n-    def __init__(\n-        self,\n-        tenant_id: str,\n-        client_id: str,\n-        client_uri: str,\n-        name: str = None,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Initialize base authentication client for credential caching.\n-\n-        Parameters\n-        ----------\n-        tenant_id : str\n-            Tenant ID of Azure User\n-        client_id : str\n-            Client ID of application client\n-        client_uri : str\n-            [description]\n-        name : str, optional\n-            Name of the secret store, by default None\n-        authority : str, optional\n-            The AAD authority - one of 'global', 'usgov', 'de' or 'chi'\n-        authority_uri : str, optional\n-            The AAD authority URI - overrides `authority`\n-        debug : bool, optional\n-            Output debug information if True, by default False\n-\n-        Notes\n-        -----\n-        The parameter values can also be obtained from the\n-        KeyVault section of msticpyconfig.yaml.\n-\n-        \"\"\"\n-        self.name = name\n-        self.debug = kwargs.pop(\"debug\", False)\n-        self.settings: KeyVaultSettings = (\n-            kwargs.pop(\"settings\", None) or KeyVaultSettings()\n-        )\n-        self.tenant_id = tenant_id or self.settings.get(\"tenantid\")\n-        if not self.tenant_id:\n-            raise MsticpyKeyVaultConfigError(\n-                \"Could not get TenantId from function parameters or configuration.\",\n-                \"Please add this to the KeyVault section of msticpyconfig.yaml\",\n-                title=\"missing tenant ID value.\",\n-            )\n-        self.authority = kwargs.pop(\n-            \"authority\", self.settings.get_tenant_authority_host(tenant_id)\n-        )\n-        self.client_id = client_id or self.settings.CLIENT_ID\n-        self.client_uri = client_uri\n-        self.authority_uri = self.settings.get_tenant_authority_uri(\n-            authority_uri=kwargs.get(\"authority_uri\"), tenant=self.tenant_id\n-        )\n-\n-        if self.debug:\n-            print(\"AuthClient for %s - %s\" % (client_id, client_uri))\n-        self._get_creds()\n-        if self._expired_creds:\n-            if self.debug:\n-                print(\"expired creds\")\n-            try:\n-                self._refresh_creds()\n-                return\n-            except AdalError:\n-                if self.debug:\n-                    print(\"Token was no longer valid, forcing a new one.\")\n-            self._get_token()\n-\n-    def _get_token(self):\n-        context = AuthenticationContext(self.authority_uri)\n-        code = context.acquire_user_code(self.client_uri, self.client_id)\n-        _prompt_for_code(code)\n-        self.config_data = context.acquire_token_with_device_code(\n-            self.client_uri, code, self.client_id\n-        )\n-        self._cache_creds()\n-\n-    def _get_creds(self):\n-        self._get_token()\n-\n-    def _cache_creds(self):\n-        pass\n-\n-    def _is_valid_config_data(self):\n-        keys = [\"accessToken\", \"refreshToken\", \"expiresOn\"]\n-        return (\n-            all(key in self.config_data for key in keys)\n-            and all(self.config_data.get(key) for key in keys)\n-            and all(len(self.config_data.get(key)) > 0 for key in keys)\n-        )\n-\n-    @property\n-    def auth_id(self) -> str:\n-        \"\"\"Return name or ID of client.\"\"\"\n-        return self.name if self.name is not None else self.client_id\n-\n-    @property\n-    def user_oid(self) -> str:\n-        \"\"\"\n-        Return the user Object ID.\n-\n-        Returns\n-        -------\n-        str\n-            User OID.\n-\n-        \"\"\"\n-        data = self._get_parsed_token_data()\n-        return data.get(\"oid\")\n-\n-    def _get_parsed_token_data(self) -> Any:\n-        tok_data = self.token\n-        tok_data = tok_data.split(\".\")[1]\n-        tok_data += \"=\" * ((4 - len(tok_data) % 4) % 4)\n-        return json.loads(base64.b64decode(tok_data))\n-\n-    def _refresh_creds(self):\n-        context = AuthenticationContext(self.authority_uri)\n-        self.config_data = context.acquire_token_with_refresh_token(\n-            self.config_data[\"refreshToken\"], self.client_id, self.client_uri\n-        )\n-        if self.debug:\n-            print(f\"got new token expiring {self.config_data['expiresOn']}\")\n-            self._cache_creds()\n-\n-    @property\n-    def _expired_creds(self) -> bool:\n-        return self._expires_on < datetime.now()\n-\n-    @property\n-    def _expires_on(self) -> datetime:\n-        \"\"\"Return token expiry date as string.\"\"\"\n-        return datetime.strptime(self.config_data[\"expiresOn\"], \"%Y-%m-%d %H:%M:%S.%f\")\n-\n-    @property\n-    def token(self) -> str:\n-        \"\"\"\n-        Return the access token.\n-\n-        Returns\n-        -------\n-        str\n-            Access Token\n-\n-        \"\"\"\n-        if self._expired_creds:\n-            try:\n-                self._refresh_creds()\n-            except AdalError:\n-                self._get_token()\n-        return self.config_data[\"accessToken\"]\n-\n-    def _adal_callback(self, server: str, resource: str, scope: str, scheme: str):\n-        \"\"\"\n-        ADAL Callback for authentication.\n-\n-        Parameters\n-        ----------\n-        server : str\n-            Not used\n-        resource : str\n-            Not used\n-        scope : str\n-            Not used\n-        scheme : str\n-            Not used\n-\n-        Returns\n-        -------\n-        Tuple(str, str)\n-            Bearer, Token\n-\n-        Notes\n-        -----\n-        None of the parameters are used in this function. However,\n-        they are required because of the expected callback signature.\n-\n-        \"\"\"\n-        del (server, resource, scope, scheme)\n-        return \"Bearer\", self.token\n-\n-\n-# pylint: enable=too-many-instance-attributes\n-\n-\n-@export\n-class KeyringAuthClient(AuthClient):\n-    \"\"\"\n-    Key Authentication Client.\n-\n-    Handles management of authentication and refresh tokens\n-    via keyring\n-    \"\"\"\n-\n-    # pylint: disable=too-many-arguments\n-    def __init__(\n-        self,\n-        tenant_id: str,\n-        client_id: str,\n-        client_url: str,\n-        name: str = None,\n-        debug: bool = False,\n-    ):\n-        \"\"\"\n-        Initialize KeyringAuthClient.\n-\n-        Parameters\n-        ----------\n-        tenant_id : str\n-            Tenant ID of Azure User\n-        client_id : str\n-            Client ID of application client\n-        client_url : str\n-            [description]\n-        name : str, optional\n-            Name of the secret store, by default None\n-        debug : bool, optional\n-            Output debug information if True, by default False\n-\n-        \"\"\"\n-        self.name = name\n-        self.keyring = self.auth_id\n-        super().__init__(tenant_id, client_id, client_url, name=name, debug=debug)\n-\n-    # pylint: enable=too-many-arguments\n-\n-    def _get_creds(self):\n-        if self.debug:\n-            print(\"Fetching creds from keyring\")\n-        try:\n-            access_token = (\n-                keyring.get_password(self.keyring, \"adal_context_1\")\n-                + keyring.get_password(self.keyring, \"adal_context_2\")\n-                + keyring.get_password(self.keyring, \"adal_context_3\")\n-                + keyring.get_password(self.keyring, \"adal_context_4\")\n-            )\n-            refresh_token = (\n-                keyring.get_password(self.keyring, \"adal_context_5\")\n-                + keyring.get_password(self.keyring, \"adal_context_6\")\n-                + keyring.get_password(self.keyring, \"adal_context_7\")\n-            )\n-            expires_on = keyring.get_password(self.keyring, \"adal_context_8\")\n-            self.config_data = {\n-                \"accessToken\": access_token,\n-                \"refreshToken\": refresh_token,\n-                \"expiresOn\": expires_on,\n-            }\n-        except (TypeError, KeyringError):\n-            if self.debug:\n-                print(\"No valid credentials in keyring %s\" % self.keyring)\n-            self._get_token()\n-        if not self._is_valid_config_data():\n-            if self.debug:\n-                print(\"No valid authtoken config found in keyring\")\n-            self._get_token()\n-\n-    def _cache_creds(self):\n-        if self.debug:\n-            print(\"Saving config data to keyring %s\" % self.keyring)\n-        keyring.set_password(\n-            self.keyring, \"adal_context_1\", self.config_data[\"accessToken\"][:400]\n-        )\n-        keyring.set_password(\n-            self.keyring, \"adal_context_2\", self.config_data[\"accessToken\"][400:800]\n-        )\n-        keyring.set_password(\n-            self.keyring, \"adal_context_3\", self.config_data[\"accessToken\"][800:1200]\n-        )\n-        keyring.set_password(\n-            self.keyring, \"adal_context_4\", self.config_data[\"accessToken\"][1200:]\n-        )\n-        keyring.set_password(\n-            self.keyring, \"adal_context_5\", self.config_data[\"refreshToken\"][:400]\n-        )\n-        keyring.set_password(\n-            self.keyring, \"adal_context_6\", self.config_data[\"refreshToken\"][400:800]\n-        )\n-        keyring.set_password(\n-            self.keyring, \"adal_context_7\", self.config_data[\"refreshToken\"][800:]\n-        )\n-        keyring.set_password(\n-            self.keyring, \"adal_context_8\", self.config_data[\"expiresOn\"]\n-        )\n-\n-\n-# class KeyVaultAuthClient(AuthClient):\n-#     \"\"\"\n-#     Keyvault Auth client.\n-\n-#     Handles management of authentication tokens in keyvault.\n-#     \"\"\"\n-\n-#     def __init__(\n-#         self,\n-#         tenant_id: str,\n-#         client_id: str,\n-#         client_url: str,\n-#         secret_name: str,\n-#         name: str = None,\n-#         debug: bool = False,\n-#     ):\n-#         \"\"\"\n-#         Initialize KeyvaultAuthClient.\n-\n-#         Parameters\n-#         ----------\n-#         tenant_id : str\n-#             Tenant ID of Azure User\n-#         client_id : str\n-#             Client ID of application client\n-#         client_url : str\n-#             [description]\n-#         name : str, optional\n-#             Name of the secret store, by default None\n-#         debug : bool, optional\n-#             Output debug information if True, by default False\n-\n-#         \"\"\"\n-#         self.secret_name = secret_name\n-#         self._get_creds = self._get_keyvault_creds\n-#         self._cache_creds = self._cache_creds_keyvault\n-#         self.keyvault_client = BHKeyVaultClient(\n-#             tenant_id=tenant_id, vault_uri=client_url\n-#         )\n-#         self.config_data: Any = None\n-#         super().__init__(tenant_id, client_id, client_url, name=name, debug=debug)\n-\n-#     def _get_keyvault_creds(self):\n-#         if self.debug:\n-#             print(\"getting tokens from keyvault\")\n-#         try:\n-#             self.config_data = json.loads(\n-#                 self.keyvault_client.get_secret(self.secret_name)\n-#             )\n-#         except KeyVaultMissingSecretException:\n-#             if self.debug:\n-#                 print(\"missing secret from keyvault, fetching manually\")\n-#             self._get_token()\n-#         except KeyVaultErrorException as err:\n-#             if self.debug:\n-#                 print(\"bad creds in keyvault, you gotta getem\")\n-#                 print(\"here is what went wrong: %s\" % str(err))\n-#             self._get_token()\n-\n-#     def _cache_creds_keyvault(self):\n-#         self.keyvault_client.set_secret(self.secret_name, json.dumps(self.config_data))\n \n",
        "source_code_with_indent": "__author__ = \"Matt Richard, Ian Hellen\"\n\n\n# pylint: disable=too-many-instance-attributes\n@export\nclass AuthClient:\n    <IND>\"\"\"Authentication class base.\"\"\"\n\n    def __init__(\n        self,\n        tenant_id: str,\n        client_id: str,\n        client_uri: str,\n        name: str = None,\n        **kwargs,\n    ):\n        <IND>\"\"\"\n        Initialize base authentication client for credential caching.\n\n        Parameters\n        ----------\n        tenant_id : str\n            Tenant ID of Azure User\n        client_id : str\n            Client ID of application client\n        client_uri : str\n            [description]\n        name : str, optional\n            Name of the secret store, by default None\n        authority : str, optional\n            The AAD authority - one of 'global', 'usgov', 'de' or 'chi'\n        authority_uri : str, optional\n            The AAD authority URI - overrides `authority`\n        debug : bool, optional\n            Output debug information if True, by default False\n\n        Notes\n        -----\n        The parameter values can also be obtained from the\n        KeyVault section of msticpyconfig.yaml.\n\n        \"\"\"\n        self.name = name\n        self.debug = kwargs.pop(\"debug\", False)\n        self.settings: KeyVaultSettings = (\n            kwargs.pop(\"settings\", None) or KeyVaultSettings()\n        )\n        self.tenant_id = tenant_id or self.settings.get(\"tenantid\")\n        if not self.tenant_id:\n            <IND>raise MsticpyKeyVaultConfigError(\n                \"Could not get TenantId from function parameters or configuration.\",\n                \"Please add this to the KeyVault section of msticpyconfig.yaml\",\n                title=\"missing tenant ID value.\",\n            )\n        <DED>self.authority = kwargs.pop(\n            \"authority\", self.settings.get_tenant_authority_host(tenant_id)\n        )\n        self.client_id = client_id or self.settings.CLIENT_ID\n        self.client_uri = client_uri\n        self.authority_uri = self.settings.get_tenant_authority_uri(\n            authority_uri=kwargs.get(\"authority_uri\"), tenant=self.tenant_id\n        )\n\n        if self.debug:\n            <IND>print(\"AuthClient for %s - %s\" % (client_id, client_uri))\n        <DED>self._get_creds()\n        if self._expired_creds:\n            <IND>if self.debug:\n                <IND>print(\"expired creds\")\n            <DED>try:\n                <IND>self._refresh_creds()\n                return\n            <DED>except AdalError:\n                <IND>if self.debug:\n                    <IND>print(\"Token was no longer valid, forcing a new one.\")\n            <DED><DED>self._get_token()\n\n    <DED><DED>def _get_token(self):\n        <IND>context = AuthenticationContext(self.authority_uri)\n        code = context.acquire_user_code(self.client_uri, self.client_id)\n        _prompt_for_code(code)\n        self.config_data = context.acquire_token_with_device_code(\n            self.client_uri, code, self.client_id\n        )\n        self._cache_creds()\n\n    <DED>def _get_creds(self):\n        <IND>self._get_token()\n\n    <DED>def _cache_creds(self):\n        <IND>pass\n\n    <DED>def _is_valid_config_data(self):\n        <IND>keys = [\"accessToken\", \"refreshToken\", \"expiresOn\"]\n        return (\n            all(key in self.config_data for key in keys)\n            and all(self.config_data.get(key) for key in keys)\n            and all(len(self.config_data.get(key)) > 0 for key in keys)\n        )\n\n    <DED>@property\n    def auth_id(self) -> str:\n        <IND>\"\"\"Return name or ID of client.\"\"\"\n        return self.name if self.name is not None else self.client_id\n\n    <DED>@property\n    def user_oid(self) -> str:\n        <IND>\"\"\"\n        Return the user Object ID.\n\n        Returns\n        -------\n        str\n            User OID.\n\n        \"\"\"\n        data = self._get_parsed_token_data()\n        return data.get(\"oid\")\n\n    <DED>def _get_parsed_token_data(self) -> Any:\n        <IND>tok_data = self.token\n        tok_data = tok_data.split(\".\")[1]\n        tok_data += \"=\" * ((4 - len(tok_data) % 4) % 4)\n        return json.loads(base64.b64decode(tok_data))\n\n    <DED>def _refresh_creds(self):\n        <IND>context = AuthenticationContext(self.authority_uri)\n        self.config_data = context.acquire_token_with_refresh_token(\n            self.config_data[\"refreshToken\"], self.client_id, self.client_uri\n        )\n        if self.debug:\n            <IND>print(f\"got new token expiring {self.config_data['expiresOn']}\")\n            self._cache_creds()\n\n    <DED><DED>@property\n    def _expired_creds(self) -> bool:\n        <IND>return self._expires_on < datetime.now()\n\n    <DED>@property\n    def _expires_on(self) -> datetime:\n        <IND>\"\"\"Return token expiry date as string.\"\"\"\n        return datetime.strptime(self.config_data[\"expiresOn\"], \"%Y-%m-%d %H:%M:%S.%f\")\n\n    <DED>@property\n    def token(self) -> str:\n        <IND>\"\"\"\n        Return the access token.\n\n        Returns\n        -------\n        str\n            Access Token\n\n        \"\"\"\n        if self._expired_creds:\n            <IND>try:\n                <IND>self._refresh_creds()\n            <DED>except AdalError:\n                <IND>self._get_token()\n        <DED><DED>return self.config_data[\"accessToken\"]\n\n    <DED>def _adal_callback(self, server: str, resource: str, scope: str, scheme: str):\n        <IND>\"\"\"\n        ADAL Callback for authentication.\n\n        Parameters\n        ----------\n        server : str\n            Not used\n        resource : str\n            Not used\n        scope : str\n            Not used\n        scheme : str\n            Not used\n\n        Returns\n        -------\n        Tuple(str, str)\n            Bearer, Token\n\n        Notes\n        -----\n        None of the parameters are used in this function. However,\n        they are required because of the expected callback signature.\n\n        \"\"\"\n        del (server, resource, scope, scheme)\n        return \"Bearer\", self.token\n\n\n# pylint: enable=too-many-instance-attributes\n\n\n<DED><DED>@export\nclass KeyringAuthClient(AuthClient):\n    <IND>\"\"\"\n    Key Authentication Client.\n\n    Handles management of authentication and refresh tokens\n    via keyring\n    \"\"\"\n\n    # pylint: disable=too-many-arguments\n    def __init__(\n        self,\n        tenant_id: str,\n        client_id: str,\n        client_url: str,\n        name: str = None,\n        debug: bool = False,\n    ):\n        <IND>\"\"\"\n        Initialize KeyringAuthClient.\n\n        Parameters\n        ----------\n        tenant_id : str\n            Tenant ID of Azure User\n        client_id : str\n            Client ID of application client\n        client_url : str\n            [description]\n        name : str, optional\n            Name of the secret store, by default None\n        debug : bool, optional\n            Output debug information if True, by default False\n\n        \"\"\"\n        self.name = name\n        self.keyring = self.auth_id\n        super().__init__(tenant_id, client_id, client_url, name=name, debug=debug)\n\n    # pylint: enable=too-many-arguments\n\n    <DED>def _get_creds(self):\n        <IND>if self.debug:\n            <IND>print(\"Fetching creds from keyring\")\n        <DED>try:\n            <IND>access_token = (\n                keyring.get_password(self.keyring, \"adal_context_1\")\n                + keyring.get_password(self.keyring, \"adal_context_2\")\n                + keyring.get_password(self.keyring, \"adal_context_3\")\n                + keyring.get_password(self.keyring, \"adal_context_4\")\n            )\n            refresh_token = (\n                keyring.get_password(self.keyring, \"adal_context_5\")\n                + keyring.get_password(self.keyring, \"adal_context_6\")\n                + keyring.get_password(self.keyring, \"adal_context_7\")\n            )\n            expires_on = keyring.get_password(self.keyring, \"adal_context_8\")\n            self.config_data = {\n                \"accessToken\": access_token,\n                \"refreshToken\": refresh_token,\n                \"expiresOn\": expires_on,\n            }\n        <DED>except (TypeError, KeyringError):\n            <IND>if self.debug:\n                <IND>print(\"No valid credentials in keyring %s\" % self.keyring)\n            <DED>self._get_token()\n        <DED>if not self._is_valid_config_data():\n            <IND>if self.debug:\n                <IND>print(\"No valid authtoken config found in keyring\")\n            <DED>self._get_token()\n\n    <DED><DED>def _cache_creds(self):\n        <IND>if self.debug:\n            <IND>print(\"Saving config data to keyring %s\" % self.keyring)\n        <DED>keyring.set_password(\n            self.keyring, \"adal_context_1\", self.config_data[\"accessToken\"][:400]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_2\", self.config_data[\"accessToken\"][400:800]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_3\", self.config_data[\"accessToken\"][800:1200]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_4\", self.config_data[\"accessToken\"][1200:]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_5\", self.config_data[\"refreshToken\"][:400]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_6\", self.config_data[\"refreshToken\"][400:800]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_7\", self.config_data[\"refreshToken\"][800:]\n        )\n        keyring.set_password(\n            self.keyring, \"adal_context_8\", self.config_data[\"expiresOn\"]\n        )\n\n\n# class KeyVaultAuthClient(AuthClient):\n#     \"\"\"\n#     Keyvault Auth client.\n\n#     Handles management of authentication tokens in keyvault.\n#     \"\"\"\n\n#     def __init__(\n#         self,\n#         tenant_id: str,\n#         client_id: str,\n#         client_url: str,\n#         secret_name: str,\n#         name: str = None,\n#         debug: bool = False,\n#     ):\n#         \"\"\"\n#         Initialize KeyvaultAuthClient.\n\n#         Parameters\n#         ----------\n#         tenant_id : str\n#             Tenant ID of Azure User\n#         client_id : str\n#             Client ID of application client\n#         client_url : str\n#             [description]\n#         name : str, optional\n#             Name of the secret store, by default None\n#         debug : bool, optional\n#             Output debug information if True, by default False\n\n#         \"\"\"\n#         self.secret_name = secret_name\n#         self._get_creds = self._get_keyvault_creds\n#         self._cache_creds = self._cache_creds_keyvault\n#         self.keyvault_client = BHKeyVaultClient(\n#             tenant_id=tenant_id, vault_uri=client_url\n#         )\n#         self.config_data: Any = None\n#         super().__init__(tenant_id, client_id, client_url, name=name, debug=debug)\n\n#     def _get_keyvault_creds(self):\n#         if self.debug:\n#             print(\"getting tokens from keyvault\")\n#         try:\n#             self.config_data = json.loads(\n#                 self.keyvault_client.get_secret(self.secret_name)\n#             )\n#         except KeyVaultMissingSecretException:\n#             if self.debug:\n#                 print(\"missing secret from keyvault, fetching manually\")\n#             self._get_token()\n#         except KeyVaultErrorException as err:\n#             if self.debug:\n#                 print(\"bad creds in keyvault, you gotta getem\")\n#                 print(\"here is what went wrong: %s\" % str(err))\n#             self._get_token()\n\n#     def _cache_creds_keyvault(self):\n#         self.keyvault_client.set_secret(self.secret_name, json.dumps(self.config_data))\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "__author__ = \"Matt Richard, Ian Hellen\"\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "microsoft/msticpy",
    "commit": "28466d681e261394e18d9f4e063cebfa06ed04b4",
    "filename": "msticpy/sectools/auditdextract.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/microsoft-msticpy/msticpy/sectools/auditdextract.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "msticpy/sectools/auditdextract.py:520:55 Incompatible variable type [9]: app is declared to have type `str` but is used as type `None`.",
    "message": " app is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 520,
    "warning_line": "def cluster_auditd_processes(audit_data: pd.DataFrame, app: str = None) -> pd.DataFrame:",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "    return build_process_tree(audit_data)\n\n\ndef cluster_auditd_processes(audit_data: pd.DataFrame, app: str = None) -> pd.DataFrame:\n    \"\"\"\n    Clusters process data into specific processes.\n\n    Parameters\n    ----------\n    audit_data : pd.DataFrame\n        The Audit data containing process creation events\n    app: str, optional\n        The name of a specific app you wish to cluster\n\n    Returns\n    -------\n    pd.DataFrame\n        Details of the clustered process\n\n    \"\"\"\n    if app is not None:\n        processes = audit_data[audit_data[\"exe\"].str.contains(app, na=False)]\n    else:\n        processes = audit_data\n    processes = processes.rename(\n        columns={\n            \"acct\": \"SubjectUserName\",\n            \"uid\": \"SubjectUserSid\",\n            \"user\": \"SubjectUserName\",\n            \"ses\": \"SubjectLogonId\",\n            \"pid\": \"NewProcessId\",\n            \"exe\": \"NewProcessName\",\n            \"ppid\": \"ProcessId\",\n            \"cmdline\": \"CommandLine\",\n        }\n    )\n    req_cols = [\n        \"cwd\",\n        \"SubjectUserName\",\n        \"SubjectUserSid\",\n        \"SubjectUserName\",\n        \"SubjectLogonId\",\n        \"NewProcessId\",\n        \"NewProcessName\",\n        \"ProcessId\",\n        \"CommandLine\",\n    ]\n    for col in req_cols:\n        if col not in processes:\n            processes[col] = \"\"\n\n    feature_procs_h1 = add_process_features(input_frame=processes)\n\n    clus_events, _, _ = dbcluster_events(\n        data=feature_procs_h1,\n        cluster_columns=[\"pathScore\", \"SubjectUserSid\"],\n        time_column=\"TimeGenerated\",\n        max_cluster_distance=0.0001,\n    )\n    (\n        clus_events.sort_values(\"TimeGenerated\")[\n            [\n                \"TimeGenerated\",\n                \"LastEventTime\",\n                \"NewProcessName\",\n                \"CommandLine\",\n                \"SubjectLogonId\",\n                \"SubjectUserSid\",\n                \"pathScore\",\n                \"isSystemSession\",\n                \"ProcessId\",\n                \"ClusterSize\",\n            ]\n        ].sort_values(\"ClusterSize\", ascending=True)\n    )\n\n    procs = clus_events[\n        [\n            \"TimeGenerated\",\n            \"NewProcessName\",\n            \"CommandLine\",\n            \"NewProcessId\",\n            \"SubjectUserSid\",\n            \"cwd\",\n            \"ClusterSize\",\n            \"ProcessId\",\n        ]\n    ]\n    procs = procs.rename(columns={\"NewProcessId\": \"pid\", \"ProcessId\": \"ppid\"})\n\n    return procs\n",
        "source_code_len": 2424,
        "target_code": "    return build_process_tree(audit_data)\n",
        "target_code_len": 42,
        "diff_format": "@@ -517,91 +521,1 @@\n     return build_process_tree(audit_data)\n-\n-\n-def cluster_auditd_processes(audit_data: pd.DataFrame, app: str = None) -> pd.DataFrame:\n-    \"\"\"\n-    Clusters process data into specific processes.\n-\n-    Parameters\n-    ----------\n-    audit_data : pd.DataFrame\n-        The Audit data containing process creation events\n-    app: str, optional\n-        The name of a specific app you wish to cluster\n-\n-    Returns\n-    -------\n-    pd.DataFrame\n-        Details of the clustered process\n-\n-    \"\"\"\n-    if app is not None:\n-        processes = audit_data[audit_data[\"exe\"].str.contains(app, na=False)]\n-    else:\n-        processes = audit_data\n-    processes = processes.rename(\n-        columns={\n-            \"acct\": \"SubjectUserName\",\n-            \"uid\": \"SubjectUserSid\",\n-            \"user\": \"SubjectUserName\",\n-            \"ses\": \"SubjectLogonId\",\n-            \"pid\": \"NewProcessId\",\n-            \"exe\": \"NewProcessName\",\n-            \"ppid\": \"ProcessId\",\n-            \"cmdline\": \"CommandLine\",\n-        }\n-    )\n-    req_cols = [\n-        \"cwd\",\n-        \"SubjectUserName\",\n-        \"SubjectUserSid\",\n-        \"SubjectUserName\",\n-        \"SubjectLogonId\",\n-        \"NewProcessId\",\n-        \"NewProcessName\",\n-        \"ProcessId\",\n-        \"CommandLine\",\n-    ]\n-    for col in req_cols:\n-        if col not in processes:\n-            processes[col] = \"\"\n-\n-    feature_procs_h1 = add_process_features(input_frame=processes)\n-\n-    clus_events, _, _ = dbcluster_events(\n-        data=feature_procs_h1,\n-        cluster_columns=[\"pathScore\", \"SubjectUserSid\"],\n-        time_column=\"TimeGenerated\",\n-        max_cluster_distance=0.0001,\n-    )\n-    (\n-        clus_events.sort_values(\"TimeGenerated\")[\n-            [\n-                \"TimeGenerated\",\n-                \"LastEventTime\",\n-                \"NewProcessName\",\n-                \"CommandLine\",\n-                \"SubjectLogonId\",\n-                \"SubjectUserSid\",\n-                \"pathScore\",\n-                \"isSystemSession\",\n-                \"ProcessId\",\n-                \"ClusterSize\",\n-            ]\n-        ].sort_values(\"ClusterSize\", ascending=True)\n-    )\n-\n-    procs = clus_events[\n-        [\n-            \"TimeGenerated\",\n-            \"NewProcessName\",\n-            \"CommandLine\",\n-            \"NewProcessId\",\n-            \"SubjectUserSid\",\n-            \"cwd\",\n-            \"ClusterSize\",\n-            \"ProcessId\",\n-        ]\n-    ]\n-    procs = procs.rename(columns={\"NewProcessId\": \"pid\", \"ProcessId\": \"ppid\"})\n-\n-    return procs\n",
        "source_code_with_indent": "    return build_process_tree(audit_data)\n\n\n<DED>def cluster_auditd_processes(audit_data: pd.DataFrame, app: str = None) -> pd.DataFrame:\n    <IND>\"\"\"\n    Clusters process data into specific processes.\n\n    Parameters\n    ----------\n    audit_data : pd.DataFrame\n        The Audit data containing process creation events\n    app: str, optional\n        The name of a specific app you wish to cluster\n\n    Returns\n    -------\n    pd.DataFrame\n        Details of the clustered process\n\n    \"\"\"\n    if app is not None:\n        <IND>processes = audit_data[audit_data[\"exe\"].str.contains(app, na=False)]\n    <DED>else:\n        <IND>processes = audit_data\n    <DED>processes = processes.rename(\n        columns={\n            \"acct\": \"SubjectUserName\",\n            \"uid\": \"SubjectUserSid\",\n            \"user\": \"SubjectUserName\",\n            \"ses\": \"SubjectLogonId\",\n            \"pid\": \"NewProcessId\",\n            \"exe\": \"NewProcessName\",\n            \"ppid\": \"ProcessId\",\n            \"cmdline\": \"CommandLine\",\n        }\n    )\n    req_cols = [\n        \"cwd\",\n        \"SubjectUserName\",\n        \"SubjectUserSid\",\n        \"SubjectUserName\",\n        \"SubjectLogonId\",\n        \"NewProcessId\",\n        \"NewProcessName\",\n        \"ProcessId\",\n        \"CommandLine\",\n    ]\n    for col in req_cols:\n        <IND>if col not in processes:\n            <IND>processes[col] = \"\"\n\n    <DED><DED>feature_procs_h1 = add_process_features(input_frame=processes)\n\n    clus_events, _, _ = dbcluster_events(\n        data=feature_procs_h1,\n        cluster_columns=[\"pathScore\", \"SubjectUserSid\"],\n        time_column=\"TimeGenerated\",\n        max_cluster_distance=0.0001,\n    )\n    (\n        clus_events.sort_values(\"TimeGenerated\")[\n            [\n                \"TimeGenerated\",\n                \"LastEventTime\",\n                \"NewProcessName\",\n                \"CommandLine\",\n                \"SubjectLogonId\",\n                \"SubjectUserSid\",\n                \"pathScore\",\n                \"isSystemSession\",\n                \"ProcessId\",\n                \"ClusterSize\",\n            ]\n        ].sort_values(\"ClusterSize\", ascending=True)\n    )\n\n    procs = clus_events[\n        [\n            \"TimeGenerated\",\n            \"NewProcessName\",\n            \"CommandLine\",\n            \"NewProcessId\",\n            \"SubjectUserSid\",\n            \"cwd\",\n            \"ClusterSize\",\n            \"ProcessId\",\n        ]\n    ]\n    procs = procs.rename(columns={\"NewProcessId\": \"pid\", \"ProcessId\": \"ppid\"})\n\n    return procs\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    return build_process_tree(audit_data)\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "microsoft/msticpy",
    "commit": "28466d681e261394e18d9f4e063cebfa06ed04b4",
    "filename": "msticpy/sectools/eventcluster.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/microsoft-msticpy/msticpy/sectools/eventcluster.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "msticpy/sectools/eventcluster.py:55:4 Incompatible variable type [9]: cluster_columns is declared to have type `List[typing.Any]` but is used as type `None`.",
    "message": " cluster_columns is declared to have type `List[typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 55,
    "warning_line": "    cluster_columns: List[Any] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "# --------------------------------------------------------------------------\nr\"\"\"\neventcluster module.\n\nThis module is intended to be used to summarize large numbers of events\ninto clusters of different patterns. High volume repeating events can\noften make it difficult to see unique and interesting items.\n\nThe module contains functions to generate clusterable features from\nstring data. For example, an administration command that does some\nmaintenance on thousands of servers with a commandline such as:\n``install-update -hostname {host.fqdn} -tmp:/tmp/{GUID}/rollback``\\  can\nbe collapsed into a single cluster pattern by ignoring the character\nvalues in the string and using delimiters or tokens to group the values.\n\nThis is an unsupervised learning module implemented using SciKit Learn\nDBScan.\n\nContains:\ndbcluster_events: generic clustering method using DBSCAN designed to summarize\nprocess events and other similar data by grouping on common features.\n\nadd_process_features: derives numerical features from text features such as\ncommandline and process path.\n\n\"\"\"\nfrom binascii import crc32\nfrom functools import lru_cache\nfrom math import log10, floor\nimport re\nfrom typing import List, Any, Tuple, Union\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import Normalizer\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nfrom ..common.utility import export\nfrom .._version import VERSION\n",
        "source_code_len": 1462,
        "target_code": "# --------------------------------------------------------------------------\n\"\"\"Deprecated placeholder for eventcluster.py.\"\"\"\nimport warnings\nfrom .._version import VERSION\n",
        "target_code_len": 174,
        "diff_format": "@@ -5,41 +5,4 @@\n # --------------------------------------------------------------------------\n-r\"\"\"\n-eventcluster module.\n-\n-This module is intended to be used to summarize large numbers of events\n-into clusters of different patterns. High volume repeating events can\n-often make it difficult to see unique and interesting items.\n-\n-The module contains functions to generate clusterable features from\n-string data. For example, an administration command that does some\n-maintenance on thousands of servers with a commandline such as:\n-``install-update -hostname {host.fqdn} -tmp:/tmp/{GUID}/rollback``\\  can\n-be collapsed into a single cluster pattern by ignoring the character\n-values in the string and using delimiters or tokens to group the values.\n-\n-This is an unsupervised learning module implemented using SciKit Learn\n-DBScan.\n-\n-Contains:\n-dbcluster_events: generic clustering method using DBSCAN designed to summarize\n-process events and other similar data by grouping on common features.\n-\n-add_process_features: derives numerical features from text features such as\n-commandline and process path.\n-\n-\"\"\"\n-from binascii import crc32\n-from functools import lru_cache\n-from math import log10, floor\n-import re\n-from typing import List, Any, Tuple, Union\n-\n-import numpy as np\n-import pandas as pd\n-from sklearn.cluster import DBSCAN\n-from sklearn.preprocessing import Normalizer\n-import matplotlib.pyplot as plt\n-from matplotlib import cm\n-\n-from ..common.utility import export\n+\"\"\"Deprecated placeholder for eventcluster.py.\"\"\"\n+import warnings\n from .._version import VERSION\n",
        "source_code_with_indent": "# --------------------------------------------------------------------------\nr\"\"\"\neventcluster module.\n\nThis module is intended to be used to summarize large numbers of events\ninto clusters of different patterns. High volume repeating events can\noften make it difficult to see unique and interesting items.\n\nThe module contains functions to generate clusterable features from\nstring data. For example, an administration command that does some\nmaintenance on thousands of servers with a commandline such as:\n``install-update -hostname {host.fqdn} -tmp:/tmp/{GUID}/rollback``\\  can\nbe collapsed into a single cluster pattern by ignoring the character\nvalues in the string and using delimiters or tokens to group the values.\n\nThis is an unsupervised learning module implemented using SciKit Learn\nDBScan.\n\nContains:\ndbcluster_events: generic clustering method using DBSCAN designed to summarize\nprocess events and other similar data by grouping on common features.\n\nadd_process_features: derives numerical features from text features such as\ncommandline and process path.\n\n\"\"\"\nfrom binascii import crc32\nfrom functools import lru_cache\nfrom math import log10, floor\nimport re\nfrom typing import List, Any, Tuple, Union\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import Normalizer\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nfrom ..common.utility import export\nfrom .._version import VERSION\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "# --------------------------------------------------------------------------\n\"\"\"Deprecated placeholder for eventcluster.py.\"\"\"\nimport warnings\nfrom .._version import VERSION\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n# pylint: disable=too-many-arguments, too-many-locals\n@export\ndef dbcluster_events(\n    data: Any,\n    cluster_columns: List[Any] = None,\n    verbose: bool = False,\n    normalize: bool = True,\n    time_column: str = \"TimeCreatedUtc\",\n    max_cluster_distance: float = 0.01,\n    min_cluster_samples: int = 2,\n    **kwargs,\n) -> Tuple[pd.DataFrame, DBSCAN, np.ndarray]:\n    \"\"\"\n    Cluster data set according to cluster_columns features.\n\n    Parameters\n    ----------\n    data : Any\n        Input data as a pandas DataFrame or numpy array\n    cluster_columns : List[Any], optional\n        List of columns to use for features\n        - for DataFrame this is a list of column names\n        - for numpy array this is a list of column indexes\n    verbose : bool, optional\n        Print additional information about clustering results (the default is False)\n    normalize : bool, optional\n        Normalize the input data (should probably always be True)\n    time_column : str, optional\n        If there is a time column the output data will be ordered by this\n        (the default is 'TimeCreatedUtc')\n    max_cluster_distance : float, optional\n        DBSCAN eps (max cluster member distance) (the default is 0.01)\n    min_cluster_samples : int, optional\n        DBSCAN min_samples (the minimum cluster size) (the default is 2)\n\n    Other Parameters\n    ----------------\n    kwargs: Other arguments are passed to DBSCAN constructor\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, DBSCAN, np.ndarray]\n        Output dataframe with clustered rows\n        DBSCAN model\n        Normalized data set\n\n    \"\"\"\n    allowed_types = [np.ndarray, pd.DataFrame]\n\n    x_input = None\n    if isinstance(data, pd.DataFrame):\n        if cluster_columns is None:\n            x_input = data.values\n        else:\n            x_input = data[cluster_columns].values\n    elif isinstance(data, np.ndarray):\n        x_input = data if cluster_columns is None else data[:, cluster_columns].values\n    if x_input is None:\n        mssg = \"Input data not in expected format.\\n{} is not one of allowed types {}\"\n        type_list = \", \".join([str(t) for t in allowed_types])\n        mssg = mssg.format(str(type(data)), type_list)\n        raise ValueError(mssg)\n\n    # Create DBSCAN cluster object\n    db_cluster = DBSCAN(\n        eps=max_cluster_distance, min_samples=min_cluster_samples, **kwargs\n    )\n\n    # Normalize the data (most clustering algorithms don't do well with\n    # unnormalized data)\n    x_norm = Normalizer().fit_transform(x_input) if normalize else x_input\n    # fit the data set\n    db_cluster.fit(x_norm)\n    labels = db_cluster.labels_\n    cluster_set, counts = np.unique(labels, return_counts=True)\n    if verbose:\n        print(\n            \"Clustering for set size \",\n            len(x_norm),\n            \" - \",\n            len(cluster_set),\n            \" clusters\",\n        )\n        print(\"Individual cluster sizes: \", \", \".join([str(c) for c in counts]))\n\n    clustered_events = _merge_clustered_items(\n        cluster_set, labels, data, time_column, counts\n    )\n\n    if verbose:\n        print(\"Cluster output rows: \", len(clustered_events))\n\n    return clustered_events, db_cluster, x_norm\n\n\ndef _merge_clustered_items(\n    cluster_set: np.array,\n    labels: np.array,\n    data: Union[pd.DataFrame, np.array],\n    time_column: str,\n    counts: np.array,\n) -> pd.DataFrame:\n    \"\"\"\n    Merge outliers and core clusters into single DataFrame.\n\n    Parameters\n    ----------\n    cluster_set : np.array\n        The set of clusters\n    labels : np.array\n        The cluster labels\n    data : Union[pd.DataFrame, np.array]\n        The source data\n    time_column : str\n        Name of the Time column\n    counts : np.array\n        The counts of members in each cluster\n\n    Returns\n    -------\n    pd.DataFrame\n        Merged dataframe\n\n    \"\"\"\n    tz_aware = data.iloc[0][time_column].tz\n    ts_type = \"datetime64[ns, UTC]\" if tz_aware is not None else \"datetime64[ns]\"\n\n    cluster_list = []\n    # Iterate through clusters, adding exemplar to output frame\n    # pylint: disable=consider-using-enumerate\n    # we need to know the index of the item within the loop\n    for idx in range(len(cluster_set)):\n        cluster_id = cluster_set[idx]\n        class_members = labels == cluster_id\n        if isinstance(data, pd.DataFrame):\n            time_ordered = data[class_members].sort_values(time_column, ascending=True)\n            first_event_time = time_ordered[0:][time_column].iat[0]\n            last_event_time = time_ordered[-1:][time_column].iat[0]\n        else:\n            first_event_time = None\n            last_event_time = None\n\n        if cluster_id == -1:\n            # 'Noise' events are individual items that could not be assigned\n            # to a cluster and so are unique\n            cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=False,\n                    ClusterId=cluster_id,\n                    ClusterSize=1,\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n        else:\n            # Otherwise, just choose the first example of the cluster set\n            cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=True,\n                    ClusterId=cluster_id,\n                    ClusterSize=counts[idx],\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )[0:1]\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n    # pylint: enable=consider-using-enumerate\n    return pd.concat(cluster_list)\n\n\n@export\ndef add_process_features(\n    input_frame: pd.DataFrame, path_separator: str = None, force: bool = False\n) -> pd.DataFrame:\n    r\"\"\"\n    Add numerical features based on patterns of command line and process name.\n\n    Parameters\n    ----------\n    input_frame : pd.DataFrame\n        The input dataframe\n    path_separator : str, optional\n        Path separator. If not supplied, try to determine\n        from 'NewProcessName' column of first 10 rows\n        (the default is None)\n    force : bool, optional\n        Forces re-calculation of feature columns even if they\n        already exist (the default is False)\n\n    Returns\n    -------\n    pd.DataFrame\n        Copy of the dataframe with the additional numeric features\n\n    Notes\n    -----\n    Features added:\n\n    - processNameLen: length of process file name (inc path)\n    - processNameTokens: the number of elements in the path\n    - processName: the process file name (minus path)\n    - commandlineTokens: number of space-separated tokens in the command line\n    - commandlineLen: length of the command line\n    - commandlineLogLen: log10 length of commandline\n    - isSystemSession: 1 if session Id is 0x3e7 for Windows or -1 for Linux\n    - commandlineTokensFull: counts number of token separators in commandline\n      [\\\\s\\-\\\\/\\.,\"\\'\\|&:;%$()]\n    - pathScore: sum of ord() value of characters in path\n    - pathLogScore: log10 of pathScore\n    - commandlineScore: sum of ord() value of characters in commandline\n    - commandlineLogScore: log10 of commandlineScore\n\n    \"\"\"\n    output_df = input_frame.copy()\n\n    # Set any NaN values to empty string\n    if \"NewProcessName\" in output_df and \"CommandLine\" in output_df:\n        output_df[[\"NewProcessName\", \"CommandLine\"]] = output_df[\n            [\"NewProcessName\", \"CommandLine\"]\n        ].fillna(value=\"\")\n\n    # try to determine the path separator\n    if path_separator is None:\n        sample_df = output_df.head(10)\n        lx_path = len(sample_df[sample_df[\"NewProcessName\"].str.contains(\"/\")])\n        path_separator = \"/\" if lx_path else \"\\\\\"\n    # Create features from process name and command line\n    if \"NewProcessName\" in output_df:\n        _add_processname_features(output_df, force, path_separator)\n\n    if \"CommandLine\" in output_df:\n        _add_commandline_features(output_df, force)\n\n    if \"SubjectLogonId\" in output_df and (\"isSystemSession\" not in output_df or force):\n        output_df[\"isSystemSession\"] = output_df[\"SubjectLogonId\"].isin([\"0x3e7\", \"-1\"])\n\n    return output_df\n\n\ndef _add_processname_features(\n    output_df: pd.DataFrame, force: bool, path_separator: str\n):\n    \"\"\"\n    Add process name default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n    path_separator : str\n        Path separator for OS\n\n    \"\"\"\n    if \"processName\" not in output_df or force:\n        output_df[\"processName\"] = output_df.apply(\n            lambda x: x.NewProcessName.split(path_separator)[-1], axis=1\n        )\n    if \"pathScore\" not in output_df or force:\n        output_df[\"pathScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.NewProcessName), axis=1\n        )\n    if \"pathLogScore\" not in output_df or force:\n        output_df[\"pathLogScore\"] = output_df.apply(\n            lambda x: log10(x.pathScore) if x.pathScore else 0, axis=1\n        )\n    if \"pathHash\" not in output_df or force:\n        output_df[\"pathHash\"] = output_df.apply(\n            lambda x: crc32_hash(x.NewProcessName), axis=1\n        )\n\n\ndef _add_commandline_features(output_df: pd.DataFrame, force: bool):\n    \"\"\"\n    Add commandline default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n\n    \"\"\"\n    if \"commandlineLen\" not in output_df or force:\n        output_df[\"commandlineLen\"] = output_df.apply(\n            lambda x: len(x.CommandLine), axis=1\n        )\n    if \"commandlineLogLen\" not in output_df or force:\n        output_df[\"commandlineLogLen\"] = output_df.apply(\n            lambda x: log10(x.commandlineLen) if x.commandlineLen else 0, axis=1\n        )\n    if \"commandlineTokensFull\" not in output_df or force:\n        output_df[\"commandlineTokensFull\"] = output_df[[\"CommandLine\"]].apply(\n            lambda x: delim_count(x.CommandLine), axis=1\n        )\n\n    if \"commandlineScore\" not in output_df or force:\n        output_df[\"commandlineScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.CommandLine), axis=1\n        )\n    if \"commandlineTokensHash\" not in output_df or force:\n        output_df[\"commandlineTokensHash\"] = output_df.apply(\n            lambda x: delim_hash(x.CommandLine), axis=1\n        )\n\n\n@export\n@lru_cache(maxsize=1024)\ndef delim_count(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Count of delimiters in the string.\n\n    \"\"\"\n    return len(re.findall(delim_list, value))\n\n\n@export\n@lru_cache(maxsize=1024)\ndef delim_hash(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    r\"\"\"\n    Return a hash (CRC32) of the delimiters from input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Hash of delimiter set in the string.\n\n    \"\"\"\n    return crc32(bytes(\"\".join(re.findall(delim_list, value)), \"utf-8\"))\n\n\n@export\n@lru_cache(maxsize=1024)\ndef char_ord_score(value: str, scale: int = 1) -> int:\n    \"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    int\n        [description]\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return floor(sum(ord(x) for x in value) / scale)\n\n\n@export\n@lru_cache(maxsize=1024)\ndef token_count(value: str, delimiter: str = \" \") -> int:\n    \"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    int\n        count of tokens\n\n    \"\"\"\n    return len(value.split(delimiter))\n\n\ndef _string_score(input_str):\n    \"\"\"Sum the ord(c) for characters in a string.\"\"\"\n    return sum(ord(x) for x in input_str)\n\n\n@export\n@lru_cache(maxsize=1024)\ndef crc32_hash(value: str) -> int:\n    \"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n\n    Returns\n    -------\n    int\n        CRC32 hash\n\n    \"\"\"\n    return crc32(bytes(value.encode(\"utf-8\")))\n\n\ndef delim_count_df(\n    data: pd.DataFrame, column: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]'\n) -> pd.Series:\n    r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        The name of the column to process\n    delim_list : str, optional\n        delimiters to use. (the default is r\\'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]\\')\n\n    Returns\n    -------\n    pd.Series\n        Count of delimiters in the string in `column`.\n\n    \"\"\"\n    return data[column].str.count(delim_list)\n\n\ndef char_ord_score_df(data: pd.DataFrame, column: str, scale: int = 1) -> pd.Series:\n    \"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    pd.Series\n        The sum of the ordinal values of the characters\n        in `column`.\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return data.apply(lambda x: sum(ord(char) for char in x[column]) / scale, axis=1)\n\n\ndef token_count_df(data: pd.DataFrame, column: str, delimiter: str = \" \") -> pd.Series:\n    \"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    pd.Series\n        count of tokens in strings in `column`\n\n    \"\"\"\n    return data.apply(lambda x: len(x[column].split(delimiter)), axis=1)\n\n\ndef crc32_hash_df(data: pd.DataFrame, column: str) -> pd.Series:\n    \"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n\n    Returns\n    -------\n    pd.Series\n        CRC32 hash of input column\n\n    \"\"\"\n    return data.apply(lambda x: crc32(bytes(x[column].encode(\"utf-8\"))), axis=1)\n\n\n# pylint: disable=too-many-arguments, too-many-statements\n@export  # noqa: C901, MC0001\ndef plot_cluster(\n    db_cluster: DBSCAN,\n    data: pd.DataFrame,\n    x_predict: np.ndarray,\n    plot_label: str = None,\n    plot_features: Tuple[int, int] = (0, 1),\n    verbose: bool = False,\n    cut_off: int = 3,\n    xlabel: str = None,\n    ylabel: str = None,\n):\n    \"\"\"\n    Plot clustered data as scatter chart.\n\n    Parameters\n    ----------\n    db_cluster : DBSCAN\n        DBScan Cluster (from SkLearn DBSCAN).\n    data : pd.DataFrame\n        Dataframe containing original data.\n    x_predict : np.ndarray\n        The DBSCAN predict numpy array\n    plot_label : str, optional\n         If set the column to use to label data points\n         (the default is None)\n    plot_features :  Tuple[int, int], optional\n        Which two features in x_predict to plot (the default is (0, 1))\n    verbose : bool, optional\n        Verbose execution with some extra info\n        (the default is False)\n    cut_off : int, optional\n        The cluster size below which items are considered outliers\n        (the default is 3)\n    xlabel : str, optional\n        x-axis label (the default is None)\n    ylabel : str, optional\n        y-axis label (the default is None)\n\n    \"\"\"\n    max_idx = x_predict.shape[1] - 1\n    if plot_features[0] >= x_predict.shape[1]:\n        raise ValueError(\n            \"plot_features[0] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    if plot_features[1] >= x_predict.shape[1]:\n        raise ValueError(\n            \"plot_features[1] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    if plot_features[0] == plot_features[1]:\n        mssg = \"plot_features indexes must be 2 different values in range 0 to\"\n        raise ValueError(mssg + f\" {max_idx}.\")\n\n    labels = db_cluster.labels_\n    core_samples_mask = np.zeros_like(labels, dtype=bool)\n\n    # pylint: disable=unsupported-assignment-operation\n    # (assignment of numpy array is valid)\n    core_samples_mask[db_cluster.core_sample_indices_] = True\n    unique_labels = set(labels)\n\n    # pylint: disable=no-member\n    # Spectral color map does exist\n    colors = [cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n    # Number of clusters in labels, ignoring noise if present.\n    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n    n_noise_ = list(labels).count(-1)\n    _, counts = np.unique(labels, return_counts=True)\n\n    if verbose:\n        print(\"Estimated number of clusters: %d\" % n_clusters_)\n        print(\"Estimated number of noise points: %d\" % n_noise_)\n        # print(\"Silhouette Coefficient: %0.3f\"\n        #       % metrics.silhouette_score(x_predict, labels))\n\n    if not isinstance(data, pd.DataFrame):\n        plot_label = None\n    elif plot_label is not None and plot_label not in data:\n        plot_label = None\n\n    p_label = None\n    for cluster_id, color in zip(unique_labels, colors):\n        if cluster_id == -1:\n            # Black used for noise.\n            color = [0, 0, 0, 1]\n        class_member_mask = labels == cluster_id\n\n        cluster_size = counts[cluster_id]\n        marker_size = cluster_size\n        marker = \"o\"\n        font_size = \"small\"\n        alpha = 0.4\n\n        if cluster_size < cut_off:\n            marker = \"+\"\n            marker_size = 10\n            font_size = \"large\"\n            alpha = 1.0\n        xy_pos = x_predict[class_member_mask & core_samples_mask]\n        plt.plot(\n            xy_pos[:, plot_features[0]],\n            xy_pos[:, plot_features[1]],\n            marker,\n            markerfacecolor=tuple(color),\n            markersize=marker_size,\n        )\n\n        if plot_label:\n            first_row = data[class_member_mask].iloc[0]\n            if not first_row.empty and plot_label in first_row:\n                p_label = first_row[plot_label]\n                try:\n                    plt.annotate(\n                        p_label,\n                        xy=(xy_pos[0, plot_features[0]], xy_pos[0, plot_features[1]]),\n                        fontsize=font_size,\n                        alpha=alpha,\n                    )\n                except IndexError:\n                    pass\n\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n    plt.show()\n    return plt\n",
        "source_code_len": 21140,
        "target_code": "\n# flake8: noqa: F403, F401\n# pylint: disable=wildcard-import, unused-wildcard-import, unused-import\nfrom ..analysis.eventcluster import *\n\nWARN_MSSG = (\n    \"This module has moved to msticpy.analysis.eventcluster\\n\"\n    + \"Please change your import to reflect this new location.\"\n)\nwarnings.warn(WARN_MSSG, category=DeprecationWarning)\n",
        "target_code_len": 337,
        "diff_format": "@@ -50,683 +13,10 @@\n \n-# pylint: disable=too-many-arguments, too-many-locals\n-@export\n-def dbcluster_events(\n-    data: Any,\n-    cluster_columns: List[Any] = None,\n-    verbose: bool = False,\n-    normalize: bool = True,\n-    time_column: str = \"TimeCreatedUtc\",\n-    max_cluster_distance: float = 0.01,\n-    min_cluster_samples: int = 2,\n-    **kwargs,\n-) -> Tuple[pd.DataFrame, DBSCAN, np.ndarray]:\n-    \"\"\"\n-    Cluster data set according to cluster_columns features.\n+# flake8: noqa: F403, F401\n+# pylint: disable=wildcard-import, unused-wildcard-import, unused-import\n+from ..analysis.eventcluster import *\n \n-    Parameters\n-    ----------\n-    data : Any\n-        Input data as a pandas DataFrame or numpy array\n-    cluster_columns : List[Any], optional\n-        List of columns to use for features\n-        - for DataFrame this is a list of column names\n-        - for numpy array this is a list of column indexes\n-    verbose : bool, optional\n-        Print additional information about clustering results (the default is False)\n-    normalize : bool, optional\n-        Normalize the input data (should probably always be True)\n-    time_column : str, optional\n-        If there is a time column the output data will be ordered by this\n-        (the default is 'TimeCreatedUtc')\n-    max_cluster_distance : float, optional\n-        DBSCAN eps (max cluster member distance) (the default is 0.01)\n-    min_cluster_samples : int, optional\n-        DBSCAN min_samples (the minimum cluster size) (the default is 2)\n-\n-    Other Parameters\n-    ----------------\n-    kwargs: Other arguments are passed to DBSCAN constructor\n-\n-    Returns\n-    -------\n-    Tuple[pd.DataFrame, DBSCAN, np.ndarray]\n-        Output dataframe with clustered rows\n-        DBSCAN model\n-        Normalized data set\n-\n-    \"\"\"\n-    allowed_types = [np.ndarray, pd.DataFrame]\n-\n-    x_input = None\n-    if isinstance(data, pd.DataFrame):\n-        if cluster_columns is None:\n-            x_input = data.values\n-        else:\n-            x_input = data[cluster_columns].values\n-    elif isinstance(data, np.ndarray):\n-        x_input = data if cluster_columns is None else data[:, cluster_columns].values\n-    if x_input is None:\n-        mssg = \"Input data not in expected format.\\n{} is not one of allowed types {}\"\n-        type_list = \", \".join([str(t) for t in allowed_types])\n-        mssg = mssg.format(str(type(data)), type_list)\n-        raise ValueError(mssg)\n-\n-    # Create DBSCAN cluster object\n-    db_cluster = DBSCAN(\n-        eps=max_cluster_distance, min_samples=min_cluster_samples, **kwargs\n-    )\n-\n-    # Normalize the data (most clustering algorithms don't do well with\n-    # unnormalized data)\n-    x_norm = Normalizer().fit_transform(x_input) if normalize else x_input\n-    # fit the data set\n-    db_cluster.fit(x_norm)\n-    labels = db_cluster.labels_\n-    cluster_set, counts = np.unique(labels, return_counts=True)\n-    if verbose:\n-        print(\n-            \"Clustering for set size \",\n-            len(x_norm),\n-            \" - \",\n-            len(cluster_set),\n-            \" clusters\",\n-        )\n-        print(\"Individual cluster sizes: \", \", \".join([str(c) for c in counts]))\n-\n-    clustered_events = _merge_clustered_items(\n-        cluster_set, labels, data, time_column, counts\n-    )\n-\n-    if verbose:\n-        print(\"Cluster output rows: \", len(clustered_events))\n-\n-    return clustered_events, db_cluster, x_norm\n-\n-\n-def _merge_clustered_items(\n-    cluster_set: np.array,\n-    labels: np.array,\n-    data: Union[pd.DataFrame, np.array],\n-    time_column: str,\n-    counts: np.array,\n-) -> pd.DataFrame:\n-    \"\"\"\n-    Merge outliers and core clusters into single DataFrame.\n-\n-    Parameters\n-    ----------\n-    cluster_set : np.array\n-        The set of clusters\n-    labels : np.array\n-        The cluster labels\n-    data : Union[pd.DataFrame, np.array]\n-        The source data\n-    time_column : str\n-        Name of the Time column\n-    counts : np.array\n-        The counts of members in each cluster\n-\n-    Returns\n-    -------\n-    pd.DataFrame\n-        Merged dataframe\n-\n-    \"\"\"\n-    tz_aware = data.iloc[0][time_column].tz\n-    ts_type = \"datetime64[ns, UTC]\" if tz_aware is not None else \"datetime64[ns]\"\n-\n-    cluster_list = []\n-    # Iterate through clusters, adding exemplar to output frame\n-    # pylint: disable=consider-using-enumerate\n-    # we need to know the index of the item within the loop\n-    for idx in range(len(cluster_set)):\n-        cluster_id = cluster_set[idx]\n-        class_members = labels == cluster_id\n-        if isinstance(data, pd.DataFrame):\n-            time_ordered = data[class_members].sort_values(time_column, ascending=True)\n-            first_event_time = time_ordered[0:][time_column].iat[0]\n-            last_event_time = time_ordered[-1:][time_column].iat[0]\n-        else:\n-            first_event_time = None\n-            last_event_time = None\n-\n-        if cluster_id == -1:\n-            # 'Noise' events are individual items that could not be assigned\n-            # to a cluster and so are unique\n-            cluster_list.append(\n-                data[class_members]\n-                .assign(\n-                    Clustered=False,\n-                    ClusterId=cluster_id,\n-                    ClusterSize=1,\n-                    TimeGenerated=first_event_time,\n-                    FirstEventTime=first_event_time,\n-                    LastEventTime=last_event_time,\n-                )\n-                .astype(\n-                    dtype={\n-                        \"TimeGenerated\": ts_type,\n-                        \"FirstEventTime\": ts_type,\n-                        \"LastEventTime\": ts_type,\n-                    }\n-                )\n-            )\n-        else:\n-            # Otherwise, just choose the first example of the cluster set\n-            cluster_list.append(\n-                data[class_members]\n-                .assign(\n-                    Clustered=True,\n-                    ClusterId=cluster_id,\n-                    ClusterSize=counts[idx],\n-                    TimeGenerated=first_event_time,\n-                    FirstEventTime=first_event_time,\n-                    LastEventTime=last_event_time,\n-                )[0:1]\n-                .astype(\n-                    dtype={\n-                        \"TimeGenerated\": ts_type,\n-                        \"FirstEventTime\": ts_type,\n-                        \"LastEventTime\": ts_type,\n-                    }\n-                )\n-            )\n-    # pylint: enable=consider-using-enumerate\n-    return pd.concat(cluster_list)\n-\n-\n-@export\n-def add_process_features(\n-    input_frame: pd.DataFrame, path_separator: str = None, force: bool = False\n-) -> pd.DataFrame:\n-    r\"\"\"\n-    Add numerical features based on patterns of command line and process name.\n-\n-    Parameters\n-    ----------\n-    input_frame : pd.DataFrame\n-        The input dataframe\n-    path_separator : str, optional\n-        Path separator. If not supplied, try to determine\n-        from 'NewProcessName' column of first 10 rows\n-        (the default is None)\n-    force : bool, optional\n-        Forces re-calculation of feature columns even if they\n-        already exist (the default is False)\n-\n-    Returns\n-    -------\n-    pd.DataFrame\n-        Copy of the dataframe with the additional numeric features\n-\n-    Notes\n-    -----\n-    Features added:\n-\n-    - processNameLen: length of process file name (inc path)\n-    - processNameTokens: the number of elements in the path\n-    - processName: the process file name (minus path)\n-    - commandlineTokens: number of space-separated tokens in the command line\n-    - commandlineLen: length of the command line\n-    - commandlineLogLen: log10 length of commandline\n-    - isSystemSession: 1 if session Id is 0x3e7 for Windows or -1 for Linux\n-    - commandlineTokensFull: counts number of token separators in commandline\n-      [\\\\s\\-\\\\/\\.,\"\\'\\|&:;%$()]\n-    - pathScore: sum of ord() value of characters in path\n-    - pathLogScore: log10 of pathScore\n-    - commandlineScore: sum of ord() value of characters in commandline\n-    - commandlineLogScore: log10 of commandlineScore\n-\n-    \"\"\"\n-    output_df = input_frame.copy()\n-\n-    # Set any NaN values to empty string\n-    if \"NewProcessName\" in output_df and \"CommandLine\" in output_df:\n-        output_df[[\"NewProcessName\", \"CommandLine\"]] = output_df[\n-            [\"NewProcessName\", \"CommandLine\"]\n-        ].fillna(value=\"\")\n-\n-    # try to determine the path separator\n-    if path_separator is None:\n-        sample_df = output_df.head(10)\n-        lx_path = len(sample_df[sample_df[\"NewProcessName\"].str.contains(\"/\")])\n-        path_separator = \"/\" if lx_path else \"\\\\\"\n-    # Create features from process name and command line\n-    if \"NewProcessName\" in output_df:\n-        _add_processname_features(output_df, force, path_separator)\n-\n-    if \"CommandLine\" in output_df:\n-        _add_commandline_features(output_df, force)\n-\n-    if \"SubjectLogonId\" in output_df and (\"isSystemSession\" not in output_df or force):\n-        output_df[\"isSystemSession\"] = output_df[\"SubjectLogonId\"].isin([\"0x3e7\", \"-1\"])\n-\n-    return output_df\n-\n-\n-def _add_processname_features(\n-    output_df: pd.DataFrame, force: bool, path_separator: str\n-):\n-    \"\"\"\n-    Add process name default features.\n-\n-    Parameters\n-    ----------\n-    output_df : pd.DataFrame\n-        The dataframe to add features to\n-    force : bool\n-        If True overwrite existing feature columns\n-    path_separator : str\n-        Path separator for OS\n-\n-    \"\"\"\n-    if \"processName\" not in output_df or force:\n-        output_df[\"processName\"] = output_df.apply(\n-            lambda x: x.NewProcessName.split(path_separator)[-1], axis=1\n-        )\n-    if \"pathScore\" not in output_df or force:\n-        output_df[\"pathScore\"] = output_df.apply(\n-            lambda x: char_ord_score(x.NewProcessName), axis=1\n-        )\n-    if \"pathLogScore\" not in output_df or force:\n-        output_df[\"pathLogScore\"] = output_df.apply(\n-            lambda x: log10(x.pathScore) if x.pathScore else 0, axis=1\n-        )\n-    if \"pathHash\" not in output_df or force:\n-        output_df[\"pathHash\"] = output_df.apply(\n-            lambda x: crc32_hash(x.NewProcessName), axis=1\n-        )\n-\n-\n-def _add_commandline_features(output_df: pd.DataFrame, force: bool):\n-    \"\"\"\n-    Add commandline default features.\n-\n-    Parameters\n-    ----------\n-    output_df : pd.DataFrame\n-        The dataframe to add features to\n-    force : bool\n-        If True overwrite existing feature columns\n-\n-    \"\"\"\n-    if \"commandlineLen\" not in output_df or force:\n-        output_df[\"commandlineLen\"] = output_df.apply(\n-            lambda x: len(x.CommandLine), axis=1\n-        )\n-    if \"commandlineLogLen\" not in output_df or force:\n-        output_df[\"commandlineLogLen\"] = output_df.apply(\n-            lambda x: log10(x.commandlineLen) if x.commandlineLen else 0, axis=1\n-        )\n-    if \"commandlineTokensFull\" not in output_df or force:\n-        output_df[\"commandlineTokensFull\"] = output_df[[\"CommandLine\"]].apply(\n-            lambda x: delim_count(x.CommandLine), axis=1\n-        )\n-\n-    if \"commandlineScore\" not in output_df or force:\n-        output_df[\"commandlineScore\"] = output_df.apply(\n-            lambda x: char_ord_score(x.CommandLine), axis=1\n-        )\n-    if \"commandlineTokensHash\" not in output_df or force:\n-        output_df[\"commandlineTokensHash\"] = output_df.apply(\n-            lambda x: delim_hash(x.CommandLine), axis=1\n-        )\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def delim_count(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n-    r\"\"\"\n-    Count the delimiters in input column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    delim_list : str, optional\n-        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n-\n-    Returns\n-    -------\n-    int\n-        Count of delimiters in the string.\n-\n-    \"\"\"\n-    return len(re.findall(delim_list, value))\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def delim_hash(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n-    r\"\"\"\n-    Return a hash (CRC32) of the delimiters from input column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    delim_list : str, optional\n-        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n-\n-    Returns\n-    -------\n-    int\n-        Hash of delimiter set in the string.\n-\n-    \"\"\"\n-    return crc32(bytes(\"\".join(re.findall(delim_list, value)), \"utf-8\"))\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def char_ord_score(value: str, scale: int = 1) -> int:\n-    \"\"\"\n-    Return sum of ord values of characters in string.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    scale : int, optional\n-        reduce the scale of the feature (reducing the\n-        influence of variations this feature on the clustering\n-        algorithm (the default is 1)\n-\n-    Returns\n-    -------\n-    int\n-        [description]\n-\n-    Notes\n-    -----\n-    This function sums the ordinal value of each character in the\n-    input string. Two strings with minor differences will result in\n-    a similar score. However, for strings with highly variable content\n-    (e.g. command lines or http requests containing GUIDs) this may result\n-    in too much variance to be useful when you are trying to detect\n-    similar patterns. You can use the scale parameter to reduce the\n-    influence of features using this function on clustering and anomaly\n-    algorithms.\n-\n-    \"\"\"\n-    return floor(sum(ord(x) for x in value) / scale)\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def token_count(value: str, delimiter: str = \" \") -> int:\n-    \"\"\"\n-    Return count of delimiter-separated tokens pd.Series column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    delimiter : str, optional\n-        Delimiter used to split the column string.\n-        (the default is ' ')\n-\n-    Returns\n-    -------\n-    int\n-        count of tokens\n-\n-    \"\"\"\n-    return len(value.split(delimiter))\n-\n-\n-def _string_score(input_str):\n-    \"\"\"Sum the ord(c) for characters in a string.\"\"\"\n-    return sum(ord(x) for x in input_str)\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def crc32_hash(value: str) -> int:\n-    \"\"\"\n-    Return the CRC32 hash of the input column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-\n-    Returns\n-    -------\n-    int\n-        CRC32 hash\n-\n-    \"\"\"\n-    return crc32(bytes(value.encode(\"utf-8\")))\n-\n-\n-def delim_count_df(\n-    data: pd.DataFrame, column: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]'\n-) -> pd.Series:\n-    r\"\"\"\n-    Count the delimiters in input column.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        The name of the column to process\n-    delim_list : str, optional\n-        delimiters to use. (the default is r\\'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]\\')\n-\n-    Returns\n-    -------\n-    pd.Series\n-        Count of delimiters in the string in `column`.\n-\n-    \"\"\"\n-    return data[column].str.count(delim_list)\n-\n-\n-def char_ord_score_df(data: pd.DataFrame, column: str, scale: int = 1) -> pd.Series:\n-    \"\"\"\n-    Return sum of ord values of characters in string.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        Column name to process\n-    scale : int, optional\n-        reduce the scale of the feature (reducing the\n-        influence of variations this feature on the clustering\n-        algorithm (the default is 1)\n-\n-    Returns\n-    -------\n-    pd.Series\n-        The sum of the ordinal values of the characters\n-        in `column`.\n-\n-    Notes\n-    -----\n-    This function sums the ordinal value of each character in the\n-    input string. Two strings with minor differences will result in\n-    a similar score. However, for strings with highly variable content\n-    (e.g. command lines or http requests containing GUIDs) this may result\n-    in too much variance to be useful when you are trying to detect\n-    similar patterns. You can use the scale parameter to reduce the\n-    influence of features using this function on clustering and anomaly\n-    algorithms.\n-\n-    \"\"\"\n-    return data.apply(lambda x: sum(ord(char) for char in x[column]) / scale, axis=1)\n-\n-\n-def token_count_df(data: pd.DataFrame, column: str, delimiter: str = \" \") -> pd.Series:\n-    \"\"\"\n-    Return count of delimiter-separated tokens pd.Series column.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        Column name to process\n-    delimiter : str, optional\n-        Delimiter used to split the column string.\n-        (the default is ' ')\n-\n-    Returns\n-    -------\n-    pd.Series\n-        count of tokens in strings in `column`\n-\n-    \"\"\"\n-    return data.apply(lambda x: len(x[column].split(delimiter)), axis=1)\n-\n-\n-def crc32_hash_df(data: pd.DataFrame, column: str) -> pd.Series:\n-    \"\"\"\n-    Return the CRC32 hash of the input column.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        Column name to process\n-\n-    Returns\n-    -------\n-    pd.Series\n-        CRC32 hash of input column\n-\n-    \"\"\"\n-    return data.apply(lambda x: crc32(bytes(x[column].encode(\"utf-8\"))), axis=1)\n-\n-\n-# pylint: disable=too-many-arguments, too-many-statements\n-@export  # noqa: C901, MC0001\n-def plot_cluster(\n-    db_cluster: DBSCAN,\n-    data: pd.DataFrame,\n-    x_predict: np.ndarray,\n-    plot_label: str = None,\n-    plot_features: Tuple[int, int] = (0, 1),\n-    verbose: bool = False,\n-    cut_off: int = 3,\n-    xlabel: str = None,\n-    ylabel: str = None,\n-):\n-    \"\"\"\n-    Plot clustered data as scatter chart.\n-\n-    Parameters\n-    ----------\n-    db_cluster : DBSCAN\n-        DBScan Cluster (from SkLearn DBSCAN).\n-    data : pd.DataFrame\n-        Dataframe containing original data.\n-    x_predict : np.ndarray\n-        The DBSCAN predict numpy array\n-    plot_label : str, optional\n-         If set the column to use to label data points\n-         (the default is None)\n-    plot_features :  Tuple[int, int], optional\n-        Which two features in x_predict to plot (the default is (0, 1))\n-    verbose : bool, optional\n-        Verbose execution with some extra info\n-        (the default is False)\n-    cut_off : int, optional\n-        The cluster size below which items are considered outliers\n-        (the default is 3)\n-    xlabel : str, optional\n-        x-axis label (the default is None)\n-    ylabel : str, optional\n-        y-axis label (the default is None)\n-\n-    \"\"\"\n-    max_idx = x_predict.shape[1] - 1\n-    if plot_features[0] >= x_predict.shape[1]:\n-        raise ValueError(\n-            \"plot_features[0] index must be a value from 0 to {}.\".format(max_idx)\n-        )\n-    if plot_features[1] >= x_predict.shape[1]:\n-        raise ValueError(\n-            \"plot_features[1] index must be a value from 0 to {}.\".format(max_idx)\n-        )\n-    if plot_features[0] == plot_features[1]:\n-        mssg = \"plot_features indexes must be 2 different values in range 0 to\"\n-        raise ValueError(mssg + f\" {max_idx}.\")\n-\n-    labels = db_cluster.labels_\n-    core_samples_mask = np.zeros_like(labels, dtype=bool)\n-\n-    # pylint: disable=unsupported-assignment-operation\n-    # (assignment of numpy array is valid)\n-    core_samples_mask[db_cluster.core_sample_indices_] = True\n-    unique_labels = set(labels)\n-\n-    # pylint: disable=no-member\n-    # Spectral color map does exist\n-    colors = [cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n-    # Number of clusters in labels, ignoring noise if present.\n-    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n-    n_noise_ = list(labels).count(-1)\n-    _, counts = np.unique(labels, return_counts=True)\n-\n-    if verbose:\n-        print(\"Estimated number of clusters: %d\" % n_clusters_)\n-        print(\"Estimated number of noise points: %d\" % n_noise_)\n-        # print(\"Silhouette Coefficient: %0.3f\"\n-        #       % metrics.silhouette_score(x_predict, labels))\n-\n-    if not isinstance(data, pd.DataFrame):\n-        plot_label = None\n-    elif plot_label is not None and plot_label not in data:\n-        plot_label = None\n-\n-    p_label = None\n-    for cluster_id, color in zip(unique_labels, colors):\n-        if cluster_id == -1:\n-            # Black used for noise.\n-            color = [0, 0, 0, 1]\n-        class_member_mask = labels == cluster_id\n-\n-        cluster_size = counts[cluster_id]\n-        marker_size = cluster_size\n-        marker = \"o\"\n-        font_size = \"small\"\n-        alpha = 0.4\n-\n-        if cluster_size < cut_off:\n-            marker = \"+\"\n-            marker_size = 10\n-            font_size = \"large\"\n-            alpha = 1.0\n-        xy_pos = x_predict[class_member_mask & core_samples_mask]\n-        plt.plot(\n-            xy_pos[:, plot_features[0]],\n-            xy_pos[:, plot_features[1]],\n-            marker,\n-            markerfacecolor=tuple(color),\n-            markersize=marker_size,\n-        )\n-\n-        if plot_label:\n-            first_row = data[class_member_mask].iloc[0]\n-            if not first_row.empty and plot_label in first_row:\n-                p_label = first_row[plot_label]\n-                try:\n-                    plt.annotate(\n-                        p_label,\n-                        xy=(xy_pos[0, plot_features[0]], xy_pos[0, plot_features[1]]),\n-                        fontsize=font_size,\n-                        alpha=alpha,\n-                    )\n-                except IndexError:\n-                    pass\n-\n-    plt.xlabel(xlabel)\n-    plt.ylabel(ylabel)\n-    plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n-    plt.show()\n-    return plt\n+WARN_MSSG = (\n+    \"This module has moved to msticpy.analysis.eventcluster\\n\"\n+    + \"Please change your import to reflect this new location.\"\n+)\n+warnings.warn(WARN_MSSG, category=DeprecationWarning)\n",
        "source_code_with_indent": "\n# pylint: disable=too-many-arguments, too-many-locals\n@export\ndef dbcluster_events(\n    data: Any,\n    cluster_columns: List[Any] = None,\n    verbose: bool = False,\n    normalize: bool = True,\n    time_column: str = \"TimeCreatedUtc\",\n    max_cluster_distance: float = 0.01,\n    min_cluster_samples: int = 2,\n    **kwargs,\n) -> Tuple[pd.DataFrame, DBSCAN, np.ndarray]:\n    <IND>\"\"\"\n    Cluster data set according to cluster_columns features.\n\n    Parameters\n    ----------\n    data : Any\n        Input data as a pandas DataFrame or numpy array\n    cluster_columns : List[Any], optional\n        List of columns to use for features\n        - for DataFrame this is a list of column names\n        - for numpy array this is a list of column indexes\n    verbose : bool, optional\n        Print additional information about clustering results (the default is False)\n    normalize : bool, optional\n        Normalize the input data (should probably always be True)\n    time_column : str, optional\n        If there is a time column the output data will be ordered by this\n        (the default is 'TimeCreatedUtc')\n    max_cluster_distance : float, optional\n        DBSCAN eps (max cluster member distance) (the default is 0.01)\n    min_cluster_samples : int, optional\n        DBSCAN min_samples (the minimum cluster size) (the default is 2)\n\n    Other Parameters\n    ----------------\n    kwargs: Other arguments are passed to DBSCAN constructor\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, DBSCAN, np.ndarray]\n        Output dataframe with clustered rows\n        DBSCAN model\n        Normalized data set\n\n    \"\"\"\n    allowed_types = [np.ndarray, pd.DataFrame]\n\n    x_input = None\n    if isinstance(data, pd.DataFrame):\n        <IND>if cluster_columns is None:\n            <IND>x_input = data.values\n        <DED>else:\n            <IND>x_input = data[cluster_columns].values\n    <DED><DED>elif isinstance(data, np.ndarray):\n        <IND>x_input = data if cluster_columns is None else data[:, cluster_columns].values\n    <DED>if x_input is None:\n        <IND>mssg = \"Input data not in expected format.\\n{} is not one of allowed types {}\"\n        type_list = \", \".join([str(t) for t in allowed_types])\n        mssg = mssg.format(str(type(data)), type_list)\n        raise ValueError(mssg)\n\n    # Create DBSCAN cluster object\n    <DED>db_cluster = DBSCAN(\n        eps=max_cluster_distance, min_samples=min_cluster_samples, **kwargs\n    )\n\n    # Normalize the data (most clustering algorithms don't do well with\n    # unnormalized data)\n    x_norm = Normalizer().fit_transform(x_input) if normalize else x_input\n    # fit the data set\n    db_cluster.fit(x_norm)\n    labels = db_cluster.labels_\n    cluster_set, counts = np.unique(labels, return_counts=True)\n    if verbose:\n        <IND>print(\n            \"Clustering for set size \",\n            len(x_norm),\n            \" - \",\n            len(cluster_set),\n            \" clusters\",\n        )\n        print(\"Individual cluster sizes: \", \", \".join([str(c) for c in counts]))\n\n    <DED>clustered_events = _merge_clustered_items(\n        cluster_set, labels, data, time_column, counts\n    )\n\n    if verbose:\n        <IND>print(\"Cluster output rows: \", len(clustered_events))\n\n    <DED>return clustered_events, db_cluster, x_norm\n\n\n<DED>def _merge_clustered_items(\n    cluster_set: np.array,\n    labels: np.array,\n    data: Union[pd.DataFrame, np.array],\n    time_column: str,\n    counts: np.array,\n) -> pd.DataFrame:\n    <IND>\"\"\"\n    Merge outliers and core clusters into single DataFrame.\n\n    Parameters\n    ----------\n    cluster_set : np.array\n        The set of clusters\n    labels : np.array\n        The cluster labels\n    data : Union[pd.DataFrame, np.array]\n        The source data\n    time_column : str\n        Name of the Time column\n    counts : np.array\n        The counts of members in each cluster\n\n    Returns\n    -------\n    pd.DataFrame\n        Merged dataframe\n\n    \"\"\"\n    tz_aware = data.iloc[0][time_column].tz\n    ts_type = \"datetime64[ns, UTC]\" if tz_aware is not None else \"datetime64[ns]\"\n\n    cluster_list = []\n    # Iterate through clusters, adding exemplar to output frame\n    # pylint: disable=consider-using-enumerate\n    # we need to know the index of the item within the loop\n    for idx in range(len(cluster_set)):\n        <IND>cluster_id = cluster_set[idx]\n        class_members = labels == cluster_id\n        if isinstance(data, pd.DataFrame):\n            <IND>time_ordered = data[class_members].sort_values(time_column, ascending=True)\n            first_event_time = time_ordered[0:][time_column].iat[0]\n            last_event_time = time_ordered[-1:][time_column].iat[0]\n        <DED>else:\n            <IND>first_event_time = None\n            last_event_time = None\n\n        <DED>if cluster_id == -1:\n            # 'Noise' events are individual items that could not be assigned\n            # to a cluster and so are unique\n            <IND>cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=False,\n                    ClusterId=cluster_id,\n                    ClusterSize=1,\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n        <DED>else:\n            # Otherwise, just choose the first example of the cluster set\n            <IND>cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=True,\n                    ClusterId=cluster_id,\n                    ClusterSize=counts[idx],\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )[0:1]\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n    # pylint: enable=consider-using-enumerate\n    <DED><DED>return pd.concat(cluster_list)\n\n\n<DED>@export\ndef add_process_features(\n    input_frame: pd.DataFrame, path_separator: str = None, force: bool = False\n) -> pd.DataFrame:\n    <IND>r\"\"\"\n    Add numerical features based on patterns of command line and process name.\n\n    Parameters\n    ----------\n    input_frame : pd.DataFrame\n        The input dataframe\n    path_separator : str, optional\n        Path separator. If not supplied, try to determine\n        from 'NewProcessName' column of first 10 rows\n        (the default is None)\n    force : bool, optional\n        Forces re-calculation of feature columns even if they\n        already exist (the default is False)\n\n    Returns\n    -------\n    pd.DataFrame\n        Copy of the dataframe with the additional numeric features\n\n    Notes\n    -----\n    Features added:\n\n    - processNameLen: length of process file name (inc path)\n    - processNameTokens: the number of elements in the path\n    - processName: the process file name (minus path)\n    - commandlineTokens: number of space-separated tokens in the command line\n    - commandlineLen: length of the command line\n    - commandlineLogLen: log10 length of commandline\n    - isSystemSession: 1 if session Id is 0x3e7 for Windows or -1 for Linux\n    - commandlineTokensFull: counts number of token separators in commandline\n      [\\\\s\\-\\\\/\\.,\"\\'\\|&:;%$()]\n    - pathScore: sum of ord() value of characters in path\n    - pathLogScore: log10 of pathScore\n    - commandlineScore: sum of ord() value of characters in commandline\n    - commandlineLogScore: log10 of commandlineScore\n\n    \"\"\"\n    output_df = input_frame.copy()\n\n    # Set any NaN values to empty string\n    if \"NewProcessName\" in output_df and \"CommandLine\" in output_df:\n        <IND>output_df[[\"NewProcessName\", \"CommandLine\"]] = output_df[\n            [\"NewProcessName\", \"CommandLine\"]\n        ].fillna(value=\"\")\n\n    # try to determine the path separator\n    <DED>if path_separator is None:\n        <IND>sample_df = output_df.head(10)\n        lx_path = len(sample_df[sample_df[\"NewProcessName\"].str.contains(\"/\")])\n        path_separator = \"/\" if lx_path else \"\\\\\"\n    # Create features from process name and command line\n    <DED>if \"NewProcessName\" in output_df:\n        <IND>_add_processname_features(output_df, force, path_separator)\n\n    <DED>if \"CommandLine\" in output_df:\n        <IND>_add_commandline_features(output_df, force)\n\n    <DED>if \"SubjectLogonId\" in output_df and (\"isSystemSession\" not in output_df or force):\n        <IND>output_df[\"isSystemSession\"] = output_df[\"SubjectLogonId\"].isin([\"0x3e7\", \"-1\"])\n\n    <DED>return output_df\n\n\n<DED>def _add_processname_features(\n    output_df: pd.DataFrame, force: bool, path_separator: str\n):\n    <IND>\"\"\"\n    Add process name default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n    path_separator : str\n        Path separator for OS\n\n    \"\"\"\n    if \"processName\" not in output_df or force:\n        <IND>output_df[\"processName\"] = output_df.apply(\n            lambda x: x.NewProcessName.split(path_separator)[-1], axis=1\n        )\n    <DED>if \"pathScore\" not in output_df or force:\n        <IND>output_df[\"pathScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.NewProcessName), axis=1\n        )\n    <DED>if \"pathLogScore\" not in output_df or force:\n        <IND>output_df[\"pathLogScore\"] = output_df.apply(\n            lambda x: log10(x.pathScore) if x.pathScore else 0, axis=1\n        )\n    <DED>if \"pathHash\" not in output_df or force:\n        <IND>output_df[\"pathHash\"] = output_df.apply(\n            lambda x: crc32_hash(x.NewProcessName), axis=1\n        )\n\n\n<DED><DED>def _add_commandline_features(output_df: pd.DataFrame, force: bool):\n    <IND>\"\"\"\n    Add commandline default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n\n    \"\"\"\n    if \"commandlineLen\" not in output_df or force:\n        <IND>output_df[\"commandlineLen\"] = output_df.apply(\n            lambda x: len(x.CommandLine), axis=1\n        )\n    <DED>if \"commandlineLogLen\" not in output_df or force:\n        <IND>output_df[\"commandlineLogLen\"] = output_df.apply(\n            lambda x: log10(x.commandlineLen) if x.commandlineLen else 0, axis=1\n        )\n    <DED>if \"commandlineTokensFull\" not in output_df or force:\n        <IND>output_df[\"commandlineTokensFull\"] = output_df[[\"CommandLine\"]].apply(\n            lambda x: delim_count(x.CommandLine), axis=1\n        )\n\n    <DED>if \"commandlineScore\" not in output_df or force:\n        <IND>output_df[\"commandlineScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.CommandLine), axis=1\n        )\n    <DED>if \"commandlineTokensHash\" not in output_df or force:\n        <IND>output_df[\"commandlineTokensHash\"] = output_df.apply(\n            lambda x: delim_hash(x.CommandLine), axis=1\n        )\n\n\n<DED><DED>@export\n@lru_cache(maxsize=1024)\ndef delim_count(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    <IND>r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Count of delimiters in the string.\n\n    \"\"\"\n    return len(re.findall(delim_list, value))\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef delim_hash(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    <IND>r\"\"\"\n    Return a hash (CRC32) of the delimiters from input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Hash of delimiter set in the string.\n\n    \"\"\"\n    return crc32(bytes(\"\".join(re.findall(delim_list, value)), \"utf-8\"))\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef char_ord_score(value: str, scale: int = 1) -> int:\n    <IND>\"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    int\n        [description]\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return floor(sum(ord(x) for x in value) / scale)\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef token_count(value: str, delimiter: str = \" \") -> int:\n    <IND>\"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    int\n        count of tokens\n\n    \"\"\"\n    return len(value.split(delimiter))\n\n\n<DED>def _string_score(input_str):\n    <IND>\"\"\"Sum the ord(c) for characters in a string.\"\"\"\n    return sum(ord(x) for x in input_str)\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef crc32_hash(value: str) -> int:\n    <IND>\"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n\n    Returns\n    -------\n    int\n        CRC32 hash\n\n    \"\"\"\n    return crc32(bytes(value.encode(\"utf-8\")))\n\n\n<DED>def delim_count_df(\n    data: pd.DataFrame, column: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]'\n) -> pd.Series:\n    <IND>r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        The name of the column to process\n    delim_list : str, optional\n        delimiters to use. (the default is r\\'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]\\')\n\n    Returns\n    -------\n    pd.Series\n        Count of delimiters in the string in `column`.\n\n    \"\"\"\n    return data[column].str.count(delim_list)\n\n\n<DED>def char_ord_score_df(data: pd.DataFrame, column: str, scale: int = 1) -> pd.Series:\n    <IND>\"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    pd.Series\n        The sum of the ordinal values of the characters\n        in `column`.\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return data.apply(lambda x: sum(ord(char) for char in x[column]) / scale, axis=1)\n\n\n<DED>def token_count_df(data: pd.DataFrame, column: str, delimiter: str = \" \") -> pd.Series:\n    <IND>\"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    pd.Series\n        count of tokens in strings in `column`\n\n    \"\"\"\n    return data.apply(lambda x: len(x[column].split(delimiter)), axis=1)\n\n\n<DED>def crc32_hash_df(data: pd.DataFrame, column: str) -> pd.Series:\n    <IND>\"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n\n    Returns\n    -------\n    pd.Series\n        CRC32 hash of input column\n\n    \"\"\"\n    return data.apply(lambda x: crc32(bytes(x[column].encode(\"utf-8\"))), axis=1)\n\n\n# pylint: disable=too-many-arguments, too-many-statements\n<DED>@export  # noqa: C901, MC0001\ndef plot_cluster(\n    db_cluster: DBSCAN,\n    data: pd.DataFrame,\n    x_predict: np.ndarray,\n    plot_label: str = None,\n    plot_features: Tuple[int, int] = (0, 1),\n    verbose: bool = False,\n    cut_off: int = 3,\n    xlabel: str = None,\n    ylabel: str = None,\n):\n    <IND>\"\"\"\n    Plot clustered data as scatter chart.\n\n    Parameters\n    ----------\n    db_cluster : DBSCAN\n        DBScan Cluster (from SkLearn DBSCAN).\n    data : pd.DataFrame\n        Dataframe containing original data.\n    x_predict : np.ndarray\n        The DBSCAN predict numpy array\n    plot_label : str, optional\n         If set the column to use to label data points\n         (the default is None)\n    plot_features :  Tuple[int, int], optional\n        Which two features in x_predict to plot (the default is (0, 1))\n    verbose : bool, optional\n        Verbose execution with some extra info\n        (the default is False)\n    cut_off : int, optional\n        The cluster size below which items are considered outliers\n        (the default is 3)\n    xlabel : str, optional\n        x-axis label (the default is None)\n    ylabel : str, optional\n        y-axis label (the default is None)\n\n    \"\"\"\n    max_idx = x_predict.shape[1] - 1\n    if plot_features[0] >= x_predict.shape[1]:\n        <IND>raise ValueError(\n            \"plot_features[0] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    <DED>if plot_features[1] >= x_predict.shape[1]:\n        <IND>raise ValueError(\n            \"plot_features[1] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    <DED>if plot_features[0] == plot_features[1]:\n        <IND>mssg = \"plot_features indexes must be 2 different values in range 0 to\"\n        raise ValueError(mssg + f\" {max_idx}.\")\n\n    <DED>labels = db_cluster.labels_\n    core_samples_mask = np.zeros_like(labels, dtype=bool)\n\n    # pylint: disable=unsupported-assignment-operation\n    # (assignment of numpy array is valid)\n    core_samples_mask[db_cluster.core_sample_indices_] = True\n    unique_labels = set(labels)\n\n    # pylint: disable=no-member\n    # Spectral color map does exist\n    colors = [cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n    # Number of clusters in labels, ignoring noise if present.\n    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n    n_noise_ = list(labels).count(-1)\n    _, counts = np.unique(labels, return_counts=True)\n\n    if verbose:\n        <IND>print(\"Estimated number of clusters: %d\" % n_clusters_)\n        print(\"Estimated number of noise points: %d\" % n_noise_)\n        # print(\"Silhouette Coefficient: %0.3f\"\n        #       % metrics.silhouette_score(x_predict, labels))\n\n    <DED>if not isinstance(data, pd.DataFrame):\n        <IND>plot_label = None\n    <DED>elif plot_label is not None and plot_label not in data:\n        <IND>plot_label = None\n\n    <DED>p_label = None\n    for cluster_id, color in zip(unique_labels, colors):\n        <IND>if cluster_id == -1:\n            # Black used for noise.\n            <IND>color = [0, 0, 0, 1]\n        <DED>class_member_mask = labels == cluster_id\n\n        cluster_size = counts[cluster_id]\n        marker_size = cluster_size\n        marker = \"o\"\n        font_size = \"small\"\n        alpha = 0.4\n\n        if cluster_size < cut_off:\n            <IND>marker = \"+\"\n            marker_size = 10\n            font_size = \"large\"\n            alpha = 1.0\n        <DED>xy_pos = x_predict[class_member_mask & core_samples_mask]\n        plt.plot(\n            xy_pos[:, plot_features[0]],\n            xy_pos[:, plot_features[1]],\n            marker,\n            markerfacecolor=tuple(color),\n            markersize=marker_size,\n        )\n\n        if plot_label:\n            <IND>first_row = data[class_member_mask].iloc[0]\n            if not first_row.empty and plot_label in first_row:\n                <IND>p_label = first_row[plot_label]\n                try:\n                    <IND>plt.annotate(\n                        p_label,\n                        xy=(xy_pos[0, plot_features[0]], xy_pos[0, plot_features[1]]),\n                        fontsize=font_size,\n                        alpha=alpha,\n                    )\n                <DED>except IndexError:\n                    <IND>pass\n\n    <DED><DED><DED><DED>plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n    plt.show()\n    return plt\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n# flake8: noqa: F403, F401\n# pylint: disable=wildcard-import, unused-wildcard-import, unused-import\nfrom ..analysis.eventcluster import *\n\nWARN_MSSG = (\n    \"This module has moved to msticpy.analysis.eventcluster\\n\"\n    + \"Please change your import to reflect this new location.\"\n)\nwarnings.warn(WARN_MSSG, category=DeprecationWarning)\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "microsoft/msticpy",
    "commit": "28466d681e261394e18d9f4e063cebfa06ed04b4",
    "filename": "msticpy/sectools/eventcluster.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/microsoft-msticpy/msticpy/sectools/eventcluster.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "msticpy/sectools/eventcluster.py:240:31 Incompatible variable type [9]: path_separator is declared to have type `str` but is used as type `None`.",
    "message": " path_separator is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 240,
    "warning_line": "    input_frame: pd.DataFrame, path_separator: str = None, force: bool = False",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "# --------------------------------------------------------------------------\nr\"\"\"\neventcluster module.\n\nThis module is intended to be used to summarize large numbers of events\ninto clusters of different patterns. High volume repeating events can\noften make it difficult to see unique and interesting items.\n\nThe module contains functions to generate clusterable features from\nstring data. For example, an administration command that does some\nmaintenance on thousands of servers with a commandline such as:\n``install-update -hostname {host.fqdn} -tmp:/tmp/{GUID}/rollback``\\  can\nbe collapsed into a single cluster pattern by ignoring the character\nvalues in the string and using delimiters or tokens to group the values.\n\nThis is an unsupervised learning module implemented using SciKit Learn\nDBScan.\n\nContains:\ndbcluster_events: generic clustering method using DBSCAN designed to summarize\nprocess events and other similar data by grouping on common features.\n\nadd_process_features: derives numerical features from text features such as\ncommandline and process path.\n\n\"\"\"\nfrom binascii import crc32\nfrom functools import lru_cache\nfrom math import log10, floor\nimport re\nfrom typing import List, Any, Tuple, Union\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import Normalizer\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nfrom ..common.utility import export\nfrom .._version import VERSION\n",
        "source_code_len": 1462,
        "target_code": "# --------------------------------------------------------------------------\n\"\"\"Deprecated placeholder for eventcluster.py.\"\"\"\nimport warnings\nfrom .._version import VERSION\n",
        "target_code_len": 174,
        "diff_format": "@@ -5,41 +5,4 @@\n # --------------------------------------------------------------------------\n-r\"\"\"\n-eventcluster module.\n-\n-This module is intended to be used to summarize large numbers of events\n-into clusters of different patterns. High volume repeating events can\n-often make it difficult to see unique and interesting items.\n-\n-The module contains functions to generate clusterable features from\n-string data. For example, an administration command that does some\n-maintenance on thousands of servers with a commandline such as:\n-``install-update -hostname {host.fqdn} -tmp:/tmp/{GUID}/rollback``\\  can\n-be collapsed into a single cluster pattern by ignoring the character\n-values in the string and using delimiters or tokens to group the values.\n-\n-This is an unsupervised learning module implemented using SciKit Learn\n-DBScan.\n-\n-Contains:\n-dbcluster_events: generic clustering method using DBSCAN designed to summarize\n-process events and other similar data by grouping on common features.\n-\n-add_process_features: derives numerical features from text features such as\n-commandline and process path.\n-\n-\"\"\"\n-from binascii import crc32\n-from functools import lru_cache\n-from math import log10, floor\n-import re\n-from typing import List, Any, Tuple, Union\n-\n-import numpy as np\n-import pandas as pd\n-from sklearn.cluster import DBSCAN\n-from sklearn.preprocessing import Normalizer\n-import matplotlib.pyplot as plt\n-from matplotlib import cm\n-\n-from ..common.utility import export\n+\"\"\"Deprecated placeholder for eventcluster.py.\"\"\"\n+import warnings\n from .._version import VERSION\n",
        "source_code_with_indent": "# --------------------------------------------------------------------------\nr\"\"\"\neventcluster module.\n\nThis module is intended to be used to summarize large numbers of events\ninto clusters of different patterns. High volume repeating events can\noften make it difficult to see unique and interesting items.\n\nThe module contains functions to generate clusterable features from\nstring data. For example, an administration command that does some\nmaintenance on thousands of servers with a commandline such as:\n``install-update -hostname {host.fqdn} -tmp:/tmp/{GUID}/rollback``\\  can\nbe collapsed into a single cluster pattern by ignoring the character\nvalues in the string and using delimiters or tokens to group the values.\n\nThis is an unsupervised learning module implemented using SciKit Learn\nDBScan.\n\nContains:\ndbcluster_events: generic clustering method using DBSCAN designed to summarize\nprocess events and other similar data by grouping on common features.\n\nadd_process_features: derives numerical features from text features such as\ncommandline and process path.\n\n\"\"\"\nfrom binascii import crc32\nfrom functools import lru_cache\nfrom math import log10, floor\nimport re\nfrom typing import List, Any, Tuple, Union\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import Normalizer\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nfrom ..common.utility import export\nfrom .._version import VERSION\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "# --------------------------------------------------------------------------\n\"\"\"Deprecated placeholder for eventcluster.py.\"\"\"\nimport warnings\nfrom .._version import VERSION\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n# pylint: disable=too-many-arguments, too-many-locals\n@export\ndef dbcluster_events(\n    data: Any,\n    cluster_columns: List[Any] = None,\n    verbose: bool = False,\n    normalize: bool = True,\n    time_column: str = \"TimeCreatedUtc\",\n    max_cluster_distance: float = 0.01,\n    min_cluster_samples: int = 2,\n    **kwargs,\n) -> Tuple[pd.DataFrame, DBSCAN, np.ndarray]:\n    \"\"\"\n    Cluster data set according to cluster_columns features.\n\n    Parameters\n    ----------\n    data : Any\n        Input data as a pandas DataFrame or numpy array\n    cluster_columns : List[Any], optional\n        List of columns to use for features\n        - for DataFrame this is a list of column names\n        - for numpy array this is a list of column indexes\n    verbose : bool, optional\n        Print additional information about clustering results (the default is False)\n    normalize : bool, optional\n        Normalize the input data (should probably always be True)\n    time_column : str, optional\n        If there is a time column the output data will be ordered by this\n        (the default is 'TimeCreatedUtc')\n    max_cluster_distance : float, optional\n        DBSCAN eps (max cluster member distance) (the default is 0.01)\n    min_cluster_samples : int, optional\n        DBSCAN min_samples (the minimum cluster size) (the default is 2)\n\n    Other Parameters\n    ----------------\n    kwargs: Other arguments are passed to DBSCAN constructor\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, DBSCAN, np.ndarray]\n        Output dataframe with clustered rows\n        DBSCAN model\n        Normalized data set\n\n    \"\"\"\n    allowed_types = [np.ndarray, pd.DataFrame]\n\n    x_input = None\n    if isinstance(data, pd.DataFrame):\n        if cluster_columns is None:\n            x_input = data.values\n        else:\n            x_input = data[cluster_columns].values\n    elif isinstance(data, np.ndarray):\n        x_input = data if cluster_columns is None else data[:, cluster_columns].values\n    if x_input is None:\n        mssg = \"Input data not in expected format.\\n{} is not one of allowed types {}\"\n        type_list = \", \".join([str(t) for t in allowed_types])\n        mssg = mssg.format(str(type(data)), type_list)\n        raise ValueError(mssg)\n\n    # Create DBSCAN cluster object\n    db_cluster = DBSCAN(\n        eps=max_cluster_distance, min_samples=min_cluster_samples, **kwargs\n    )\n\n    # Normalize the data (most clustering algorithms don't do well with\n    # unnormalized data)\n    x_norm = Normalizer().fit_transform(x_input) if normalize else x_input\n    # fit the data set\n    db_cluster.fit(x_norm)\n    labels = db_cluster.labels_\n    cluster_set, counts = np.unique(labels, return_counts=True)\n    if verbose:\n        print(\n            \"Clustering for set size \",\n            len(x_norm),\n            \" - \",\n            len(cluster_set),\n            \" clusters\",\n        )\n        print(\"Individual cluster sizes: \", \", \".join([str(c) for c in counts]))\n\n    clustered_events = _merge_clustered_items(\n        cluster_set, labels, data, time_column, counts\n    )\n\n    if verbose:\n        print(\"Cluster output rows: \", len(clustered_events))\n\n    return clustered_events, db_cluster, x_norm\n\n\ndef _merge_clustered_items(\n    cluster_set: np.array,\n    labels: np.array,\n    data: Union[pd.DataFrame, np.array],\n    time_column: str,\n    counts: np.array,\n) -> pd.DataFrame:\n    \"\"\"\n    Merge outliers and core clusters into single DataFrame.\n\n    Parameters\n    ----------\n    cluster_set : np.array\n        The set of clusters\n    labels : np.array\n        The cluster labels\n    data : Union[pd.DataFrame, np.array]\n        The source data\n    time_column : str\n        Name of the Time column\n    counts : np.array\n        The counts of members in each cluster\n\n    Returns\n    -------\n    pd.DataFrame\n        Merged dataframe\n\n    \"\"\"\n    tz_aware = data.iloc[0][time_column].tz\n    ts_type = \"datetime64[ns, UTC]\" if tz_aware is not None else \"datetime64[ns]\"\n\n    cluster_list = []\n    # Iterate through clusters, adding exemplar to output frame\n    # pylint: disable=consider-using-enumerate\n    # we need to know the index of the item within the loop\n    for idx in range(len(cluster_set)):\n        cluster_id = cluster_set[idx]\n        class_members = labels == cluster_id\n        if isinstance(data, pd.DataFrame):\n            time_ordered = data[class_members].sort_values(time_column, ascending=True)\n            first_event_time = time_ordered[0:][time_column].iat[0]\n            last_event_time = time_ordered[-1:][time_column].iat[0]\n        else:\n            first_event_time = None\n            last_event_time = None\n\n        if cluster_id == -1:\n            # 'Noise' events are individual items that could not be assigned\n            # to a cluster and so are unique\n            cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=False,\n                    ClusterId=cluster_id,\n                    ClusterSize=1,\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n        else:\n            # Otherwise, just choose the first example of the cluster set\n            cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=True,\n                    ClusterId=cluster_id,\n                    ClusterSize=counts[idx],\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )[0:1]\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n    # pylint: enable=consider-using-enumerate\n    return pd.concat(cluster_list)\n\n\n@export\ndef add_process_features(\n    input_frame: pd.DataFrame, path_separator: str = None, force: bool = False\n) -> pd.DataFrame:\n    r\"\"\"\n    Add numerical features based on patterns of command line and process name.\n\n    Parameters\n    ----------\n    input_frame : pd.DataFrame\n        The input dataframe\n    path_separator : str, optional\n        Path separator. If not supplied, try to determine\n        from 'NewProcessName' column of first 10 rows\n        (the default is None)\n    force : bool, optional\n        Forces re-calculation of feature columns even if they\n        already exist (the default is False)\n\n    Returns\n    -------\n    pd.DataFrame\n        Copy of the dataframe with the additional numeric features\n\n    Notes\n    -----\n    Features added:\n\n    - processNameLen: length of process file name (inc path)\n    - processNameTokens: the number of elements in the path\n    - processName: the process file name (minus path)\n    - commandlineTokens: number of space-separated tokens in the command line\n    - commandlineLen: length of the command line\n    - commandlineLogLen: log10 length of commandline\n    - isSystemSession: 1 if session Id is 0x3e7 for Windows or -1 for Linux\n    - commandlineTokensFull: counts number of token separators in commandline\n      [\\\\s\\-\\\\/\\.,\"\\'\\|&:;%$()]\n    - pathScore: sum of ord() value of characters in path\n    - pathLogScore: log10 of pathScore\n    - commandlineScore: sum of ord() value of characters in commandline\n    - commandlineLogScore: log10 of commandlineScore\n\n    \"\"\"\n    output_df = input_frame.copy()\n\n    # Set any NaN values to empty string\n    if \"NewProcessName\" in output_df and \"CommandLine\" in output_df:\n        output_df[[\"NewProcessName\", \"CommandLine\"]] = output_df[\n            [\"NewProcessName\", \"CommandLine\"]\n        ].fillna(value=\"\")\n\n    # try to determine the path separator\n    if path_separator is None:\n        sample_df = output_df.head(10)\n        lx_path = len(sample_df[sample_df[\"NewProcessName\"].str.contains(\"/\")])\n        path_separator = \"/\" if lx_path else \"\\\\\"\n    # Create features from process name and command line\n    if \"NewProcessName\" in output_df:\n        _add_processname_features(output_df, force, path_separator)\n\n    if \"CommandLine\" in output_df:\n        _add_commandline_features(output_df, force)\n\n    if \"SubjectLogonId\" in output_df and (\"isSystemSession\" not in output_df or force):\n        output_df[\"isSystemSession\"] = output_df[\"SubjectLogonId\"].isin([\"0x3e7\", \"-1\"])\n\n    return output_df\n\n\ndef _add_processname_features(\n    output_df: pd.DataFrame, force: bool, path_separator: str\n):\n    \"\"\"\n    Add process name default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n    path_separator : str\n        Path separator for OS\n\n    \"\"\"\n    if \"processName\" not in output_df or force:\n        output_df[\"processName\"] = output_df.apply(\n            lambda x: x.NewProcessName.split(path_separator)[-1], axis=1\n        )\n    if \"pathScore\" not in output_df or force:\n        output_df[\"pathScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.NewProcessName), axis=1\n        )\n    if \"pathLogScore\" not in output_df or force:\n        output_df[\"pathLogScore\"] = output_df.apply(\n            lambda x: log10(x.pathScore) if x.pathScore else 0, axis=1\n        )\n    if \"pathHash\" not in output_df or force:\n        output_df[\"pathHash\"] = output_df.apply(\n            lambda x: crc32_hash(x.NewProcessName), axis=1\n        )\n\n\ndef _add_commandline_features(output_df: pd.DataFrame, force: bool):\n    \"\"\"\n    Add commandline default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n\n    \"\"\"\n    if \"commandlineLen\" not in output_df or force:\n        output_df[\"commandlineLen\"] = output_df.apply(\n            lambda x: len(x.CommandLine), axis=1\n        )\n    if \"commandlineLogLen\" not in output_df or force:\n        output_df[\"commandlineLogLen\"] = output_df.apply(\n            lambda x: log10(x.commandlineLen) if x.commandlineLen else 0, axis=1\n        )\n    if \"commandlineTokensFull\" not in output_df or force:\n        output_df[\"commandlineTokensFull\"] = output_df[[\"CommandLine\"]].apply(\n            lambda x: delim_count(x.CommandLine), axis=1\n        )\n\n    if \"commandlineScore\" not in output_df or force:\n        output_df[\"commandlineScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.CommandLine), axis=1\n        )\n    if \"commandlineTokensHash\" not in output_df or force:\n        output_df[\"commandlineTokensHash\"] = output_df.apply(\n            lambda x: delim_hash(x.CommandLine), axis=1\n        )\n\n\n@export\n@lru_cache(maxsize=1024)\ndef delim_count(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Count of delimiters in the string.\n\n    \"\"\"\n    return len(re.findall(delim_list, value))\n\n\n@export\n@lru_cache(maxsize=1024)\ndef delim_hash(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    r\"\"\"\n    Return a hash (CRC32) of the delimiters from input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Hash of delimiter set in the string.\n\n    \"\"\"\n    return crc32(bytes(\"\".join(re.findall(delim_list, value)), \"utf-8\"))\n\n\n@export\n@lru_cache(maxsize=1024)\ndef char_ord_score(value: str, scale: int = 1) -> int:\n    \"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    int\n        [description]\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return floor(sum(ord(x) for x in value) / scale)\n\n\n@export\n@lru_cache(maxsize=1024)\ndef token_count(value: str, delimiter: str = \" \") -> int:\n    \"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    int\n        count of tokens\n\n    \"\"\"\n    return len(value.split(delimiter))\n\n\ndef _string_score(input_str):\n    \"\"\"Sum the ord(c) for characters in a string.\"\"\"\n    return sum(ord(x) for x in input_str)\n\n\n@export\n@lru_cache(maxsize=1024)\ndef crc32_hash(value: str) -> int:\n    \"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n\n    Returns\n    -------\n    int\n        CRC32 hash\n\n    \"\"\"\n    return crc32(bytes(value.encode(\"utf-8\")))\n\n\ndef delim_count_df(\n    data: pd.DataFrame, column: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]'\n) -> pd.Series:\n    r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        The name of the column to process\n    delim_list : str, optional\n        delimiters to use. (the default is r\\'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]\\')\n\n    Returns\n    -------\n    pd.Series\n        Count of delimiters in the string in `column`.\n\n    \"\"\"\n    return data[column].str.count(delim_list)\n\n\ndef char_ord_score_df(data: pd.DataFrame, column: str, scale: int = 1) -> pd.Series:\n    \"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    pd.Series\n        The sum of the ordinal values of the characters\n        in `column`.\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return data.apply(lambda x: sum(ord(char) for char in x[column]) / scale, axis=1)\n\n\ndef token_count_df(data: pd.DataFrame, column: str, delimiter: str = \" \") -> pd.Series:\n    \"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    pd.Series\n        count of tokens in strings in `column`\n\n    \"\"\"\n    return data.apply(lambda x: len(x[column].split(delimiter)), axis=1)\n\n\ndef crc32_hash_df(data: pd.DataFrame, column: str) -> pd.Series:\n    \"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n\n    Returns\n    -------\n    pd.Series\n        CRC32 hash of input column\n\n    \"\"\"\n    return data.apply(lambda x: crc32(bytes(x[column].encode(\"utf-8\"))), axis=1)\n\n\n# pylint: disable=too-many-arguments, too-many-statements\n@export  # noqa: C901, MC0001\ndef plot_cluster(\n    db_cluster: DBSCAN,\n    data: pd.DataFrame,\n    x_predict: np.ndarray,\n    plot_label: str = None,\n    plot_features: Tuple[int, int] = (0, 1),\n    verbose: bool = False,\n    cut_off: int = 3,\n    xlabel: str = None,\n    ylabel: str = None,\n):\n    \"\"\"\n    Plot clustered data as scatter chart.\n\n    Parameters\n    ----------\n    db_cluster : DBSCAN\n        DBScan Cluster (from SkLearn DBSCAN).\n    data : pd.DataFrame\n        Dataframe containing original data.\n    x_predict : np.ndarray\n        The DBSCAN predict numpy array\n    plot_label : str, optional\n         If set the column to use to label data points\n         (the default is None)\n    plot_features :  Tuple[int, int], optional\n        Which two features in x_predict to plot (the default is (0, 1))\n    verbose : bool, optional\n        Verbose execution with some extra info\n        (the default is False)\n    cut_off : int, optional\n        The cluster size below which items are considered outliers\n        (the default is 3)\n    xlabel : str, optional\n        x-axis label (the default is None)\n    ylabel : str, optional\n        y-axis label (the default is None)\n\n    \"\"\"\n    max_idx = x_predict.shape[1] - 1\n    if plot_features[0] >= x_predict.shape[1]:\n        raise ValueError(\n            \"plot_features[0] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    if plot_features[1] >= x_predict.shape[1]:\n        raise ValueError(\n            \"plot_features[1] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    if plot_features[0] == plot_features[1]:\n        mssg = \"plot_features indexes must be 2 different values in range 0 to\"\n        raise ValueError(mssg + f\" {max_idx}.\")\n\n    labels = db_cluster.labels_\n    core_samples_mask = np.zeros_like(labels, dtype=bool)\n\n    # pylint: disable=unsupported-assignment-operation\n    # (assignment of numpy array is valid)\n    core_samples_mask[db_cluster.core_sample_indices_] = True\n    unique_labels = set(labels)\n\n    # pylint: disable=no-member\n    # Spectral color map does exist\n    colors = [cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n    # Number of clusters in labels, ignoring noise if present.\n    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n    n_noise_ = list(labels).count(-1)\n    _, counts = np.unique(labels, return_counts=True)\n\n    if verbose:\n        print(\"Estimated number of clusters: %d\" % n_clusters_)\n        print(\"Estimated number of noise points: %d\" % n_noise_)\n        # print(\"Silhouette Coefficient: %0.3f\"\n        #       % metrics.silhouette_score(x_predict, labels))\n\n    if not isinstance(data, pd.DataFrame):\n        plot_label = None\n    elif plot_label is not None and plot_label not in data:\n        plot_label = None\n\n    p_label = None\n    for cluster_id, color in zip(unique_labels, colors):\n        if cluster_id == -1:\n            # Black used for noise.\n            color = [0, 0, 0, 1]\n        class_member_mask = labels == cluster_id\n\n        cluster_size = counts[cluster_id]\n        marker_size = cluster_size\n        marker = \"o\"\n        font_size = \"small\"\n        alpha = 0.4\n\n        if cluster_size < cut_off:\n            marker = \"+\"\n            marker_size = 10\n            font_size = \"large\"\n            alpha = 1.0\n        xy_pos = x_predict[class_member_mask & core_samples_mask]\n        plt.plot(\n            xy_pos[:, plot_features[0]],\n            xy_pos[:, plot_features[1]],\n            marker,\n            markerfacecolor=tuple(color),\n            markersize=marker_size,\n        )\n\n        if plot_label:\n            first_row = data[class_member_mask].iloc[0]\n            if not first_row.empty and plot_label in first_row:\n                p_label = first_row[plot_label]\n                try:\n                    plt.annotate(\n                        p_label,\n                        xy=(xy_pos[0, plot_features[0]], xy_pos[0, plot_features[1]]),\n                        fontsize=font_size,\n                        alpha=alpha,\n                    )\n                except IndexError:\n                    pass\n\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n    plt.show()\n    return plt\n",
        "source_code_len": 21140,
        "target_code": "\n# flake8: noqa: F403, F401\n# pylint: disable=wildcard-import, unused-wildcard-import, unused-import\nfrom ..analysis.eventcluster import *\n\nWARN_MSSG = (\n    \"This module has moved to msticpy.analysis.eventcluster\\n\"\n    + \"Please change your import to reflect this new location.\"\n)\nwarnings.warn(WARN_MSSG, category=DeprecationWarning)\n",
        "target_code_len": 337,
        "diff_format": "@@ -50,683 +13,10 @@\n \n-# pylint: disable=too-many-arguments, too-many-locals\n-@export\n-def dbcluster_events(\n-    data: Any,\n-    cluster_columns: List[Any] = None,\n-    verbose: bool = False,\n-    normalize: bool = True,\n-    time_column: str = \"TimeCreatedUtc\",\n-    max_cluster_distance: float = 0.01,\n-    min_cluster_samples: int = 2,\n-    **kwargs,\n-) -> Tuple[pd.DataFrame, DBSCAN, np.ndarray]:\n-    \"\"\"\n-    Cluster data set according to cluster_columns features.\n+# flake8: noqa: F403, F401\n+# pylint: disable=wildcard-import, unused-wildcard-import, unused-import\n+from ..analysis.eventcluster import *\n \n-    Parameters\n-    ----------\n-    data : Any\n-        Input data as a pandas DataFrame or numpy array\n-    cluster_columns : List[Any], optional\n-        List of columns to use for features\n-        - for DataFrame this is a list of column names\n-        - for numpy array this is a list of column indexes\n-    verbose : bool, optional\n-        Print additional information about clustering results (the default is False)\n-    normalize : bool, optional\n-        Normalize the input data (should probably always be True)\n-    time_column : str, optional\n-        If there is a time column the output data will be ordered by this\n-        (the default is 'TimeCreatedUtc')\n-    max_cluster_distance : float, optional\n-        DBSCAN eps (max cluster member distance) (the default is 0.01)\n-    min_cluster_samples : int, optional\n-        DBSCAN min_samples (the minimum cluster size) (the default is 2)\n-\n-    Other Parameters\n-    ----------------\n-    kwargs: Other arguments are passed to DBSCAN constructor\n-\n-    Returns\n-    -------\n-    Tuple[pd.DataFrame, DBSCAN, np.ndarray]\n-        Output dataframe with clustered rows\n-        DBSCAN model\n-        Normalized data set\n-\n-    \"\"\"\n-    allowed_types = [np.ndarray, pd.DataFrame]\n-\n-    x_input = None\n-    if isinstance(data, pd.DataFrame):\n-        if cluster_columns is None:\n-            x_input = data.values\n-        else:\n-            x_input = data[cluster_columns].values\n-    elif isinstance(data, np.ndarray):\n-        x_input = data if cluster_columns is None else data[:, cluster_columns].values\n-    if x_input is None:\n-        mssg = \"Input data not in expected format.\\n{} is not one of allowed types {}\"\n-        type_list = \", \".join([str(t) for t in allowed_types])\n-        mssg = mssg.format(str(type(data)), type_list)\n-        raise ValueError(mssg)\n-\n-    # Create DBSCAN cluster object\n-    db_cluster = DBSCAN(\n-        eps=max_cluster_distance, min_samples=min_cluster_samples, **kwargs\n-    )\n-\n-    # Normalize the data (most clustering algorithms don't do well with\n-    # unnormalized data)\n-    x_norm = Normalizer().fit_transform(x_input) if normalize else x_input\n-    # fit the data set\n-    db_cluster.fit(x_norm)\n-    labels = db_cluster.labels_\n-    cluster_set, counts = np.unique(labels, return_counts=True)\n-    if verbose:\n-        print(\n-            \"Clustering for set size \",\n-            len(x_norm),\n-            \" - \",\n-            len(cluster_set),\n-            \" clusters\",\n-        )\n-        print(\"Individual cluster sizes: \", \", \".join([str(c) for c in counts]))\n-\n-    clustered_events = _merge_clustered_items(\n-        cluster_set, labels, data, time_column, counts\n-    )\n-\n-    if verbose:\n-        print(\"Cluster output rows: \", len(clustered_events))\n-\n-    return clustered_events, db_cluster, x_norm\n-\n-\n-def _merge_clustered_items(\n-    cluster_set: np.array,\n-    labels: np.array,\n-    data: Union[pd.DataFrame, np.array],\n-    time_column: str,\n-    counts: np.array,\n-) -> pd.DataFrame:\n-    \"\"\"\n-    Merge outliers and core clusters into single DataFrame.\n-\n-    Parameters\n-    ----------\n-    cluster_set : np.array\n-        The set of clusters\n-    labels : np.array\n-        The cluster labels\n-    data : Union[pd.DataFrame, np.array]\n-        The source data\n-    time_column : str\n-        Name of the Time column\n-    counts : np.array\n-        The counts of members in each cluster\n-\n-    Returns\n-    -------\n-    pd.DataFrame\n-        Merged dataframe\n-\n-    \"\"\"\n-    tz_aware = data.iloc[0][time_column].tz\n-    ts_type = \"datetime64[ns, UTC]\" if tz_aware is not None else \"datetime64[ns]\"\n-\n-    cluster_list = []\n-    # Iterate through clusters, adding exemplar to output frame\n-    # pylint: disable=consider-using-enumerate\n-    # we need to know the index of the item within the loop\n-    for idx in range(len(cluster_set)):\n-        cluster_id = cluster_set[idx]\n-        class_members = labels == cluster_id\n-        if isinstance(data, pd.DataFrame):\n-            time_ordered = data[class_members].sort_values(time_column, ascending=True)\n-            first_event_time = time_ordered[0:][time_column].iat[0]\n-            last_event_time = time_ordered[-1:][time_column].iat[0]\n-        else:\n-            first_event_time = None\n-            last_event_time = None\n-\n-        if cluster_id == -1:\n-            # 'Noise' events are individual items that could not be assigned\n-            # to a cluster and so are unique\n-            cluster_list.append(\n-                data[class_members]\n-                .assign(\n-                    Clustered=False,\n-                    ClusterId=cluster_id,\n-                    ClusterSize=1,\n-                    TimeGenerated=first_event_time,\n-                    FirstEventTime=first_event_time,\n-                    LastEventTime=last_event_time,\n-                )\n-                .astype(\n-                    dtype={\n-                        \"TimeGenerated\": ts_type,\n-                        \"FirstEventTime\": ts_type,\n-                        \"LastEventTime\": ts_type,\n-                    }\n-                )\n-            )\n-        else:\n-            # Otherwise, just choose the first example of the cluster set\n-            cluster_list.append(\n-                data[class_members]\n-                .assign(\n-                    Clustered=True,\n-                    ClusterId=cluster_id,\n-                    ClusterSize=counts[idx],\n-                    TimeGenerated=first_event_time,\n-                    FirstEventTime=first_event_time,\n-                    LastEventTime=last_event_time,\n-                )[0:1]\n-                .astype(\n-                    dtype={\n-                        \"TimeGenerated\": ts_type,\n-                        \"FirstEventTime\": ts_type,\n-                        \"LastEventTime\": ts_type,\n-                    }\n-                )\n-            )\n-    # pylint: enable=consider-using-enumerate\n-    return pd.concat(cluster_list)\n-\n-\n-@export\n-def add_process_features(\n-    input_frame: pd.DataFrame, path_separator: str = None, force: bool = False\n-) -> pd.DataFrame:\n-    r\"\"\"\n-    Add numerical features based on patterns of command line and process name.\n-\n-    Parameters\n-    ----------\n-    input_frame : pd.DataFrame\n-        The input dataframe\n-    path_separator : str, optional\n-        Path separator. If not supplied, try to determine\n-        from 'NewProcessName' column of first 10 rows\n-        (the default is None)\n-    force : bool, optional\n-        Forces re-calculation of feature columns even if they\n-        already exist (the default is False)\n-\n-    Returns\n-    -------\n-    pd.DataFrame\n-        Copy of the dataframe with the additional numeric features\n-\n-    Notes\n-    -----\n-    Features added:\n-\n-    - processNameLen: length of process file name (inc path)\n-    - processNameTokens: the number of elements in the path\n-    - processName: the process file name (minus path)\n-    - commandlineTokens: number of space-separated tokens in the command line\n-    - commandlineLen: length of the command line\n-    - commandlineLogLen: log10 length of commandline\n-    - isSystemSession: 1 if session Id is 0x3e7 for Windows or -1 for Linux\n-    - commandlineTokensFull: counts number of token separators in commandline\n-      [\\\\s\\-\\\\/\\.,\"\\'\\|&:;%$()]\n-    - pathScore: sum of ord() value of characters in path\n-    - pathLogScore: log10 of pathScore\n-    - commandlineScore: sum of ord() value of characters in commandline\n-    - commandlineLogScore: log10 of commandlineScore\n-\n-    \"\"\"\n-    output_df = input_frame.copy()\n-\n-    # Set any NaN values to empty string\n-    if \"NewProcessName\" in output_df and \"CommandLine\" in output_df:\n-        output_df[[\"NewProcessName\", \"CommandLine\"]] = output_df[\n-            [\"NewProcessName\", \"CommandLine\"]\n-        ].fillna(value=\"\")\n-\n-    # try to determine the path separator\n-    if path_separator is None:\n-        sample_df = output_df.head(10)\n-        lx_path = len(sample_df[sample_df[\"NewProcessName\"].str.contains(\"/\")])\n-        path_separator = \"/\" if lx_path else \"\\\\\"\n-    # Create features from process name and command line\n-    if \"NewProcessName\" in output_df:\n-        _add_processname_features(output_df, force, path_separator)\n-\n-    if \"CommandLine\" in output_df:\n-        _add_commandline_features(output_df, force)\n-\n-    if \"SubjectLogonId\" in output_df and (\"isSystemSession\" not in output_df or force):\n-        output_df[\"isSystemSession\"] = output_df[\"SubjectLogonId\"].isin([\"0x3e7\", \"-1\"])\n-\n-    return output_df\n-\n-\n-def _add_processname_features(\n-    output_df: pd.DataFrame, force: bool, path_separator: str\n-):\n-    \"\"\"\n-    Add process name default features.\n-\n-    Parameters\n-    ----------\n-    output_df : pd.DataFrame\n-        The dataframe to add features to\n-    force : bool\n-        If True overwrite existing feature columns\n-    path_separator : str\n-        Path separator for OS\n-\n-    \"\"\"\n-    if \"processName\" not in output_df or force:\n-        output_df[\"processName\"] = output_df.apply(\n-            lambda x: x.NewProcessName.split(path_separator)[-1], axis=1\n-        )\n-    if \"pathScore\" not in output_df or force:\n-        output_df[\"pathScore\"] = output_df.apply(\n-            lambda x: char_ord_score(x.NewProcessName), axis=1\n-        )\n-    if \"pathLogScore\" not in output_df or force:\n-        output_df[\"pathLogScore\"] = output_df.apply(\n-            lambda x: log10(x.pathScore) if x.pathScore else 0, axis=1\n-        )\n-    if \"pathHash\" not in output_df or force:\n-        output_df[\"pathHash\"] = output_df.apply(\n-            lambda x: crc32_hash(x.NewProcessName), axis=1\n-        )\n-\n-\n-def _add_commandline_features(output_df: pd.DataFrame, force: bool):\n-    \"\"\"\n-    Add commandline default features.\n-\n-    Parameters\n-    ----------\n-    output_df : pd.DataFrame\n-        The dataframe to add features to\n-    force : bool\n-        If True overwrite existing feature columns\n-\n-    \"\"\"\n-    if \"commandlineLen\" not in output_df or force:\n-        output_df[\"commandlineLen\"] = output_df.apply(\n-            lambda x: len(x.CommandLine), axis=1\n-        )\n-    if \"commandlineLogLen\" not in output_df or force:\n-        output_df[\"commandlineLogLen\"] = output_df.apply(\n-            lambda x: log10(x.commandlineLen) if x.commandlineLen else 0, axis=1\n-        )\n-    if \"commandlineTokensFull\" not in output_df or force:\n-        output_df[\"commandlineTokensFull\"] = output_df[[\"CommandLine\"]].apply(\n-            lambda x: delim_count(x.CommandLine), axis=1\n-        )\n-\n-    if \"commandlineScore\" not in output_df or force:\n-        output_df[\"commandlineScore\"] = output_df.apply(\n-            lambda x: char_ord_score(x.CommandLine), axis=1\n-        )\n-    if \"commandlineTokensHash\" not in output_df or force:\n-        output_df[\"commandlineTokensHash\"] = output_df.apply(\n-            lambda x: delim_hash(x.CommandLine), axis=1\n-        )\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def delim_count(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n-    r\"\"\"\n-    Count the delimiters in input column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    delim_list : str, optional\n-        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n-\n-    Returns\n-    -------\n-    int\n-        Count of delimiters in the string.\n-\n-    \"\"\"\n-    return len(re.findall(delim_list, value))\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def delim_hash(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n-    r\"\"\"\n-    Return a hash (CRC32) of the delimiters from input column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    delim_list : str, optional\n-        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n-\n-    Returns\n-    -------\n-    int\n-        Hash of delimiter set in the string.\n-\n-    \"\"\"\n-    return crc32(bytes(\"\".join(re.findall(delim_list, value)), \"utf-8\"))\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def char_ord_score(value: str, scale: int = 1) -> int:\n-    \"\"\"\n-    Return sum of ord values of characters in string.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    scale : int, optional\n-        reduce the scale of the feature (reducing the\n-        influence of variations this feature on the clustering\n-        algorithm (the default is 1)\n-\n-    Returns\n-    -------\n-    int\n-        [description]\n-\n-    Notes\n-    -----\n-    This function sums the ordinal value of each character in the\n-    input string. Two strings with minor differences will result in\n-    a similar score. However, for strings with highly variable content\n-    (e.g. command lines or http requests containing GUIDs) this may result\n-    in too much variance to be useful when you are trying to detect\n-    similar patterns. You can use the scale parameter to reduce the\n-    influence of features using this function on clustering and anomaly\n-    algorithms.\n-\n-    \"\"\"\n-    return floor(sum(ord(x) for x in value) / scale)\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def token_count(value: str, delimiter: str = \" \") -> int:\n-    \"\"\"\n-    Return count of delimiter-separated tokens pd.Series column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    delimiter : str, optional\n-        Delimiter used to split the column string.\n-        (the default is ' ')\n-\n-    Returns\n-    -------\n-    int\n-        count of tokens\n-\n-    \"\"\"\n-    return len(value.split(delimiter))\n-\n-\n-def _string_score(input_str):\n-    \"\"\"Sum the ord(c) for characters in a string.\"\"\"\n-    return sum(ord(x) for x in input_str)\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def crc32_hash(value: str) -> int:\n-    \"\"\"\n-    Return the CRC32 hash of the input column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-\n-    Returns\n-    -------\n-    int\n-        CRC32 hash\n-\n-    \"\"\"\n-    return crc32(bytes(value.encode(\"utf-8\")))\n-\n-\n-def delim_count_df(\n-    data: pd.DataFrame, column: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]'\n-) -> pd.Series:\n-    r\"\"\"\n-    Count the delimiters in input column.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        The name of the column to process\n-    delim_list : str, optional\n-        delimiters to use. (the default is r\\'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]\\')\n-\n-    Returns\n-    -------\n-    pd.Series\n-        Count of delimiters in the string in `column`.\n-\n-    \"\"\"\n-    return data[column].str.count(delim_list)\n-\n-\n-def char_ord_score_df(data: pd.DataFrame, column: str, scale: int = 1) -> pd.Series:\n-    \"\"\"\n-    Return sum of ord values of characters in string.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        Column name to process\n-    scale : int, optional\n-        reduce the scale of the feature (reducing the\n-        influence of variations this feature on the clustering\n-        algorithm (the default is 1)\n-\n-    Returns\n-    -------\n-    pd.Series\n-        The sum of the ordinal values of the characters\n-        in `column`.\n-\n-    Notes\n-    -----\n-    This function sums the ordinal value of each character in the\n-    input string. Two strings with minor differences will result in\n-    a similar score. However, for strings with highly variable content\n-    (e.g. command lines or http requests containing GUIDs) this may result\n-    in too much variance to be useful when you are trying to detect\n-    similar patterns. You can use the scale parameter to reduce the\n-    influence of features using this function on clustering and anomaly\n-    algorithms.\n-\n-    \"\"\"\n-    return data.apply(lambda x: sum(ord(char) for char in x[column]) / scale, axis=1)\n-\n-\n-def token_count_df(data: pd.DataFrame, column: str, delimiter: str = \" \") -> pd.Series:\n-    \"\"\"\n-    Return count of delimiter-separated tokens pd.Series column.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        Column name to process\n-    delimiter : str, optional\n-        Delimiter used to split the column string.\n-        (the default is ' ')\n-\n-    Returns\n-    -------\n-    pd.Series\n-        count of tokens in strings in `column`\n-\n-    \"\"\"\n-    return data.apply(lambda x: len(x[column].split(delimiter)), axis=1)\n-\n-\n-def crc32_hash_df(data: pd.DataFrame, column: str) -> pd.Series:\n-    \"\"\"\n-    Return the CRC32 hash of the input column.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        Column name to process\n-\n-    Returns\n-    -------\n-    pd.Series\n-        CRC32 hash of input column\n-\n-    \"\"\"\n-    return data.apply(lambda x: crc32(bytes(x[column].encode(\"utf-8\"))), axis=1)\n-\n-\n-# pylint: disable=too-many-arguments, too-many-statements\n-@export  # noqa: C901, MC0001\n-def plot_cluster(\n-    db_cluster: DBSCAN,\n-    data: pd.DataFrame,\n-    x_predict: np.ndarray,\n-    plot_label: str = None,\n-    plot_features: Tuple[int, int] = (0, 1),\n-    verbose: bool = False,\n-    cut_off: int = 3,\n-    xlabel: str = None,\n-    ylabel: str = None,\n-):\n-    \"\"\"\n-    Plot clustered data as scatter chart.\n-\n-    Parameters\n-    ----------\n-    db_cluster : DBSCAN\n-        DBScan Cluster (from SkLearn DBSCAN).\n-    data : pd.DataFrame\n-        Dataframe containing original data.\n-    x_predict : np.ndarray\n-        The DBSCAN predict numpy array\n-    plot_label : str, optional\n-         If set the column to use to label data points\n-         (the default is None)\n-    plot_features :  Tuple[int, int], optional\n-        Which two features in x_predict to plot (the default is (0, 1))\n-    verbose : bool, optional\n-        Verbose execution with some extra info\n-        (the default is False)\n-    cut_off : int, optional\n-        The cluster size below which items are considered outliers\n-        (the default is 3)\n-    xlabel : str, optional\n-        x-axis label (the default is None)\n-    ylabel : str, optional\n-        y-axis label (the default is None)\n-\n-    \"\"\"\n-    max_idx = x_predict.shape[1] - 1\n-    if plot_features[0] >= x_predict.shape[1]:\n-        raise ValueError(\n-            \"plot_features[0] index must be a value from 0 to {}.\".format(max_idx)\n-        )\n-    if plot_features[1] >= x_predict.shape[1]:\n-        raise ValueError(\n-            \"plot_features[1] index must be a value from 0 to {}.\".format(max_idx)\n-        )\n-    if plot_features[0] == plot_features[1]:\n-        mssg = \"plot_features indexes must be 2 different values in range 0 to\"\n-        raise ValueError(mssg + f\" {max_idx}.\")\n-\n-    labels = db_cluster.labels_\n-    core_samples_mask = np.zeros_like(labels, dtype=bool)\n-\n-    # pylint: disable=unsupported-assignment-operation\n-    # (assignment of numpy array is valid)\n-    core_samples_mask[db_cluster.core_sample_indices_] = True\n-    unique_labels = set(labels)\n-\n-    # pylint: disable=no-member\n-    # Spectral color map does exist\n-    colors = [cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n-    # Number of clusters in labels, ignoring noise if present.\n-    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n-    n_noise_ = list(labels).count(-1)\n-    _, counts = np.unique(labels, return_counts=True)\n-\n-    if verbose:\n-        print(\"Estimated number of clusters: %d\" % n_clusters_)\n-        print(\"Estimated number of noise points: %d\" % n_noise_)\n-        # print(\"Silhouette Coefficient: %0.3f\"\n-        #       % metrics.silhouette_score(x_predict, labels))\n-\n-    if not isinstance(data, pd.DataFrame):\n-        plot_label = None\n-    elif plot_label is not None and plot_label not in data:\n-        plot_label = None\n-\n-    p_label = None\n-    for cluster_id, color in zip(unique_labels, colors):\n-        if cluster_id == -1:\n-            # Black used for noise.\n-            color = [0, 0, 0, 1]\n-        class_member_mask = labels == cluster_id\n-\n-        cluster_size = counts[cluster_id]\n-        marker_size = cluster_size\n-        marker = \"o\"\n-        font_size = \"small\"\n-        alpha = 0.4\n-\n-        if cluster_size < cut_off:\n-            marker = \"+\"\n-            marker_size = 10\n-            font_size = \"large\"\n-            alpha = 1.0\n-        xy_pos = x_predict[class_member_mask & core_samples_mask]\n-        plt.plot(\n-            xy_pos[:, plot_features[0]],\n-            xy_pos[:, plot_features[1]],\n-            marker,\n-            markerfacecolor=tuple(color),\n-            markersize=marker_size,\n-        )\n-\n-        if plot_label:\n-            first_row = data[class_member_mask].iloc[0]\n-            if not first_row.empty and plot_label in first_row:\n-                p_label = first_row[plot_label]\n-                try:\n-                    plt.annotate(\n-                        p_label,\n-                        xy=(xy_pos[0, plot_features[0]], xy_pos[0, plot_features[1]]),\n-                        fontsize=font_size,\n-                        alpha=alpha,\n-                    )\n-                except IndexError:\n-                    pass\n-\n-    plt.xlabel(xlabel)\n-    plt.ylabel(ylabel)\n-    plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n-    plt.show()\n-    return plt\n+WARN_MSSG = (\n+    \"This module has moved to msticpy.analysis.eventcluster\\n\"\n+    + \"Please change your import to reflect this new location.\"\n+)\n+warnings.warn(WARN_MSSG, category=DeprecationWarning)\n",
        "source_code_with_indent": "\n# pylint: disable=too-many-arguments, too-many-locals\n@export\ndef dbcluster_events(\n    data: Any,\n    cluster_columns: List[Any] = None,\n    verbose: bool = False,\n    normalize: bool = True,\n    time_column: str = \"TimeCreatedUtc\",\n    max_cluster_distance: float = 0.01,\n    min_cluster_samples: int = 2,\n    **kwargs,\n) -> Tuple[pd.DataFrame, DBSCAN, np.ndarray]:\n    <IND>\"\"\"\n    Cluster data set according to cluster_columns features.\n\n    Parameters\n    ----------\n    data : Any\n        Input data as a pandas DataFrame or numpy array\n    cluster_columns : List[Any], optional\n        List of columns to use for features\n        - for DataFrame this is a list of column names\n        - for numpy array this is a list of column indexes\n    verbose : bool, optional\n        Print additional information about clustering results (the default is False)\n    normalize : bool, optional\n        Normalize the input data (should probably always be True)\n    time_column : str, optional\n        If there is a time column the output data will be ordered by this\n        (the default is 'TimeCreatedUtc')\n    max_cluster_distance : float, optional\n        DBSCAN eps (max cluster member distance) (the default is 0.01)\n    min_cluster_samples : int, optional\n        DBSCAN min_samples (the minimum cluster size) (the default is 2)\n\n    Other Parameters\n    ----------------\n    kwargs: Other arguments are passed to DBSCAN constructor\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, DBSCAN, np.ndarray]\n        Output dataframe with clustered rows\n        DBSCAN model\n        Normalized data set\n\n    \"\"\"\n    allowed_types = [np.ndarray, pd.DataFrame]\n\n    x_input = None\n    if isinstance(data, pd.DataFrame):\n        <IND>if cluster_columns is None:\n            <IND>x_input = data.values\n        <DED>else:\n            <IND>x_input = data[cluster_columns].values\n    <DED><DED>elif isinstance(data, np.ndarray):\n        <IND>x_input = data if cluster_columns is None else data[:, cluster_columns].values\n    <DED>if x_input is None:\n        <IND>mssg = \"Input data not in expected format.\\n{} is not one of allowed types {}\"\n        type_list = \", \".join([str(t) for t in allowed_types])\n        mssg = mssg.format(str(type(data)), type_list)\n        raise ValueError(mssg)\n\n    # Create DBSCAN cluster object\n    <DED>db_cluster = DBSCAN(\n        eps=max_cluster_distance, min_samples=min_cluster_samples, **kwargs\n    )\n\n    # Normalize the data (most clustering algorithms don't do well with\n    # unnormalized data)\n    x_norm = Normalizer().fit_transform(x_input) if normalize else x_input\n    # fit the data set\n    db_cluster.fit(x_norm)\n    labels = db_cluster.labels_\n    cluster_set, counts = np.unique(labels, return_counts=True)\n    if verbose:\n        <IND>print(\n            \"Clustering for set size \",\n            len(x_norm),\n            \" - \",\n            len(cluster_set),\n            \" clusters\",\n        )\n        print(\"Individual cluster sizes: \", \", \".join([str(c) for c in counts]))\n\n    <DED>clustered_events = _merge_clustered_items(\n        cluster_set, labels, data, time_column, counts\n    )\n\n    if verbose:\n        <IND>print(\"Cluster output rows: \", len(clustered_events))\n\n    <DED>return clustered_events, db_cluster, x_norm\n\n\n<DED>def _merge_clustered_items(\n    cluster_set: np.array,\n    labels: np.array,\n    data: Union[pd.DataFrame, np.array],\n    time_column: str,\n    counts: np.array,\n) -> pd.DataFrame:\n    <IND>\"\"\"\n    Merge outliers and core clusters into single DataFrame.\n\n    Parameters\n    ----------\n    cluster_set : np.array\n        The set of clusters\n    labels : np.array\n        The cluster labels\n    data : Union[pd.DataFrame, np.array]\n        The source data\n    time_column : str\n        Name of the Time column\n    counts : np.array\n        The counts of members in each cluster\n\n    Returns\n    -------\n    pd.DataFrame\n        Merged dataframe\n\n    \"\"\"\n    tz_aware = data.iloc[0][time_column].tz\n    ts_type = \"datetime64[ns, UTC]\" if tz_aware is not None else \"datetime64[ns]\"\n\n    cluster_list = []\n    # Iterate through clusters, adding exemplar to output frame\n    # pylint: disable=consider-using-enumerate\n    # we need to know the index of the item within the loop\n    for idx in range(len(cluster_set)):\n        <IND>cluster_id = cluster_set[idx]\n        class_members = labels == cluster_id\n        if isinstance(data, pd.DataFrame):\n            <IND>time_ordered = data[class_members].sort_values(time_column, ascending=True)\n            first_event_time = time_ordered[0:][time_column].iat[0]\n            last_event_time = time_ordered[-1:][time_column].iat[0]\n        <DED>else:\n            <IND>first_event_time = None\n            last_event_time = None\n\n        <DED>if cluster_id == -1:\n            # 'Noise' events are individual items that could not be assigned\n            # to a cluster and so are unique\n            <IND>cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=False,\n                    ClusterId=cluster_id,\n                    ClusterSize=1,\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n        <DED>else:\n            # Otherwise, just choose the first example of the cluster set\n            <IND>cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=True,\n                    ClusterId=cluster_id,\n                    ClusterSize=counts[idx],\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )[0:1]\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n    # pylint: enable=consider-using-enumerate\n    <DED><DED>return pd.concat(cluster_list)\n\n\n<DED>@export\ndef add_process_features(\n    input_frame: pd.DataFrame, path_separator: str = None, force: bool = False\n) -> pd.DataFrame:\n    <IND>r\"\"\"\n    Add numerical features based on patterns of command line and process name.\n\n    Parameters\n    ----------\n    input_frame : pd.DataFrame\n        The input dataframe\n    path_separator : str, optional\n        Path separator. If not supplied, try to determine\n        from 'NewProcessName' column of first 10 rows\n        (the default is None)\n    force : bool, optional\n        Forces re-calculation of feature columns even if they\n        already exist (the default is False)\n\n    Returns\n    -------\n    pd.DataFrame\n        Copy of the dataframe with the additional numeric features\n\n    Notes\n    -----\n    Features added:\n\n    - processNameLen: length of process file name (inc path)\n    - processNameTokens: the number of elements in the path\n    - processName: the process file name (minus path)\n    - commandlineTokens: number of space-separated tokens in the command line\n    - commandlineLen: length of the command line\n    - commandlineLogLen: log10 length of commandline\n    - isSystemSession: 1 if session Id is 0x3e7 for Windows or -1 for Linux\n    - commandlineTokensFull: counts number of token separators in commandline\n      [\\\\s\\-\\\\/\\.,\"\\'\\|&:;%$()]\n    - pathScore: sum of ord() value of characters in path\n    - pathLogScore: log10 of pathScore\n    - commandlineScore: sum of ord() value of characters in commandline\n    - commandlineLogScore: log10 of commandlineScore\n\n    \"\"\"\n    output_df = input_frame.copy()\n\n    # Set any NaN values to empty string\n    if \"NewProcessName\" in output_df and \"CommandLine\" in output_df:\n        <IND>output_df[[\"NewProcessName\", \"CommandLine\"]] = output_df[\n            [\"NewProcessName\", \"CommandLine\"]\n        ].fillna(value=\"\")\n\n    # try to determine the path separator\n    <DED>if path_separator is None:\n        <IND>sample_df = output_df.head(10)\n        lx_path = len(sample_df[sample_df[\"NewProcessName\"].str.contains(\"/\")])\n        path_separator = \"/\" if lx_path else \"\\\\\"\n    # Create features from process name and command line\n    <DED>if \"NewProcessName\" in output_df:\n        <IND>_add_processname_features(output_df, force, path_separator)\n\n    <DED>if \"CommandLine\" in output_df:\n        <IND>_add_commandline_features(output_df, force)\n\n    <DED>if \"SubjectLogonId\" in output_df and (\"isSystemSession\" not in output_df or force):\n        <IND>output_df[\"isSystemSession\"] = output_df[\"SubjectLogonId\"].isin([\"0x3e7\", \"-1\"])\n\n    <DED>return output_df\n\n\n<DED>def _add_processname_features(\n    output_df: pd.DataFrame, force: bool, path_separator: str\n):\n    <IND>\"\"\"\n    Add process name default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n    path_separator : str\n        Path separator for OS\n\n    \"\"\"\n    if \"processName\" not in output_df or force:\n        <IND>output_df[\"processName\"] = output_df.apply(\n            lambda x: x.NewProcessName.split(path_separator)[-1], axis=1\n        )\n    <DED>if \"pathScore\" not in output_df or force:\n        <IND>output_df[\"pathScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.NewProcessName), axis=1\n        )\n    <DED>if \"pathLogScore\" not in output_df or force:\n        <IND>output_df[\"pathLogScore\"] = output_df.apply(\n            lambda x: log10(x.pathScore) if x.pathScore else 0, axis=1\n        )\n    <DED>if \"pathHash\" not in output_df or force:\n        <IND>output_df[\"pathHash\"] = output_df.apply(\n            lambda x: crc32_hash(x.NewProcessName), axis=1\n        )\n\n\n<DED><DED>def _add_commandline_features(output_df: pd.DataFrame, force: bool):\n    <IND>\"\"\"\n    Add commandline default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n\n    \"\"\"\n    if \"commandlineLen\" not in output_df or force:\n        <IND>output_df[\"commandlineLen\"] = output_df.apply(\n            lambda x: len(x.CommandLine), axis=1\n        )\n    <DED>if \"commandlineLogLen\" not in output_df or force:\n        <IND>output_df[\"commandlineLogLen\"] = output_df.apply(\n            lambda x: log10(x.commandlineLen) if x.commandlineLen else 0, axis=1\n        )\n    <DED>if \"commandlineTokensFull\" not in output_df or force:\n        <IND>output_df[\"commandlineTokensFull\"] = output_df[[\"CommandLine\"]].apply(\n            lambda x: delim_count(x.CommandLine), axis=1\n        )\n\n    <DED>if \"commandlineScore\" not in output_df or force:\n        <IND>output_df[\"commandlineScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.CommandLine), axis=1\n        )\n    <DED>if \"commandlineTokensHash\" not in output_df or force:\n        <IND>output_df[\"commandlineTokensHash\"] = output_df.apply(\n            lambda x: delim_hash(x.CommandLine), axis=1\n        )\n\n\n<DED><DED>@export\n@lru_cache(maxsize=1024)\ndef delim_count(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    <IND>r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Count of delimiters in the string.\n\n    \"\"\"\n    return len(re.findall(delim_list, value))\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef delim_hash(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    <IND>r\"\"\"\n    Return a hash (CRC32) of the delimiters from input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Hash of delimiter set in the string.\n\n    \"\"\"\n    return crc32(bytes(\"\".join(re.findall(delim_list, value)), \"utf-8\"))\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef char_ord_score(value: str, scale: int = 1) -> int:\n    <IND>\"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    int\n        [description]\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return floor(sum(ord(x) for x in value) / scale)\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef token_count(value: str, delimiter: str = \" \") -> int:\n    <IND>\"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    int\n        count of tokens\n\n    \"\"\"\n    return len(value.split(delimiter))\n\n\n<DED>def _string_score(input_str):\n    <IND>\"\"\"Sum the ord(c) for characters in a string.\"\"\"\n    return sum(ord(x) for x in input_str)\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef crc32_hash(value: str) -> int:\n    <IND>\"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n\n    Returns\n    -------\n    int\n        CRC32 hash\n\n    \"\"\"\n    return crc32(bytes(value.encode(\"utf-8\")))\n\n\n<DED>def delim_count_df(\n    data: pd.DataFrame, column: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]'\n) -> pd.Series:\n    <IND>r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        The name of the column to process\n    delim_list : str, optional\n        delimiters to use. (the default is r\\'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]\\')\n\n    Returns\n    -------\n    pd.Series\n        Count of delimiters in the string in `column`.\n\n    \"\"\"\n    return data[column].str.count(delim_list)\n\n\n<DED>def char_ord_score_df(data: pd.DataFrame, column: str, scale: int = 1) -> pd.Series:\n    <IND>\"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    pd.Series\n        The sum of the ordinal values of the characters\n        in `column`.\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return data.apply(lambda x: sum(ord(char) for char in x[column]) / scale, axis=1)\n\n\n<DED>def token_count_df(data: pd.DataFrame, column: str, delimiter: str = \" \") -> pd.Series:\n    <IND>\"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    pd.Series\n        count of tokens in strings in `column`\n\n    \"\"\"\n    return data.apply(lambda x: len(x[column].split(delimiter)), axis=1)\n\n\n<DED>def crc32_hash_df(data: pd.DataFrame, column: str) -> pd.Series:\n    <IND>\"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n\n    Returns\n    -------\n    pd.Series\n        CRC32 hash of input column\n\n    \"\"\"\n    return data.apply(lambda x: crc32(bytes(x[column].encode(\"utf-8\"))), axis=1)\n\n\n# pylint: disable=too-many-arguments, too-many-statements\n<DED>@export  # noqa: C901, MC0001\ndef plot_cluster(\n    db_cluster: DBSCAN,\n    data: pd.DataFrame,\n    x_predict: np.ndarray,\n    plot_label: str = None,\n    plot_features: Tuple[int, int] = (0, 1),\n    verbose: bool = False,\n    cut_off: int = 3,\n    xlabel: str = None,\n    ylabel: str = None,\n):\n    <IND>\"\"\"\n    Plot clustered data as scatter chart.\n\n    Parameters\n    ----------\n    db_cluster : DBSCAN\n        DBScan Cluster (from SkLearn DBSCAN).\n    data : pd.DataFrame\n        Dataframe containing original data.\n    x_predict : np.ndarray\n        The DBSCAN predict numpy array\n    plot_label : str, optional\n         If set the column to use to label data points\n         (the default is None)\n    plot_features :  Tuple[int, int], optional\n        Which two features in x_predict to plot (the default is (0, 1))\n    verbose : bool, optional\n        Verbose execution with some extra info\n        (the default is False)\n    cut_off : int, optional\n        The cluster size below which items are considered outliers\n        (the default is 3)\n    xlabel : str, optional\n        x-axis label (the default is None)\n    ylabel : str, optional\n        y-axis label (the default is None)\n\n    \"\"\"\n    max_idx = x_predict.shape[1] - 1\n    if plot_features[0] >= x_predict.shape[1]:\n        <IND>raise ValueError(\n            \"plot_features[0] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    <DED>if plot_features[1] >= x_predict.shape[1]:\n        <IND>raise ValueError(\n            \"plot_features[1] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    <DED>if plot_features[0] == plot_features[1]:\n        <IND>mssg = \"plot_features indexes must be 2 different values in range 0 to\"\n        raise ValueError(mssg + f\" {max_idx}.\")\n\n    <DED>labels = db_cluster.labels_\n    core_samples_mask = np.zeros_like(labels, dtype=bool)\n\n    # pylint: disable=unsupported-assignment-operation\n    # (assignment of numpy array is valid)\n    core_samples_mask[db_cluster.core_sample_indices_] = True\n    unique_labels = set(labels)\n\n    # pylint: disable=no-member\n    # Spectral color map does exist\n    colors = [cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n    # Number of clusters in labels, ignoring noise if present.\n    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n    n_noise_ = list(labels).count(-1)\n    _, counts = np.unique(labels, return_counts=True)\n\n    if verbose:\n        <IND>print(\"Estimated number of clusters: %d\" % n_clusters_)\n        print(\"Estimated number of noise points: %d\" % n_noise_)\n        # print(\"Silhouette Coefficient: %0.3f\"\n        #       % metrics.silhouette_score(x_predict, labels))\n\n    <DED>if not isinstance(data, pd.DataFrame):\n        <IND>plot_label = None\n    <DED>elif plot_label is not None and plot_label not in data:\n        <IND>plot_label = None\n\n    <DED>p_label = None\n    for cluster_id, color in zip(unique_labels, colors):\n        <IND>if cluster_id == -1:\n            # Black used for noise.\n            <IND>color = [0, 0, 0, 1]\n        <DED>class_member_mask = labels == cluster_id\n\n        cluster_size = counts[cluster_id]\n        marker_size = cluster_size\n        marker = \"o\"\n        font_size = \"small\"\n        alpha = 0.4\n\n        if cluster_size < cut_off:\n            <IND>marker = \"+\"\n            marker_size = 10\n            font_size = \"large\"\n            alpha = 1.0\n        <DED>xy_pos = x_predict[class_member_mask & core_samples_mask]\n        plt.plot(\n            xy_pos[:, plot_features[0]],\n            xy_pos[:, plot_features[1]],\n            marker,\n            markerfacecolor=tuple(color),\n            markersize=marker_size,\n        )\n\n        if plot_label:\n            <IND>first_row = data[class_member_mask].iloc[0]\n            if not first_row.empty and plot_label in first_row:\n                <IND>p_label = first_row[plot_label]\n                try:\n                    <IND>plt.annotate(\n                        p_label,\n                        xy=(xy_pos[0, plot_features[0]], xy_pos[0, plot_features[1]]),\n                        fontsize=font_size,\n                        alpha=alpha,\n                    )\n                <DED>except IndexError:\n                    <IND>pass\n\n    <DED><DED><DED><DED>plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n    plt.show()\n    return plt\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n# flake8: noqa: F403, F401\n# pylint: disable=wildcard-import, unused-wildcard-import, unused-import\nfrom ..analysis.eventcluster import *\n\nWARN_MSSG = (\n    \"This module has moved to msticpy.analysis.eventcluster\\n\"\n    + \"Please change your import to reflect this new location.\"\n)\nwarnings.warn(WARN_MSSG, category=DeprecationWarning)\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "microsoft/msticpy",
    "commit": "28466d681e261394e18d9f4e063cebfa06ed04b4",
    "filename": "msticpy/sectools/eventcluster.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/microsoft-msticpy/msticpy/sectools/eventcluster.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "msticpy/sectools/eventcluster.py:612:4 Incompatible variable type [9]: plot_label is declared to have type `str` but is used as type `None`.",
    "message": " plot_label is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 612,
    "warning_line": "    plot_label: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "# --------------------------------------------------------------------------\nr\"\"\"\neventcluster module.\n\nThis module is intended to be used to summarize large numbers of events\ninto clusters of different patterns. High volume repeating events can\noften make it difficult to see unique and interesting items.\n\nThe module contains functions to generate clusterable features from\nstring data. For example, an administration command that does some\nmaintenance on thousands of servers with a commandline such as:\n``install-update -hostname {host.fqdn} -tmp:/tmp/{GUID}/rollback``\\  can\nbe collapsed into a single cluster pattern by ignoring the character\nvalues in the string and using delimiters or tokens to group the values.\n\nThis is an unsupervised learning module implemented using SciKit Learn\nDBScan.\n\nContains:\ndbcluster_events: generic clustering method using DBSCAN designed to summarize\nprocess events and other similar data by grouping on common features.\n\nadd_process_features: derives numerical features from text features such as\ncommandline and process path.\n\n\"\"\"\nfrom binascii import crc32\nfrom functools import lru_cache\nfrom math import log10, floor\nimport re\nfrom typing import List, Any, Tuple, Union\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import Normalizer\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nfrom ..common.utility import export\nfrom .._version import VERSION\n",
        "source_code_len": 1462,
        "target_code": "# --------------------------------------------------------------------------\n\"\"\"Deprecated placeholder for eventcluster.py.\"\"\"\nimport warnings\nfrom .._version import VERSION\n",
        "target_code_len": 174,
        "diff_format": "@@ -5,41 +5,4 @@\n # --------------------------------------------------------------------------\n-r\"\"\"\n-eventcluster module.\n-\n-This module is intended to be used to summarize large numbers of events\n-into clusters of different patterns. High volume repeating events can\n-often make it difficult to see unique and interesting items.\n-\n-The module contains functions to generate clusterable features from\n-string data. For example, an administration command that does some\n-maintenance on thousands of servers with a commandline such as:\n-``install-update -hostname {host.fqdn} -tmp:/tmp/{GUID}/rollback``\\  can\n-be collapsed into a single cluster pattern by ignoring the character\n-values in the string and using delimiters or tokens to group the values.\n-\n-This is an unsupervised learning module implemented using SciKit Learn\n-DBScan.\n-\n-Contains:\n-dbcluster_events: generic clustering method using DBSCAN designed to summarize\n-process events and other similar data by grouping on common features.\n-\n-add_process_features: derives numerical features from text features such as\n-commandline and process path.\n-\n-\"\"\"\n-from binascii import crc32\n-from functools import lru_cache\n-from math import log10, floor\n-import re\n-from typing import List, Any, Tuple, Union\n-\n-import numpy as np\n-import pandas as pd\n-from sklearn.cluster import DBSCAN\n-from sklearn.preprocessing import Normalizer\n-import matplotlib.pyplot as plt\n-from matplotlib import cm\n-\n-from ..common.utility import export\n+\"\"\"Deprecated placeholder for eventcluster.py.\"\"\"\n+import warnings\n from .._version import VERSION\n",
        "source_code_with_indent": "# --------------------------------------------------------------------------\nr\"\"\"\neventcluster module.\n\nThis module is intended to be used to summarize large numbers of events\ninto clusters of different patterns. High volume repeating events can\noften make it difficult to see unique and interesting items.\n\nThe module contains functions to generate clusterable features from\nstring data. For example, an administration command that does some\nmaintenance on thousands of servers with a commandline such as:\n``install-update -hostname {host.fqdn} -tmp:/tmp/{GUID}/rollback``\\  can\nbe collapsed into a single cluster pattern by ignoring the character\nvalues in the string and using delimiters or tokens to group the values.\n\nThis is an unsupervised learning module implemented using SciKit Learn\nDBScan.\n\nContains:\ndbcluster_events: generic clustering method using DBSCAN designed to summarize\nprocess events and other similar data by grouping on common features.\n\nadd_process_features: derives numerical features from text features such as\ncommandline and process path.\n\n\"\"\"\nfrom binascii import crc32\nfrom functools import lru_cache\nfrom math import log10, floor\nimport re\nfrom typing import List, Any, Tuple, Union\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import Normalizer\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nfrom ..common.utility import export\nfrom .._version import VERSION\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "# --------------------------------------------------------------------------\n\"\"\"Deprecated placeholder for eventcluster.py.\"\"\"\nimport warnings\nfrom .._version import VERSION\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n# pylint: disable=too-many-arguments, too-many-locals\n@export\ndef dbcluster_events(\n    data: Any,\n    cluster_columns: List[Any] = None,\n    verbose: bool = False,\n    normalize: bool = True,\n    time_column: str = \"TimeCreatedUtc\",\n    max_cluster_distance: float = 0.01,\n    min_cluster_samples: int = 2,\n    **kwargs,\n) -> Tuple[pd.DataFrame, DBSCAN, np.ndarray]:\n    \"\"\"\n    Cluster data set according to cluster_columns features.\n\n    Parameters\n    ----------\n    data : Any\n        Input data as a pandas DataFrame or numpy array\n    cluster_columns : List[Any], optional\n        List of columns to use for features\n        - for DataFrame this is a list of column names\n        - for numpy array this is a list of column indexes\n    verbose : bool, optional\n        Print additional information about clustering results (the default is False)\n    normalize : bool, optional\n        Normalize the input data (should probably always be True)\n    time_column : str, optional\n        If there is a time column the output data will be ordered by this\n        (the default is 'TimeCreatedUtc')\n    max_cluster_distance : float, optional\n        DBSCAN eps (max cluster member distance) (the default is 0.01)\n    min_cluster_samples : int, optional\n        DBSCAN min_samples (the minimum cluster size) (the default is 2)\n\n    Other Parameters\n    ----------------\n    kwargs: Other arguments are passed to DBSCAN constructor\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, DBSCAN, np.ndarray]\n        Output dataframe with clustered rows\n        DBSCAN model\n        Normalized data set\n\n    \"\"\"\n    allowed_types = [np.ndarray, pd.DataFrame]\n\n    x_input = None\n    if isinstance(data, pd.DataFrame):\n        if cluster_columns is None:\n            x_input = data.values\n        else:\n            x_input = data[cluster_columns].values\n    elif isinstance(data, np.ndarray):\n        x_input = data if cluster_columns is None else data[:, cluster_columns].values\n    if x_input is None:\n        mssg = \"Input data not in expected format.\\n{} is not one of allowed types {}\"\n        type_list = \", \".join([str(t) for t in allowed_types])\n        mssg = mssg.format(str(type(data)), type_list)\n        raise ValueError(mssg)\n\n    # Create DBSCAN cluster object\n    db_cluster = DBSCAN(\n        eps=max_cluster_distance, min_samples=min_cluster_samples, **kwargs\n    )\n\n    # Normalize the data (most clustering algorithms don't do well with\n    # unnormalized data)\n    x_norm = Normalizer().fit_transform(x_input) if normalize else x_input\n    # fit the data set\n    db_cluster.fit(x_norm)\n    labels = db_cluster.labels_\n    cluster_set, counts = np.unique(labels, return_counts=True)\n    if verbose:\n        print(\n            \"Clustering for set size \",\n            len(x_norm),\n            \" - \",\n            len(cluster_set),\n            \" clusters\",\n        )\n        print(\"Individual cluster sizes: \", \", \".join([str(c) for c in counts]))\n\n    clustered_events = _merge_clustered_items(\n        cluster_set, labels, data, time_column, counts\n    )\n\n    if verbose:\n        print(\"Cluster output rows: \", len(clustered_events))\n\n    return clustered_events, db_cluster, x_norm\n\n\ndef _merge_clustered_items(\n    cluster_set: np.array,\n    labels: np.array,\n    data: Union[pd.DataFrame, np.array],\n    time_column: str,\n    counts: np.array,\n) -> pd.DataFrame:\n    \"\"\"\n    Merge outliers and core clusters into single DataFrame.\n\n    Parameters\n    ----------\n    cluster_set : np.array\n        The set of clusters\n    labels : np.array\n        The cluster labels\n    data : Union[pd.DataFrame, np.array]\n        The source data\n    time_column : str\n        Name of the Time column\n    counts : np.array\n        The counts of members in each cluster\n\n    Returns\n    -------\n    pd.DataFrame\n        Merged dataframe\n\n    \"\"\"\n    tz_aware = data.iloc[0][time_column].tz\n    ts_type = \"datetime64[ns, UTC]\" if tz_aware is not None else \"datetime64[ns]\"\n\n    cluster_list = []\n    # Iterate through clusters, adding exemplar to output frame\n    # pylint: disable=consider-using-enumerate\n    # we need to know the index of the item within the loop\n    for idx in range(len(cluster_set)):\n        cluster_id = cluster_set[idx]\n        class_members = labels == cluster_id\n        if isinstance(data, pd.DataFrame):\n            time_ordered = data[class_members].sort_values(time_column, ascending=True)\n            first_event_time = time_ordered[0:][time_column].iat[0]\n            last_event_time = time_ordered[-1:][time_column].iat[0]\n        else:\n            first_event_time = None\n            last_event_time = None\n\n        if cluster_id == -1:\n            # 'Noise' events are individual items that could not be assigned\n            # to a cluster and so are unique\n            cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=False,\n                    ClusterId=cluster_id,\n                    ClusterSize=1,\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n        else:\n            # Otherwise, just choose the first example of the cluster set\n            cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=True,\n                    ClusterId=cluster_id,\n                    ClusterSize=counts[idx],\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )[0:1]\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n    # pylint: enable=consider-using-enumerate\n    return pd.concat(cluster_list)\n\n\n@export\ndef add_process_features(\n    input_frame: pd.DataFrame, path_separator: str = None, force: bool = False\n) -> pd.DataFrame:\n    r\"\"\"\n    Add numerical features based on patterns of command line and process name.\n\n    Parameters\n    ----------\n    input_frame : pd.DataFrame\n        The input dataframe\n    path_separator : str, optional\n        Path separator. If not supplied, try to determine\n        from 'NewProcessName' column of first 10 rows\n        (the default is None)\n    force : bool, optional\n        Forces re-calculation of feature columns even if they\n        already exist (the default is False)\n\n    Returns\n    -------\n    pd.DataFrame\n        Copy of the dataframe with the additional numeric features\n\n    Notes\n    -----\n    Features added:\n\n    - processNameLen: length of process file name (inc path)\n    - processNameTokens: the number of elements in the path\n    - processName: the process file name (minus path)\n    - commandlineTokens: number of space-separated tokens in the command line\n    - commandlineLen: length of the command line\n    - commandlineLogLen: log10 length of commandline\n    - isSystemSession: 1 if session Id is 0x3e7 for Windows or -1 for Linux\n    - commandlineTokensFull: counts number of token separators in commandline\n      [\\\\s\\-\\\\/\\.,\"\\'\\|&:;%$()]\n    - pathScore: sum of ord() value of characters in path\n    - pathLogScore: log10 of pathScore\n    - commandlineScore: sum of ord() value of characters in commandline\n    - commandlineLogScore: log10 of commandlineScore\n\n    \"\"\"\n    output_df = input_frame.copy()\n\n    # Set any NaN values to empty string\n    if \"NewProcessName\" in output_df and \"CommandLine\" in output_df:\n        output_df[[\"NewProcessName\", \"CommandLine\"]] = output_df[\n            [\"NewProcessName\", \"CommandLine\"]\n        ].fillna(value=\"\")\n\n    # try to determine the path separator\n    if path_separator is None:\n        sample_df = output_df.head(10)\n        lx_path = len(sample_df[sample_df[\"NewProcessName\"].str.contains(\"/\")])\n        path_separator = \"/\" if lx_path else \"\\\\\"\n    # Create features from process name and command line\n    if \"NewProcessName\" in output_df:\n        _add_processname_features(output_df, force, path_separator)\n\n    if \"CommandLine\" in output_df:\n        _add_commandline_features(output_df, force)\n\n    if \"SubjectLogonId\" in output_df and (\"isSystemSession\" not in output_df or force):\n        output_df[\"isSystemSession\"] = output_df[\"SubjectLogonId\"].isin([\"0x3e7\", \"-1\"])\n\n    return output_df\n\n\ndef _add_processname_features(\n    output_df: pd.DataFrame, force: bool, path_separator: str\n):\n    \"\"\"\n    Add process name default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n    path_separator : str\n        Path separator for OS\n\n    \"\"\"\n    if \"processName\" not in output_df or force:\n        output_df[\"processName\"] = output_df.apply(\n            lambda x: x.NewProcessName.split(path_separator)[-1], axis=1\n        )\n    if \"pathScore\" not in output_df or force:\n        output_df[\"pathScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.NewProcessName), axis=1\n        )\n    if \"pathLogScore\" not in output_df or force:\n        output_df[\"pathLogScore\"] = output_df.apply(\n            lambda x: log10(x.pathScore) if x.pathScore else 0, axis=1\n        )\n    if \"pathHash\" not in output_df or force:\n        output_df[\"pathHash\"] = output_df.apply(\n            lambda x: crc32_hash(x.NewProcessName), axis=1\n        )\n\n\ndef _add_commandline_features(output_df: pd.DataFrame, force: bool):\n    \"\"\"\n    Add commandline default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n\n    \"\"\"\n    if \"commandlineLen\" not in output_df or force:\n        output_df[\"commandlineLen\"] = output_df.apply(\n            lambda x: len(x.CommandLine), axis=1\n        )\n    if \"commandlineLogLen\" not in output_df or force:\n        output_df[\"commandlineLogLen\"] = output_df.apply(\n            lambda x: log10(x.commandlineLen) if x.commandlineLen else 0, axis=1\n        )\n    if \"commandlineTokensFull\" not in output_df or force:\n        output_df[\"commandlineTokensFull\"] = output_df[[\"CommandLine\"]].apply(\n            lambda x: delim_count(x.CommandLine), axis=1\n        )\n\n    if \"commandlineScore\" not in output_df or force:\n        output_df[\"commandlineScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.CommandLine), axis=1\n        )\n    if \"commandlineTokensHash\" not in output_df or force:\n        output_df[\"commandlineTokensHash\"] = output_df.apply(\n            lambda x: delim_hash(x.CommandLine), axis=1\n        )\n\n\n@export\n@lru_cache(maxsize=1024)\ndef delim_count(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Count of delimiters in the string.\n\n    \"\"\"\n    return len(re.findall(delim_list, value))\n\n\n@export\n@lru_cache(maxsize=1024)\ndef delim_hash(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    r\"\"\"\n    Return a hash (CRC32) of the delimiters from input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Hash of delimiter set in the string.\n\n    \"\"\"\n    return crc32(bytes(\"\".join(re.findall(delim_list, value)), \"utf-8\"))\n\n\n@export\n@lru_cache(maxsize=1024)\ndef char_ord_score(value: str, scale: int = 1) -> int:\n    \"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    int\n        [description]\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return floor(sum(ord(x) for x in value) / scale)\n\n\n@export\n@lru_cache(maxsize=1024)\ndef token_count(value: str, delimiter: str = \" \") -> int:\n    \"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    int\n        count of tokens\n\n    \"\"\"\n    return len(value.split(delimiter))\n\n\ndef _string_score(input_str):\n    \"\"\"Sum the ord(c) for characters in a string.\"\"\"\n    return sum(ord(x) for x in input_str)\n\n\n@export\n@lru_cache(maxsize=1024)\ndef crc32_hash(value: str) -> int:\n    \"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n\n    Returns\n    -------\n    int\n        CRC32 hash\n\n    \"\"\"\n    return crc32(bytes(value.encode(\"utf-8\")))\n\n\ndef delim_count_df(\n    data: pd.DataFrame, column: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]'\n) -> pd.Series:\n    r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        The name of the column to process\n    delim_list : str, optional\n        delimiters to use. (the default is r\\'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]\\')\n\n    Returns\n    -------\n    pd.Series\n        Count of delimiters in the string in `column`.\n\n    \"\"\"\n    return data[column].str.count(delim_list)\n\n\ndef char_ord_score_df(data: pd.DataFrame, column: str, scale: int = 1) -> pd.Series:\n    \"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    pd.Series\n        The sum of the ordinal values of the characters\n        in `column`.\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return data.apply(lambda x: sum(ord(char) for char in x[column]) / scale, axis=1)\n\n\ndef token_count_df(data: pd.DataFrame, column: str, delimiter: str = \" \") -> pd.Series:\n    \"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    pd.Series\n        count of tokens in strings in `column`\n\n    \"\"\"\n    return data.apply(lambda x: len(x[column].split(delimiter)), axis=1)\n\n\ndef crc32_hash_df(data: pd.DataFrame, column: str) -> pd.Series:\n    \"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n\n    Returns\n    -------\n    pd.Series\n        CRC32 hash of input column\n\n    \"\"\"\n    return data.apply(lambda x: crc32(bytes(x[column].encode(\"utf-8\"))), axis=1)\n\n\n# pylint: disable=too-many-arguments, too-many-statements\n@export  # noqa: C901, MC0001\ndef plot_cluster(\n    db_cluster: DBSCAN,\n    data: pd.DataFrame,\n    x_predict: np.ndarray,\n    plot_label: str = None,\n    plot_features: Tuple[int, int] = (0, 1),\n    verbose: bool = False,\n    cut_off: int = 3,\n    xlabel: str = None,\n    ylabel: str = None,\n):\n    \"\"\"\n    Plot clustered data as scatter chart.\n\n    Parameters\n    ----------\n    db_cluster : DBSCAN\n        DBScan Cluster (from SkLearn DBSCAN).\n    data : pd.DataFrame\n        Dataframe containing original data.\n    x_predict : np.ndarray\n        The DBSCAN predict numpy array\n    plot_label : str, optional\n         If set the column to use to label data points\n         (the default is None)\n    plot_features :  Tuple[int, int], optional\n        Which two features in x_predict to plot (the default is (0, 1))\n    verbose : bool, optional\n        Verbose execution with some extra info\n        (the default is False)\n    cut_off : int, optional\n        The cluster size below which items are considered outliers\n        (the default is 3)\n    xlabel : str, optional\n        x-axis label (the default is None)\n    ylabel : str, optional\n        y-axis label (the default is None)\n\n    \"\"\"\n    max_idx = x_predict.shape[1] - 1\n    if plot_features[0] >= x_predict.shape[1]:\n        raise ValueError(\n            \"plot_features[0] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    if plot_features[1] >= x_predict.shape[1]:\n        raise ValueError(\n            \"plot_features[1] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    if plot_features[0] == plot_features[1]:\n        mssg = \"plot_features indexes must be 2 different values in range 0 to\"\n        raise ValueError(mssg + f\" {max_idx}.\")\n\n    labels = db_cluster.labels_\n    core_samples_mask = np.zeros_like(labels, dtype=bool)\n\n    # pylint: disable=unsupported-assignment-operation\n    # (assignment of numpy array is valid)\n    core_samples_mask[db_cluster.core_sample_indices_] = True\n    unique_labels = set(labels)\n\n    # pylint: disable=no-member\n    # Spectral color map does exist\n    colors = [cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n    # Number of clusters in labels, ignoring noise if present.\n    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n    n_noise_ = list(labels).count(-1)\n    _, counts = np.unique(labels, return_counts=True)\n\n    if verbose:\n        print(\"Estimated number of clusters: %d\" % n_clusters_)\n        print(\"Estimated number of noise points: %d\" % n_noise_)\n        # print(\"Silhouette Coefficient: %0.3f\"\n        #       % metrics.silhouette_score(x_predict, labels))\n\n    if not isinstance(data, pd.DataFrame):\n        plot_label = None\n    elif plot_label is not None and plot_label not in data:\n        plot_label = None\n\n    p_label = None\n    for cluster_id, color in zip(unique_labels, colors):\n        if cluster_id == -1:\n            # Black used for noise.\n            color = [0, 0, 0, 1]\n        class_member_mask = labels == cluster_id\n\n        cluster_size = counts[cluster_id]\n        marker_size = cluster_size\n        marker = \"o\"\n        font_size = \"small\"\n        alpha = 0.4\n\n        if cluster_size < cut_off:\n            marker = \"+\"\n            marker_size = 10\n            font_size = \"large\"\n            alpha = 1.0\n        xy_pos = x_predict[class_member_mask & core_samples_mask]\n        plt.plot(\n            xy_pos[:, plot_features[0]],\n            xy_pos[:, plot_features[1]],\n            marker,\n            markerfacecolor=tuple(color),\n            markersize=marker_size,\n        )\n\n        if plot_label:\n            first_row = data[class_member_mask].iloc[0]\n            if not first_row.empty and plot_label in first_row:\n                p_label = first_row[plot_label]\n                try:\n                    plt.annotate(\n                        p_label,\n                        xy=(xy_pos[0, plot_features[0]], xy_pos[0, plot_features[1]]),\n                        fontsize=font_size,\n                        alpha=alpha,\n                    )\n                except IndexError:\n                    pass\n\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n    plt.show()\n    return plt\n",
        "source_code_len": 21140,
        "target_code": "\n# flake8: noqa: F403, F401\n# pylint: disable=wildcard-import, unused-wildcard-import, unused-import\nfrom ..analysis.eventcluster import *\n\nWARN_MSSG = (\n    \"This module has moved to msticpy.analysis.eventcluster\\n\"\n    + \"Please change your import to reflect this new location.\"\n)\nwarnings.warn(WARN_MSSG, category=DeprecationWarning)\n",
        "target_code_len": 337,
        "diff_format": "@@ -50,683 +13,10 @@\n \n-# pylint: disable=too-many-arguments, too-many-locals\n-@export\n-def dbcluster_events(\n-    data: Any,\n-    cluster_columns: List[Any] = None,\n-    verbose: bool = False,\n-    normalize: bool = True,\n-    time_column: str = \"TimeCreatedUtc\",\n-    max_cluster_distance: float = 0.01,\n-    min_cluster_samples: int = 2,\n-    **kwargs,\n-) -> Tuple[pd.DataFrame, DBSCAN, np.ndarray]:\n-    \"\"\"\n-    Cluster data set according to cluster_columns features.\n+# flake8: noqa: F403, F401\n+# pylint: disable=wildcard-import, unused-wildcard-import, unused-import\n+from ..analysis.eventcluster import *\n \n-    Parameters\n-    ----------\n-    data : Any\n-        Input data as a pandas DataFrame or numpy array\n-    cluster_columns : List[Any], optional\n-        List of columns to use for features\n-        - for DataFrame this is a list of column names\n-        - for numpy array this is a list of column indexes\n-    verbose : bool, optional\n-        Print additional information about clustering results (the default is False)\n-    normalize : bool, optional\n-        Normalize the input data (should probably always be True)\n-    time_column : str, optional\n-        If there is a time column the output data will be ordered by this\n-        (the default is 'TimeCreatedUtc')\n-    max_cluster_distance : float, optional\n-        DBSCAN eps (max cluster member distance) (the default is 0.01)\n-    min_cluster_samples : int, optional\n-        DBSCAN min_samples (the minimum cluster size) (the default is 2)\n-\n-    Other Parameters\n-    ----------------\n-    kwargs: Other arguments are passed to DBSCAN constructor\n-\n-    Returns\n-    -------\n-    Tuple[pd.DataFrame, DBSCAN, np.ndarray]\n-        Output dataframe with clustered rows\n-        DBSCAN model\n-        Normalized data set\n-\n-    \"\"\"\n-    allowed_types = [np.ndarray, pd.DataFrame]\n-\n-    x_input = None\n-    if isinstance(data, pd.DataFrame):\n-        if cluster_columns is None:\n-            x_input = data.values\n-        else:\n-            x_input = data[cluster_columns].values\n-    elif isinstance(data, np.ndarray):\n-        x_input = data if cluster_columns is None else data[:, cluster_columns].values\n-    if x_input is None:\n-        mssg = \"Input data not in expected format.\\n{} is not one of allowed types {}\"\n-        type_list = \", \".join([str(t) for t in allowed_types])\n-        mssg = mssg.format(str(type(data)), type_list)\n-        raise ValueError(mssg)\n-\n-    # Create DBSCAN cluster object\n-    db_cluster = DBSCAN(\n-        eps=max_cluster_distance, min_samples=min_cluster_samples, **kwargs\n-    )\n-\n-    # Normalize the data (most clustering algorithms don't do well with\n-    # unnormalized data)\n-    x_norm = Normalizer().fit_transform(x_input) if normalize else x_input\n-    # fit the data set\n-    db_cluster.fit(x_norm)\n-    labels = db_cluster.labels_\n-    cluster_set, counts = np.unique(labels, return_counts=True)\n-    if verbose:\n-        print(\n-            \"Clustering for set size \",\n-            len(x_norm),\n-            \" - \",\n-            len(cluster_set),\n-            \" clusters\",\n-        )\n-        print(\"Individual cluster sizes: \", \", \".join([str(c) for c in counts]))\n-\n-    clustered_events = _merge_clustered_items(\n-        cluster_set, labels, data, time_column, counts\n-    )\n-\n-    if verbose:\n-        print(\"Cluster output rows: \", len(clustered_events))\n-\n-    return clustered_events, db_cluster, x_norm\n-\n-\n-def _merge_clustered_items(\n-    cluster_set: np.array,\n-    labels: np.array,\n-    data: Union[pd.DataFrame, np.array],\n-    time_column: str,\n-    counts: np.array,\n-) -> pd.DataFrame:\n-    \"\"\"\n-    Merge outliers and core clusters into single DataFrame.\n-\n-    Parameters\n-    ----------\n-    cluster_set : np.array\n-        The set of clusters\n-    labels : np.array\n-        The cluster labels\n-    data : Union[pd.DataFrame, np.array]\n-        The source data\n-    time_column : str\n-        Name of the Time column\n-    counts : np.array\n-        The counts of members in each cluster\n-\n-    Returns\n-    -------\n-    pd.DataFrame\n-        Merged dataframe\n-\n-    \"\"\"\n-    tz_aware = data.iloc[0][time_column].tz\n-    ts_type = \"datetime64[ns, UTC]\" if tz_aware is not None else \"datetime64[ns]\"\n-\n-    cluster_list = []\n-    # Iterate through clusters, adding exemplar to output frame\n-    # pylint: disable=consider-using-enumerate\n-    # we need to know the index of the item within the loop\n-    for idx in range(len(cluster_set)):\n-        cluster_id = cluster_set[idx]\n-        class_members = labels == cluster_id\n-        if isinstance(data, pd.DataFrame):\n-            time_ordered = data[class_members].sort_values(time_column, ascending=True)\n-            first_event_time = time_ordered[0:][time_column].iat[0]\n-            last_event_time = time_ordered[-1:][time_column].iat[0]\n-        else:\n-            first_event_time = None\n-            last_event_time = None\n-\n-        if cluster_id == -1:\n-            # 'Noise' events are individual items that could not be assigned\n-            # to a cluster and so are unique\n-            cluster_list.append(\n-                data[class_members]\n-                .assign(\n-                    Clustered=False,\n-                    ClusterId=cluster_id,\n-                    ClusterSize=1,\n-                    TimeGenerated=first_event_time,\n-                    FirstEventTime=first_event_time,\n-                    LastEventTime=last_event_time,\n-                )\n-                .astype(\n-                    dtype={\n-                        \"TimeGenerated\": ts_type,\n-                        \"FirstEventTime\": ts_type,\n-                        \"LastEventTime\": ts_type,\n-                    }\n-                )\n-            )\n-        else:\n-            # Otherwise, just choose the first example of the cluster set\n-            cluster_list.append(\n-                data[class_members]\n-                .assign(\n-                    Clustered=True,\n-                    ClusterId=cluster_id,\n-                    ClusterSize=counts[idx],\n-                    TimeGenerated=first_event_time,\n-                    FirstEventTime=first_event_time,\n-                    LastEventTime=last_event_time,\n-                )[0:1]\n-                .astype(\n-                    dtype={\n-                        \"TimeGenerated\": ts_type,\n-                        \"FirstEventTime\": ts_type,\n-                        \"LastEventTime\": ts_type,\n-                    }\n-                )\n-            )\n-    # pylint: enable=consider-using-enumerate\n-    return pd.concat(cluster_list)\n-\n-\n-@export\n-def add_process_features(\n-    input_frame: pd.DataFrame, path_separator: str = None, force: bool = False\n-) -> pd.DataFrame:\n-    r\"\"\"\n-    Add numerical features based on patterns of command line and process name.\n-\n-    Parameters\n-    ----------\n-    input_frame : pd.DataFrame\n-        The input dataframe\n-    path_separator : str, optional\n-        Path separator. If not supplied, try to determine\n-        from 'NewProcessName' column of first 10 rows\n-        (the default is None)\n-    force : bool, optional\n-        Forces re-calculation of feature columns even if they\n-        already exist (the default is False)\n-\n-    Returns\n-    -------\n-    pd.DataFrame\n-        Copy of the dataframe with the additional numeric features\n-\n-    Notes\n-    -----\n-    Features added:\n-\n-    - processNameLen: length of process file name (inc path)\n-    - processNameTokens: the number of elements in the path\n-    - processName: the process file name (minus path)\n-    - commandlineTokens: number of space-separated tokens in the command line\n-    - commandlineLen: length of the command line\n-    - commandlineLogLen: log10 length of commandline\n-    - isSystemSession: 1 if session Id is 0x3e7 for Windows or -1 for Linux\n-    - commandlineTokensFull: counts number of token separators in commandline\n-      [\\\\s\\-\\\\/\\.,\"\\'\\|&:;%$()]\n-    - pathScore: sum of ord() value of characters in path\n-    - pathLogScore: log10 of pathScore\n-    - commandlineScore: sum of ord() value of characters in commandline\n-    - commandlineLogScore: log10 of commandlineScore\n-\n-    \"\"\"\n-    output_df = input_frame.copy()\n-\n-    # Set any NaN values to empty string\n-    if \"NewProcessName\" in output_df and \"CommandLine\" in output_df:\n-        output_df[[\"NewProcessName\", \"CommandLine\"]] = output_df[\n-            [\"NewProcessName\", \"CommandLine\"]\n-        ].fillna(value=\"\")\n-\n-    # try to determine the path separator\n-    if path_separator is None:\n-        sample_df = output_df.head(10)\n-        lx_path = len(sample_df[sample_df[\"NewProcessName\"].str.contains(\"/\")])\n-        path_separator = \"/\" if lx_path else \"\\\\\"\n-    # Create features from process name and command line\n-    if \"NewProcessName\" in output_df:\n-        _add_processname_features(output_df, force, path_separator)\n-\n-    if \"CommandLine\" in output_df:\n-        _add_commandline_features(output_df, force)\n-\n-    if \"SubjectLogonId\" in output_df and (\"isSystemSession\" not in output_df or force):\n-        output_df[\"isSystemSession\"] = output_df[\"SubjectLogonId\"].isin([\"0x3e7\", \"-1\"])\n-\n-    return output_df\n-\n-\n-def _add_processname_features(\n-    output_df: pd.DataFrame, force: bool, path_separator: str\n-):\n-    \"\"\"\n-    Add process name default features.\n-\n-    Parameters\n-    ----------\n-    output_df : pd.DataFrame\n-        The dataframe to add features to\n-    force : bool\n-        If True overwrite existing feature columns\n-    path_separator : str\n-        Path separator for OS\n-\n-    \"\"\"\n-    if \"processName\" not in output_df or force:\n-        output_df[\"processName\"] = output_df.apply(\n-            lambda x: x.NewProcessName.split(path_separator)[-1], axis=1\n-        )\n-    if \"pathScore\" not in output_df or force:\n-        output_df[\"pathScore\"] = output_df.apply(\n-            lambda x: char_ord_score(x.NewProcessName), axis=1\n-        )\n-    if \"pathLogScore\" not in output_df or force:\n-        output_df[\"pathLogScore\"] = output_df.apply(\n-            lambda x: log10(x.pathScore) if x.pathScore else 0, axis=1\n-        )\n-    if \"pathHash\" not in output_df or force:\n-        output_df[\"pathHash\"] = output_df.apply(\n-            lambda x: crc32_hash(x.NewProcessName), axis=1\n-        )\n-\n-\n-def _add_commandline_features(output_df: pd.DataFrame, force: bool):\n-    \"\"\"\n-    Add commandline default features.\n-\n-    Parameters\n-    ----------\n-    output_df : pd.DataFrame\n-        The dataframe to add features to\n-    force : bool\n-        If True overwrite existing feature columns\n-\n-    \"\"\"\n-    if \"commandlineLen\" not in output_df or force:\n-        output_df[\"commandlineLen\"] = output_df.apply(\n-            lambda x: len(x.CommandLine), axis=1\n-        )\n-    if \"commandlineLogLen\" not in output_df or force:\n-        output_df[\"commandlineLogLen\"] = output_df.apply(\n-            lambda x: log10(x.commandlineLen) if x.commandlineLen else 0, axis=1\n-        )\n-    if \"commandlineTokensFull\" not in output_df or force:\n-        output_df[\"commandlineTokensFull\"] = output_df[[\"CommandLine\"]].apply(\n-            lambda x: delim_count(x.CommandLine), axis=1\n-        )\n-\n-    if \"commandlineScore\" not in output_df or force:\n-        output_df[\"commandlineScore\"] = output_df.apply(\n-            lambda x: char_ord_score(x.CommandLine), axis=1\n-        )\n-    if \"commandlineTokensHash\" not in output_df or force:\n-        output_df[\"commandlineTokensHash\"] = output_df.apply(\n-            lambda x: delim_hash(x.CommandLine), axis=1\n-        )\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def delim_count(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n-    r\"\"\"\n-    Count the delimiters in input column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    delim_list : str, optional\n-        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n-\n-    Returns\n-    -------\n-    int\n-        Count of delimiters in the string.\n-\n-    \"\"\"\n-    return len(re.findall(delim_list, value))\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def delim_hash(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n-    r\"\"\"\n-    Return a hash (CRC32) of the delimiters from input column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    delim_list : str, optional\n-        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n-\n-    Returns\n-    -------\n-    int\n-        Hash of delimiter set in the string.\n-\n-    \"\"\"\n-    return crc32(bytes(\"\".join(re.findall(delim_list, value)), \"utf-8\"))\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def char_ord_score(value: str, scale: int = 1) -> int:\n-    \"\"\"\n-    Return sum of ord values of characters in string.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    scale : int, optional\n-        reduce the scale of the feature (reducing the\n-        influence of variations this feature on the clustering\n-        algorithm (the default is 1)\n-\n-    Returns\n-    -------\n-    int\n-        [description]\n-\n-    Notes\n-    -----\n-    This function sums the ordinal value of each character in the\n-    input string. Two strings with minor differences will result in\n-    a similar score. However, for strings with highly variable content\n-    (e.g. command lines or http requests containing GUIDs) this may result\n-    in too much variance to be useful when you are trying to detect\n-    similar patterns. You can use the scale parameter to reduce the\n-    influence of features using this function on clustering and anomaly\n-    algorithms.\n-\n-    \"\"\"\n-    return floor(sum(ord(x) for x in value) / scale)\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def token_count(value: str, delimiter: str = \" \") -> int:\n-    \"\"\"\n-    Return count of delimiter-separated tokens pd.Series column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    delimiter : str, optional\n-        Delimiter used to split the column string.\n-        (the default is ' ')\n-\n-    Returns\n-    -------\n-    int\n-        count of tokens\n-\n-    \"\"\"\n-    return len(value.split(delimiter))\n-\n-\n-def _string_score(input_str):\n-    \"\"\"Sum the ord(c) for characters in a string.\"\"\"\n-    return sum(ord(x) for x in input_str)\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def crc32_hash(value: str) -> int:\n-    \"\"\"\n-    Return the CRC32 hash of the input column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-\n-    Returns\n-    -------\n-    int\n-        CRC32 hash\n-\n-    \"\"\"\n-    return crc32(bytes(value.encode(\"utf-8\")))\n-\n-\n-def delim_count_df(\n-    data: pd.DataFrame, column: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]'\n-) -> pd.Series:\n-    r\"\"\"\n-    Count the delimiters in input column.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        The name of the column to process\n-    delim_list : str, optional\n-        delimiters to use. (the default is r\\'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]\\')\n-\n-    Returns\n-    -------\n-    pd.Series\n-        Count of delimiters in the string in `column`.\n-\n-    \"\"\"\n-    return data[column].str.count(delim_list)\n-\n-\n-def char_ord_score_df(data: pd.DataFrame, column: str, scale: int = 1) -> pd.Series:\n-    \"\"\"\n-    Return sum of ord values of characters in string.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        Column name to process\n-    scale : int, optional\n-        reduce the scale of the feature (reducing the\n-        influence of variations this feature on the clustering\n-        algorithm (the default is 1)\n-\n-    Returns\n-    -------\n-    pd.Series\n-        The sum of the ordinal values of the characters\n-        in `column`.\n-\n-    Notes\n-    -----\n-    This function sums the ordinal value of each character in the\n-    input string. Two strings with minor differences will result in\n-    a similar score. However, for strings with highly variable content\n-    (e.g. command lines or http requests containing GUIDs) this may result\n-    in too much variance to be useful when you are trying to detect\n-    similar patterns. You can use the scale parameter to reduce the\n-    influence of features using this function on clustering and anomaly\n-    algorithms.\n-\n-    \"\"\"\n-    return data.apply(lambda x: sum(ord(char) for char in x[column]) / scale, axis=1)\n-\n-\n-def token_count_df(data: pd.DataFrame, column: str, delimiter: str = \" \") -> pd.Series:\n-    \"\"\"\n-    Return count of delimiter-separated tokens pd.Series column.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        Column name to process\n-    delimiter : str, optional\n-        Delimiter used to split the column string.\n-        (the default is ' ')\n-\n-    Returns\n-    -------\n-    pd.Series\n-        count of tokens in strings in `column`\n-\n-    \"\"\"\n-    return data.apply(lambda x: len(x[column].split(delimiter)), axis=1)\n-\n-\n-def crc32_hash_df(data: pd.DataFrame, column: str) -> pd.Series:\n-    \"\"\"\n-    Return the CRC32 hash of the input column.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        Column name to process\n-\n-    Returns\n-    -------\n-    pd.Series\n-        CRC32 hash of input column\n-\n-    \"\"\"\n-    return data.apply(lambda x: crc32(bytes(x[column].encode(\"utf-8\"))), axis=1)\n-\n-\n-# pylint: disable=too-many-arguments, too-many-statements\n-@export  # noqa: C901, MC0001\n-def plot_cluster(\n-    db_cluster: DBSCAN,\n-    data: pd.DataFrame,\n-    x_predict: np.ndarray,\n-    plot_label: str = None,\n-    plot_features: Tuple[int, int] = (0, 1),\n-    verbose: bool = False,\n-    cut_off: int = 3,\n-    xlabel: str = None,\n-    ylabel: str = None,\n-):\n-    \"\"\"\n-    Plot clustered data as scatter chart.\n-\n-    Parameters\n-    ----------\n-    db_cluster : DBSCAN\n-        DBScan Cluster (from SkLearn DBSCAN).\n-    data : pd.DataFrame\n-        Dataframe containing original data.\n-    x_predict : np.ndarray\n-        The DBSCAN predict numpy array\n-    plot_label : str, optional\n-         If set the column to use to label data points\n-         (the default is None)\n-    plot_features :  Tuple[int, int], optional\n-        Which two features in x_predict to plot (the default is (0, 1))\n-    verbose : bool, optional\n-        Verbose execution with some extra info\n-        (the default is False)\n-    cut_off : int, optional\n-        The cluster size below which items are considered outliers\n-        (the default is 3)\n-    xlabel : str, optional\n-        x-axis label (the default is None)\n-    ylabel : str, optional\n-        y-axis label (the default is None)\n-\n-    \"\"\"\n-    max_idx = x_predict.shape[1] - 1\n-    if plot_features[0] >= x_predict.shape[1]:\n-        raise ValueError(\n-            \"plot_features[0] index must be a value from 0 to {}.\".format(max_idx)\n-        )\n-    if plot_features[1] >= x_predict.shape[1]:\n-        raise ValueError(\n-            \"plot_features[1] index must be a value from 0 to {}.\".format(max_idx)\n-        )\n-    if plot_features[0] == plot_features[1]:\n-        mssg = \"plot_features indexes must be 2 different values in range 0 to\"\n-        raise ValueError(mssg + f\" {max_idx}.\")\n-\n-    labels = db_cluster.labels_\n-    core_samples_mask = np.zeros_like(labels, dtype=bool)\n-\n-    # pylint: disable=unsupported-assignment-operation\n-    # (assignment of numpy array is valid)\n-    core_samples_mask[db_cluster.core_sample_indices_] = True\n-    unique_labels = set(labels)\n-\n-    # pylint: disable=no-member\n-    # Spectral color map does exist\n-    colors = [cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n-    # Number of clusters in labels, ignoring noise if present.\n-    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n-    n_noise_ = list(labels).count(-1)\n-    _, counts = np.unique(labels, return_counts=True)\n-\n-    if verbose:\n-        print(\"Estimated number of clusters: %d\" % n_clusters_)\n-        print(\"Estimated number of noise points: %d\" % n_noise_)\n-        # print(\"Silhouette Coefficient: %0.3f\"\n-        #       % metrics.silhouette_score(x_predict, labels))\n-\n-    if not isinstance(data, pd.DataFrame):\n-        plot_label = None\n-    elif plot_label is not None and plot_label not in data:\n-        plot_label = None\n-\n-    p_label = None\n-    for cluster_id, color in zip(unique_labels, colors):\n-        if cluster_id == -1:\n-            # Black used for noise.\n-            color = [0, 0, 0, 1]\n-        class_member_mask = labels == cluster_id\n-\n-        cluster_size = counts[cluster_id]\n-        marker_size = cluster_size\n-        marker = \"o\"\n-        font_size = \"small\"\n-        alpha = 0.4\n-\n-        if cluster_size < cut_off:\n-            marker = \"+\"\n-            marker_size = 10\n-            font_size = \"large\"\n-            alpha = 1.0\n-        xy_pos = x_predict[class_member_mask & core_samples_mask]\n-        plt.plot(\n-            xy_pos[:, plot_features[0]],\n-            xy_pos[:, plot_features[1]],\n-            marker,\n-            markerfacecolor=tuple(color),\n-            markersize=marker_size,\n-        )\n-\n-        if plot_label:\n-            first_row = data[class_member_mask].iloc[0]\n-            if not first_row.empty and plot_label in first_row:\n-                p_label = first_row[plot_label]\n-                try:\n-                    plt.annotate(\n-                        p_label,\n-                        xy=(xy_pos[0, plot_features[0]], xy_pos[0, plot_features[1]]),\n-                        fontsize=font_size,\n-                        alpha=alpha,\n-                    )\n-                except IndexError:\n-                    pass\n-\n-    plt.xlabel(xlabel)\n-    plt.ylabel(ylabel)\n-    plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n-    plt.show()\n-    return plt\n+WARN_MSSG = (\n+    \"This module has moved to msticpy.analysis.eventcluster\\n\"\n+    + \"Please change your import to reflect this new location.\"\n+)\n+warnings.warn(WARN_MSSG, category=DeprecationWarning)\n",
        "source_code_with_indent": "\n# pylint: disable=too-many-arguments, too-many-locals\n@export\ndef dbcluster_events(\n    data: Any,\n    cluster_columns: List[Any] = None,\n    verbose: bool = False,\n    normalize: bool = True,\n    time_column: str = \"TimeCreatedUtc\",\n    max_cluster_distance: float = 0.01,\n    min_cluster_samples: int = 2,\n    **kwargs,\n) -> Tuple[pd.DataFrame, DBSCAN, np.ndarray]:\n    <IND>\"\"\"\n    Cluster data set according to cluster_columns features.\n\n    Parameters\n    ----------\n    data : Any\n        Input data as a pandas DataFrame or numpy array\n    cluster_columns : List[Any], optional\n        List of columns to use for features\n        - for DataFrame this is a list of column names\n        - for numpy array this is a list of column indexes\n    verbose : bool, optional\n        Print additional information about clustering results (the default is False)\n    normalize : bool, optional\n        Normalize the input data (should probably always be True)\n    time_column : str, optional\n        If there is a time column the output data will be ordered by this\n        (the default is 'TimeCreatedUtc')\n    max_cluster_distance : float, optional\n        DBSCAN eps (max cluster member distance) (the default is 0.01)\n    min_cluster_samples : int, optional\n        DBSCAN min_samples (the minimum cluster size) (the default is 2)\n\n    Other Parameters\n    ----------------\n    kwargs: Other arguments are passed to DBSCAN constructor\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, DBSCAN, np.ndarray]\n        Output dataframe with clustered rows\n        DBSCAN model\n        Normalized data set\n\n    \"\"\"\n    allowed_types = [np.ndarray, pd.DataFrame]\n\n    x_input = None\n    if isinstance(data, pd.DataFrame):\n        <IND>if cluster_columns is None:\n            <IND>x_input = data.values\n        <DED>else:\n            <IND>x_input = data[cluster_columns].values\n    <DED><DED>elif isinstance(data, np.ndarray):\n        <IND>x_input = data if cluster_columns is None else data[:, cluster_columns].values\n    <DED>if x_input is None:\n        <IND>mssg = \"Input data not in expected format.\\n{} is not one of allowed types {}\"\n        type_list = \", \".join([str(t) for t in allowed_types])\n        mssg = mssg.format(str(type(data)), type_list)\n        raise ValueError(mssg)\n\n    # Create DBSCAN cluster object\n    <DED>db_cluster = DBSCAN(\n        eps=max_cluster_distance, min_samples=min_cluster_samples, **kwargs\n    )\n\n    # Normalize the data (most clustering algorithms don't do well with\n    # unnormalized data)\n    x_norm = Normalizer().fit_transform(x_input) if normalize else x_input\n    # fit the data set\n    db_cluster.fit(x_norm)\n    labels = db_cluster.labels_\n    cluster_set, counts = np.unique(labels, return_counts=True)\n    if verbose:\n        <IND>print(\n            \"Clustering for set size \",\n            len(x_norm),\n            \" - \",\n            len(cluster_set),\n            \" clusters\",\n        )\n        print(\"Individual cluster sizes: \", \", \".join([str(c) for c in counts]))\n\n    <DED>clustered_events = _merge_clustered_items(\n        cluster_set, labels, data, time_column, counts\n    )\n\n    if verbose:\n        <IND>print(\"Cluster output rows: \", len(clustered_events))\n\n    <DED>return clustered_events, db_cluster, x_norm\n\n\n<DED>def _merge_clustered_items(\n    cluster_set: np.array,\n    labels: np.array,\n    data: Union[pd.DataFrame, np.array],\n    time_column: str,\n    counts: np.array,\n) -> pd.DataFrame:\n    <IND>\"\"\"\n    Merge outliers and core clusters into single DataFrame.\n\n    Parameters\n    ----------\n    cluster_set : np.array\n        The set of clusters\n    labels : np.array\n        The cluster labels\n    data : Union[pd.DataFrame, np.array]\n        The source data\n    time_column : str\n        Name of the Time column\n    counts : np.array\n        The counts of members in each cluster\n\n    Returns\n    -------\n    pd.DataFrame\n        Merged dataframe\n\n    \"\"\"\n    tz_aware = data.iloc[0][time_column].tz\n    ts_type = \"datetime64[ns, UTC]\" if tz_aware is not None else \"datetime64[ns]\"\n\n    cluster_list = []\n    # Iterate through clusters, adding exemplar to output frame\n    # pylint: disable=consider-using-enumerate\n    # we need to know the index of the item within the loop\n    for idx in range(len(cluster_set)):\n        <IND>cluster_id = cluster_set[idx]\n        class_members = labels == cluster_id\n        if isinstance(data, pd.DataFrame):\n            <IND>time_ordered = data[class_members].sort_values(time_column, ascending=True)\n            first_event_time = time_ordered[0:][time_column].iat[0]\n            last_event_time = time_ordered[-1:][time_column].iat[0]\n        <DED>else:\n            <IND>first_event_time = None\n            last_event_time = None\n\n        <DED>if cluster_id == -1:\n            # 'Noise' events are individual items that could not be assigned\n            # to a cluster and so are unique\n            <IND>cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=False,\n                    ClusterId=cluster_id,\n                    ClusterSize=1,\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n        <DED>else:\n            # Otherwise, just choose the first example of the cluster set\n            <IND>cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=True,\n                    ClusterId=cluster_id,\n                    ClusterSize=counts[idx],\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )[0:1]\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n    # pylint: enable=consider-using-enumerate\n    <DED><DED>return pd.concat(cluster_list)\n\n\n<DED>@export\ndef add_process_features(\n    input_frame: pd.DataFrame, path_separator: str = None, force: bool = False\n) -> pd.DataFrame:\n    <IND>r\"\"\"\n    Add numerical features based on patterns of command line and process name.\n\n    Parameters\n    ----------\n    input_frame : pd.DataFrame\n        The input dataframe\n    path_separator : str, optional\n        Path separator. If not supplied, try to determine\n        from 'NewProcessName' column of first 10 rows\n        (the default is None)\n    force : bool, optional\n        Forces re-calculation of feature columns even if they\n        already exist (the default is False)\n\n    Returns\n    -------\n    pd.DataFrame\n        Copy of the dataframe with the additional numeric features\n\n    Notes\n    -----\n    Features added:\n\n    - processNameLen: length of process file name (inc path)\n    - processNameTokens: the number of elements in the path\n    - processName: the process file name (minus path)\n    - commandlineTokens: number of space-separated tokens in the command line\n    - commandlineLen: length of the command line\n    - commandlineLogLen: log10 length of commandline\n    - isSystemSession: 1 if session Id is 0x3e7 for Windows or -1 for Linux\n    - commandlineTokensFull: counts number of token separators in commandline\n      [\\\\s\\-\\\\/\\.,\"\\'\\|&:;%$()]\n    - pathScore: sum of ord() value of characters in path\n    - pathLogScore: log10 of pathScore\n    - commandlineScore: sum of ord() value of characters in commandline\n    - commandlineLogScore: log10 of commandlineScore\n\n    \"\"\"\n    output_df = input_frame.copy()\n\n    # Set any NaN values to empty string\n    if \"NewProcessName\" in output_df and \"CommandLine\" in output_df:\n        <IND>output_df[[\"NewProcessName\", \"CommandLine\"]] = output_df[\n            [\"NewProcessName\", \"CommandLine\"]\n        ].fillna(value=\"\")\n\n    # try to determine the path separator\n    <DED>if path_separator is None:\n        <IND>sample_df = output_df.head(10)\n        lx_path = len(sample_df[sample_df[\"NewProcessName\"].str.contains(\"/\")])\n        path_separator = \"/\" if lx_path else \"\\\\\"\n    # Create features from process name and command line\n    <DED>if \"NewProcessName\" in output_df:\n        <IND>_add_processname_features(output_df, force, path_separator)\n\n    <DED>if \"CommandLine\" in output_df:\n        <IND>_add_commandline_features(output_df, force)\n\n    <DED>if \"SubjectLogonId\" in output_df and (\"isSystemSession\" not in output_df or force):\n        <IND>output_df[\"isSystemSession\"] = output_df[\"SubjectLogonId\"].isin([\"0x3e7\", \"-1\"])\n\n    <DED>return output_df\n\n\n<DED>def _add_processname_features(\n    output_df: pd.DataFrame, force: bool, path_separator: str\n):\n    <IND>\"\"\"\n    Add process name default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n    path_separator : str\n        Path separator for OS\n\n    \"\"\"\n    if \"processName\" not in output_df or force:\n        <IND>output_df[\"processName\"] = output_df.apply(\n            lambda x: x.NewProcessName.split(path_separator)[-1], axis=1\n        )\n    <DED>if \"pathScore\" not in output_df or force:\n        <IND>output_df[\"pathScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.NewProcessName), axis=1\n        )\n    <DED>if \"pathLogScore\" not in output_df or force:\n        <IND>output_df[\"pathLogScore\"] = output_df.apply(\n            lambda x: log10(x.pathScore) if x.pathScore else 0, axis=1\n        )\n    <DED>if \"pathHash\" not in output_df or force:\n        <IND>output_df[\"pathHash\"] = output_df.apply(\n            lambda x: crc32_hash(x.NewProcessName), axis=1\n        )\n\n\n<DED><DED>def _add_commandline_features(output_df: pd.DataFrame, force: bool):\n    <IND>\"\"\"\n    Add commandline default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n\n    \"\"\"\n    if \"commandlineLen\" not in output_df or force:\n        <IND>output_df[\"commandlineLen\"] = output_df.apply(\n            lambda x: len(x.CommandLine), axis=1\n        )\n    <DED>if \"commandlineLogLen\" not in output_df or force:\n        <IND>output_df[\"commandlineLogLen\"] = output_df.apply(\n            lambda x: log10(x.commandlineLen) if x.commandlineLen else 0, axis=1\n        )\n    <DED>if \"commandlineTokensFull\" not in output_df or force:\n        <IND>output_df[\"commandlineTokensFull\"] = output_df[[\"CommandLine\"]].apply(\n            lambda x: delim_count(x.CommandLine), axis=1\n        )\n\n    <DED>if \"commandlineScore\" not in output_df or force:\n        <IND>output_df[\"commandlineScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.CommandLine), axis=1\n        )\n    <DED>if \"commandlineTokensHash\" not in output_df or force:\n        <IND>output_df[\"commandlineTokensHash\"] = output_df.apply(\n            lambda x: delim_hash(x.CommandLine), axis=1\n        )\n\n\n<DED><DED>@export\n@lru_cache(maxsize=1024)\ndef delim_count(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    <IND>r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Count of delimiters in the string.\n\n    \"\"\"\n    return len(re.findall(delim_list, value))\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef delim_hash(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    <IND>r\"\"\"\n    Return a hash (CRC32) of the delimiters from input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Hash of delimiter set in the string.\n\n    \"\"\"\n    return crc32(bytes(\"\".join(re.findall(delim_list, value)), \"utf-8\"))\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef char_ord_score(value: str, scale: int = 1) -> int:\n    <IND>\"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    int\n        [description]\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return floor(sum(ord(x) for x in value) / scale)\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef token_count(value: str, delimiter: str = \" \") -> int:\n    <IND>\"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    int\n        count of tokens\n\n    \"\"\"\n    return len(value.split(delimiter))\n\n\n<DED>def _string_score(input_str):\n    <IND>\"\"\"Sum the ord(c) for characters in a string.\"\"\"\n    return sum(ord(x) for x in input_str)\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef crc32_hash(value: str) -> int:\n    <IND>\"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n\n    Returns\n    -------\n    int\n        CRC32 hash\n\n    \"\"\"\n    return crc32(bytes(value.encode(\"utf-8\")))\n\n\n<DED>def delim_count_df(\n    data: pd.DataFrame, column: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]'\n) -> pd.Series:\n    <IND>r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        The name of the column to process\n    delim_list : str, optional\n        delimiters to use. (the default is r\\'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]\\')\n\n    Returns\n    -------\n    pd.Series\n        Count of delimiters in the string in `column`.\n\n    \"\"\"\n    return data[column].str.count(delim_list)\n\n\n<DED>def char_ord_score_df(data: pd.DataFrame, column: str, scale: int = 1) -> pd.Series:\n    <IND>\"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    pd.Series\n        The sum of the ordinal values of the characters\n        in `column`.\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return data.apply(lambda x: sum(ord(char) for char in x[column]) / scale, axis=1)\n\n\n<DED>def token_count_df(data: pd.DataFrame, column: str, delimiter: str = \" \") -> pd.Series:\n    <IND>\"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    pd.Series\n        count of tokens in strings in `column`\n\n    \"\"\"\n    return data.apply(lambda x: len(x[column].split(delimiter)), axis=1)\n\n\n<DED>def crc32_hash_df(data: pd.DataFrame, column: str) -> pd.Series:\n    <IND>\"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n\n    Returns\n    -------\n    pd.Series\n        CRC32 hash of input column\n\n    \"\"\"\n    return data.apply(lambda x: crc32(bytes(x[column].encode(\"utf-8\"))), axis=1)\n\n\n# pylint: disable=too-many-arguments, too-many-statements\n<DED>@export  # noqa: C901, MC0001\ndef plot_cluster(\n    db_cluster: DBSCAN,\n    data: pd.DataFrame,\n    x_predict: np.ndarray,\n    plot_label: str = None,\n    plot_features: Tuple[int, int] = (0, 1),\n    verbose: bool = False,\n    cut_off: int = 3,\n    xlabel: str = None,\n    ylabel: str = None,\n):\n    <IND>\"\"\"\n    Plot clustered data as scatter chart.\n\n    Parameters\n    ----------\n    db_cluster : DBSCAN\n        DBScan Cluster (from SkLearn DBSCAN).\n    data : pd.DataFrame\n        Dataframe containing original data.\n    x_predict : np.ndarray\n        The DBSCAN predict numpy array\n    plot_label : str, optional\n         If set the column to use to label data points\n         (the default is None)\n    plot_features :  Tuple[int, int], optional\n        Which two features in x_predict to plot (the default is (0, 1))\n    verbose : bool, optional\n        Verbose execution with some extra info\n        (the default is False)\n    cut_off : int, optional\n        The cluster size below which items are considered outliers\n        (the default is 3)\n    xlabel : str, optional\n        x-axis label (the default is None)\n    ylabel : str, optional\n        y-axis label (the default is None)\n\n    \"\"\"\n    max_idx = x_predict.shape[1] - 1\n    if plot_features[0] >= x_predict.shape[1]:\n        <IND>raise ValueError(\n            \"plot_features[0] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    <DED>if plot_features[1] >= x_predict.shape[1]:\n        <IND>raise ValueError(\n            \"plot_features[1] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    <DED>if plot_features[0] == plot_features[1]:\n        <IND>mssg = \"plot_features indexes must be 2 different values in range 0 to\"\n        raise ValueError(mssg + f\" {max_idx}.\")\n\n    <DED>labels = db_cluster.labels_\n    core_samples_mask = np.zeros_like(labels, dtype=bool)\n\n    # pylint: disable=unsupported-assignment-operation\n    # (assignment of numpy array is valid)\n    core_samples_mask[db_cluster.core_sample_indices_] = True\n    unique_labels = set(labels)\n\n    # pylint: disable=no-member\n    # Spectral color map does exist\n    colors = [cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n    # Number of clusters in labels, ignoring noise if present.\n    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n    n_noise_ = list(labels).count(-1)\n    _, counts = np.unique(labels, return_counts=True)\n\n    if verbose:\n        <IND>print(\"Estimated number of clusters: %d\" % n_clusters_)\n        print(\"Estimated number of noise points: %d\" % n_noise_)\n        # print(\"Silhouette Coefficient: %0.3f\"\n        #       % metrics.silhouette_score(x_predict, labels))\n\n    <DED>if not isinstance(data, pd.DataFrame):\n        <IND>plot_label = None\n    <DED>elif plot_label is not None and plot_label not in data:\n        <IND>plot_label = None\n\n    <DED>p_label = None\n    for cluster_id, color in zip(unique_labels, colors):\n        <IND>if cluster_id == -1:\n            # Black used for noise.\n            <IND>color = [0, 0, 0, 1]\n        <DED>class_member_mask = labels == cluster_id\n\n        cluster_size = counts[cluster_id]\n        marker_size = cluster_size\n        marker = \"o\"\n        font_size = \"small\"\n        alpha = 0.4\n\n        if cluster_size < cut_off:\n            <IND>marker = \"+\"\n            marker_size = 10\n            font_size = \"large\"\n            alpha = 1.0\n        <DED>xy_pos = x_predict[class_member_mask & core_samples_mask]\n        plt.plot(\n            xy_pos[:, plot_features[0]],\n            xy_pos[:, plot_features[1]],\n            marker,\n            markerfacecolor=tuple(color),\n            markersize=marker_size,\n        )\n\n        if plot_label:\n            <IND>first_row = data[class_member_mask].iloc[0]\n            if not first_row.empty and plot_label in first_row:\n                <IND>p_label = first_row[plot_label]\n                try:\n                    <IND>plt.annotate(\n                        p_label,\n                        xy=(xy_pos[0, plot_features[0]], xy_pos[0, plot_features[1]]),\n                        fontsize=font_size,\n                        alpha=alpha,\n                    )\n                <DED>except IndexError:\n                    <IND>pass\n\n    <DED><DED><DED><DED>plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n    plt.show()\n    return plt\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n# flake8: noqa: F403, F401\n# pylint: disable=wildcard-import, unused-wildcard-import, unused-import\nfrom ..analysis.eventcluster import *\n\nWARN_MSSG = (\n    \"This module has moved to msticpy.analysis.eventcluster\\n\"\n    + \"Please change your import to reflect this new location.\"\n)\nwarnings.warn(WARN_MSSG, category=DeprecationWarning)\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "microsoft/msticpy",
    "commit": "28466d681e261394e18d9f4e063cebfa06ed04b4",
    "filename": "msticpy/sectools/eventcluster.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/microsoft-msticpy/msticpy/sectools/eventcluster.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "msticpy/sectools/eventcluster.py:616:4 Incompatible variable type [9]: xlabel is declared to have type `str` but is used as type `None`.",
    "message": " xlabel is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 616,
    "warning_line": "    xlabel: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "# --------------------------------------------------------------------------\nr\"\"\"\neventcluster module.\n\nThis module is intended to be used to summarize large numbers of events\ninto clusters of different patterns. High volume repeating events can\noften make it difficult to see unique and interesting items.\n\nThe module contains functions to generate clusterable features from\nstring data. For example, an administration command that does some\nmaintenance on thousands of servers with a commandline such as:\n``install-update -hostname {host.fqdn} -tmp:/tmp/{GUID}/rollback``\\  can\nbe collapsed into a single cluster pattern by ignoring the character\nvalues in the string and using delimiters or tokens to group the values.\n\nThis is an unsupervised learning module implemented using SciKit Learn\nDBScan.\n\nContains:\ndbcluster_events: generic clustering method using DBSCAN designed to summarize\nprocess events and other similar data by grouping on common features.\n\nadd_process_features: derives numerical features from text features such as\ncommandline and process path.\n\n\"\"\"\nfrom binascii import crc32\nfrom functools import lru_cache\nfrom math import log10, floor\nimport re\nfrom typing import List, Any, Tuple, Union\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import Normalizer\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nfrom ..common.utility import export\nfrom .._version import VERSION\n",
        "source_code_len": 1462,
        "target_code": "# --------------------------------------------------------------------------\n\"\"\"Deprecated placeholder for eventcluster.py.\"\"\"\nimport warnings\nfrom .._version import VERSION\n",
        "target_code_len": 174,
        "diff_format": "@@ -5,41 +5,4 @@\n # --------------------------------------------------------------------------\n-r\"\"\"\n-eventcluster module.\n-\n-This module is intended to be used to summarize large numbers of events\n-into clusters of different patterns. High volume repeating events can\n-often make it difficult to see unique and interesting items.\n-\n-The module contains functions to generate clusterable features from\n-string data. For example, an administration command that does some\n-maintenance on thousands of servers with a commandline such as:\n-``install-update -hostname {host.fqdn} -tmp:/tmp/{GUID}/rollback``\\  can\n-be collapsed into a single cluster pattern by ignoring the character\n-values in the string and using delimiters or tokens to group the values.\n-\n-This is an unsupervised learning module implemented using SciKit Learn\n-DBScan.\n-\n-Contains:\n-dbcluster_events: generic clustering method using DBSCAN designed to summarize\n-process events and other similar data by grouping on common features.\n-\n-add_process_features: derives numerical features from text features such as\n-commandline and process path.\n-\n-\"\"\"\n-from binascii import crc32\n-from functools import lru_cache\n-from math import log10, floor\n-import re\n-from typing import List, Any, Tuple, Union\n-\n-import numpy as np\n-import pandas as pd\n-from sklearn.cluster import DBSCAN\n-from sklearn.preprocessing import Normalizer\n-import matplotlib.pyplot as plt\n-from matplotlib import cm\n-\n-from ..common.utility import export\n+\"\"\"Deprecated placeholder for eventcluster.py.\"\"\"\n+import warnings\n from .._version import VERSION\n",
        "source_code_with_indent": "# --------------------------------------------------------------------------\nr\"\"\"\neventcluster module.\n\nThis module is intended to be used to summarize large numbers of events\ninto clusters of different patterns. High volume repeating events can\noften make it difficult to see unique and interesting items.\n\nThe module contains functions to generate clusterable features from\nstring data. For example, an administration command that does some\nmaintenance on thousands of servers with a commandline such as:\n``install-update -hostname {host.fqdn} -tmp:/tmp/{GUID}/rollback``\\  can\nbe collapsed into a single cluster pattern by ignoring the character\nvalues in the string and using delimiters or tokens to group the values.\n\nThis is an unsupervised learning module implemented using SciKit Learn\nDBScan.\n\nContains:\ndbcluster_events: generic clustering method using DBSCAN designed to summarize\nprocess events and other similar data by grouping on common features.\n\nadd_process_features: derives numerical features from text features such as\ncommandline and process path.\n\n\"\"\"\nfrom binascii import crc32\nfrom functools import lru_cache\nfrom math import log10, floor\nimport re\nfrom typing import List, Any, Tuple, Union\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import Normalizer\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nfrom ..common.utility import export\nfrom .._version import VERSION\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "# --------------------------------------------------------------------------\n\"\"\"Deprecated placeholder for eventcluster.py.\"\"\"\nimport warnings\nfrom .._version import VERSION\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n# pylint: disable=too-many-arguments, too-many-locals\n@export\ndef dbcluster_events(\n    data: Any,\n    cluster_columns: List[Any] = None,\n    verbose: bool = False,\n    normalize: bool = True,\n    time_column: str = \"TimeCreatedUtc\",\n    max_cluster_distance: float = 0.01,\n    min_cluster_samples: int = 2,\n    **kwargs,\n) -> Tuple[pd.DataFrame, DBSCAN, np.ndarray]:\n    \"\"\"\n    Cluster data set according to cluster_columns features.\n\n    Parameters\n    ----------\n    data : Any\n        Input data as a pandas DataFrame or numpy array\n    cluster_columns : List[Any], optional\n        List of columns to use for features\n        - for DataFrame this is a list of column names\n        - for numpy array this is a list of column indexes\n    verbose : bool, optional\n        Print additional information about clustering results (the default is False)\n    normalize : bool, optional\n        Normalize the input data (should probably always be True)\n    time_column : str, optional\n        If there is a time column the output data will be ordered by this\n        (the default is 'TimeCreatedUtc')\n    max_cluster_distance : float, optional\n        DBSCAN eps (max cluster member distance) (the default is 0.01)\n    min_cluster_samples : int, optional\n        DBSCAN min_samples (the minimum cluster size) (the default is 2)\n\n    Other Parameters\n    ----------------\n    kwargs: Other arguments are passed to DBSCAN constructor\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, DBSCAN, np.ndarray]\n        Output dataframe with clustered rows\n        DBSCAN model\n        Normalized data set\n\n    \"\"\"\n    allowed_types = [np.ndarray, pd.DataFrame]\n\n    x_input = None\n    if isinstance(data, pd.DataFrame):\n        if cluster_columns is None:\n            x_input = data.values\n        else:\n            x_input = data[cluster_columns].values\n    elif isinstance(data, np.ndarray):\n        x_input = data if cluster_columns is None else data[:, cluster_columns].values\n    if x_input is None:\n        mssg = \"Input data not in expected format.\\n{} is not one of allowed types {}\"\n        type_list = \", \".join([str(t) for t in allowed_types])\n        mssg = mssg.format(str(type(data)), type_list)\n        raise ValueError(mssg)\n\n    # Create DBSCAN cluster object\n    db_cluster = DBSCAN(\n        eps=max_cluster_distance, min_samples=min_cluster_samples, **kwargs\n    )\n\n    # Normalize the data (most clustering algorithms don't do well with\n    # unnormalized data)\n    x_norm = Normalizer().fit_transform(x_input) if normalize else x_input\n    # fit the data set\n    db_cluster.fit(x_norm)\n    labels = db_cluster.labels_\n    cluster_set, counts = np.unique(labels, return_counts=True)\n    if verbose:\n        print(\n            \"Clustering for set size \",\n            len(x_norm),\n            \" - \",\n            len(cluster_set),\n            \" clusters\",\n        )\n        print(\"Individual cluster sizes: \", \", \".join([str(c) for c in counts]))\n\n    clustered_events = _merge_clustered_items(\n        cluster_set, labels, data, time_column, counts\n    )\n\n    if verbose:\n        print(\"Cluster output rows: \", len(clustered_events))\n\n    return clustered_events, db_cluster, x_norm\n\n\ndef _merge_clustered_items(\n    cluster_set: np.array,\n    labels: np.array,\n    data: Union[pd.DataFrame, np.array],\n    time_column: str,\n    counts: np.array,\n) -> pd.DataFrame:\n    \"\"\"\n    Merge outliers and core clusters into single DataFrame.\n\n    Parameters\n    ----------\n    cluster_set : np.array\n        The set of clusters\n    labels : np.array\n        The cluster labels\n    data : Union[pd.DataFrame, np.array]\n        The source data\n    time_column : str\n        Name of the Time column\n    counts : np.array\n        The counts of members in each cluster\n\n    Returns\n    -------\n    pd.DataFrame\n        Merged dataframe\n\n    \"\"\"\n    tz_aware = data.iloc[0][time_column].tz\n    ts_type = \"datetime64[ns, UTC]\" if tz_aware is not None else \"datetime64[ns]\"\n\n    cluster_list = []\n    # Iterate through clusters, adding exemplar to output frame\n    # pylint: disable=consider-using-enumerate\n    # we need to know the index of the item within the loop\n    for idx in range(len(cluster_set)):\n        cluster_id = cluster_set[idx]\n        class_members = labels == cluster_id\n        if isinstance(data, pd.DataFrame):\n            time_ordered = data[class_members].sort_values(time_column, ascending=True)\n            first_event_time = time_ordered[0:][time_column].iat[0]\n            last_event_time = time_ordered[-1:][time_column].iat[0]\n        else:\n            first_event_time = None\n            last_event_time = None\n\n        if cluster_id == -1:\n            # 'Noise' events are individual items that could not be assigned\n            # to a cluster and so are unique\n            cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=False,\n                    ClusterId=cluster_id,\n                    ClusterSize=1,\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n        else:\n            # Otherwise, just choose the first example of the cluster set\n            cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=True,\n                    ClusterId=cluster_id,\n                    ClusterSize=counts[idx],\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )[0:1]\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n    # pylint: enable=consider-using-enumerate\n    return pd.concat(cluster_list)\n\n\n@export\ndef add_process_features(\n    input_frame: pd.DataFrame, path_separator: str = None, force: bool = False\n) -> pd.DataFrame:\n    r\"\"\"\n    Add numerical features based on patterns of command line and process name.\n\n    Parameters\n    ----------\n    input_frame : pd.DataFrame\n        The input dataframe\n    path_separator : str, optional\n        Path separator. If not supplied, try to determine\n        from 'NewProcessName' column of first 10 rows\n        (the default is None)\n    force : bool, optional\n        Forces re-calculation of feature columns even if they\n        already exist (the default is False)\n\n    Returns\n    -------\n    pd.DataFrame\n        Copy of the dataframe with the additional numeric features\n\n    Notes\n    -----\n    Features added:\n\n    - processNameLen: length of process file name (inc path)\n    - processNameTokens: the number of elements in the path\n    - processName: the process file name (minus path)\n    - commandlineTokens: number of space-separated tokens in the command line\n    - commandlineLen: length of the command line\n    - commandlineLogLen: log10 length of commandline\n    - isSystemSession: 1 if session Id is 0x3e7 for Windows or -1 for Linux\n    - commandlineTokensFull: counts number of token separators in commandline\n      [\\\\s\\-\\\\/\\.,\"\\'\\|&:;%$()]\n    - pathScore: sum of ord() value of characters in path\n    - pathLogScore: log10 of pathScore\n    - commandlineScore: sum of ord() value of characters in commandline\n    - commandlineLogScore: log10 of commandlineScore\n\n    \"\"\"\n    output_df = input_frame.copy()\n\n    # Set any NaN values to empty string\n    if \"NewProcessName\" in output_df and \"CommandLine\" in output_df:\n        output_df[[\"NewProcessName\", \"CommandLine\"]] = output_df[\n            [\"NewProcessName\", \"CommandLine\"]\n        ].fillna(value=\"\")\n\n    # try to determine the path separator\n    if path_separator is None:\n        sample_df = output_df.head(10)\n        lx_path = len(sample_df[sample_df[\"NewProcessName\"].str.contains(\"/\")])\n        path_separator = \"/\" if lx_path else \"\\\\\"\n    # Create features from process name and command line\n    if \"NewProcessName\" in output_df:\n        _add_processname_features(output_df, force, path_separator)\n\n    if \"CommandLine\" in output_df:\n        _add_commandline_features(output_df, force)\n\n    if \"SubjectLogonId\" in output_df and (\"isSystemSession\" not in output_df or force):\n        output_df[\"isSystemSession\"] = output_df[\"SubjectLogonId\"].isin([\"0x3e7\", \"-1\"])\n\n    return output_df\n\n\ndef _add_processname_features(\n    output_df: pd.DataFrame, force: bool, path_separator: str\n):\n    \"\"\"\n    Add process name default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n    path_separator : str\n        Path separator for OS\n\n    \"\"\"\n    if \"processName\" not in output_df or force:\n        output_df[\"processName\"] = output_df.apply(\n            lambda x: x.NewProcessName.split(path_separator)[-1], axis=1\n        )\n    if \"pathScore\" not in output_df or force:\n        output_df[\"pathScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.NewProcessName), axis=1\n        )\n    if \"pathLogScore\" not in output_df or force:\n        output_df[\"pathLogScore\"] = output_df.apply(\n            lambda x: log10(x.pathScore) if x.pathScore else 0, axis=1\n        )\n    if \"pathHash\" not in output_df or force:\n        output_df[\"pathHash\"] = output_df.apply(\n            lambda x: crc32_hash(x.NewProcessName), axis=1\n        )\n\n\ndef _add_commandline_features(output_df: pd.DataFrame, force: bool):\n    \"\"\"\n    Add commandline default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n\n    \"\"\"\n    if \"commandlineLen\" not in output_df or force:\n        output_df[\"commandlineLen\"] = output_df.apply(\n            lambda x: len(x.CommandLine), axis=1\n        )\n    if \"commandlineLogLen\" not in output_df or force:\n        output_df[\"commandlineLogLen\"] = output_df.apply(\n            lambda x: log10(x.commandlineLen) if x.commandlineLen else 0, axis=1\n        )\n    if \"commandlineTokensFull\" not in output_df or force:\n        output_df[\"commandlineTokensFull\"] = output_df[[\"CommandLine\"]].apply(\n            lambda x: delim_count(x.CommandLine), axis=1\n        )\n\n    if \"commandlineScore\" not in output_df or force:\n        output_df[\"commandlineScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.CommandLine), axis=1\n        )\n    if \"commandlineTokensHash\" not in output_df or force:\n        output_df[\"commandlineTokensHash\"] = output_df.apply(\n            lambda x: delim_hash(x.CommandLine), axis=1\n        )\n\n\n@export\n@lru_cache(maxsize=1024)\ndef delim_count(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Count of delimiters in the string.\n\n    \"\"\"\n    return len(re.findall(delim_list, value))\n\n\n@export\n@lru_cache(maxsize=1024)\ndef delim_hash(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    r\"\"\"\n    Return a hash (CRC32) of the delimiters from input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Hash of delimiter set in the string.\n\n    \"\"\"\n    return crc32(bytes(\"\".join(re.findall(delim_list, value)), \"utf-8\"))\n\n\n@export\n@lru_cache(maxsize=1024)\ndef char_ord_score(value: str, scale: int = 1) -> int:\n    \"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    int\n        [description]\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return floor(sum(ord(x) for x in value) / scale)\n\n\n@export\n@lru_cache(maxsize=1024)\ndef token_count(value: str, delimiter: str = \" \") -> int:\n    \"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    int\n        count of tokens\n\n    \"\"\"\n    return len(value.split(delimiter))\n\n\ndef _string_score(input_str):\n    \"\"\"Sum the ord(c) for characters in a string.\"\"\"\n    return sum(ord(x) for x in input_str)\n\n\n@export\n@lru_cache(maxsize=1024)\ndef crc32_hash(value: str) -> int:\n    \"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n\n    Returns\n    -------\n    int\n        CRC32 hash\n\n    \"\"\"\n    return crc32(bytes(value.encode(\"utf-8\")))\n\n\ndef delim_count_df(\n    data: pd.DataFrame, column: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]'\n) -> pd.Series:\n    r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        The name of the column to process\n    delim_list : str, optional\n        delimiters to use. (the default is r\\'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]\\')\n\n    Returns\n    -------\n    pd.Series\n        Count of delimiters in the string in `column`.\n\n    \"\"\"\n    return data[column].str.count(delim_list)\n\n\ndef char_ord_score_df(data: pd.DataFrame, column: str, scale: int = 1) -> pd.Series:\n    \"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    pd.Series\n        The sum of the ordinal values of the characters\n        in `column`.\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return data.apply(lambda x: sum(ord(char) for char in x[column]) / scale, axis=1)\n\n\ndef token_count_df(data: pd.DataFrame, column: str, delimiter: str = \" \") -> pd.Series:\n    \"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    pd.Series\n        count of tokens in strings in `column`\n\n    \"\"\"\n    return data.apply(lambda x: len(x[column].split(delimiter)), axis=1)\n\n\ndef crc32_hash_df(data: pd.DataFrame, column: str) -> pd.Series:\n    \"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n\n    Returns\n    -------\n    pd.Series\n        CRC32 hash of input column\n\n    \"\"\"\n    return data.apply(lambda x: crc32(bytes(x[column].encode(\"utf-8\"))), axis=1)\n\n\n# pylint: disable=too-many-arguments, too-many-statements\n@export  # noqa: C901, MC0001\ndef plot_cluster(\n    db_cluster: DBSCAN,\n    data: pd.DataFrame,\n    x_predict: np.ndarray,\n    plot_label: str = None,\n    plot_features: Tuple[int, int] = (0, 1),\n    verbose: bool = False,\n    cut_off: int = 3,\n    xlabel: str = None,\n    ylabel: str = None,\n):\n    \"\"\"\n    Plot clustered data as scatter chart.\n\n    Parameters\n    ----------\n    db_cluster : DBSCAN\n        DBScan Cluster (from SkLearn DBSCAN).\n    data : pd.DataFrame\n        Dataframe containing original data.\n    x_predict : np.ndarray\n        The DBSCAN predict numpy array\n    plot_label : str, optional\n         If set the column to use to label data points\n         (the default is None)\n    plot_features :  Tuple[int, int], optional\n        Which two features in x_predict to plot (the default is (0, 1))\n    verbose : bool, optional\n        Verbose execution with some extra info\n        (the default is False)\n    cut_off : int, optional\n        The cluster size below which items are considered outliers\n        (the default is 3)\n    xlabel : str, optional\n        x-axis label (the default is None)\n    ylabel : str, optional\n        y-axis label (the default is None)\n\n    \"\"\"\n    max_idx = x_predict.shape[1] - 1\n    if plot_features[0] >= x_predict.shape[1]:\n        raise ValueError(\n            \"plot_features[0] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    if plot_features[1] >= x_predict.shape[1]:\n        raise ValueError(\n            \"plot_features[1] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    if plot_features[0] == plot_features[1]:\n        mssg = \"plot_features indexes must be 2 different values in range 0 to\"\n        raise ValueError(mssg + f\" {max_idx}.\")\n\n    labels = db_cluster.labels_\n    core_samples_mask = np.zeros_like(labels, dtype=bool)\n\n    # pylint: disable=unsupported-assignment-operation\n    # (assignment of numpy array is valid)\n    core_samples_mask[db_cluster.core_sample_indices_] = True\n    unique_labels = set(labels)\n\n    # pylint: disable=no-member\n    # Spectral color map does exist\n    colors = [cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n    # Number of clusters in labels, ignoring noise if present.\n    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n    n_noise_ = list(labels).count(-1)\n    _, counts = np.unique(labels, return_counts=True)\n\n    if verbose:\n        print(\"Estimated number of clusters: %d\" % n_clusters_)\n        print(\"Estimated number of noise points: %d\" % n_noise_)\n        # print(\"Silhouette Coefficient: %0.3f\"\n        #       % metrics.silhouette_score(x_predict, labels))\n\n    if not isinstance(data, pd.DataFrame):\n        plot_label = None\n    elif plot_label is not None and plot_label not in data:\n        plot_label = None\n\n    p_label = None\n    for cluster_id, color in zip(unique_labels, colors):\n        if cluster_id == -1:\n            # Black used for noise.\n            color = [0, 0, 0, 1]\n        class_member_mask = labels == cluster_id\n\n        cluster_size = counts[cluster_id]\n        marker_size = cluster_size\n        marker = \"o\"\n        font_size = \"small\"\n        alpha = 0.4\n\n        if cluster_size < cut_off:\n            marker = \"+\"\n            marker_size = 10\n            font_size = \"large\"\n            alpha = 1.0\n        xy_pos = x_predict[class_member_mask & core_samples_mask]\n        plt.plot(\n            xy_pos[:, plot_features[0]],\n            xy_pos[:, plot_features[1]],\n            marker,\n            markerfacecolor=tuple(color),\n            markersize=marker_size,\n        )\n\n        if plot_label:\n            first_row = data[class_member_mask].iloc[0]\n            if not first_row.empty and plot_label in first_row:\n                p_label = first_row[plot_label]\n                try:\n                    plt.annotate(\n                        p_label,\n                        xy=(xy_pos[0, plot_features[0]], xy_pos[0, plot_features[1]]),\n                        fontsize=font_size,\n                        alpha=alpha,\n                    )\n                except IndexError:\n                    pass\n\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n    plt.show()\n    return plt\n",
        "source_code_len": 21140,
        "target_code": "\n# flake8: noqa: F403, F401\n# pylint: disable=wildcard-import, unused-wildcard-import, unused-import\nfrom ..analysis.eventcluster import *\n\nWARN_MSSG = (\n    \"This module has moved to msticpy.analysis.eventcluster\\n\"\n    + \"Please change your import to reflect this new location.\"\n)\nwarnings.warn(WARN_MSSG, category=DeprecationWarning)\n",
        "target_code_len": 337,
        "diff_format": "@@ -50,683 +13,10 @@\n \n-# pylint: disable=too-many-arguments, too-many-locals\n-@export\n-def dbcluster_events(\n-    data: Any,\n-    cluster_columns: List[Any] = None,\n-    verbose: bool = False,\n-    normalize: bool = True,\n-    time_column: str = \"TimeCreatedUtc\",\n-    max_cluster_distance: float = 0.01,\n-    min_cluster_samples: int = 2,\n-    **kwargs,\n-) -> Tuple[pd.DataFrame, DBSCAN, np.ndarray]:\n-    \"\"\"\n-    Cluster data set according to cluster_columns features.\n+# flake8: noqa: F403, F401\n+# pylint: disable=wildcard-import, unused-wildcard-import, unused-import\n+from ..analysis.eventcluster import *\n \n-    Parameters\n-    ----------\n-    data : Any\n-        Input data as a pandas DataFrame or numpy array\n-    cluster_columns : List[Any], optional\n-        List of columns to use for features\n-        - for DataFrame this is a list of column names\n-        - for numpy array this is a list of column indexes\n-    verbose : bool, optional\n-        Print additional information about clustering results (the default is False)\n-    normalize : bool, optional\n-        Normalize the input data (should probably always be True)\n-    time_column : str, optional\n-        If there is a time column the output data will be ordered by this\n-        (the default is 'TimeCreatedUtc')\n-    max_cluster_distance : float, optional\n-        DBSCAN eps (max cluster member distance) (the default is 0.01)\n-    min_cluster_samples : int, optional\n-        DBSCAN min_samples (the minimum cluster size) (the default is 2)\n-\n-    Other Parameters\n-    ----------------\n-    kwargs: Other arguments are passed to DBSCAN constructor\n-\n-    Returns\n-    -------\n-    Tuple[pd.DataFrame, DBSCAN, np.ndarray]\n-        Output dataframe with clustered rows\n-        DBSCAN model\n-        Normalized data set\n-\n-    \"\"\"\n-    allowed_types = [np.ndarray, pd.DataFrame]\n-\n-    x_input = None\n-    if isinstance(data, pd.DataFrame):\n-        if cluster_columns is None:\n-            x_input = data.values\n-        else:\n-            x_input = data[cluster_columns].values\n-    elif isinstance(data, np.ndarray):\n-        x_input = data if cluster_columns is None else data[:, cluster_columns].values\n-    if x_input is None:\n-        mssg = \"Input data not in expected format.\\n{} is not one of allowed types {}\"\n-        type_list = \", \".join([str(t) for t in allowed_types])\n-        mssg = mssg.format(str(type(data)), type_list)\n-        raise ValueError(mssg)\n-\n-    # Create DBSCAN cluster object\n-    db_cluster = DBSCAN(\n-        eps=max_cluster_distance, min_samples=min_cluster_samples, **kwargs\n-    )\n-\n-    # Normalize the data (most clustering algorithms don't do well with\n-    # unnormalized data)\n-    x_norm = Normalizer().fit_transform(x_input) if normalize else x_input\n-    # fit the data set\n-    db_cluster.fit(x_norm)\n-    labels = db_cluster.labels_\n-    cluster_set, counts = np.unique(labels, return_counts=True)\n-    if verbose:\n-        print(\n-            \"Clustering for set size \",\n-            len(x_norm),\n-            \" - \",\n-            len(cluster_set),\n-            \" clusters\",\n-        )\n-        print(\"Individual cluster sizes: \", \", \".join([str(c) for c in counts]))\n-\n-    clustered_events = _merge_clustered_items(\n-        cluster_set, labels, data, time_column, counts\n-    )\n-\n-    if verbose:\n-        print(\"Cluster output rows: \", len(clustered_events))\n-\n-    return clustered_events, db_cluster, x_norm\n-\n-\n-def _merge_clustered_items(\n-    cluster_set: np.array,\n-    labels: np.array,\n-    data: Union[pd.DataFrame, np.array],\n-    time_column: str,\n-    counts: np.array,\n-) -> pd.DataFrame:\n-    \"\"\"\n-    Merge outliers and core clusters into single DataFrame.\n-\n-    Parameters\n-    ----------\n-    cluster_set : np.array\n-        The set of clusters\n-    labels : np.array\n-        The cluster labels\n-    data : Union[pd.DataFrame, np.array]\n-        The source data\n-    time_column : str\n-        Name of the Time column\n-    counts : np.array\n-        The counts of members in each cluster\n-\n-    Returns\n-    -------\n-    pd.DataFrame\n-        Merged dataframe\n-\n-    \"\"\"\n-    tz_aware = data.iloc[0][time_column].tz\n-    ts_type = \"datetime64[ns, UTC]\" if tz_aware is not None else \"datetime64[ns]\"\n-\n-    cluster_list = []\n-    # Iterate through clusters, adding exemplar to output frame\n-    # pylint: disable=consider-using-enumerate\n-    # we need to know the index of the item within the loop\n-    for idx in range(len(cluster_set)):\n-        cluster_id = cluster_set[idx]\n-        class_members = labels == cluster_id\n-        if isinstance(data, pd.DataFrame):\n-            time_ordered = data[class_members].sort_values(time_column, ascending=True)\n-            first_event_time = time_ordered[0:][time_column].iat[0]\n-            last_event_time = time_ordered[-1:][time_column].iat[0]\n-        else:\n-            first_event_time = None\n-            last_event_time = None\n-\n-        if cluster_id == -1:\n-            # 'Noise' events are individual items that could not be assigned\n-            # to a cluster and so are unique\n-            cluster_list.append(\n-                data[class_members]\n-                .assign(\n-                    Clustered=False,\n-                    ClusterId=cluster_id,\n-                    ClusterSize=1,\n-                    TimeGenerated=first_event_time,\n-                    FirstEventTime=first_event_time,\n-                    LastEventTime=last_event_time,\n-                )\n-                .astype(\n-                    dtype={\n-                        \"TimeGenerated\": ts_type,\n-                        \"FirstEventTime\": ts_type,\n-                        \"LastEventTime\": ts_type,\n-                    }\n-                )\n-            )\n-        else:\n-            # Otherwise, just choose the first example of the cluster set\n-            cluster_list.append(\n-                data[class_members]\n-                .assign(\n-                    Clustered=True,\n-                    ClusterId=cluster_id,\n-                    ClusterSize=counts[idx],\n-                    TimeGenerated=first_event_time,\n-                    FirstEventTime=first_event_time,\n-                    LastEventTime=last_event_time,\n-                )[0:1]\n-                .astype(\n-                    dtype={\n-                        \"TimeGenerated\": ts_type,\n-                        \"FirstEventTime\": ts_type,\n-                        \"LastEventTime\": ts_type,\n-                    }\n-                )\n-            )\n-    # pylint: enable=consider-using-enumerate\n-    return pd.concat(cluster_list)\n-\n-\n-@export\n-def add_process_features(\n-    input_frame: pd.DataFrame, path_separator: str = None, force: bool = False\n-) -> pd.DataFrame:\n-    r\"\"\"\n-    Add numerical features based on patterns of command line and process name.\n-\n-    Parameters\n-    ----------\n-    input_frame : pd.DataFrame\n-        The input dataframe\n-    path_separator : str, optional\n-        Path separator. If not supplied, try to determine\n-        from 'NewProcessName' column of first 10 rows\n-        (the default is None)\n-    force : bool, optional\n-        Forces re-calculation of feature columns even if they\n-        already exist (the default is False)\n-\n-    Returns\n-    -------\n-    pd.DataFrame\n-        Copy of the dataframe with the additional numeric features\n-\n-    Notes\n-    -----\n-    Features added:\n-\n-    - processNameLen: length of process file name (inc path)\n-    - processNameTokens: the number of elements in the path\n-    - processName: the process file name (minus path)\n-    - commandlineTokens: number of space-separated tokens in the command line\n-    - commandlineLen: length of the command line\n-    - commandlineLogLen: log10 length of commandline\n-    - isSystemSession: 1 if session Id is 0x3e7 for Windows or -1 for Linux\n-    - commandlineTokensFull: counts number of token separators in commandline\n-      [\\\\s\\-\\\\/\\.,\"\\'\\|&:;%$()]\n-    - pathScore: sum of ord() value of characters in path\n-    - pathLogScore: log10 of pathScore\n-    - commandlineScore: sum of ord() value of characters in commandline\n-    - commandlineLogScore: log10 of commandlineScore\n-\n-    \"\"\"\n-    output_df = input_frame.copy()\n-\n-    # Set any NaN values to empty string\n-    if \"NewProcessName\" in output_df and \"CommandLine\" in output_df:\n-        output_df[[\"NewProcessName\", \"CommandLine\"]] = output_df[\n-            [\"NewProcessName\", \"CommandLine\"]\n-        ].fillna(value=\"\")\n-\n-    # try to determine the path separator\n-    if path_separator is None:\n-        sample_df = output_df.head(10)\n-        lx_path = len(sample_df[sample_df[\"NewProcessName\"].str.contains(\"/\")])\n-        path_separator = \"/\" if lx_path else \"\\\\\"\n-    # Create features from process name and command line\n-    if \"NewProcessName\" in output_df:\n-        _add_processname_features(output_df, force, path_separator)\n-\n-    if \"CommandLine\" in output_df:\n-        _add_commandline_features(output_df, force)\n-\n-    if \"SubjectLogonId\" in output_df and (\"isSystemSession\" not in output_df or force):\n-        output_df[\"isSystemSession\"] = output_df[\"SubjectLogonId\"].isin([\"0x3e7\", \"-1\"])\n-\n-    return output_df\n-\n-\n-def _add_processname_features(\n-    output_df: pd.DataFrame, force: bool, path_separator: str\n-):\n-    \"\"\"\n-    Add process name default features.\n-\n-    Parameters\n-    ----------\n-    output_df : pd.DataFrame\n-        The dataframe to add features to\n-    force : bool\n-        If True overwrite existing feature columns\n-    path_separator : str\n-        Path separator for OS\n-\n-    \"\"\"\n-    if \"processName\" not in output_df or force:\n-        output_df[\"processName\"] = output_df.apply(\n-            lambda x: x.NewProcessName.split(path_separator)[-1], axis=1\n-        )\n-    if \"pathScore\" not in output_df or force:\n-        output_df[\"pathScore\"] = output_df.apply(\n-            lambda x: char_ord_score(x.NewProcessName), axis=1\n-        )\n-    if \"pathLogScore\" not in output_df or force:\n-        output_df[\"pathLogScore\"] = output_df.apply(\n-            lambda x: log10(x.pathScore) if x.pathScore else 0, axis=1\n-        )\n-    if \"pathHash\" not in output_df or force:\n-        output_df[\"pathHash\"] = output_df.apply(\n-            lambda x: crc32_hash(x.NewProcessName), axis=1\n-        )\n-\n-\n-def _add_commandline_features(output_df: pd.DataFrame, force: bool):\n-    \"\"\"\n-    Add commandline default features.\n-\n-    Parameters\n-    ----------\n-    output_df : pd.DataFrame\n-        The dataframe to add features to\n-    force : bool\n-        If True overwrite existing feature columns\n-\n-    \"\"\"\n-    if \"commandlineLen\" not in output_df or force:\n-        output_df[\"commandlineLen\"] = output_df.apply(\n-            lambda x: len(x.CommandLine), axis=1\n-        )\n-    if \"commandlineLogLen\" not in output_df or force:\n-        output_df[\"commandlineLogLen\"] = output_df.apply(\n-            lambda x: log10(x.commandlineLen) if x.commandlineLen else 0, axis=1\n-        )\n-    if \"commandlineTokensFull\" not in output_df or force:\n-        output_df[\"commandlineTokensFull\"] = output_df[[\"CommandLine\"]].apply(\n-            lambda x: delim_count(x.CommandLine), axis=1\n-        )\n-\n-    if \"commandlineScore\" not in output_df or force:\n-        output_df[\"commandlineScore\"] = output_df.apply(\n-            lambda x: char_ord_score(x.CommandLine), axis=1\n-        )\n-    if \"commandlineTokensHash\" not in output_df or force:\n-        output_df[\"commandlineTokensHash\"] = output_df.apply(\n-            lambda x: delim_hash(x.CommandLine), axis=1\n-        )\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def delim_count(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n-    r\"\"\"\n-    Count the delimiters in input column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    delim_list : str, optional\n-        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n-\n-    Returns\n-    -------\n-    int\n-        Count of delimiters in the string.\n-\n-    \"\"\"\n-    return len(re.findall(delim_list, value))\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def delim_hash(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n-    r\"\"\"\n-    Return a hash (CRC32) of the delimiters from input column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    delim_list : str, optional\n-        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n-\n-    Returns\n-    -------\n-    int\n-        Hash of delimiter set in the string.\n-\n-    \"\"\"\n-    return crc32(bytes(\"\".join(re.findall(delim_list, value)), \"utf-8\"))\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def char_ord_score(value: str, scale: int = 1) -> int:\n-    \"\"\"\n-    Return sum of ord values of characters in string.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    scale : int, optional\n-        reduce the scale of the feature (reducing the\n-        influence of variations this feature on the clustering\n-        algorithm (the default is 1)\n-\n-    Returns\n-    -------\n-    int\n-        [description]\n-\n-    Notes\n-    -----\n-    This function sums the ordinal value of each character in the\n-    input string. Two strings with minor differences will result in\n-    a similar score. However, for strings with highly variable content\n-    (e.g. command lines or http requests containing GUIDs) this may result\n-    in too much variance to be useful when you are trying to detect\n-    similar patterns. You can use the scale parameter to reduce the\n-    influence of features using this function on clustering and anomaly\n-    algorithms.\n-\n-    \"\"\"\n-    return floor(sum(ord(x) for x in value) / scale)\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def token_count(value: str, delimiter: str = \" \") -> int:\n-    \"\"\"\n-    Return count of delimiter-separated tokens pd.Series column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    delimiter : str, optional\n-        Delimiter used to split the column string.\n-        (the default is ' ')\n-\n-    Returns\n-    -------\n-    int\n-        count of tokens\n-\n-    \"\"\"\n-    return len(value.split(delimiter))\n-\n-\n-def _string_score(input_str):\n-    \"\"\"Sum the ord(c) for characters in a string.\"\"\"\n-    return sum(ord(x) for x in input_str)\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def crc32_hash(value: str) -> int:\n-    \"\"\"\n-    Return the CRC32 hash of the input column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-\n-    Returns\n-    -------\n-    int\n-        CRC32 hash\n-\n-    \"\"\"\n-    return crc32(bytes(value.encode(\"utf-8\")))\n-\n-\n-def delim_count_df(\n-    data: pd.DataFrame, column: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]'\n-) -> pd.Series:\n-    r\"\"\"\n-    Count the delimiters in input column.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        The name of the column to process\n-    delim_list : str, optional\n-        delimiters to use. (the default is r\\'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]\\')\n-\n-    Returns\n-    -------\n-    pd.Series\n-        Count of delimiters in the string in `column`.\n-\n-    \"\"\"\n-    return data[column].str.count(delim_list)\n-\n-\n-def char_ord_score_df(data: pd.DataFrame, column: str, scale: int = 1) -> pd.Series:\n-    \"\"\"\n-    Return sum of ord values of characters in string.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        Column name to process\n-    scale : int, optional\n-        reduce the scale of the feature (reducing the\n-        influence of variations this feature on the clustering\n-        algorithm (the default is 1)\n-\n-    Returns\n-    -------\n-    pd.Series\n-        The sum of the ordinal values of the characters\n-        in `column`.\n-\n-    Notes\n-    -----\n-    This function sums the ordinal value of each character in the\n-    input string. Two strings with minor differences will result in\n-    a similar score. However, for strings with highly variable content\n-    (e.g. command lines or http requests containing GUIDs) this may result\n-    in too much variance to be useful when you are trying to detect\n-    similar patterns. You can use the scale parameter to reduce the\n-    influence of features using this function on clustering and anomaly\n-    algorithms.\n-\n-    \"\"\"\n-    return data.apply(lambda x: sum(ord(char) for char in x[column]) / scale, axis=1)\n-\n-\n-def token_count_df(data: pd.DataFrame, column: str, delimiter: str = \" \") -> pd.Series:\n-    \"\"\"\n-    Return count of delimiter-separated tokens pd.Series column.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        Column name to process\n-    delimiter : str, optional\n-        Delimiter used to split the column string.\n-        (the default is ' ')\n-\n-    Returns\n-    -------\n-    pd.Series\n-        count of tokens in strings in `column`\n-\n-    \"\"\"\n-    return data.apply(lambda x: len(x[column].split(delimiter)), axis=1)\n-\n-\n-def crc32_hash_df(data: pd.DataFrame, column: str) -> pd.Series:\n-    \"\"\"\n-    Return the CRC32 hash of the input column.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        Column name to process\n-\n-    Returns\n-    -------\n-    pd.Series\n-        CRC32 hash of input column\n-\n-    \"\"\"\n-    return data.apply(lambda x: crc32(bytes(x[column].encode(\"utf-8\"))), axis=1)\n-\n-\n-# pylint: disable=too-many-arguments, too-many-statements\n-@export  # noqa: C901, MC0001\n-def plot_cluster(\n-    db_cluster: DBSCAN,\n-    data: pd.DataFrame,\n-    x_predict: np.ndarray,\n-    plot_label: str = None,\n-    plot_features: Tuple[int, int] = (0, 1),\n-    verbose: bool = False,\n-    cut_off: int = 3,\n-    xlabel: str = None,\n-    ylabel: str = None,\n-):\n-    \"\"\"\n-    Plot clustered data as scatter chart.\n-\n-    Parameters\n-    ----------\n-    db_cluster : DBSCAN\n-        DBScan Cluster (from SkLearn DBSCAN).\n-    data : pd.DataFrame\n-        Dataframe containing original data.\n-    x_predict : np.ndarray\n-        The DBSCAN predict numpy array\n-    plot_label : str, optional\n-         If set the column to use to label data points\n-         (the default is None)\n-    plot_features :  Tuple[int, int], optional\n-        Which two features in x_predict to plot (the default is (0, 1))\n-    verbose : bool, optional\n-        Verbose execution with some extra info\n-        (the default is False)\n-    cut_off : int, optional\n-        The cluster size below which items are considered outliers\n-        (the default is 3)\n-    xlabel : str, optional\n-        x-axis label (the default is None)\n-    ylabel : str, optional\n-        y-axis label (the default is None)\n-\n-    \"\"\"\n-    max_idx = x_predict.shape[1] - 1\n-    if plot_features[0] >= x_predict.shape[1]:\n-        raise ValueError(\n-            \"plot_features[0] index must be a value from 0 to {}.\".format(max_idx)\n-        )\n-    if plot_features[1] >= x_predict.shape[1]:\n-        raise ValueError(\n-            \"plot_features[1] index must be a value from 0 to {}.\".format(max_idx)\n-        )\n-    if plot_features[0] == plot_features[1]:\n-        mssg = \"plot_features indexes must be 2 different values in range 0 to\"\n-        raise ValueError(mssg + f\" {max_idx}.\")\n-\n-    labels = db_cluster.labels_\n-    core_samples_mask = np.zeros_like(labels, dtype=bool)\n-\n-    # pylint: disable=unsupported-assignment-operation\n-    # (assignment of numpy array is valid)\n-    core_samples_mask[db_cluster.core_sample_indices_] = True\n-    unique_labels = set(labels)\n-\n-    # pylint: disable=no-member\n-    # Spectral color map does exist\n-    colors = [cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n-    # Number of clusters in labels, ignoring noise if present.\n-    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n-    n_noise_ = list(labels).count(-1)\n-    _, counts = np.unique(labels, return_counts=True)\n-\n-    if verbose:\n-        print(\"Estimated number of clusters: %d\" % n_clusters_)\n-        print(\"Estimated number of noise points: %d\" % n_noise_)\n-        # print(\"Silhouette Coefficient: %0.3f\"\n-        #       % metrics.silhouette_score(x_predict, labels))\n-\n-    if not isinstance(data, pd.DataFrame):\n-        plot_label = None\n-    elif plot_label is not None and plot_label not in data:\n-        plot_label = None\n-\n-    p_label = None\n-    for cluster_id, color in zip(unique_labels, colors):\n-        if cluster_id == -1:\n-            # Black used for noise.\n-            color = [0, 0, 0, 1]\n-        class_member_mask = labels == cluster_id\n-\n-        cluster_size = counts[cluster_id]\n-        marker_size = cluster_size\n-        marker = \"o\"\n-        font_size = \"small\"\n-        alpha = 0.4\n-\n-        if cluster_size < cut_off:\n-            marker = \"+\"\n-            marker_size = 10\n-            font_size = \"large\"\n-            alpha = 1.0\n-        xy_pos = x_predict[class_member_mask & core_samples_mask]\n-        plt.plot(\n-            xy_pos[:, plot_features[0]],\n-            xy_pos[:, plot_features[1]],\n-            marker,\n-            markerfacecolor=tuple(color),\n-            markersize=marker_size,\n-        )\n-\n-        if plot_label:\n-            first_row = data[class_member_mask].iloc[0]\n-            if not first_row.empty and plot_label in first_row:\n-                p_label = first_row[plot_label]\n-                try:\n-                    plt.annotate(\n-                        p_label,\n-                        xy=(xy_pos[0, plot_features[0]], xy_pos[0, plot_features[1]]),\n-                        fontsize=font_size,\n-                        alpha=alpha,\n-                    )\n-                except IndexError:\n-                    pass\n-\n-    plt.xlabel(xlabel)\n-    plt.ylabel(ylabel)\n-    plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n-    plt.show()\n-    return plt\n+WARN_MSSG = (\n+    \"This module has moved to msticpy.analysis.eventcluster\\n\"\n+    + \"Please change your import to reflect this new location.\"\n+)\n+warnings.warn(WARN_MSSG, category=DeprecationWarning)\n",
        "source_code_with_indent": "\n# pylint: disable=too-many-arguments, too-many-locals\n@export\ndef dbcluster_events(\n    data: Any,\n    cluster_columns: List[Any] = None,\n    verbose: bool = False,\n    normalize: bool = True,\n    time_column: str = \"TimeCreatedUtc\",\n    max_cluster_distance: float = 0.01,\n    min_cluster_samples: int = 2,\n    **kwargs,\n) -> Tuple[pd.DataFrame, DBSCAN, np.ndarray]:\n    <IND>\"\"\"\n    Cluster data set according to cluster_columns features.\n\n    Parameters\n    ----------\n    data : Any\n        Input data as a pandas DataFrame or numpy array\n    cluster_columns : List[Any], optional\n        List of columns to use for features\n        - for DataFrame this is a list of column names\n        - for numpy array this is a list of column indexes\n    verbose : bool, optional\n        Print additional information about clustering results (the default is False)\n    normalize : bool, optional\n        Normalize the input data (should probably always be True)\n    time_column : str, optional\n        If there is a time column the output data will be ordered by this\n        (the default is 'TimeCreatedUtc')\n    max_cluster_distance : float, optional\n        DBSCAN eps (max cluster member distance) (the default is 0.01)\n    min_cluster_samples : int, optional\n        DBSCAN min_samples (the minimum cluster size) (the default is 2)\n\n    Other Parameters\n    ----------------\n    kwargs: Other arguments are passed to DBSCAN constructor\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, DBSCAN, np.ndarray]\n        Output dataframe with clustered rows\n        DBSCAN model\n        Normalized data set\n\n    \"\"\"\n    allowed_types = [np.ndarray, pd.DataFrame]\n\n    x_input = None\n    if isinstance(data, pd.DataFrame):\n        <IND>if cluster_columns is None:\n            <IND>x_input = data.values\n        <DED>else:\n            <IND>x_input = data[cluster_columns].values\n    <DED><DED>elif isinstance(data, np.ndarray):\n        <IND>x_input = data if cluster_columns is None else data[:, cluster_columns].values\n    <DED>if x_input is None:\n        <IND>mssg = \"Input data not in expected format.\\n{} is not one of allowed types {}\"\n        type_list = \", \".join([str(t) for t in allowed_types])\n        mssg = mssg.format(str(type(data)), type_list)\n        raise ValueError(mssg)\n\n    # Create DBSCAN cluster object\n    <DED>db_cluster = DBSCAN(\n        eps=max_cluster_distance, min_samples=min_cluster_samples, **kwargs\n    )\n\n    # Normalize the data (most clustering algorithms don't do well with\n    # unnormalized data)\n    x_norm = Normalizer().fit_transform(x_input) if normalize else x_input\n    # fit the data set\n    db_cluster.fit(x_norm)\n    labels = db_cluster.labels_\n    cluster_set, counts = np.unique(labels, return_counts=True)\n    if verbose:\n        <IND>print(\n            \"Clustering for set size \",\n            len(x_norm),\n            \" - \",\n            len(cluster_set),\n            \" clusters\",\n        )\n        print(\"Individual cluster sizes: \", \", \".join([str(c) for c in counts]))\n\n    <DED>clustered_events = _merge_clustered_items(\n        cluster_set, labels, data, time_column, counts\n    )\n\n    if verbose:\n        <IND>print(\"Cluster output rows: \", len(clustered_events))\n\n    <DED>return clustered_events, db_cluster, x_norm\n\n\n<DED>def _merge_clustered_items(\n    cluster_set: np.array,\n    labels: np.array,\n    data: Union[pd.DataFrame, np.array],\n    time_column: str,\n    counts: np.array,\n) -> pd.DataFrame:\n    <IND>\"\"\"\n    Merge outliers and core clusters into single DataFrame.\n\n    Parameters\n    ----------\n    cluster_set : np.array\n        The set of clusters\n    labels : np.array\n        The cluster labels\n    data : Union[pd.DataFrame, np.array]\n        The source data\n    time_column : str\n        Name of the Time column\n    counts : np.array\n        The counts of members in each cluster\n\n    Returns\n    -------\n    pd.DataFrame\n        Merged dataframe\n\n    \"\"\"\n    tz_aware = data.iloc[0][time_column].tz\n    ts_type = \"datetime64[ns, UTC]\" if tz_aware is not None else \"datetime64[ns]\"\n\n    cluster_list = []\n    # Iterate through clusters, adding exemplar to output frame\n    # pylint: disable=consider-using-enumerate\n    # we need to know the index of the item within the loop\n    for idx in range(len(cluster_set)):\n        <IND>cluster_id = cluster_set[idx]\n        class_members = labels == cluster_id\n        if isinstance(data, pd.DataFrame):\n            <IND>time_ordered = data[class_members].sort_values(time_column, ascending=True)\n            first_event_time = time_ordered[0:][time_column].iat[0]\n            last_event_time = time_ordered[-1:][time_column].iat[0]\n        <DED>else:\n            <IND>first_event_time = None\n            last_event_time = None\n\n        <DED>if cluster_id == -1:\n            # 'Noise' events are individual items that could not be assigned\n            # to a cluster and so are unique\n            <IND>cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=False,\n                    ClusterId=cluster_id,\n                    ClusterSize=1,\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n        <DED>else:\n            # Otherwise, just choose the first example of the cluster set\n            <IND>cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=True,\n                    ClusterId=cluster_id,\n                    ClusterSize=counts[idx],\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )[0:1]\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n    # pylint: enable=consider-using-enumerate\n    <DED><DED>return pd.concat(cluster_list)\n\n\n<DED>@export\ndef add_process_features(\n    input_frame: pd.DataFrame, path_separator: str = None, force: bool = False\n) -> pd.DataFrame:\n    <IND>r\"\"\"\n    Add numerical features based on patterns of command line and process name.\n\n    Parameters\n    ----------\n    input_frame : pd.DataFrame\n        The input dataframe\n    path_separator : str, optional\n        Path separator. If not supplied, try to determine\n        from 'NewProcessName' column of first 10 rows\n        (the default is None)\n    force : bool, optional\n        Forces re-calculation of feature columns even if they\n        already exist (the default is False)\n\n    Returns\n    -------\n    pd.DataFrame\n        Copy of the dataframe with the additional numeric features\n\n    Notes\n    -----\n    Features added:\n\n    - processNameLen: length of process file name (inc path)\n    - processNameTokens: the number of elements in the path\n    - processName: the process file name (minus path)\n    - commandlineTokens: number of space-separated tokens in the command line\n    - commandlineLen: length of the command line\n    - commandlineLogLen: log10 length of commandline\n    - isSystemSession: 1 if session Id is 0x3e7 for Windows or -1 for Linux\n    - commandlineTokensFull: counts number of token separators in commandline\n      [\\\\s\\-\\\\/\\.,\"\\'\\|&:;%$()]\n    - pathScore: sum of ord() value of characters in path\n    - pathLogScore: log10 of pathScore\n    - commandlineScore: sum of ord() value of characters in commandline\n    - commandlineLogScore: log10 of commandlineScore\n\n    \"\"\"\n    output_df = input_frame.copy()\n\n    # Set any NaN values to empty string\n    if \"NewProcessName\" in output_df and \"CommandLine\" in output_df:\n        <IND>output_df[[\"NewProcessName\", \"CommandLine\"]] = output_df[\n            [\"NewProcessName\", \"CommandLine\"]\n        ].fillna(value=\"\")\n\n    # try to determine the path separator\n    <DED>if path_separator is None:\n        <IND>sample_df = output_df.head(10)\n        lx_path = len(sample_df[sample_df[\"NewProcessName\"].str.contains(\"/\")])\n        path_separator = \"/\" if lx_path else \"\\\\\"\n    # Create features from process name and command line\n    <DED>if \"NewProcessName\" in output_df:\n        <IND>_add_processname_features(output_df, force, path_separator)\n\n    <DED>if \"CommandLine\" in output_df:\n        <IND>_add_commandline_features(output_df, force)\n\n    <DED>if \"SubjectLogonId\" in output_df and (\"isSystemSession\" not in output_df or force):\n        <IND>output_df[\"isSystemSession\"] = output_df[\"SubjectLogonId\"].isin([\"0x3e7\", \"-1\"])\n\n    <DED>return output_df\n\n\n<DED>def _add_processname_features(\n    output_df: pd.DataFrame, force: bool, path_separator: str\n):\n    <IND>\"\"\"\n    Add process name default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n    path_separator : str\n        Path separator for OS\n\n    \"\"\"\n    if \"processName\" not in output_df or force:\n        <IND>output_df[\"processName\"] = output_df.apply(\n            lambda x: x.NewProcessName.split(path_separator)[-1], axis=1\n        )\n    <DED>if \"pathScore\" not in output_df or force:\n        <IND>output_df[\"pathScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.NewProcessName), axis=1\n        )\n    <DED>if \"pathLogScore\" not in output_df or force:\n        <IND>output_df[\"pathLogScore\"] = output_df.apply(\n            lambda x: log10(x.pathScore) if x.pathScore else 0, axis=1\n        )\n    <DED>if \"pathHash\" not in output_df or force:\n        <IND>output_df[\"pathHash\"] = output_df.apply(\n            lambda x: crc32_hash(x.NewProcessName), axis=1\n        )\n\n\n<DED><DED>def _add_commandline_features(output_df: pd.DataFrame, force: bool):\n    <IND>\"\"\"\n    Add commandline default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n\n    \"\"\"\n    if \"commandlineLen\" not in output_df or force:\n        <IND>output_df[\"commandlineLen\"] = output_df.apply(\n            lambda x: len(x.CommandLine), axis=1\n        )\n    <DED>if \"commandlineLogLen\" not in output_df or force:\n        <IND>output_df[\"commandlineLogLen\"] = output_df.apply(\n            lambda x: log10(x.commandlineLen) if x.commandlineLen else 0, axis=1\n        )\n    <DED>if \"commandlineTokensFull\" not in output_df or force:\n        <IND>output_df[\"commandlineTokensFull\"] = output_df[[\"CommandLine\"]].apply(\n            lambda x: delim_count(x.CommandLine), axis=1\n        )\n\n    <DED>if \"commandlineScore\" not in output_df or force:\n        <IND>output_df[\"commandlineScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.CommandLine), axis=1\n        )\n    <DED>if \"commandlineTokensHash\" not in output_df or force:\n        <IND>output_df[\"commandlineTokensHash\"] = output_df.apply(\n            lambda x: delim_hash(x.CommandLine), axis=1\n        )\n\n\n<DED><DED>@export\n@lru_cache(maxsize=1024)\ndef delim_count(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    <IND>r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Count of delimiters in the string.\n\n    \"\"\"\n    return len(re.findall(delim_list, value))\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef delim_hash(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    <IND>r\"\"\"\n    Return a hash (CRC32) of the delimiters from input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Hash of delimiter set in the string.\n\n    \"\"\"\n    return crc32(bytes(\"\".join(re.findall(delim_list, value)), \"utf-8\"))\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef char_ord_score(value: str, scale: int = 1) -> int:\n    <IND>\"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    int\n        [description]\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return floor(sum(ord(x) for x in value) / scale)\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef token_count(value: str, delimiter: str = \" \") -> int:\n    <IND>\"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    int\n        count of tokens\n\n    \"\"\"\n    return len(value.split(delimiter))\n\n\n<DED>def _string_score(input_str):\n    <IND>\"\"\"Sum the ord(c) for characters in a string.\"\"\"\n    return sum(ord(x) for x in input_str)\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef crc32_hash(value: str) -> int:\n    <IND>\"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n\n    Returns\n    -------\n    int\n        CRC32 hash\n\n    \"\"\"\n    return crc32(bytes(value.encode(\"utf-8\")))\n\n\n<DED>def delim_count_df(\n    data: pd.DataFrame, column: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]'\n) -> pd.Series:\n    <IND>r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        The name of the column to process\n    delim_list : str, optional\n        delimiters to use. (the default is r\\'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]\\')\n\n    Returns\n    -------\n    pd.Series\n        Count of delimiters in the string in `column`.\n\n    \"\"\"\n    return data[column].str.count(delim_list)\n\n\n<DED>def char_ord_score_df(data: pd.DataFrame, column: str, scale: int = 1) -> pd.Series:\n    <IND>\"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    pd.Series\n        The sum of the ordinal values of the characters\n        in `column`.\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return data.apply(lambda x: sum(ord(char) for char in x[column]) / scale, axis=1)\n\n\n<DED>def token_count_df(data: pd.DataFrame, column: str, delimiter: str = \" \") -> pd.Series:\n    <IND>\"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    pd.Series\n        count of tokens in strings in `column`\n\n    \"\"\"\n    return data.apply(lambda x: len(x[column].split(delimiter)), axis=1)\n\n\n<DED>def crc32_hash_df(data: pd.DataFrame, column: str) -> pd.Series:\n    <IND>\"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n\n    Returns\n    -------\n    pd.Series\n        CRC32 hash of input column\n\n    \"\"\"\n    return data.apply(lambda x: crc32(bytes(x[column].encode(\"utf-8\"))), axis=1)\n\n\n# pylint: disable=too-many-arguments, too-many-statements\n<DED>@export  # noqa: C901, MC0001\ndef plot_cluster(\n    db_cluster: DBSCAN,\n    data: pd.DataFrame,\n    x_predict: np.ndarray,\n    plot_label: str = None,\n    plot_features: Tuple[int, int] = (0, 1),\n    verbose: bool = False,\n    cut_off: int = 3,\n    xlabel: str = None,\n    ylabel: str = None,\n):\n    <IND>\"\"\"\n    Plot clustered data as scatter chart.\n\n    Parameters\n    ----------\n    db_cluster : DBSCAN\n        DBScan Cluster (from SkLearn DBSCAN).\n    data : pd.DataFrame\n        Dataframe containing original data.\n    x_predict : np.ndarray\n        The DBSCAN predict numpy array\n    plot_label : str, optional\n         If set the column to use to label data points\n         (the default is None)\n    plot_features :  Tuple[int, int], optional\n        Which two features in x_predict to plot (the default is (0, 1))\n    verbose : bool, optional\n        Verbose execution with some extra info\n        (the default is False)\n    cut_off : int, optional\n        The cluster size below which items are considered outliers\n        (the default is 3)\n    xlabel : str, optional\n        x-axis label (the default is None)\n    ylabel : str, optional\n        y-axis label (the default is None)\n\n    \"\"\"\n    max_idx = x_predict.shape[1] - 1\n    if plot_features[0] >= x_predict.shape[1]:\n        <IND>raise ValueError(\n            \"plot_features[0] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    <DED>if plot_features[1] >= x_predict.shape[1]:\n        <IND>raise ValueError(\n            \"plot_features[1] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    <DED>if plot_features[0] == plot_features[1]:\n        <IND>mssg = \"plot_features indexes must be 2 different values in range 0 to\"\n        raise ValueError(mssg + f\" {max_idx}.\")\n\n    <DED>labels = db_cluster.labels_\n    core_samples_mask = np.zeros_like(labels, dtype=bool)\n\n    # pylint: disable=unsupported-assignment-operation\n    # (assignment of numpy array is valid)\n    core_samples_mask[db_cluster.core_sample_indices_] = True\n    unique_labels = set(labels)\n\n    # pylint: disable=no-member\n    # Spectral color map does exist\n    colors = [cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n    # Number of clusters in labels, ignoring noise if present.\n    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n    n_noise_ = list(labels).count(-1)\n    _, counts = np.unique(labels, return_counts=True)\n\n    if verbose:\n        <IND>print(\"Estimated number of clusters: %d\" % n_clusters_)\n        print(\"Estimated number of noise points: %d\" % n_noise_)\n        # print(\"Silhouette Coefficient: %0.3f\"\n        #       % metrics.silhouette_score(x_predict, labels))\n\n    <DED>if not isinstance(data, pd.DataFrame):\n        <IND>plot_label = None\n    <DED>elif plot_label is not None and plot_label not in data:\n        <IND>plot_label = None\n\n    <DED>p_label = None\n    for cluster_id, color in zip(unique_labels, colors):\n        <IND>if cluster_id == -1:\n            # Black used for noise.\n            <IND>color = [0, 0, 0, 1]\n        <DED>class_member_mask = labels == cluster_id\n\n        cluster_size = counts[cluster_id]\n        marker_size = cluster_size\n        marker = \"o\"\n        font_size = \"small\"\n        alpha = 0.4\n\n        if cluster_size < cut_off:\n            <IND>marker = \"+\"\n            marker_size = 10\n            font_size = \"large\"\n            alpha = 1.0\n        <DED>xy_pos = x_predict[class_member_mask & core_samples_mask]\n        plt.plot(\n            xy_pos[:, plot_features[0]],\n            xy_pos[:, plot_features[1]],\n            marker,\n            markerfacecolor=tuple(color),\n            markersize=marker_size,\n        )\n\n        if plot_label:\n            <IND>first_row = data[class_member_mask].iloc[0]\n            if not first_row.empty and plot_label in first_row:\n                <IND>p_label = first_row[plot_label]\n                try:\n                    <IND>plt.annotate(\n                        p_label,\n                        xy=(xy_pos[0, plot_features[0]], xy_pos[0, plot_features[1]]),\n                        fontsize=font_size,\n                        alpha=alpha,\n                    )\n                <DED>except IndexError:\n                    <IND>pass\n\n    <DED><DED><DED><DED>plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n    plt.show()\n    return plt\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n# flake8: noqa: F403, F401\n# pylint: disable=wildcard-import, unused-wildcard-import, unused-import\nfrom ..analysis.eventcluster import *\n\nWARN_MSSG = (\n    \"This module has moved to msticpy.analysis.eventcluster\\n\"\n    + \"Please change your import to reflect this new location.\"\n)\nwarnings.warn(WARN_MSSG, category=DeprecationWarning)\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "microsoft/msticpy",
    "commit": "28466d681e261394e18d9f4e063cebfa06ed04b4",
    "filename": "msticpy/sectools/eventcluster.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/microsoft-msticpy/msticpy/sectools/eventcluster.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "msticpy/sectools/eventcluster.py:617:4 Incompatible variable type [9]: ylabel is declared to have type `str` but is used as type `None`.",
    "message": " ylabel is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 617,
    "warning_line": "    ylabel: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "# --------------------------------------------------------------------------\nr\"\"\"\neventcluster module.\n\nThis module is intended to be used to summarize large numbers of events\ninto clusters of different patterns. High volume repeating events can\noften make it difficult to see unique and interesting items.\n\nThe module contains functions to generate clusterable features from\nstring data. For example, an administration command that does some\nmaintenance on thousands of servers with a commandline such as:\n``install-update -hostname {host.fqdn} -tmp:/tmp/{GUID}/rollback``\\  can\nbe collapsed into a single cluster pattern by ignoring the character\nvalues in the string and using delimiters or tokens to group the values.\n\nThis is an unsupervised learning module implemented using SciKit Learn\nDBScan.\n\nContains:\ndbcluster_events: generic clustering method using DBSCAN designed to summarize\nprocess events and other similar data by grouping on common features.\n\nadd_process_features: derives numerical features from text features such as\ncommandline and process path.\n\n\"\"\"\nfrom binascii import crc32\nfrom functools import lru_cache\nfrom math import log10, floor\nimport re\nfrom typing import List, Any, Tuple, Union\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import Normalizer\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nfrom ..common.utility import export\nfrom .._version import VERSION\n",
        "source_code_len": 1462,
        "target_code": "# --------------------------------------------------------------------------\n\"\"\"Deprecated placeholder for eventcluster.py.\"\"\"\nimport warnings\nfrom .._version import VERSION\n",
        "target_code_len": 174,
        "diff_format": "@@ -5,41 +5,4 @@\n # --------------------------------------------------------------------------\n-r\"\"\"\n-eventcluster module.\n-\n-This module is intended to be used to summarize large numbers of events\n-into clusters of different patterns. High volume repeating events can\n-often make it difficult to see unique and interesting items.\n-\n-The module contains functions to generate clusterable features from\n-string data. For example, an administration command that does some\n-maintenance on thousands of servers with a commandline such as:\n-``install-update -hostname {host.fqdn} -tmp:/tmp/{GUID}/rollback``\\  can\n-be collapsed into a single cluster pattern by ignoring the character\n-values in the string and using delimiters or tokens to group the values.\n-\n-This is an unsupervised learning module implemented using SciKit Learn\n-DBScan.\n-\n-Contains:\n-dbcluster_events: generic clustering method using DBSCAN designed to summarize\n-process events and other similar data by grouping on common features.\n-\n-add_process_features: derives numerical features from text features such as\n-commandline and process path.\n-\n-\"\"\"\n-from binascii import crc32\n-from functools import lru_cache\n-from math import log10, floor\n-import re\n-from typing import List, Any, Tuple, Union\n-\n-import numpy as np\n-import pandas as pd\n-from sklearn.cluster import DBSCAN\n-from sklearn.preprocessing import Normalizer\n-import matplotlib.pyplot as plt\n-from matplotlib import cm\n-\n-from ..common.utility import export\n+\"\"\"Deprecated placeholder for eventcluster.py.\"\"\"\n+import warnings\n from .._version import VERSION\n",
        "source_code_with_indent": "# --------------------------------------------------------------------------\nr\"\"\"\neventcluster module.\n\nThis module is intended to be used to summarize large numbers of events\ninto clusters of different patterns. High volume repeating events can\noften make it difficult to see unique and interesting items.\n\nThe module contains functions to generate clusterable features from\nstring data. For example, an administration command that does some\nmaintenance on thousands of servers with a commandline such as:\n``install-update -hostname {host.fqdn} -tmp:/tmp/{GUID}/rollback``\\  can\nbe collapsed into a single cluster pattern by ignoring the character\nvalues in the string and using delimiters or tokens to group the values.\n\nThis is an unsupervised learning module implemented using SciKit Learn\nDBScan.\n\nContains:\ndbcluster_events: generic clustering method using DBSCAN designed to summarize\nprocess events and other similar data by grouping on common features.\n\nadd_process_features: derives numerical features from text features such as\ncommandline and process path.\n\n\"\"\"\nfrom binascii import crc32\nfrom functools import lru_cache\nfrom math import log10, floor\nimport re\nfrom typing import List, Any, Tuple, Union\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import Normalizer\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nfrom ..common.utility import export\nfrom .._version import VERSION\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "# --------------------------------------------------------------------------\n\"\"\"Deprecated placeholder for eventcluster.py.\"\"\"\nimport warnings\nfrom .._version import VERSION\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n# pylint: disable=too-many-arguments, too-many-locals\n@export\ndef dbcluster_events(\n    data: Any,\n    cluster_columns: List[Any] = None,\n    verbose: bool = False,\n    normalize: bool = True,\n    time_column: str = \"TimeCreatedUtc\",\n    max_cluster_distance: float = 0.01,\n    min_cluster_samples: int = 2,\n    **kwargs,\n) -> Tuple[pd.DataFrame, DBSCAN, np.ndarray]:\n    \"\"\"\n    Cluster data set according to cluster_columns features.\n\n    Parameters\n    ----------\n    data : Any\n        Input data as a pandas DataFrame or numpy array\n    cluster_columns : List[Any], optional\n        List of columns to use for features\n        - for DataFrame this is a list of column names\n        - for numpy array this is a list of column indexes\n    verbose : bool, optional\n        Print additional information about clustering results (the default is False)\n    normalize : bool, optional\n        Normalize the input data (should probably always be True)\n    time_column : str, optional\n        If there is a time column the output data will be ordered by this\n        (the default is 'TimeCreatedUtc')\n    max_cluster_distance : float, optional\n        DBSCAN eps (max cluster member distance) (the default is 0.01)\n    min_cluster_samples : int, optional\n        DBSCAN min_samples (the minimum cluster size) (the default is 2)\n\n    Other Parameters\n    ----------------\n    kwargs: Other arguments are passed to DBSCAN constructor\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, DBSCAN, np.ndarray]\n        Output dataframe with clustered rows\n        DBSCAN model\n        Normalized data set\n\n    \"\"\"\n    allowed_types = [np.ndarray, pd.DataFrame]\n\n    x_input = None\n    if isinstance(data, pd.DataFrame):\n        if cluster_columns is None:\n            x_input = data.values\n        else:\n            x_input = data[cluster_columns].values\n    elif isinstance(data, np.ndarray):\n        x_input = data if cluster_columns is None else data[:, cluster_columns].values\n    if x_input is None:\n        mssg = \"Input data not in expected format.\\n{} is not one of allowed types {}\"\n        type_list = \", \".join([str(t) for t in allowed_types])\n        mssg = mssg.format(str(type(data)), type_list)\n        raise ValueError(mssg)\n\n    # Create DBSCAN cluster object\n    db_cluster = DBSCAN(\n        eps=max_cluster_distance, min_samples=min_cluster_samples, **kwargs\n    )\n\n    # Normalize the data (most clustering algorithms don't do well with\n    # unnormalized data)\n    x_norm = Normalizer().fit_transform(x_input) if normalize else x_input\n    # fit the data set\n    db_cluster.fit(x_norm)\n    labels = db_cluster.labels_\n    cluster_set, counts = np.unique(labels, return_counts=True)\n    if verbose:\n        print(\n            \"Clustering for set size \",\n            len(x_norm),\n            \" - \",\n            len(cluster_set),\n            \" clusters\",\n        )\n        print(\"Individual cluster sizes: \", \", \".join([str(c) for c in counts]))\n\n    clustered_events = _merge_clustered_items(\n        cluster_set, labels, data, time_column, counts\n    )\n\n    if verbose:\n        print(\"Cluster output rows: \", len(clustered_events))\n\n    return clustered_events, db_cluster, x_norm\n\n\ndef _merge_clustered_items(\n    cluster_set: np.array,\n    labels: np.array,\n    data: Union[pd.DataFrame, np.array],\n    time_column: str,\n    counts: np.array,\n) -> pd.DataFrame:\n    \"\"\"\n    Merge outliers and core clusters into single DataFrame.\n\n    Parameters\n    ----------\n    cluster_set : np.array\n        The set of clusters\n    labels : np.array\n        The cluster labels\n    data : Union[pd.DataFrame, np.array]\n        The source data\n    time_column : str\n        Name of the Time column\n    counts : np.array\n        The counts of members in each cluster\n\n    Returns\n    -------\n    pd.DataFrame\n        Merged dataframe\n\n    \"\"\"\n    tz_aware = data.iloc[0][time_column].tz\n    ts_type = \"datetime64[ns, UTC]\" if tz_aware is not None else \"datetime64[ns]\"\n\n    cluster_list = []\n    # Iterate through clusters, adding exemplar to output frame\n    # pylint: disable=consider-using-enumerate\n    # we need to know the index of the item within the loop\n    for idx in range(len(cluster_set)):\n        cluster_id = cluster_set[idx]\n        class_members = labels == cluster_id\n        if isinstance(data, pd.DataFrame):\n            time_ordered = data[class_members].sort_values(time_column, ascending=True)\n            first_event_time = time_ordered[0:][time_column].iat[0]\n            last_event_time = time_ordered[-1:][time_column].iat[0]\n        else:\n            first_event_time = None\n            last_event_time = None\n\n        if cluster_id == -1:\n            # 'Noise' events are individual items that could not be assigned\n            # to a cluster and so are unique\n            cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=False,\n                    ClusterId=cluster_id,\n                    ClusterSize=1,\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n        else:\n            # Otherwise, just choose the first example of the cluster set\n            cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=True,\n                    ClusterId=cluster_id,\n                    ClusterSize=counts[idx],\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )[0:1]\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n    # pylint: enable=consider-using-enumerate\n    return pd.concat(cluster_list)\n\n\n@export\ndef add_process_features(\n    input_frame: pd.DataFrame, path_separator: str = None, force: bool = False\n) -> pd.DataFrame:\n    r\"\"\"\n    Add numerical features based on patterns of command line and process name.\n\n    Parameters\n    ----------\n    input_frame : pd.DataFrame\n        The input dataframe\n    path_separator : str, optional\n        Path separator. If not supplied, try to determine\n        from 'NewProcessName' column of first 10 rows\n        (the default is None)\n    force : bool, optional\n        Forces re-calculation of feature columns even if they\n        already exist (the default is False)\n\n    Returns\n    -------\n    pd.DataFrame\n        Copy of the dataframe with the additional numeric features\n\n    Notes\n    -----\n    Features added:\n\n    - processNameLen: length of process file name (inc path)\n    - processNameTokens: the number of elements in the path\n    - processName: the process file name (minus path)\n    - commandlineTokens: number of space-separated tokens in the command line\n    - commandlineLen: length of the command line\n    - commandlineLogLen: log10 length of commandline\n    - isSystemSession: 1 if session Id is 0x3e7 for Windows or -1 for Linux\n    - commandlineTokensFull: counts number of token separators in commandline\n      [\\\\s\\-\\\\/\\.,\"\\'\\|&:;%$()]\n    - pathScore: sum of ord() value of characters in path\n    - pathLogScore: log10 of pathScore\n    - commandlineScore: sum of ord() value of characters in commandline\n    - commandlineLogScore: log10 of commandlineScore\n\n    \"\"\"\n    output_df = input_frame.copy()\n\n    # Set any NaN values to empty string\n    if \"NewProcessName\" in output_df and \"CommandLine\" in output_df:\n        output_df[[\"NewProcessName\", \"CommandLine\"]] = output_df[\n            [\"NewProcessName\", \"CommandLine\"]\n        ].fillna(value=\"\")\n\n    # try to determine the path separator\n    if path_separator is None:\n        sample_df = output_df.head(10)\n        lx_path = len(sample_df[sample_df[\"NewProcessName\"].str.contains(\"/\")])\n        path_separator = \"/\" if lx_path else \"\\\\\"\n    # Create features from process name and command line\n    if \"NewProcessName\" in output_df:\n        _add_processname_features(output_df, force, path_separator)\n\n    if \"CommandLine\" in output_df:\n        _add_commandline_features(output_df, force)\n\n    if \"SubjectLogonId\" in output_df and (\"isSystemSession\" not in output_df or force):\n        output_df[\"isSystemSession\"] = output_df[\"SubjectLogonId\"].isin([\"0x3e7\", \"-1\"])\n\n    return output_df\n\n\ndef _add_processname_features(\n    output_df: pd.DataFrame, force: bool, path_separator: str\n):\n    \"\"\"\n    Add process name default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n    path_separator : str\n        Path separator for OS\n\n    \"\"\"\n    if \"processName\" not in output_df or force:\n        output_df[\"processName\"] = output_df.apply(\n            lambda x: x.NewProcessName.split(path_separator)[-1], axis=1\n        )\n    if \"pathScore\" not in output_df or force:\n        output_df[\"pathScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.NewProcessName), axis=1\n        )\n    if \"pathLogScore\" not in output_df or force:\n        output_df[\"pathLogScore\"] = output_df.apply(\n            lambda x: log10(x.pathScore) if x.pathScore else 0, axis=1\n        )\n    if \"pathHash\" not in output_df or force:\n        output_df[\"pathHash\"] = output_df.apply(\n            lambda x: crc32_hash(x.NewProcessName), axis=1\n        )\n\n\ndef _add_commandline_features(output_df: pd.DataFrame, force: bool):\n    \"\"\"\n    Add commandline default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n\n    \"\"\"\n    if \"commandlineLen\" not in output_df or force:\n        output_df[\"commandlineLen\"] = output_df.apply(\n            lambda x: len(x.CommandLine), axis=1\n        )\n    if \"commandlineLogLen\" not in output_df or force:\n        output_df[\"commandlineLogLen\"] = output_df.apply(\n            lambda x: log10(x.commandlineLen) if x.commandlineLen else 0, axis=1\n        )\n    if \"commandlineTokensFull\" not in output_df or force:\n        output_df[\"commandlineTokensFull\"] = output_df[[\"CommandLine\"]].apply(\n            lambda x: delim_count(x.CommandLine), axis=1\n        )\n\n    if \"commandlineScore\" not in output_df or force:\n        output_df[\"commandlineScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.CommandLine), axis=1\n        )\n    if \"commandlineTokensHash\" not in output_df or force:\n        output_df[\"commandlineTokensHash\"] = output_df.apply(\n            lambda x: delim_hash(x.CommandLine), axis=1\n        )\n\n\n@export\n@lru_cache(maxsize=1024)\ndef delim_count(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Count of delimiters in the string.\n\n    \"\"\"\n    return len(re.findall(delim_list, value))\n\n\n@export\n@lru_cache(maxsize=1024)\ndef delim_hash(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    r\"\"\"\n    Return a hash (CRC32) of the delimiters from input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Hash of delimiter set in the string.\n\n    \"\"\"\n    return crc32(bytes(\"\".join(re.findall(delim_list, value)), \"utf-8\"))\n\n\n@export\n@lru_cache(maxsize=1024)\ndef char_ord_score(value: str, scale: int = 1) -> int:\n    \"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    int\n        [description]\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return floor(sum(ord(x) for x in value) / scale)\n\n\n@export\n@lru_cache(maxsize=1024)\ndef token_count(value: str, delimiter: str = \" \") -> int:\n    \"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    int\n        count of tokens\n\n    \"\"\"\n    return len(value.split(delimiter))\n\n\ndef _string_score(input_str):\n    \"\"\"Sum the ord(c) for characters in a string.\"\"\"\n    return sum(ord(x) for x in input_str)\n\n\n@export\n@lru_cache(maxsize=1024)\ndef crc32_hash(value: str) -> int:\n    \"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n\n    Returns\n    -------\n    int\n        CRC32 hash\n\n    \"\"\"\n    return crc32(bytes(value.encode(\"utf-8\")))\n\n\ndef delim_count_df(\n    data: pd.DataFrame, column: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]'\n) -> pd.Series:\n    r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        The name of the column to process\n    delim_list : str, optional\n        delimiters to use. (the default is r\\'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]\\')\n\n    Returns\n    -------\n    pd.Series\n        Count of delimiters in the string in `column`.\n\n    \"\"\"\n    return data[column].str.count(delim_list)\n\n\ndef char_ord_score_df(data: pd.DataFrame, column: str, scale: int = 1) -> pd.Series:\n    \"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    pd.Series\n        The sum of the ordinal values of the characters\n        in `column`.\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return data.apply(lambda x: sum(ord(char) for char in x[column]) / scale, axis=1)\n\n\ndef token_count_df(data: pd.DataFrame, column: str, delimiter: str = \" \") -> pd.Series:\n    \"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    pd.Series\n        count of tokens in strings in `column`\n\n    \"\"\"\n    return data.apply(lambda x: len(x[column].split(delimiter)), axis=1)\n\n\ndef crc32_hash_df(data: pd.DataFrame, column: str) -> pd.Series:\n    \"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n\n    Returns\n    -------\n    pd.Series\n        CRC32 hash of input column\n\n    \"\"\"\n    return data.apply(lambda x: crc32(bytes(x[column].encode(\"utf-8\"))), axis=1)\n\n\n# pylint: disable=too-many-arguments, too-many-statements\n@export  # noqa: C901, MC0001\ndef plot_cluster(\n    db_cluster: DBSCAN,\n    data: pd.DataFrame,\n    x_predict: np.ndarray,\n    plot_label: str = None,\n    plot_features: Tuple[int, int] = (0, 1),\n    verbose: bool = False,\n    cut_off: int = 3,\n    xlabel: str = None,\n    ylabel: str = None,\n):\n    \"\"\"\n    Plot clustered data as scatter chart.\n\n    Parameters\n    ----------\n    db_cluster : DBSCAN\n        DBScan Cluster (from SkLearn DBSCAN).\n    data : pd.DataFrame\n        Dataframe containing original data.\n    x_predict : np.ndarray\n        The DBSCAN predict numpy array\n    plot_label : str, optional\n         If set the column to use to label data points\n         (the default is None)\n    plot_features :  Tuple[int, int], optional\n        Which two features in x_predict to plot (the default is (0, 1))\n    verbose : bool, optional\n        Verbose execution with some extra info\n        (the default is False)\n    cut_off : int, optional\n        The cluster size below which items are considered outliers\n        (the default is 3)\n    xlabel : str, optional\n        x-axis label (the default is None)\n    ylabel : str, optional\n        y-axis label (the default is None)\n\n    \"\"\"\n    max_idx = x_predict.shape[1] - 1\n    if plot_features[0] >= x_predict.shape[1]:\n        raise ValueError(\n            \"plot_features[0] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    if plot_features[1] >= x_predict.shape[1]:\n        raise ValueError(\n            \"plot_features[1] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    if plot_features[0] == plot_features[1]:\n        mssg = \"plot_features indexes must be 2 different values in range 0 to\"\n        raise ValueError(mssg + f\" {max_idx}.\")\n\n    labels = db_cluster.labels_\n    core_samples_mask = np.zeros_like(labels, dtype=bool)\n\n    # pylint: disable=unsupported-assignment-operation\n    # (assignment of numpy array is valid)\n    core_samples_mask[db_cluster.core_sample_indices_] = True\n    unique_labels = set(labels)\n\n    # pylint: disable=no-member\n    # Spectral color map does exist\n    colors = [cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n    # Number of clusters in labels, ignoring noise if present.\n    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n    n_noise_ = list(labels).count(-1)\n    _, counts = np.unique(labels, return_counts=True)\n\n    if verbose:\n        print(\"Estimated number of clusters: %d\" % n_clusters_)\n        print(\"Estimated number of noise points: %d\" % n_noise_)\n        # print(\"Silhouette Coefficient: %0.3f\"\n        #       % metrics.silhouette_score(x_predict, labels))\n\n    if not isinstance(data, pd.DataFrame):\n        plot_label = None\n    elif plot_label is not None and plot_label not in data:\n        plot_label = None\n\n    p_label = None\n    for cluster_id, color in zip(unique_labels, colors):\n        if cluster_id == -1:\n            # Black used for noise.\n            color = [0, 0, 0, 1]\n        class_member_mask = labels == cluster_id\n\n        cluster_size = counts[cluster_id]\n        marker_size = cluster_size\n        marker = \"o\"\n        font_size = \"small\"\n        alpha = 0.4\n\n        if cluster_size < cut_off:\n            marker = \"+\"\n            marker_size = 10\n            font_size = \"large\"\n            alpha = 1.0\n        xy_pos = x_predict[class_member_mask & core_samples_mask]\n        plt.plot(\n            xy_pos[:, plot_features[0]],\n            xy_pos[:, plot_features[1]],\n            marker,\n            markerfacecolor=tuple(color),\n            markersize=marker_size,\n        )\n\n        if plot_label:\n            first_row = data[class_member_mask].iloc[0]\n            if not first_row.empty and plot_label in first_row:\n                p_label = first_row[plot_label]\n                try:\n                    plt.annotate(\n                        p_label,\n                        xy=(xy_pos[0, plot_features[0]], xy_pos[0, plot_features[1]]),\n                        fontsize=font_size,\n                        alpha=alpha,\n                    )\n                except IndexError:\n                    pass\n\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n    plt.show()\n    return plt\n",
        "source_code_len": 21140,
        "target_code": "\n# flake8: noqa: F403, F401\n# pylint: disable=wildcard-import, unused-wildcard-import, unused-import\nfrom ..analysis.eventcluster import *\n\nWARN_MSSG = (\n    \"This module has moved to msticpy.analysis.eventcluster\\n\"\n    + \"Please change your import to reflect this new location.\"\n)\nwarnings.warn(WARN_MSSG, category=DeprecationWarning)\n",
        "target_code_len": 337,
        "diff_format": "@@ -50,683 +13,10 @@\n \n-# pylint: disable=too-many-arguments, too-many-locals\n-@export\n-def dbcluster_events(\n-    data: Any,\n-    cluster_columns: List[Any] = None,\n-    verbose: bool = False,\n-    normalize: bool = True,\n-    time_column: str = \"TimeCreatedUtc\",\n-    max_cluster_distance: float = 0.01,\n-    min_cluster_samples: int = 2,\n-    **kwargs,\n-) -> Tuple[pd.DataFrame, DBSCAN, np.ndarray]:\n-    \"\"\"\n-    Cluster data set according to cluster_columns features.\n+# flake8: noqa: F403, F401\n+# pylint: disable=wildcard-import, unused-wildcard-import, unused-import\n+from ..analysis.eventcluster import *\n \n-    Parameters\n-    ----------\n-    data : Any\n-        Input data as a pandas DataFrame or numpy array\n-    cluster_columns : List[Any], optional\n-        List of columns to use for features\n-        - for DataFrame this is a list of column names\n-        - for numpy array this is a list of column indexes\n-    verbose : bool, optional\n-        Print additional information about clustering results (the default is False)\n-    normalize : bool, optional\n-        Normalize the input data (should probably always be True)\n-    time_column : str, optional\n-        If there is a time column the output data will be ordered by this\n-        (the default is 'TimeCreatedUtc')\n-    max_cluster_distance : float, optional\n-        DBSCAN eps (max cluster member distance) (the default is 0.01)\n-    min_cluster_samples : int, optional\n-        DBSCAN min_samples (the minimum cluster size) (the default is 2)\n-\n-    Other Parameters\n-    ----------------\n-    kwargs: Other arguments are passed to DBSCAN constructor\n-\n-    Returns\n-    -------\n-    Tuple[pd.DataFrame, DBSCAN, np.ndarray]\n-        Output dataframe with clustered rows\n-        DBSCAN model\n-        Normalized data set\n-\n-    \"\"\"\n-    allowed_types = [np.ndarray, pd.DataFrame]\n-\n-    x_input = None\n-    if isinstance(data, pd.DataFrame):\n-        if cluster_columns is None:\n-            x_input = data.values\n-        else:\n-            x_input = data[cluster_columns].values\n-    elif isinstance(data, np.ndarray):\n-        x_input = data if cluster_columns is None else data[:, cluster_columns].values\n-    if x_input is None:\n-        mssg = \"Input data not in expected format.\\n{} is not one of allowed types {}\"\n-        type_list = \", \".join([str(t) for t in allowed_types])\n-        mssg = mssg.format(str(type(data)), type_list)\n-        raise ValueError(mssg)\n-\n-    # Create DBSCAN cluster object\n-    db_cluster = DBSCAN(\n-        eps=max_cluster_distance, min_samples=min_cluster_samples, **kwargs\n-    )\n-\n-    # Normalize the data (most clustering algorithms don't do well with\n-    # unnormalized data)\n-    x_norm = Normalizer().fit_transform(x_input) if normalize else x_input\n-    # fit the data set\n-    db_cluster.fit(x_norm)\n-    labels = db_cluster.labels_\n-    cluster_set, counts = np.unique(labels, return_counts=True)\n-    if verbose:\n-        print(\n-            \"Clustering for set size \",\n-            len(x_norm),\n-            \" - \",\n-            len(cluster_set),\n-            \" clusters\",\n-        )\n-        print(\"Individual cluster sizes: \", \", \".join([str(c) for c in counts]))\n-\n-    clustered_events = _merge_clustered_items(\n-        cluster_set, labels, data, time_column, counts\n-    )\n-\n-    if verbose:\n-        print(\"Cluster output rows: \", len(clustered_events))\n-\n-    return clustered_events, db_cluster, x_norm\n-\n-\n-def _merge_clustered_items(\n-    cluster_set: np.array,\n-    labels: np.array,\n-    data: Union[pd.DataFrame, np.array],\n-    time_column: str,\n-    counts: np.array,\n-) -> pd.DataFrame:\n-    \"\"\"\n-    Merge outliers and core clusters into single DataFrame.\n-\n-    Parameters\n-    ----------\n-    cluster_set : np.array\n-        The set of clusters\n-    labels : np.array\n-        The cluster labels\n-    data : Union[pd.DataFrame, np.array]\n-        The source data\n-    time_column : str\n-        Name of the Time column\n-    counts : np.array\n-        The counts of members in each cluster\n-\n-    Returns\n-    -------\n-    pd.DataFrame\n-        Merged dataframe\n-\n-    \"\"\"\n-    tz_aware = data.iloc[0][time_column].tz\n-    ts_type = \"datetime64[ns, UTC]\" if tz_aware is not None else \"datetime64[ns]\"\n-\n-    cluster_list = []\n-    # Iterate through clusters, adding exemplar to output frame\n-    # pylint: disable=consider-using-enumerate\n-    # we need to know the index of the item within the loop\n-    for idx in range(len(cluster_set)):\n-        cluster_id = cluster_set[idx]\n-        class_members = labels == cluster_id\n-        if isinstance(data, pd.DataFrame):\n-            time_ordered = data[class_members].sort_values(time_column, ascending=True)\n-            first_event_time = time_ordered[0:][time_column].iat[0]\n-            last_event_time = time_ordered[-1:][time_column].iat[0]\n-        else:\n-            first_event_time = None\n-            last_event_time = None\n-\n-        if cluster_id == -1:\n-            # 'Noise' events are individual items that could not be assigned\n-            # to a cluster and so are unique\n-            cluster_list.append(\n-                data[class_members]\n-                .assign(\n-                    Clustered=False,\n-                    ClusterId=cluster_id,\n-                    ClusterSize=1,\n-                    TimeGenerated=first_event_time,\n-                    FirstEventTime=first_event_time,\n-                    LastEventTime=last_event_time,\n-                )\n-                .astype(\n-                    dtype={\n-                        \"TimeGenerated\": ts_type,\n-                        \"FirstEventTime\": ts_type,\n-                        \"LastEventTime\": ts_type,\n-                    }\n-                )\n-            )\n-        else:\n-            # Otherwise, just choose the first example of the cluster set\n-            cluster_list.append(\n-                data[class_members]\n-                .assign(\n-                    Clustered=True,\n-                    ClusterId=cluster_id,\n-                    ClusterSize=counts[idx],\n-                    TimeGenerated=first_event_time,\n-                    FirstEventTime=first_event_time,\n-                    LastEventTime=last_event_time,\n-                )[0:1]\n-                .astype(\n-                    dtype={\n-                        \"TimeGenerated\": ts_type,\n-                        \"FirstEventTime\": ts_type,\n-                        \"LastEventTime\": ts_type,\n-                    }\n-                )\n-            )\n-    # pylint: enable=consider-using-enumerate\n-    return pd.concat(cluster_list)\n-\n-\n-@export\n-def add_process_features(\n-    input_frame: pd.DataFrame, path_separator: str = None, force: bool = False\n-) -> pd.DataFrame:\n-    r\"\"\"\n-    Add numerical features based on patterns of command line and process name.\n-\n-    Parameters\n-    ----------\n-    input_frame : pd.DataFrame\n-        The input dataframe\n-    path_separator : str, optional\n-        Path separator. If not supplied, try to determine\n-        from 'NewProcessName' column of first 10 rows\n-        (the default is None)\n-    force : bool, optional\n-        Forces re-calculation of feature columns even if they\n-        already exist (the default is False)\n-\n-    Returns\n-    -------\n-    pd.DataFrame\n-        Copy of the dataframe with the additional numeric features\n-\n-    Notes\n-    -----\n-    Features added:\n-\n-    - processNameLen: length of process file name (inc path)\n-    - processNameTokens: the number of elements in the path\n-    - processName: the process file name (minus path)\n-    - commandlineTokens: number of space-separated tokens in the command line\n-    - commandlineLen: length of the command line\n-    - commandlineLogLen: log10 length of commandline\n-    - isSystemSession: 1 if session Id is 0x3e7 for Windows or -1 for Linux\n-    - commandlineTokensFull: counts number of token separators in commandline\n-      [\\\\s\\-\\\\/\\.,\"\\'\\|&:;%$()]\n-    - pathScore: sum of ord() value of characters in path\n-    - pathLogScore: log10 of pathScore\n-    - commandlineScore: sum of ord() value of characters in commandline\n-    - commandlineLogScore: log10 of commandlineScore\n-\n-    \"\"\"\n-    output_df = input_frame.copy()\n-\n-    # Set any NaN values to empty string\n-    if \"NewProcessName\" in output_df and \"CommandLine\" in output_df:\n-        output_df[[\"NewProcessName\", \"CommandLine\"]] = output_df[\n-            [\"NewProcessName\", \"CommandLine\"]\n-        ].fillna(value=\"\")\n-\n-    # try to determine the path separator\n-    if path_separator is None:\n-        sample_df = output_df.head(10)\n-        lx_path = len(sample_df[sample_df[\"NewProcessName\"].str.contains(\"/\")])\n-        path_separator = \"/\" if lx_path else \"\\\\\"\n-    # Create features from process name and command line\n-    if \"NewProcessName\" in output_df:\n-        _add_processname_features(output_df, force, path_separator)\n-\n-    if \"CommandLine\" in output_df:\n-        _add_commandline_features(output_df, force)\n-\n-    if \"SubjectLogonId\" in output_df and (\"isSystemSession\" not in output_df or force):\n-        output_df[\"isSystemSession\"] = output_df[\"SubjectLogonId\"].isin([\"0x3e7\", \"-1\"])\n-\n-    return output_df\n-\n-\n-def _add_processname_features(\n-    output_df: pd.DataFrame, force: bool, path_separator: str\n-):\n-    \"\"\"\n-    Add process name default features.\n-\n-    Parameters\n-    ----------\n-    output_df : pd.DataFrame\n-        The dataframe to add features to\n-    force : bool\n-        If True overwrite existing feature columns\n-    path_separator : str\n-        Path separator for OS\n-\n-    \"\"\"\n-    if \"processName\" not in output_df or force:\n-        output_df[\"processName\"] = output_df.apply(\n-            lambda x: x.NewProcessName.split(path_separator)[-1], axis=1\n-        )\n-    if \"pathScore\" not in output_df or force:\n-        output_df[\"pathScore\"] = output_df.apply(\n-            lambda x: char_ord_score(x.NewProcessName), axis=1\n-        )\n-    if \"pathLogScore\" not in output_df or force:\n-        output_df[\"pathLogScore\"] = output_df.apply(\n-            lambda x: log10(x.pathScore) if x.pathScore else 0, axis=1\n-        )\n-    if \"pathHash\" not in output_df or force:\n-        output_df[\"pathHash\"] = output_df.apply(\n-            lambda x: crc32_hash(x.NewProcessName), axis=1\n-        )\n-\n-\n-def _add_commandline_features(output_df: pd.DataFrame, force: bool):\n-    \"\"\"\n-    Add commandline default features.\n-\n-    Parameters\n-    ----------\n-    output_df : pd.DataFrame\n-        The dataframe to add features to\n-    force : bool\n-        If True overwrite existing feature columns\n-\n-    \"\"\"\n-    if \"commandlineLen\" not in output_df or force:\n-        output_df[\"commandlineLen\"] = output_df.apply(\n-            lambda x: len(x.CommandLine), axis=1\n-        )\n-    if \"commandlineLogLen\" not in output_df or force:\n-        output_df[\"commandlineLogLen\"] = output_df.apply(\n-            lambda x: log10(x.commandlineLen) if x.commandlineLen else 0, axis=1\n-        )\n-    if \"commandlineTokensFull\" not in output_df or force:\n-        output_df[\"commandlineTokensFull\"] = output_df[[\"CommandLine\"]].apply(\n-            lambda x: delim_count(x.CommandLine), axis=1\n-        )\n-\n-    if \"commandlineScore\" not in output_df or force:\n-        output_df[\"commandlineScore\"] = output_df.apply(\n-            lambda x: char_ord_score(x.CommandLine), axis=1\n-        )\n-    if \"commandlineTokensHash\" not in output_df or force:\n-        output_df[\"commandlineTokensHash\"] = output_df.apply(\n-            lambda x: delim_hash(x.CommandLine), axis=1\n-        )\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def delim_count(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n-    r\"\"\"\n-    Count the delimiters in input column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    delim_list : str, optional\n-        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n-\n-    Returns\n-    -------\n-    int\n-        Count of delimiters in the string.\n-\n-    \"\"\"\n-    return len(re.findall(delim_list, value))\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def delim_hash(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n-    r\"\"\"\n-    Return a hash (CRC32) of the delimiters from input column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    delim_list : str, optional\n-        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n-\n-    Returns\n-    -------\n-    int\n-        Hash of delimiter set in the string.\n-\n-    \"\"\"\n-    return crc32(bytes(\"\".join(re.findall(delim_list, value)), \"utf-8\"))\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def char_ord_score(value: str, scale: int = 1) -> int:\n-    \"\"\"\n-    Return sum of ord values of characters in string.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    scale : int, optional\n-        reduce the scale of the feature (reducing the\n-        influence of variations this feature on the clustering\n-        algorithm (the default is 1)\n-\n-    Returns\n-    -------\n-    int\n-        [description]\n-\n-    Notes\n-    -----\n-    This function sums the ordinal value of each character in the\n-    input string. Two strings with minor differences will result in\n-    a similar score. However, for strings with highly variable content\n-    (e.g. command lines or http requests containing GUIDs) this may result\n-    in too much variance to be useful when you are trying to detect\n-    similar patterns. You can use the scale parameter to reduce the\n-    influence of features using this function on clustering and anomaly\n-    algorithms.\n-\n-    \"\"\"\n-    return floor(sum(ord(x) for x in value) / scale)\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def token_count(value: str, delimiter: str = \" \") -> int:\n-    \"\"\"\n-    Return count of delimiter-separated tokens pd.Series column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    delimiter : str, optional\n-        Delimiter used to split the column string.\n-        (the default is ' ')\n-\n-    Returns\n-    -------\n-    int\n-        count of tokens\n-\n-    \"\"\"\n-    return len(value.split(delimiter))\n-\n-\n-def _string_score(input_str):\n-    \"\"\"Sum the ord(c) for characters in a string.\"\"\"\n-    return sum(ord(x) for x in input_str)\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def crc32_hash(value: str) -> int:\n-    \"\"\"\n-    Return the CRC32 hash of the input column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-\n-    Returns\n-    -------\n-    int\n-        CRC32 hash\n-\n-    \"\"\"\n-    return crc32(bytes(value.encode(\"utf-8\")))\n-\n-\n-def delim_count_df(\n-    data: pd.DataFrame, column: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]'\n-) -> pd.Series:\n-    r\"\"\"\n-    Count the delimiters in input column.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        The name of the column to process\n-    delim_list : str, optional\n-        delimiters to use. (the default is r\\'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]\\')\n-\n-    Returns\n-    -------\n-    pd.Series\n-        Count of delimiters in the string in `column`.\n-\n-    \"\"\"\n-    return data[column].str.count(delim_list)\n-\n-\n-def char_ord_score_df(data: pd.DataFrame, column: str, scale: int = 1) -> pd.Series:\n-    \"\"\"\n-    Return sum of ord values of characters in string.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        Column name to process\n-    scale : int, optional\n-        reduce the scale of the feature (reducing the\n-        influence of variations this feature on the clustering\n-        algorithm (the default is 1)\n-\n-    Returns\n-    -------\n-    pd.Series\n-        The sum of the ordinal values of the characters\n-        in `column`.\n-\n-    Notes\n-    -----\n-    This function sums the ordinal value of each character in the\n-    input string. Two strings with minor differences will result in\n-    a similar score. However, for strings with highly variable content\n-    (e.g. command lines or http requests containing GUIDs) this may result\n-    in too much variance to be useful when you are trying to detect\n-    similar patterns. You can use the scale parameter to reduce the\n-    influence of features using this function on clustering and anomaly\n-    algorithms.\n-\n-    \"\"\"\n-    return data.apply(lambda x: sum(ord(char) for char in x[column]) / scale, axis=1)\n-\n-\n-def token_count_df(data: pd.DataFrame, column: str, delimiter: str = \" \") -> pd.Series:\n-    \"\"\"\n-    Return count of delimiter-separated tokens pd.Series column.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        Column name to process\n-    delimiter : str, optional\n-        Delimiter used to split the column string.\n-        (the default is ' ')\n-\n-    Returns\n-    -------\n-    pd.Series\n-        count of tokens in strings in `column`\n-\n-    \"\"\"\n-    return data.apply(lambda x: len(x[column].split(delimiter)), axis=1)\n-\n-\n-def crc32_hash_df(data: pd.DataFrame, column: str) -> pd.Series:\n-    \"\"\"\n-    Return the CRC32 hash of the input column.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        Column name to process\n-\n-    Returns\n-    -------\n-    pd.Series\n-        CRC32 hash of input column\n-\n-    \"\"\"\n-    return data.apply(lambda x: crc32(bytes(x[column].encode(\"utf-8\"))), axis=1)\n-\n-\n-# pylint: disable=too-many-arguments, too-many-statements\n-@export  # noqa: C901, MC0001\n-def plot_cluster(\n-    db_cluster: DBSCAN,\n-    data: pd.DataFrame,\n-    x_predict: np.ndarray,\n-    plot_label: str = None,\n-    plot_features: Tuple[int, int] = (0, 1),\n-    verbose: bool = False,\n-    cut_off: int = 3,\n-    xlabel: str = None,\n-    ylabel: str = None,\n-):\n-    \"\"\"\n-    Plot clustered data as scatter chart.\n-\n-    Parameters\n-    ----------\n-    db_cluster : DBSCAN\n-        DBScan Cluster (from SkLearn DBSCAN).\n-    data : pd.DataFrame\n-        Dataframe containing original data.\n-    x_predict : np.ndarray\n-        The DBSCAN predict numpy array\n-    plot_label : str, optional\n-         If set the column to use to label data points\n-         (the default is None)\n-    plot_features :  Tuple[int, int], optional\n-        Which two features in x_predict to plot (the default is (0, 1))\n-    verbose : bool, optional\n-        Verbose execution with some extra info\n-        (the default is False)\n-    cut_off : int, optional\n-        The cluster size below which items are considered outliers\n-        (the default is 3)\n-    xlabel : str, optional\n-        x-axis label (the default is None)\n-    ylabel : str, optional\n-        y-axis label (the default is None)\n-\n-    \"\"\"\n-    max_idx = x_predict.shape[1] - 1\n-    if plot_features[0] >= x_predict.shape[1]:\n-        raise ValueError(\n-            \"plot_features[0] index must be a value from 0 to {}.\".format(max_idx)\n-        )\n-    if plot_features[1] >= x_predict.shape[1]:\n-        raise ValueError(\n-            \"plot_features[1] index must be a value from 0 to {}.\".format(max_idx)\n-        )\n-    if plot_features[0] == plot_features[1]:\n-        mssg = \"plot_features indexes must be 2 different values in range 0 to\"\n-        raise ValueError(mssg + f\" {max_idx}.\")\n-\n-    labels = db_cluster.labels_\n-    core_samples_mask = np.zeros_like(labels, dtype=bool)\n-\n-    # pylint: disable=unsupported-assignment-operation\n-    # (assignment of numpy array is valid)\n-    core_samples_mask[db_cluster.core_sample_indices_] = True\n-    unique_labels = set(labels)\n-\n-    # pylint: disable=no-member\n-    # Spectral color map does exist\n-    colors = [cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n-    # Number of clusters in labels, ignoring noise if present.\n-    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n-    n_noise_ = list(labels).count(-1)\n-    _, counts = np.unique(labels, return_counts=True)\n-\n-    if verbose:\n-        print(\"Estimated number of clusters: %d\" % n_clusters_)\n-        print(\"Estimated number of noise points: %d\" % n_noise_)\n-        # print(\"Silhouette Coefficient: %0.3f\"\n-        #       % metrics.silhouette_score(x_predict, labels))\n-\n-    if not isinstance(data, pd.DataFrame):\n-        plot_label = None\n-    elif plot_label is not None and plot_label not in data:\n-        plot_label = None\n-\n-    p_label = None\n-    for cluster_id, color in zip(unique_labels, colors):\n-        if cluster_id == -1:\n-            # Black used for noise.\n-            color = [0, 0, 0, 1]\n-        class_member_mask = labels == cluster_id\n-\n-        cluster_size = counts[cluster_id]\n-        marker_size = cluster_size\n-        marker = \"o\"\n-        font_size = \"small\"\n-        alpha = 0.4\n-\n-        if cluster_size < cut_off:\n-            marker = \"+\"\n-            marker_size = 10\n-            font_size = \"large\"\n-            alpha = 1.0\n-        xy_pos = x_predict[class_member_mask & core_samples_mask]\n-        plt.plot(\n-            xy_pos[:, plot_features[0]],\n-            xy_pos[:, plot_features[1]],\n-            marker,\n-            markerfacecolor=tuple(color),\n-            markersize=marker_size,\n-        )\n-\n-        if plot_label:\n-            first_row = data[class_member_mask].iloc[0]\n-            if not first_row.empty and plot_label in first_row:\n-                p_label = first_row[plot_label]\n-                try:\n-                    plt.annotate(\n-                        p_label,\n-                        xy=(xy_pos[0, plot_features[0]], xy_pos[0, plot_features[1]]),\n-                        fontsize=font_size,\n-                        alpha=alpha,\n-                    )\n-                except IndexError:\n-                    pass\n-\n-    plt.xlabel(xlabel)\n-    plt.ylabel(ylabel)\n-    plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n-    plt.show()\n-    return plt\n+WARN_MSSG = (\n+    \"This module has moved to msticpy.analysis.eventcluster\\n\"\n+    + \"Please change your import to reflect this new location.\"\n+)\n+warnings.warn(WARN_MSSG, category=DeprecationWarning)\n",
        "source_code_with_indent": "\n# pylint: disable=too-many-arguments, too-many-locals\n@export\ndef dbcluster_events(\n    data: Any,\n    cluster_columns: List[Any] = None,\n    verbose: bool = False,\n    normalize: bool = True,\n    time_column: str = \"TimeCreatedUtc\",\n    max_cluster_distance: float = 0.01,\n    min_cluster_samples: int = 2,\n    **kwargs,\n) -> Tuple[pd.DataFrame, DBSCAN, np.ndarray]:\n    <IND>\"\"\"\n    Cluster data set according to cluster_columns features.\n\n    Parameters\n    ----------\n    data : Any\n        Input data as a pandas DataFrame or numpy array\n    cluster_columns : List[Any], optional\n        List of columns to use for features\n        - for DataFrame this is a list of column names\n        - for numpy array this is a list of column indexes\n    verbose : bool, optional\n        Print additional information about clustering results (the default is False)\n    normalize : bool, optional\n        Normalize the input data (should probably always be True)\n    time_column : str, optional\n        If there is a time column the output data will be ordered by this\n        (the default is 'TimeCreatedUtc')\n    max_cluster_distance : float, optional\n        DBSCAN eps (max cluster member distance) (the default is 0.01)\n    min_cluster_samples : int, optional\n        DBSCAN min_samples (the minimum cluster size) (the default is 2)\n\n    Other Parameters\n    ----------------\n    kwargs: Other arguments are passed to DBSCAN constructor\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, DBSCAN, np.ndarray]\n        Output dataframe with clustered rows\n        DBSCAN model\n        Normalized data set\n\n    \"\"\"\n    allowed_types = [np.ndarray, pd.DataFrame]\n\n    x_input = None\n    if isinstance(data, pd.DataFrame):\n        <IND>if cluster_columns is None:\n            <IND>x_input = data.values\n        <DED>else:\n            <IND>x_input = data[cluster_columns].values\n    <DED><DED>elif isinstance(data, np.ndarray):\n        <IND>x_input = data if cluster_columns is None else data[:, cluster_columns].values\n    <DED>if x_input is None:\n        <IND>mssg = \"Input data not in expected format.\\n{} is not one of allowed types {}\"\n        type_list = \", \".join([str(t) for t in allowed_types])\n        mssg = mssg.format(str(type(data)), type_list)\n        raise ValueError(mssg)\n\n    # Create DBSCAN cluster object\n    <DED>db_cluster = DBSCAN(\n        eps=max_cluster_distance, min_samples=min_cluster_samples, **kwargs\n    )\n\n    # Normalize the data (most clustering algorithms don't do well with\n    # unnormalized data)\n    x_norm = Normalizer().fit_transform(x_input) if normalize else x_input\n    # fit the data set\n    db_cluster.fit(x_norm)\n    labels = db_cluster.labels_\n    cluster_set, counts = np.unique(labels, return_counts=True)\n    if verbose:\n        <IND>print(\n            \"Clustering for set size \",\n            len(x_norm),\n            \" - \",\n            len(cluster_set),\n            \" clusters\",\n        )\n        print(\"Individual cluster sizes: \", \", \".join([str(c) for c in counts]))\n\n    <DED>clustered_events = _merge_clustered_items(\n        cluster_set, labels, data, time_column, counts\n    )\n\n    if verbose:\n        <IND>print(\"Cluster output rows: \", len(clustered_events))\n\n    <DED>return clustered_events, db_cluster, x_norm\n\n\n<DED>def _merge_clustered_items(\n    cluster_set: np.array,\n    labels: np.array,\n    data: Union[pd.DataFrame, np.array],\n    time_column: str,\n    counts: np.array,\n) -> pd.DataFrame:\n    <IND>\"\"\"\n    Merge outliers and core clusters into single DataFrame.\n\n    Parameters\n    ----------\n    cluster_set : np.array\n        The set of clusters\n    labels : np.array\n        The cluster labels\n    data : Union[pd.DataFrame, np.array]\n        The source data\n    time_column : str\n        Name of the Time column\n    counts : np.array\n        The counts of members in each cluster\n\n    Returns\n    -------\n    pd.DataFrame\n        Merged dataframe\n\n    \"\"\"\n    tz_aware = data.iloc[0][time_column].tz\n    ts_type = \"datetime64[ns, UTC]\" if tz_aware is not None else \"datetime64[ns]\"\n\n    cluster_list = []\n    # Iterate through clusters, adding exemplar to output frame\n    # pylint: disable=consider-using-enumerate\n    # we need to know the index of the item within the loop\n    for idx in range(len(cluster_set)):\n        <IND>cluster_id = cluster_set[idx]\n        class_members = labels == cluster_id\n        if isinstance(data, pd.DataFrame):\n            <IND>time_ordered = data[class_members].sort_values(time_column, ascending=True)\n            first_event_time = time_ordered[0:][time_column].iat[0]\n            last_event_time = time_ordered[-1:][time_column].iat[0]\n        <DED>else:\n            <IND>first_event_time = None\n            last_event_time = None\n\n        <DED>if cluster_id == -1:\n            # 'Noise' events are individual items that could not be assigned\n            # to a cluster and so are unique\n            <IND>cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=False,\n                    ClusterId=cluster_id,\n                    ClusterSize=1,\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n        <DED>else:\n            # Otherwise, just choose the first example of the cluster set\n            <IND>cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=True,\n                    ClusterId=cluster_id,\n                    ClusterSize=counts[idx],\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )[0:1]\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n    # pylint: enable=consider-using-enumerate\n    <DED><DED>return pd.concat(cluster_list)\n\n\n<DED>@export\ndef add_process_features(\n    input_frame: pd.DataFrame, path_separator: str = None, force: bool = False\n) -> pd.DataFrame:\n    <IND>r\"\"\"\n    Add numerical features based on patterns of command line and process name.\n\n    Parameters\n    ----------\n    input_frame : pd.DataFrame\n        The input dataframe\n    path_separator : str, optional\n        Path separator. If not supplied, try to determine\n        from 'NewProcessName' column of first 10 rows\n        (the default is None)\n    force : bool, optional\n        Forces re-calculation of feature columns even if they\n        already exist (the default is False)\n\n    Returns\n    -------\n    pd.DataFrame\n        Copy of the dataframe with the additional numeric features\n\n    Notes\n    -----\n    Features added:\n\n    - processNameLen: length of process file name (inc path)\n    - processNameTokens: the number of elements in the path\n    - processName: the process file name (minus path)\n    - commandlineTokens: number of space-separated tokens in the command line\n    - commandlineLen: length of the command line\n    - commandlineLogLen: log10 length of commandline\n    - isSystemSession: 1 if session Id is 0x3e7 for Windows or -1 for Linux\n    - commandlineTokensFull: counts number of token separators in commandline\n      [\\\\s\\-\\\\/\\.,\"\\'\\|&:;%$()]\n    - pathScore: sum of ord() value of characters in path\n    - pathLogScore: log10 of pathScore\n    - commandlineScore: sum of ord() value of characters in commandline\n    - commandlineLogScore: log10 of commandlineScore\n\n    \"\"\"\n    output_df = input_frame.copy()\n\n    # Set any NaN values to empty string\n    if \"NewProcessName\" in output_df and \"CommandLine\" in output_df:\n        <IND>output_df[[\"NewProcessName\", \"CommandLine\"]] = output_df[\n            [\"NewProcessName\", \"CommandLine\"]\n        ].fillna(value=\"\")\n\n    # try to determine the path separator\n    <DED>if path_separator is None:\n        <IND>sample_df = output_df.head(10)\n        lx_path = len(sample_df[sample_df[\"NewProcessName\"].str.contains(\"/\")])\n        path_separator = \"/\" if lx_path else \"\\\\\"\n    # Create features from process name and command line\n    <DED>if \"NewProcessName\" in output_df:\n        <IND>_add_processname_features(output_df, force, path_separator)\n\n    <DED>if \"CommandLine\" in output_df:\n        <IND>_add_commandline_features(output_df, force)\n\n    <DED>if \"SubjectLogonId\" in output_df and (\"isSystemSession\" not in output_df or force):\n        <IND>output_df[\"isSystemSession\"] = output_df[\"SubjectLogonId\"].isin([\"0x3e7\", \"-1\"])\n\n    <DED>return output_df\n\n\n<DED>def _add_processname_features(\n    output_df: pd.DataFrame, force: bool, path_separator: str\n):\n    <IND>\"\"\"\n    Add process name default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n    path_separator : str\n        Path separator for OS\n\n    \"\"\"\n    if \"processName\" not in output_df or force:\n        <IND>output_df[\"processName\"] = output_df.apply(\n            lambda x: x.NewProcessName.split(path_separator)[-1], axis=1\n        )\n    <DED>if \"pathScore\" not in output_df or force:\n        <IND>output_df[\"pathScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.NewProcessName), axis=1\n        )\n    <DED>if \"pathLogScore\" not in output_df or force:\n        <IND>output_df[\"pathLogScore\"] = output_df.apply(\n            lambda x: log10(x.pathScore) if x.pathScore else 0, axis=1\n        )\n    <DED>if \"pathHash\" not in output_df or force:\n        <IND>output_df[\"pathHash\"] = output_df.apply(\n            lambda x: crc32_hash(x.NewProcessName), axis=1\n        )\n\n\n<DED><DED>def _add_commandline_features(output_df: pd.DataFrame, force: bool):\n    <IND>\"\"\"\n    Add commandline default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n\n    \"\"\"\n    if \"commandlineLen\" not in output_df or force:\n        <IND>output_df[\"commandlineLen\"] = output_df.apply(\n            lambda x: len(x.CommandLine), axis=1\n        )\n    <DED>if \"commandlineLogLen\" not in output_df or force:\n        <IND>output_df[\"commandlineLogLen\"] = output_df.apply(\n            lambda x: log10(x.commandlineLen) if x.commandlineLen else 0, axis=1\n        )\n    <DED>if \"commandlineTokensFull\" not in output_df or force:\n        <IND>output_df[\"commandlineTokensFull\"] = output_df[[\"CommandLine\"]].apply(\n            lambda x: delim_count(x.CommandLine), axis=1\n        )\n\n    <DED>if \"commandlineScore\" not in output_df or force:\n        <IND>output_df[\"commandlineScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.CommandLine), axis=1\n        )\n    <DED>if \"commandlineTokensHash\" not in output_df or force:\n        <IND>output_df[\"commandlineTokensHash\"] = output_df.apply(\n            lambda x: delim_hash(x.CommandLine), axis=1\n        )\n\n\n<DED><DED>@export\n@lru_cache(maxsize=1024)\ndef delim_count(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    <IND>r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Count of delimiters in the string.\n\n    \"\"\"\n    return len(re.findall(delim_list, value))\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef delim_hash(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    <IND>r\"\"\"\n    Return a hash (CRC32) of the delimiters from input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Hash of delimiter set in the string.\n\n    \"\"\"\n    return crc32(bytes(\"\".join(re.findall(delim_list, value)), \"utf-8\"))\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef char_ord_score(value: str, scale: int = 1) -> int:\n    <IND>\"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    int\n        [description]\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return floor(sum(ord(x) for x in value) / scale)\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef token_count(value: str, delimiter: str = \" \") -> int:\n    <IND>\"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    int\n        count of tokens\n\n    \"\"\"\n    return len(value.split(delimiter))\n\n\n<DED>def _string_score(input_str):\n    <IND>\"\"\"Sum the ord(c) for characters in a string.\"\"\"\n    return sum(ord(x) for x in input_str)\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef crc32_hash(value: str) -> int:\n    <IND>\"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n\n    Returns\n    -------\n    int\n        CRC32 hash\n\n    \"\"\"\n    return crc32(bytes(value.encode(\"utf-8\")))\n\n\n<DED>def delim_count_df(\n    data: pd.DataFrame, column: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]'\n) -> pd.Series:\n    <IND>r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        The name of the column to process\n    delim_list : str, optional\n        delimiters to use. (the default is r\\'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]\\')\n\n    Returns\n    -------\n    pd.Series\n        Count of delimiters in the string in `column`.\n\n    \"\"\"\n    return data[column].str.count(delim_list)\n\n\n<DED>def char_ord_score_df(data: pd.DataFrame, column: str, scale: int = 1) -> pd.Series:\n    <IND>\"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    pd.Series\n        The sum of the ordinal values of the characters\n        in `column`.\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return data.apply(lambda x: sum(ord(char) for char in x[column]) / scale, axis=1)\n\n\n<DED>def token_count_df(data: pd.DataFrame, column: str, delimiter: str = \" \") -> pd.Series:\n    <IND>\"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    pd.Series\n        count of tokens in strings in `column`\n\n    \"\"\"\n    return data.apply(lambda x: len(x[column].split(delimiter)), axis=1)\n\n\n<DED>def crc32_hash_df(data: pd.DataFrame, column: str) -> pd.Series:\n    <IND>\"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n\n    Returns\n    -------\n    pd.Series\n        CRC32 hash of input column\n\n    \"\"\"\n    return data.apply(lambda x: crc32(bytes(x[column].encode(\"utf-8\"))), axis=1)\n\n\n# pylint: disable=too-many-arguments, too-many-statements\n<DED>@export  # noqa: C901, MC0001\ndef plot_cluster(\n    db_cluster: DBSCAN,\n    data: pd.DataFrame,\n    x_predict: np.ndarray,\n    plot_label: str = None,\n    plot_features: Tuple[int, int] = (0, 1),\n    verbose: bool = False,\n    cut_off: int = 3,\n    xlabel: str = None,\n    ylabel: str = None,\n):\n    <IND>\"\"\"\n    Plot clustered data as scatter chart.\n\n    Parameters\n    ----------\n    db_cluster : DBSCAN\n        DBScan Cluster (from SkLearn DBSCAN).\n    data : pd.DataFrame\n        Dataframe containing original data.\n    x_predict : np.ndarray\n        The DBSCAN predict numpy array\n    plot_label : str, optional\n         If set the column to use to label data points\n         (the default is None)\n    plot_features :  Tuple[int, int], optional\n        Which two features in x_predict to plot (the default is (0, 1))\n    verbose : bool, optional\n        Verbose execution with some extra info\n        (the default is False)\n    cut_off : int, optional\n        The cluster size below which items are considered outliers\n        (the default is 3)\n    xlabel : str, optional\n        x-axis label (the default is None)\n    ylabel : str, optional\n        y-axis label (the default is None)\n\n    \"\"\"\n    max_idx = x_predict.shape[1] - 1\n    if plot_features[0] >= x_predict.shape[1]:\n        <IND>raise ValueError(\n            \"plot_features[0] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    <DED>if plot_features[1] >= x_predict.shape[1]:\n        <IND>raise ValueError(\n            \"plot_features[1] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    <DED>if plot_features[0] == plot_features[1]:\n        <IND>mssg = \"plot_features indexes must be 2 different values in range 0 to\"\n        raise ValueError(mssg + f\" {max_idx}.\")\n\n    <DED>labels = db_cluster.labels_\n    core_samples_mask = np.zeros_like(labels, dtype=bool)\n\n    # pylint: disable=unsupported-assignment-operation\n    # (assignment of numpy array is valid)\n    core_samples_mask[db_cluster.core_sample_indices_] = True\n    unique_labels = set(labels)\n\n    # pylint: disable=no-member\n    # Spectral color map does exist\n    colors = [cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n    # Number of clusters in labels, ignoring noise if present.\n    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n    n_noise_ = list(labels).count(-1)\n    _, counts = np.unique(labels, return_counts=True)\n\n    if verbose:\n        <IND>print(\"Estimated number of clusters: %d\" % n_clusters_)\n        print(\"Estimated number of noise points: %d\" % n_noise_)\n        # print(\"Silhouette Coefficient: %0.3f\"\n        #       % metrics.silhouette_score(x_predict, labels))\n\n    <DED>if not isinstance(data, pd.DataFrame):\n        <IND>plot_label = None\n    <DED>elif plot_label is not None and plot_label not in data:\n        <IND>plot_label = None\n\n    <DED>p_label = None\n    for cluster_id, color in zip(unique_labels, colors):\n        <IND>if cluster_id == -1:\n            # Black used for noise.\n            <IND>color = [0, 0, 0, 1]\n        <DED>class_member_mask = labels == cluster_id\n\n        cluster_size = counts[cluster_id]\n        marker_size = cluster_size\n        marker = \"o\"\n        font_size = \"small\"\n        alpha = 0.4\n\n        if cluster_size < cut_off:\n            <IND>marker = \"+\"\n            marker_size = 10\n            font_size = \"large\"\n            alpha = 1.0\n        <DED>xy_pos = x_predict[class_member_mask & core_samples_mask]\n        plt.plot(\n            xy_pos[:, plot_features[0]],\n            xy_pos[:, plot_features[1]],\n            marker,\n            markerfacecolor=tuple(color),\n            markersize=marker_size,\n        )\n\n        if plot_label:\n            <IND>first_row = data[class_member_mask].iloc[0]\n            if not first_row.empty and plot_label in first_row:\n                <IND>p_label = first_row[plot_label]\n                try:\n                    <IND>plt.annotate(\n                        p_label,\n                        xy=(xy_pos[0, plot_features[0]], xy_pos[0, plot_features[1]]),\n                        fontsize=font_size,\n                        alpha=alpha,\n                    )\n                <DED>except IndexError:\n                    <IND>pass\n\n    <DED><DED><DED><DED>plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n    plt.show()\n    return plt\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n# flake8: noqa: F403, F401\n# pylint: disable=wildcard-import, unused-wildcard-import, unused-import\nfrom ..analysis.eventcluster import *\n\nWARN_MSSG = (\n    \"This module has moved to msticpy.analysis.eventcluster\\n\"\n    + \"Please change your import to reflect this new location.\"\n)\nwarnings.warn(WARN_MSSG, category=DeprecationWarning)\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "microsoft/msticpy",
    "commit": "28466d681e261394e18d9f4e063cebfa06ed04b4",
    "filename": "msticpy/sectools/eventcluster.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/microsoft-msticpy/msticpy/sectools/eventcluster.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "msticpy/sectools/eventcluster.py:683:8 Incompatible variable type [9]: plot_label is declared to have type `str` but is used as type `None`.",
    "message": " plot_label is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 683,
    "warning_line": "        plot_label = None",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "# --------------------------------------------------------------------------\nr\"\"\"\neventcluster module.\n\nThis module is intended to be used to summarize large numbers of events\ninto clusters of different patterns. High volume repeating events can\noften make it difficult to see unique and interesting items.\n\nThe module contains functions to generate clusterable features from\nstring data. For example, an administration command that does some\nmaintenance on thousands of servers with a commandline such as:\n``install-update -hostname {host.fqdn} -tmp:/tmp/{GUID}/rollback``\\  can\nbe collapsed into a single cluster pattern by ignoring the character\nvalues in the string and using delimiters or tokens to group the values.\n\nThis is an unsupervised learning module implemented using SciKit Learn\nDBScan.\n\nContains:\ndbcluster_events: generic clustering method using DBSCAN designed to summarize\nprocess events and other similar data by grouping on common features.\n\nadd_process_features: derives numerical features from text features such as\ncommandline and process path.\n\n\"\"\"\nfrom binascii import crc32\nfrom functools import lru_cache\nfrom math import log10, floor\nimport re\nfrom typing import List, Any, Tuple, Union\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import Normalizer\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nfrom ..common.utility import export\nfrom .._version import VERSION\n",
        "source_code_len": 1462,
        "target_code": "# --------------------------------------------------------------------------\n\"\"\"Deprecated placeholder for eventcluster.py.\"\"\"\nimport warnings\nfrom .._version import VERSION\n",
        "target_code_len": 174,
        "diff_format": "@@ -5,41 +5,4 @@\n # --------------------------------------------------------------------------\n-r\"\"\"\n-eventcluster module.\n-\n-This module is intended to be used to summarize large numbers of events\n-into clusters of different patterns. High volume repeating events can\n-often make it difficult to see unique and interesting items.\n-\n-The module contains functions to generate clusterable features from\n-string data. For example, an administration command that does some\n-maintenance on thousands of servers with a commandline such as:\n-``install-update -hostname {host.fqdn} -tmp:/tmp/{GUID}/rollback``\\  can\n-be collapsed into a single cluster pattern by ignoring the character\n-values in the string and using delimiters or tokens to group the values.\n-\n-This is an unsupervised learning module implemented using SciKit Learn\n-DBScan.\n-\n-Contains:\n-dbcluster_events: generic clustering method using DBSCAN designed to summarize\n-process events and other similar data by grouping on common features.\n-\n-add_process_features: derives numerical features from text features such as\n-commandline and process path.\n-\n-\"\"\"\n-from binascii import crc32\n-from functools import lru_cache\n-from math import log10, floor\n-import re\n-from typing import List, Any, Tuple, Union\n-\n-import numpy as np\n-import pandas as pd\n-from sklearn.cluster import DBSCAN\n-from sklearn.preprocessing import Normalizer\n-import matplotlib.pyplot as plt\n-from matplotlib import cm\n-\n-from ..common.utility import export\n+\"\"\"Deprecated placeholder for eventcluster.py.\"\"\"\n+import warnings\n from .._version import VERSION\n",
        "source_code_with_indent": "# --------------------------------------------------------------------------\nr\"\"\"\neventcluster module.\n\nThis module is intended to be used to summarize large numbers of events\ninto clusters of different patterns. High volume repeating events can\noften make it difficult to see unique and interesting items.\n\nThe module contains functions to generate clusterable features from\nstring data. For example, an administration command that does some\nmaintenance on thousands of servers with a commandline such as:\n``install-update -hostname {host.fqdn} -tmp:/tmp/{GUID}/rollback``\\  can\nbe collapsed into a single cluster pattern by ignoring the character\nvalues in the string and using delimiters or tokens to group the values.\n\nThis is an unsupervised learning module implemented using SciKit Learn\nDBScan.\n\nContains:\ndbcluster_events: generic clustering method using DBSCAN designed to summarize\nprocess events and other similar data by grouping on common features.\n\nadd_process_features: derives numerical features from text features such as\ncommandline and process path.\n\n\"\"\"\nfrom binascii import crc32\nfrom functools import lru_cache\nfrom math import log10, floor\nimport re\nfrom typing import List, Any, Tuple, Union\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import Normalizer\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nfrom ..common.utility import export\nfrom .._version import VERSION\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "# --------------------------------------------------------------------------\n\"\"\"Deprecated placeholder for eventcluster.py.\"\"\"\nimport warnings\nfrom .._version import VERSION\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n# pylint: disable=too-many-arguments, too-many-locals\n@export\ndef dbcluster_events(\n    data: Any,\n    cluster_columns: List[Any] = None,\n    verbose: bool = False,\n    normalize: bool = True,\n    time_column: str = \"TimeCreatedUtc\",\n    max_cluster_distance: float = 0.01,\n    min_cluster_samples: int = 2,\n    **kwargs,\n) -> Tuple[pd.DataFrame, DBSCAN, np.ndarray]:\n    \"\"\"\n    Cluster data set according to cluster_columns features.\n\n    Parameters\n    ----------\n    data : Any\n        Input data as a pandas DataFrame or numpy array\n    cluster_columns : List[Any], optional\n        List of columns to use for features\n        - for DataFrame this is a list of column names\n        - for numpy array this is a list of column indexes\n    verbose : bool, optional\n        Print additional information about clustering results (the default is False)\n    normalize : bool, optional\n        Normalize the input data (should probably always be True)\n    time_column : str, optional\n        If there is a time column the output data will be ordered by this\n        (the default is 'TimeCreatedUtc')\n    max_cluster_distance : float, optional\n        DBSCAN eps (max cluster member distance) (the default is 0.01)\n    min_cluster_samples : int, optional\n        DBSCAN min_samples (the minimum cluster size) (the default is 2)\n\n    Other Parameters\n    ----------------\n    kwargs: Other arguments are passed to DBSCAN constructor\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, DBSCAN, np.ndarray]\n        Output dataframe with clustered rows\n        DBSCAN model\n        Normalized data set\n\n    \"\"\"\n    allowed_types = [np.ndarray, pd.DataFrame]\n\n    x_input = None\n    if isinstance(data, pd.DataFrame):\n        if cluster_columns is None:\n            x_input = data.values\n        else:\n            x_input = data[cluster_columns].values\n    elif isinstance(data, np.ndarray):\n        x_input = data if cluster_columns is None else data[:, cluster_columns].values\n    if x_input is None:\n        mssg = \"Input data not in expected format.\\n{} is not one of allowed types {}\"\n        type_list = \", \".join([str(t) for t in allowed_types])\n        mssg = mssg.format(str(type(data)), type_list)\n        raise ValueError(mssg)\n\n    # Create DBSCAN cluster object\n    db_cluster = DBSCAN(\n        eps=max_cluster_distance, min_samples=min_cluster_samples, **kwargs\n    )\n\n    # Normalize the data (most clustering algorithms don't do well with\n    # unnormalized data)\n    x_norm = Normalizer().fit_transform(x_input) if normalize else x_input\n    # fit the data set\n    db_cluster.fit(x_norm)\n    labels = db_cluster.labels_\n    cluster_set, counts = np.unique(labels, return_counts=True)\n    if verbose:\n        print(\n            \"Clustering for set size \",\n            len(x_norm),\n            \" - \",\n            len(cluster_set),\n            \" clusters\",\n        )\n        print(\"Individual cluster sizes: \", \", \".join([str(c) for c in counts]))\n\n    clustered_events = _merge_clustered_items(\n        cluster_set, labels, data, time_column, counts\n    )\n\n    if verbose:\n        print(\"Cluster output rows: \", len(clustered_events))\n\n    return clustered_events, db_cluster, x_norm\n\n\ndef _merge_clustered_items(\n    cluster_set: np.array,\n    labels: np.array,\n    data: Union[pd.DataFrame, np.array],\n    time_column: str,\n    counts: np.array,\n) -> pd.DataFrame:\n    \"\"\"\n    Merge outliers and core clusters into single DataFrame.\n\n    Parameters\n    ----------\n    cluster_set : np.array\n        The set of clusters\n    labels : np.array\n        The cluster labels\n    data : Union[pd.DataFrame, np.array]\n        The source data\n    time_column : str\n        Name of the Time column\n    counts : np.array\n        The counts of members in each cluster\n\n    Returns\n    -------\n    pd.DataFrame\n        Merged dataframe\n\n    \"\"\"\n    tz_aware = data.iloc[0][time_column].tz\n    ts_type = \"datetime64[ns, UTC]\" if tz_aware is not None else \"datetime64[ns]\"\n\n    cluster_list = []\n    # Iterate through clusters, adding exemplar to output frame\n    # pylint: disable=consider-using-enumerate\n    # we need to know the index of the item within the loop\n    for idx in range(len(cluster_set)):\n        cluster_id = cluster_set[idx]\n        class_members = labels == cluster_id\n        if isinstance(data, pd.DataFrame):\n            time_ordered = data[class_members].sort_values(time_column, ascending=True)\n            first_event_time = time_ordered[0:][time_column].iat[0]\n            last_event_time = time_ordered[-1:][time_column].iat[0]\n        else:\n            first_event_time = None\n            last_event_time = None\n\n        if cluster_id == -1:\n            # 'Noise' events are individual items that could not be assigned\n            # to a cluster and so are unique\n            cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=False,\n                    ClusterId=cluster_id,\n                    ClusterSize=1,\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n        else:\n            # Otherwise, just choose the first example of the cluster set\n            cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=True,\n                    ClusterId=cluster_id,\n                    ClusterSize=counts[idx],\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )[0:1]\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n    # pylint: enable=consider-using-enumerate\n    return pd.concat(cluster_list)\n\n\n@export\ndef add_process_features(\n    input_frame: pd.DataFrame, path_separator: str = None, force: bool = False\n) -> pd.DataFrame:\n    r\"\"\"\n    Add numerical features based on patterns of command line and process name.\n\n    Parameters\n    ----------\n    input_frame : pd.DataFrame\n        The input dataframe\n    path_separator : str, optional\n        Path separator. If not supplied, try to determine\n        from 'NewProcessName' column of first 10 rows\n        (the default is None)\n    force : bool, optional\n        Forces re-calculation of feature columns even if they\n        already exist (the default is False)\n\n    Returns\n    -------\n    pd.DataFrame\n        Copy of the dataframe with the additional numeric features\n\n    Notes\n    -----\n    Features added:\n\n    - processNameLen: length of process file name (inc path)\n    - processNameTokens: the number of elements in the path\n    - processName: the process file name (minus path)\n    - commandlineTokens: number of space-separated tokens in the command line\n    - commandlineLen: length of the command line\n    - commandlineLogLen: log10 length of commandline\n    - isSystemSession: 1 if session Id is 0x3e7 for Windows or -1 for Linux\n    - commandlineTokensFull: counts number of token separators in commandline\n      [\\\\s\\-\\\\/\\.,\"\\'\\|&:;%$()]\n    - pathScore: sum of ord() value of characters in path\n    - pathLogScore: log10 of pathScore\n    - commandlineScore: sum of ord() value of characters in commandline\n    - commandlineLogScore: log10 of commandlineScore\n\n    \"\"\"\n    output_df = input_frame.copy()\n\n    # Set any NaN values to empty string\n    if \"NewProcessName\" in output_df and \"CommandLine\" in output_df:\n        output_df[[\"NewProcessName\", \"CommandLine\"]] = output_df[\n            [\"NewProcessName\", \"CommandLine\"]\n        ].fillna(value=\"\")\n\n    # try to determine the path separator\n    if path_separator is None:\n        sample_df = output_df.head(10)\n        lx_path = len(sample_df[sample_df[\"NewProcessName\"].str.contains(\"/\")])\n        path_separator = \"/\" if lx_path else \"\\\\\"\n    # Create features from process name and command line\n    if \"NewProcessName\" in output_df:\n        _add_processname_features(output_df, force, path_separator)\n\n    if \"CommandLine\" in output_df:\n        _add_commandline_features(output_df, force)\n\n    if \"SubjectLogonId\" in output_df and (\"isSystemSession\" not in output_df or force):\n        output_df[\"isSystemSession\"] = output_df[\"SubjectLogonId\"].isin([\"0x3e7\", \"-1\"])\n\n    return output_df\n\n\ndef _add_processname_features(\n    output_df: pd.DataFrame, force: bool, path_separator: str\n):\n    \"\"\"\n    Add process name default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n    path_separator : str\n        Path separator for OS\n\n    \"\"\"\n    if \"processName\" not in output_df or force:\n        output_df[\"processName\"] = output_df.apply(\n            lambda x: x.NewProcessName.split(path_separator)[-1], axis=1\n        )\n    if \"pathScore\" not in output_df or force:\n        output_df[\"pathScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.NewProcessName), axis=1\n        )\n    if \"pathLogScore\" not in output_df or force:\n        output_df[\"pathLogScore\"] = output_df.apply(\n            lambda x: log10(x.pathScore) if x.pathScore else 0, axis=1\n        )\n    if \"pathHash\" not in output_df or force:\n        output_df[\"pathHash\"] = output_df.apply(\n            lambda x: crc32_hash(x.NewProcessName), axis=1\n        )\n\n\ndef _add_commandline_features(output_df: pd.DataFrame, force: bool):\n    \"\"\"\n    Add commandline default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n\n    \"\"\"\n    if \"commandlineLen\" not in output_df or force:\n        output_df[\"commandlineLen\"] = output_df.apply(\n            lambda x: len(x.CommandLine), axis=1\n        )\n    if \"commandlineLogLen\" not in output_df or force:\n        output_df[\"commandlineLogLen\"] = output_df.apply(\n            lambda x: log10(x.commandlineLen) if x.commandlineLen else 0, axis=1\n        )\n    if \"commandlineTokensFull\" not in output_df or force:\n        output_df[\"commandlineTokensFull\"] = output_df[[\"CommandLine\"]].apply(\n            lambda x: delim_count(x.CommandLine), axis=1\n        )\n\n    if \"commandlineScore\" not in output_df or force:\n        output_df[\"commandlineScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.CommandLine), axis=1\n        )\n    if \"commandlineTokensHash\" not in output_df or force:\n        output_df[\"commandlineTokensHash\"] = output_df.apply(\n            lambda x: delim_hash(x.CommandLine), axis=1\n        )\n\n\n@export\n@lru_cache(maxsize=1024)\ndef delim_count(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Count of delimiters in the string.\n\n    \"\"\"\n    return len(re.findall(delim_list, value))\n\n\n@export\n@lru_cache(maxsize=1024)\ndef delim_hash(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    r\"\"\"\n    Return a hash (CRC32) of the delimiters from input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Hash of delimiter set in the string.\n\n    \"\"\"\n    return crc32(bytes(\"\".join(re.findall(delim_list, value)), \"utf-8\"))\n\n\n@export\n@lru_cache(maxsize=1024)\ndef char_ord_score(value: str, scale: int = 1) -> int:\n    \"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    int\n        [description]\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return floor(sum(ord(x) for x in value) / scale)\n\n\n@export\n@lru_cache(maxsize=1024)\ndef token_count(value: str, delimiter: str = \" \") -> int:\n    \"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    int\n        count of tokens\n\n    \"\"\"\n    return len(value.split(delimiter))\n\n\ndef _string_score(input_str):\n    \"\"\"Sum the ord(c) for characters in a string.\"\"\"\n    return sum(ord(x) for x in input_str)\n\n\n@export\n@lru_cache(maxsize=1024)\ndef crc32_hash(value: str) -> int:\n    \"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n\n    Returns\n    -------\n    int\n        CRC32 hash\n\n    \"\"\"\n    return crc32(bytes(value.encode(\"utf-8\")))\n\n\ndef delim_count_df(\n    data: pd.DataFrame, column: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]'\n) -> pd.Series:\n    r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        The name of the column to process\n    delim_list : str, optional\n        delimiters to use. (the default is r\\'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]\\')\n\n    Returns\n    -------\n    pd.Series\n        Count of delimiters in the string in `column`.\n\n    \"\"\"\n    return data[column].str.count(delim_list)\n\n\ndef char_ord_score_df(data: pd.DataFrame, column: str, scale: int = 1) -> pd.Series:\n    \"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    pd.Series\n        The sum of the ordinal values of the characters\n        in `column`.\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return data.apply(lambda x: sum(ord(char) for char in x[column]) / scale, axis=1)\n\n\ndef token_count_df(data: pd.DataFrame, column: str, delimiter: str = \" \") -> pd.Series:\n    \"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    pd.Series\n        count of tokens in strings in `column`\n\n    \"\"\"\n    return data.apply(lambda x: len(x[column].split(delimiter)), axis=1)\n\n\ndef crc32_hash_df(data: pd.DataFrame, column: str) -> pd.Series:\n    \"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n\n    Returns\n    -------\n    pd.Series\n        CRC32 hash of input column\n\n    \"\"\"\n    return data.apply(lambda x: crc32(bytes(x[column].encode(\"utf-8\"))), axis=1)\n\n\n# pylint: disable=too-many-arguments, too-many-statements\n@export  # noqa: C901, MC0001\ndef plot_cluster(\n    db_cluster: DBSCAN,\n    data: pd.DataFrame,\n    x_predict: np.ndarray,\n    plot_label: str = None,\n    plot_features: Tuple[int, int] = (0, 1),\n    verbose: bool = False,\n    cut_off: int = 3,\n    xlabel: str = None,\n    ylabel: str = None,\n):\n    \"\"\"\n    Plot clustered data as scatter chart.\n\n    Parameters\n    ----------\n    db_cluster : DBSCAN\n        DBScan Cluster (from SkLearn DBSCAN).\n    data : pd.DataFrame\n        Dataframe containing original data.\n    x_predict : np.ndarray\n        The DBSCAN predict numpy array\n    plot_label : str, optional\n         If set the column to use to label data points\n         (the default is None)\n    plot_features :  Tuple[int, int], optional\n        Which two features in x_predict to plot (the default is (0, 1))\n    verbose : bool, optional\n        Verbose execution with some extra info\n        (the default is False)\n    cut_off : int, optional\n        The cluster size below which items are considered outliers\n        (the default is 3)\n    xlabel : str, optional\n        x-axis label (the default is None)\n    ylabel : str, optional\n        y-axis label (the default is None)\n\n    \"\"\"\n    max_idx = x_predict.shape[1] - 1\n    if plot_features[0] >= x_predict.shape[1]:\n        raise ValueError(\n            \"plot_features[0] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    if plot_features[1] >= x_predict.shape[1]:\n        raise ValueError(\n            \"plot_features[1] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    if plot_features[0] == plot_features[1]:\n        mssg = \"plot_features indexes must be 2 different values in range 0 to\"\n        raise ValueError(mssg + f\" {max_idx}.\")\n\n    labels = db_cluster.labels_\n    core_samples_mask = np.zeros_like(labels, dtype=bool)\n\n    # pylint: disable=unsupported-assignment-operation\n    # (assignment of numpy array is valid)\n    core_samples_mask[db_cluster.core_sample_indices_] = True\n    unique_labels = set(labels)\n\n    # pylint: disable=no-member\n    # Spectral color map does exist\n    colors = [cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n    # Number of clusters in labels, ignoring noise if present.\n    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n    n_noise_ = list(labels).count(-1)\n    _, counts = np.unique(labels, return_counts=True)\n\n    if verbose:\n        print(\"Estimated number of clusters: %d\" % n_clusters_)\n        print(\"Estimated number of noise points: %d\" % n_noise_)\n        # print(\"Silhouette Coefficient: %0.3f\"\n        #       % metrics.silhouette_score(x_predict, labels))\n\n    if not isinstance(data, pd.DataFrame):\n        plot_label = None\n    elif plot_label is not None and plot_label not in data:\n        plot_label = None\n\n    p_label = None\n    for cluster_id, color in zip(unique_labels, colors):\n        if cluster_id == -1:\n            # Black used for noise.\n            color = [0, 0, 0, 1]\n        class_member_mask = labels == cluster_id\n\n        cluster_size = counts[cluster_id]\n        marker_size = cluster_size\n        marker = \"o\"\n        font_size = \"small\"\n        alpha = 0.4\n\n        if cluster_size < cut_off:\n            marker = \"+\"\n            marker_size = 10\n            font_size = \"large\"\n            alpha = 1.0\n        xy_pos = x_predict[class_member_mask & core_samples_mask]\n        plt.plot(\n            xy_pos[:, plot_features[0]],\n            xy_pos[:, plot_features[1]],\n            marker,\n            markerfacecolor=tuple(color),\n            markersize=marker_size,\n        )\n\n        if plot_label:\n            first_row = data[class_member_mask].iloc[0]\n            if not first_row.empty and plot_label in first_row:\n                p_label = first_row[plot_label]\n                try:\n                    plt.annotate(\n                        p_label,\n                        xy=(xy_pos[0, plot_features[0]], xy_pos[0, plot_features[1]]),\n                        fontsize=font_size,\n                        alpha=alpha,\n                    )\n                except IndexError:\n                    pass\n\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n    plt.show()\n    return plt\n",
        "source_code_len": 21140,
        "target_code": "\n# flake8: noqa: F403, F401\n# pylint: disable=wildcard-import, unused-wildcard-import, unused-import\nfrom ..analysis.eventcluster import *\n\nWARN_MSSG = (\n    \"This module has moved to msticpy.analysis.eventcluster\\n\"\n    + \"Please change your import to reflect this new location.\"\n)\nwarnings.warn(WARN_MSSG, category=DeprecationWarning)\n",
        "target_code_len": 337,
        "diff_format": "@@ -50,683 +13,10 @@\n \n-# pylint: disable=too-many-arguments, too-many-locals\n-@export\n-def dbcluster_events(\n-    data: Any,\n-    cluster_columns: List[Any] = None,\n-    verbose: bool = False,\n-    normalize: bool = True,\n-    time_column: str = \"TimeCreatedUtc\",\n-    max_cluster_distance: float = 0.01,\n-    min_cluster_samples: int = 2,\n-    **kwargs,\n-) -> Tuple[pd.DataFrame, DBSCAN, np.ndarray]:\n-    \"\"\"\n-    Cluster data set according to cluster_columns features.\n+# flake8: noqa: F403, F401\n+# pylint: disable=wildcard-import, unused-wildcard-import, unused-import\n+from ..analysis.eventcluster import *\n \n-    Parameters\n-    ----------\n-    data : Any\n-        Input data as a pandas DataFrame or numpy array\n-    cluster_columns : List[Any], optional\n-        List of columns to use for features\n-        - for DataFrame this is a list of column names\n-        - for numpy array this is a list of column indexes\n-    verbose : bool, optional\n-        Print additional information about clustering results (the default is False)\n-    normalize : bool, optional\n-        Normalize the input data (should probably always be True)\n-    time_column : str, optional\n-        If there is a time column the output data will be ordered by this\n-        (the default is 'TimeCreatedUtc')\n-    max_cluster_distance : float, optional\n-        DBSCAN eps (max cluster member distance) (the default is 0.01)\n-    min_cluster_samples : int, optional\n-        DBSCAN min_samples (the minimum cluster size) (the default is 2)\n-\n-    Other Parameters\n-    ----------------\n-    kwargs: Other arguments are passed to DBSCAN constructor\n-\n-    Returns\n-    -------\n-    Tuple[pd.DataFrame, DBSCAN, np.ndarray]\n-        Output dataframe with clustered rows\n-        DBSCAN model\n-        Normalized data set\n-\n-    \"\"\"\n-    allowed_types = [np.ndarray, pd.DataFrame]\n-\n-    x_input = None\n-    if isinstance(data, pd.DataFrame):\n-        if cluster_columns is None:\n-            x_input = data.values\n-        else:\n-            x_input = data[cluster_columns].values\n-    elif isinstance(data, np.ndarray):\n-        x_input = data if cluster_columns is None else data[:, cluster_columns].values\n-    if x_input is None:\n-        mssg = \"Input data not in expected format.\\n{} is not one of allowed types {}\"\n-        type_list = \", \".join([str(t) for t in allowed_types])\n-        mssg = mssg.format(str(type(data)), type_list)\n-        raise ValueError(mssg)\n-\n-    # Create DBSCAN cluster object\n-    db_cluster = DBSCAN(\n-        eps=max_cluster_distance, min_samples=min_cluster_samples, **kwargs\n-    )\n-\n-    # Normalize the data (most clustering algorithms don't do well with\n-    # unnormalized data)\n-    x_norm = Normalizer().fit_transform(x_input) if normalize else x_input\n-    # fit the data set\n-    db_cluster.fit(x_norm)\n-    labels = db_cluster.labels_\n-    cluster_set, counts = np.unique(labels, return_counts=True)\n-    if verbose:\n-        print(\n-            \"Clustering for set size \",\n-            len(x_norm),\n-            \" - \",\n-            len(cluster_set),\n-            \" clusters\",\n-        )\n-        print(\"Individual cluster sizes: \", \", \".join([str(c) for c in counts]))\n-\n-    clustered_events = _merge_clustered_items(\n-        cluster_set, labels, data, time_column, counts\n-    )\n-\n-    if verbose:\n-        print(\"Cluster output rows: \", len(clustered_events))\n-\n-    return clustered_events, db_cluster, x_norm\n-\n-\n-def _merge_clustered_items(\n-    cluster_set: np.array,\n-    labels: np.array,\n-    data: Union[pd.DataFrame, np.array],\n-    time_column: str,\n-    counts: np.array,\n-) -> pd.DataFrame:\n-    \"\"\"\n-    Merge outliers and core clusters into single DataFrame.\n-\n-    Parameters\n-    ----------\n-    cluster_set : np.array\n-        The set of clusters\n-    labels : np.array\n-        The cluster labels\n-    data : Union[pd.DataFrame, np.array]\n-        The source data\n-    time_column : str\n-        Name of the Time column\n-    counts : np.array\n-        The counts of members in each cluster\n-\n-    Returns\n-    -------\n-    pd.DataFrame\n-        Merged dataframe\n-\n-    \"\"\"\n-    tz_aware = data.iloc[0][time_column].tz\n-    ts_type = \"datetime64[ns, UTC]\" if tz_aware is not None else \"datetime64[ns]\"\n-\n-    cluster_list = []\n-    # Iterate through clusters, adding exemplar to output frame\n-    # pylint: disable=consider-using-enumerate\n-    # we need to know the index of the item within the loop\n-    for idx in range(len(cluster_set)):\n-        cluster_id = cluster_set[idx]\n-        class_members = labels == cluster_id\n-        if isinstance(data, pd.DataFrame):\n-            time_ordered = data[class_members].sort_values(time_column, ascending=True)\n-            first_event_time = time_ordered[0:][time_column].iat[0]\n-            last_event_time = time_ordered[-1:][time_column].iat[0]\n-        else:\n-            first_event_time = None\n-            last_event_time = None\n-\n-        if cluster_id == -1:\n-            # 'Noise' events are individual items that could not be assigned\n-            # to a cluster and so are unique\n-            cluster_list.append(\n-                data[class_members]\n-                .assign(\n-                    Clustered=False,\n-                    ClusterId=cluster_id,\n-                    ClusterSize=1,\n-                    TimeGenerated=first_event_time,\n-                    FirstEventTime=first_event_time,\n-                    LastEventTime=last_event_time,\n-                )\n-                .astype(\n-                    dtype={\n-                        \"TimeGenerated\": ts_type,\n-                        \"FirstEventTime\": ts_type,\n-                        \"LastEventTime\": ts_type,\n-                    }\n-                )\n-            )\n-        else:\n-            # Otherwise, just choose the first example of the cluster set\n-            cluster_list.append(\n-                data[class_members]\n-                .assign(\n-                    Clustered=True,\n-                    ClusterId=cluster_id,\n-                    ClusterSize=counts[idx],\n-                    TimeGenerated=first_event_time,\n-                    FirstEventTime=first_event_time,\n-                    LastEventTime=last_event_time,\n-                )[0:1]\n-                .astype(\n-                    dtype={\n-                        \"TimeGenerated\": ts_type,\n-                        \"FirstEventTime\": ts_type,\n-                        \"LastEventTime\": ts_type,\n-                    }\n-                )\n-            )\n-    # pylint: enable=consider-using-enumerate\n-    return pd.concat(cluster_list)\n-\n-\n-@export\n-def add_process_features(\n-    input_frame: pd.DataFrame, path_separator: str = None, force: bool = False\n-) -> pd.DataFrame:\n-    r\"\"\"\n-    Add numerical features based on patterns of command line and process name.\n-\n-    Parameters\n-    ----------\n-    input_frame : pd.DataFrame\n-        The input dataframe\n-    path_separator : str, optional\n-        Path separator. If not supplied, try to determine\n-        from 'NewProcessName' column of first 10 rows\n-        (the default is None)\n-    force : bool, optional\n-        Forces re-calculation of feature columns even if they\n-        already exist (the default is False)\n-\n-    Returns\n-    -------\n-    pd.DataFrame\n-        Copy of the dataframe with the additional numeric features\n-\n-    Notes\n-    -----\n-    Features added:\n-\n-    - processNameLen: length of process file name (inc path)\n-    - processNameTokens: the number of elements in the path\n-    - processName: the process file name (minus path)\n-    - commandlineTokens: number of space-separated tokens in the command line\n-    - commandlineLen: length of the command line\n-    - commandlineLogLen: log10 length of commandline\n-    - isSystemSession: 1 if session Id is 0x3e7 for Windows or -1 for Linux\n-    - commandlineTokensFull: counts number of token separators in commandline\n-      [\\\\s\\-\\\\/\\.,\"\\'\\|&:;%$()]\n-    - pathScore: sum of ord() value of characters in path\n-    - pathLogScore: log10 of pathScore\n-    - commandlineScore: sum of ord() value of characters in commandline\n-    - commandlineLogScore: log10 of commandlineScore\n-\n-    \"\"\"\n-    output_df = input_frame.copy()\n-\n-    # Set any NaN values to empty string\n-    if \"NewProcessName\" in output_df and \"CommandLine\" in output_df:\n-        output_df[[\"NewProcessName\", \"CommandLine\"]] = output_df[\n-            [\"NewProcessName\", \"CommandLine\"]\n-        ].fillna(value=\"\")\n-\n-    # try to determine the path separator\n-    if path_separator is None:\n-        sample_df = output_df.head(10)\n-        lx_path = len(sample_df[sample_df[\"NewProcessName\"].str.contains(\"/\")])\n-        path_separator = \"/\" if lx_path else \"\\\\\"\n-    # Create features from process name and command line\n-    if \"NewProcessName\" in output_df:\n-        _add_processname_features(output_df, force, path_separator)\n-\n-    if \"CommandLine\" in output_df:\n-        _add_commandline_features(output_df, force)\n-\n-    if \"SubjectLogonId\" in output_df and (\"isSystemSession\" not in output_df or force):\n-        output_df[\"isSystemSession\"] = output_df[\"SubjectLogonId\"].isin([\"0x3e7\", \"-1\"])\n-\n-    return output_df\n-\n-\n-def _add_processname_features(\n-    output_df: pd.DataFrame, force: bool, path_separator: str\n-):\n-    \"\"\"\n-    Add process name default features.\n-\n-    Parameters\n-    ----------\n-    output_df : pd.DataFrame\n-        The dataframe to add features to\n-    force : bool\n-        If True overwrite existing feature columns\n-    path_separator : str\n-        Path separator for OS\n-\n-    \"\"\"\n-    if \"processName\" not in output_df or force:\n-        output_df[\"processName\"] = output_df.apply(\n-            lambda x: x.NewProcessName.split(path_separator)[-1], axis=1\n-        )\n-    if \"pathScore\" not in output_df or force:\n-        output_df[\"pathScore\"] = output_df.apply(\n-            lambda x: char_ord_score(x.NewProcessName), axis=1\n-        )\n-    if \"pathLogScore\" not in output_df or force:\n-        output_df[\"pathLogScore\"] = output_df.apply(\n-            lambda x: log10(x.pathScore) if x.pathScore else 0, axis=1\n-        )\n-    if \"pathHash\" not in output_df or force:\n-        output_df[\"pathHash\"] = output_df.apply(\n-            lambda x: crc32_hash(x.NewProcessName), axis=1\n-        )\n-\n-\n-def _add_commandline_features(output_df: pd.DataFrame, force: bool):\n-    \"\"\"\n-    Add commandline default features.\n-\n-    Parameters\n-    ----------\n-    output_df : pd.DataFrame\n-        The dataframe to add features to\n-    force : bool\n-        If True overwrite existing feature columns\n-\n-    \"\"\"\n-    if \"commandlineLen\" not in output_df or force:\n-        output_df[\"commandlineLen\"] = output_df.apply(\n-            lambda x: len(x.CommandLine), axis=1\n-        )\n-    if \"commandlineLogLen\" not in output_df or force:\n-        output_df[\"commandlineLogLen\"] = output_df.apply(\n-            lambda x: log10(x.commandlineLen) if x.commandlineLen else 0, axis=1\n-        )\n-    if \"commandlineTokensFull\" not in output_df or force:\n-        output_df[\"commandlineTokensFull\"] = output_df[[\"CommandLine\"]].apply(\n-            lambda x: delim_count(x.CommandLine), axis=1\n-        )\n-\n-    if \"commandlineScore\" not in output_df or force:\n-        output_df[\"commandlineScore\"] = output_df.apply(\n-            lambda x: char_ord_score(x.CommandLine), axis=1\n-        )\n-    if \"commandlineTokensHash\" not in output_df or force:\n-        output_df[\"commandlineTokensHash\"] = output_df.apply(\n-            lambda x: delim_hash(x.CommandLine), axis=1\n-        )\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def delim_count(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n-    r\"\"\"\n-    Count the delimiters in input column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    delim_list : str, optional\n-        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n-\n-    Returns\n-    -------\n-    int\n-        Count of delimiters in the string.\n-\n-    \"\"\"\n-    return len(re.findall(delim_list, value))\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def delim_hash(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n-    r\"\"\"\n-    Return a hash (CRC32) of the delimiters from input column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    delim_list : str, optional\n-        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n-\n-    Returns\n-    -------\n-    int\n-        Hash of delimiter set in the string.\n-\n-    \"\"\"\n-    return crc32(bytes(\"\".join(re.findall(delim_list, value)), \"utf-8\"))\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def char_ord_score(value: str, scale: int = 1) -> int:\n-    \"\"\"\n-    Return sum of ord values of characters in string.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    scale : int, optional\n-        reduce the scale of the feature (reducing the\n-        influence of variations this feature on the clustering\n-        algorithm (the default is 1)\n-\n-    Returns\n-    -------\n-    int\n-        [description]\n-\n-    Notes\n-    -----\n-    This function sums the ordinal value of each character in the\n-    input string. Two strings with minor differences will result in\n-    a similar score. However, for strings with highly variable content\n-    (e.g. command lines or http requests containing GUIDs) this may result\n-    in too much variance to be useful when you are trying to detect\n-    similar patterns. You can use the scale parameter to reduce the\n-    influence of features using this function on clustering and anomaly\n-    algorithms.\n-\n-    \"\"\"\n-    return floor(sum(ord(x) for x in value) / scale)\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def token_count(value: str, delimiter: str = \" \") -> int:\n-    \"\"\"\n-    Return count of delimiter-separated tokens pd.Series column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    delimiter : str, optional\n-        Delimiter used to split the column string.\n-        (the default is ' ')\n-\n-    Returns\n-    -------\n-    int\n-        count of tokens\n-\n-    \"\"\"\n-    return len(value.split(delimiter))\n-\n-\n-def _string_score(input_str):\n-    \"\"\"Sum the ord(c) for characters in a string.\"\"\"\n-    return sum(ord(x) for x in input_str)\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def crc32_hash(value: str) -> int:\n-    \"\"\"\n-    Return the CRC32 hash of the input column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-\n-    Returns\n-    -------\n-    int\n-        CRC32 hash\n-\n-    \"\"\"\n-    return crc32(bytes(value.encode(\"utf-8\")))\n-\n-\n-def delim_count_df(\n-    data: pd.DataFrame, column: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]'\n-) -> pd.Series:\n-    r\"\"\"\n-    Count the delimiters in input column.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        The name of the column to process\n-    delim_list : str, optional\n-        delimiters to use. (the default is r\\'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]\\')\n-\n-    Returns\n-    -------\n-    pd.Series\n-        Count of delimiters in the string in `column`.\n-\n-    \"\"\"\n-    return data[column].str.count(delim_list)\n-\n-\n-def char_ord_score_df(data: pd.DataFrame, column: str, scale: int = 1) -> pd.Series:\n-    \"\"\"\n-    Return sum of ord values of characters in string.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        Column name to process\n-    scale : int, optional\n-        reduce the scale of the feature (reducing the\n-        influence of variations this feature on the clustering\n-        algorithm (the default is 1)\n-\n-    Returns\n-    -------\n-    pd.Series\n-        The sum of the ordinal values of the characters\n-        in `column`.\n-\n-    Notes\n-    -----\n-    This function sums the ordinal value of each character in the\n-    input string. Two strings with minor differences will result in\n-    a similar score. However, for strings with highly variable content\n-    (e.g. command lines or http requests containing GUIDs) this may result\n-    in too much variance to be useful when you are trying to detect\n-    similar patterns. You can use the scale parameter to reduce the\n-    influence of features using this function on clustering and anomaly\n-    algorithms.\n-\n-    \"\"\"\n-    return data.apply(lambda x: sum(ord(char) for char in x[column]) / scale, axis=1)\n-\n-\n-def token_count_df(data: pd.DataFrame, column: str, delimiter: str = \" \") -> pd.Series:\n-    \"\"\"\n-    Return count of delimiter-separated tokens pd.Series column.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        Column name to process\n-    delimiter : str, optional\n-        Delimiter used to split the column string.\n-        (the default is ' ')\n-\n-    Returns\n-    -------\n-    pd.Series\n-        count of tokens in strings in `column`\n-\n-    \"\"\"\n-    return data.apply(lambda x: len(x[column].split(delimiter)), axis=1)\n-\n-\n-def crc32_hash_df(data: pd.DataFrame, column: str) -> pd.Series:\n-    \"\"\"\n-    Return the CRC32 hash of the input column.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        Column name to process\n-\n-    Returns\n-    -------\n-    pd.Series\n-        CRC32 hash of input column\n-\n-    \"\"\"\n-    return data.apply(lambda x: crc32(bytes(x[column].encode(\"utf-8\"))), axis=1)\n-\n-\n-# pylint: disable=too-many-arguments, too-many-statements\n-@export  # noqa: C901, MC0001\n-def plot_cluster(\n-    db_cluster: DBSCAN,\n-    data: pd.DataFrame,\n-    x_predict: np.ndarray,\n-    plot_label: str = None,\n-    plot_features: Tuple[int, int] = (0, 1),\n-    verbose: bool = False,\n-    cut_off: int = 3,\n-    xlabel: str = None,\n-    ylabel: str = None,\n-):\n-    \"\"\"\n-    Plot clustered data as scatter chart.\n-\n-    Parameters\n-    ----------\n-    db_cluster : DBSCAN\n-        DBScan Cluster (from SkLearn DBSCAN).\n-    data : pd.DataFrame\n-        Dataframe containing original data.\n-    x_predict : np.ndarray\n-        The DBSCAN predict numpy array\n-    plot_label : str, optional\n-         If set the column to use to label data points\n-         (the default is None)\n-    plot_features :  Tuple[int, int], optional\n-        Which two features in x_predict to plot (the default is (0, 1))\n-    verbose : bool, optional\n-        Verbose execution with some extra info\n-        (the default is False)\n-    cut_off : int, optional\n-        The cluster size below which items are considered outliers\n-        (the default is 3)\n-    xlabel : str, optional\n-        x-axis label (the default is None)\n-    ylabel : str, optional\n-        y-axis label (the default is None)\n-\n-    \"\"\"\n-    max_idx = x_predict.shape[1] - 1\n-    if plot_features[0] >= x_predict.shape[1]:\n-        raise ValueError(\n-            \"plot_features[0] index must be a value from 0 to {}.\".format(max_idx)\n-        )\n-    if plot_features[1] >= x_predict.shape[1]:\n-        raise ValueError(\n-            \"plot_features[1] index must be a value from 0 to {}.\".format(max_idx)\n-        )\n-    if plot_features[0] == plot_features[1]:\n-        mssg = \"plot_features indexes must be 2 different values in range 0 to\"\n-        raise ValueError(mssg + f\" {max_idx}.\")\n-\n-    labels = db_cluster.labels_\n-    core_samples_mask = np.zeros_like(labels, dtype=bool)\n-\n-    # pylint: disable=unsupported-assignment-operation\n-    # (assignment of numpy array is valid)\n-    core_samples_mask[db_cluster.core_sample_indices_] = True\n-    unique_labels = set(labels)\n-\n-    # pylint: disable=no-member\n-    # Spectral color map does exist\n-    colors = [cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n-    # Number of clusters in labels, ignoring noise if present.\n-    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n-    n_noise_ = list(labels).count(-1)\n-    _, counts = np.unique(labels, return_counts=True)\n-\n-    if verbose:\n-        print(\"Estimated number of clusters: %d\" % n_clusters_)\n-        print(\"Estimated number of noise points: %d\" % n_noise_)\n-        # print(\"Silhouette Coefficient: %0.3f\"\n-        #       % metrics.silhouette_score(x_predict, labels))\n-\n-    if not isinstance(data, pd.DataFrame):\n-        plot_label = None\n-    elif plot_label is not None and plot_label not in data:\n-        plot_label = None\n-\n-    p_label = None\n-    for cluster_id, color in zip(unique_labels, colors):\n-        if cluster_id == -1:\n-            # Black used for noise.\n-            color = [0, 0, 0, 1]\n-        class_member_mask = labels == cluster_id\n-\n-        cluster_size = counts[cluster_id]\n-        marker_size = cluster_size\n-        marker = \"o\"\n-        font_size = \"small\"\n-        alpha = 0.4\n-\n-        if cluster_size < cut_off:\n-            marker = \"+\"\n-            marker_size = 10\n-            font_size = \"large\"\n-            alpha = 1.0\n-        xy_pos = x_predict[class_member_mask & core_samples_mask]\n-        plt.plot(\n-            xy_pos[:, plot_features[0]],\n-            xy_pos[:, plot_features[1]],\n-            marker,\n-            markerfacecolor=tuple(color),\n-            markersize=marker_size,\n-        )\n-\n-        if plot_label:\n-            first_row = data[class_member_mask].iloc[0]\n-            if not first_row.empty and plot_label in first_row:\n-                p_label = first_row[plot_label]\n-                try:\n-                    plt.annotate(\n-                        p_label,\n-                        xy=(xy_pos[0, plot_features[0]], xy_pos[0, plot_features[1]]),\n-                        fontsize=font_size,\n-                        alpha=alpha,\n-                    )\n-                except IndexError:\n-                    pass\n-\n-    plt.xlabel(xlabel)\n-    plt.ylabel(ylabel)\n-    plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n-    plt.show()\n-    return plt\n+WARN_MSSG = (\n+    \"This module has moved to msticpy.analysis.eventcluster\\n\"\n+    + \"Please change your import to reflect this new location.\"\n+)\n+warnings.warn(WARN_MSSG, category=DeprecationWarning)\n",
        "source_code_with_indent": "\n# pylint: disable=too-many-arguments, too-many-locals\n@export\ndef dbcluster_events(\n    data: Any,\n    cluster_columns: List[Any] = None,\n    verbose: bool = False,\n    normalize: bool = True,\n    time_column: str = \"TimeCreatedUtc\",\n    max_cluster_distance: float = 0.01,\n    min_cluster_samples: int = 2,\n    **kwargs,\n) -> Tuple[pd.DataFrame, DBSCAN, np.ndarray]:\n    <IND>\"\"\"\n    Cluster data set according to cluster_columns features.\n\n    Parameters\n    ----------\n    data : Any\n        Input data as a pandas DataFrame or numpy array\n    cluster_columns : List[Any], optional\n        List of columns to use for features\n        - for DataFrame this is a list of column names\n        - for numpy array this is a list of column indexes\n    verbose : bool, optional\n        Print additional information about clustering results (the default is False)\n    normalize : bool, optional\n        Normalize the input data (should probably always be True)\n    time_column : str, optional\n        If there is a time column the output data will be ordered by this\n        (the default is 'TimeCreatedUtc')\n    max_cluster_distance : float, optional\n        DBSCAN eps (max cluster member distance) (the default is 0.01)\n    min_cluster_samples : int, optional\n        DBSCAN min_samples (the minimum cluster size) (the default is 2)\n\n    Other Parameters\n    ----------------\n    kwargs: Other arguments are passed to DBSCAN constructor\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, DBSCAN, np.ndarray]\n        Output dataframe with clustered rows\n        DBSCAN model\n        Normalized data set\n\n    \"\"\"\n    allowed_types = [np.ndarray, pd.DataFrame]\n\n    x_input = None\n    if isinstance(data, pd.DataFrame):\n        <IND>if cluster_columns is None:\n            <IND>x_input = data.values\n        <DED>else:\n            <IND>x_input = data[cluster_columns].values\n    <DED><DED>elif isinstance(data, np.ndarray):\n        <IND>x_input = data if cluster_columns is None else data[:, cluster_columns].values\n    <DED>if x_input is None:\n        <IND>mssg = \"Input data not in expected format.\\n{} is not one of allowed types {}\"\n        type_list = \", \".join([str(t) for t in allowed_types])\n        mssg = mssg.format(str(type(data)), type_list)\n        raise ValueError(mssg)\n\n    # Create DBSCAN cluster object\n    <DED>db_cluster = DBSCAN(\n        eps=max_cluster_distance, min_samples=min_cluster_samples, **kwargs\n    )\n\n    # Normalize the data (most clustering algorithms don't do well with\n    # unnormalized data)\n    x_norm = Normalizer().fit_transform(x_input) if normalize else x_input\n    # fit the data set\n    db_cluster.fit(x_norm)\n    labels = db_cluster.labels_\n    cluster_set, counts = np.unique(labels, return_counts=True)\n    if verbose:\n        <IND>print(\n            \"Clustering for set size \",\n            len(x_norm),\n            \" - \",\n            len(cluster_set),\n            \" clusters\",\n        )\n        print(\"Individual cluster sizes: \", \", \".join([str(c) for c in counts]))\n\n    <DED>clustered_events = _merge_clustered_items(\n        cluster_set, labels, data, time_column, counts\n    )\n\n    if verbose:\n        <IND>print(\"Cluster output rows: \", len(clustered_events))\n\n    <DED>return clustered_events, db_cluster, x_norm\n\n\n<DED>def _merge_clustered_items(\n    cluster_set: np.array,\n    labels: np.array,\n    data: Union[pd.DataFrame, np.array],\n    time_column: str,\n    counts: np.array,\n) -> pd.DataFrame:\n    <IND>\"\"\"\n    Merge outliers and core clusters into single DataFrame.\n\n    Parameters\n    ----------\n    cluster_set : np.array\n        The set of clusters\n    labels : np.array\n        The cluster labels\n    data : Union[pd.DataFrame, np.array]\n        The source data\n    time_column : str\n        Name of the Time column\n    counts : np.array\n        The counts of members in each cluster\n\n    Returns\n    -------\n    pd.DataFrame\n        Merged dataframe\n\n    \"\"\"\n    tz_aware = data.iloc[0][time_column].tz\n    ts_type = \"datetime64[ns, UTC]\" if tz_aware is not None else \"datetime64[ns]\"\n\n    cluster_list = []\n    # Iterate through clusters, adding exemplar to output frame\n    # pylint: disable=consider-using-enumerate\n    # we need to know the index of the item within the loop\n    for idx in range(len(cluster_set)):\n        <IND>cluster_id = cluster_set[idx]\n        class_members = labels == cluster_id\n        if isinstance(data, pd.DataFrame):\n            <IND>time_ordered = data[class_members].sort_values(time_column, ascending=True)\n            first_event_time = time_ordered[0:][time_column].iat[0]\n            last_event_time = time_ordered[-1:][time_column].iat[0]\n        <DED>else:\n            <IND>first_event_time = None\n            last_event_time = None\n\n        <DED>if cluster_id == -1:\n            # 'Noise' events are individual items that could not be assigned\n            # to a cluster and so are unique\n            <IND>cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=False,\n                    ClusterId=cluster_id,\n                    ClusterSize=1,\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n        <DED>else:\n            # Otherwise, just choose the first example of the cluster set\n            <IND>cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=True,\n                    ClusterId=cluster_id,\n                    ClusterSize=counts[idx],\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )[0:1]\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n    # pylint: enable=consider-using-enumerate\n    <DED><DED>return pd.concat(cluster_list)\n\n\n<DED>@export\ndef add_process_features(\n    input_frame: pd.DataFrame, path_separator: str = None, force: bool = False\n) -> pd.DataFrame:\n    <IND>r\"\"\"\n    Add numerical features based on patterns of command line and process name.\n\n    Parameters\n    ----------\n    input_frame : pd.DataFrame\n        The input dataframe\n    path_separator : str, optional\n        Path separator. If not supplied, try to determine\n        from 'NewProcessName' column of first 10 rows\n        (the default is None)\n    force : bool, optional\n        Forces re-calculation of feature columns even if they\n        already exist (the default is False)\n\n    Returns\n    -------\n    pd.DataFrame\n        Copy of the dataframe with the additional numeric features\n\n    Notes\n    -----\n    Features added:\n\n    - processNameLen: length of process file name (inc path)\n    - processNameTokens: the number of elements in the path\n    - processName: the process file name (minus path)\n    - commandlineTokens: number of space-separated tokens in the command line\n    - commandlineLen: length of the command line\n    - commandlineLogLen: log10 length of commandline\n    - isSystemSession: 1 if session Id is 0x3e7 for Windows or -1 for Linux\n    - commandlineTokensFull: counts number of token separators in commandline\n      [\\\\s\\-\\\\/\\.,\"\\'\\|&:;%$()]\n    - pathScore: sum of ord() value of characters in path\n    - pathLogScore: log10 of pathScore\n    - commandlineScore: sum of ord() value of characters in commandline\n    - commandlineLogScore: log10 of commandlineScore\n\n    \"\"\"\n    output_df = input_frame.copy()\n\n    # Set any NaN values to empty string\n    if \"NewProcessName\" in output_df and \"CommandLine\" in output_df:\n        <IND>output_df[[\"NewProcessName\", \"CommandLine\"]] = output_df[\n            [\"NewProcessName\", \"CommandLine\"]\n        ].fillna(value=\"\")\n\n    # try to determine the path separator\n    <DED>if path_separator is None:\n        <IND>sample_df = output_df.head(10)\n        lx_path = len(sample_df[sample_df[\"NewProcessName\"].str.contains(\"/\")])\n        path_separator = \"/\" if lx_path else \"\\\\\"\n    # Create features from process name and command line\n    <DED>if \"NewProcessName\" in output_df:\n        <IND>_add_processname_features(output_df, force, path_separator)\n\n    <DED>if \"CommandLine\" in output_df:\n        <IND>_add_commandline_features(output_df, force)\n\n    <DED>if \"SubjectLogonId\" in output_df and (\"isSystemSession\" not in output_df or force):\n        <IND>output_df[\"isSystemSession\"] = output_df[\"SubjectLogonId\"].isin([\"0x3e7\", \"-1\"])\n\n    <DED>return output_df\n\n\n<DED>def _add_processname_features(\n    output_df: pd.DataFrame, force: bool, path_separator: str\n):\n    <IND>\"\"\"\n    Add process name default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n    path_separator : str\n        Path separator for OS\n\n    \"\"\"\n    if \"processName\" not in output_df or force:\n        <IND>output_df[\"processName\"] = output_df.apply(\n            lambda x: x.NewProcessName.split(path_separator)[-1], axis=1\n        )\n    <DED>if \"pathScore\" not in output_df or force:\n        <IND>output_df[\"pathScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.NewProcessName), axis=1\n        )\n    <DED>if \"pathLogScore\" not in output_df or force:\n        <IND>output_df[\"pathLogScore\"] = output_df.apply(\n            lambda x: log10(x.pathScore) if x.pathScore else 0, axis=1\n        )\n    <DED>if \"pathHash\" not in output_df or force:\n        <IND>output_df[\"pathHash\"] = output_df.apply(\n            lambda x: crc32_hash(x.NewProcessName), axis=1\n        )\n\n\n<DED><DED>def _add_commandline_features(output_df: pd.DataFrame, force: bool):\n    <IND>\"\"\"\n    Add commandline default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n\n    \"\"\"\n    if \"commandlineLen\" not in output_df or force:\n        <IND>output_df[\"commandlineLen\"] = output_df.apply(\n            lambda x: len(x.CommandLine), axis=1\n        )\n    <DED>if \"commandlineLogLen\" not in output_df or force:\n        <IND>output_df[\"commandlineLogLen\"] = output_df.apply(\n            lambda x: log10(x.commandlineLen) if x.commandlineLen else 0, axis=1\n        )\n    <DED>if \"commandlineTokensFull\" not in output_df or force:\n        <IND>output_df[\"commandlineTokensFull\"] = output_df[[\"CommandLine\"]].apply(\n            lambda x: delim_count(x.CommandLine), axis=1\n        )\n\n    <DED>if \"commandlineScore\" not in output_df or force:\n        <IND>output_df[\"commandlineScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.CommandLine), axis=1\n        )\n    <DED>if \"commandlineTokensHash\" not in output_df or force:\n        <IND>output_df[\"commandlineTokensHash\"] = output_df.apply(\n            lambda x: delim_hash(x.CommandLine), axis=1\n        )\n\n\n<DED><DED>@export\n@lru_cache(maxsize=1024)\ndef delim_count(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    <IND>r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Count of delimiters in the string.\n\n    \"\"\"\n    return len(re.findall(delim_list, value))\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef delim_hash(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    <IND>r\"\"\"\n    Return a hash (CRC32) of the delimiters from input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Hash of delimiter set in the string.\n\n    \"\"\"\n    return crc32(bytes(\"\".join(re.findall(delim_list, value)), \"utf-8\"))\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef char_ord_score(value: str, scale: int = 1) -> int:\n    <IND>\"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    int\n        [description]\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return floor(sum(ord(x) for x in value) / scale)\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef token_count(value: str, delimiter: str = \" \") -> int:\n    <IND>\"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    int\n        count of tokens\n\n    \"\"\"\n    return len(value.split(delimiter))\n\n\n<DED>def _string_score(input_str):\n    <IND>\"\"\"Sum the ord(c) for characters in a string.\"\"\"\n    return sum(ord(x) for x in input_str)\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef crc32_hash(value: str) -> int:\n    <IND>\"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n\n    Returns\n    -------\n    int\n        CRC32 hash\n\n    \"\"\"\n    return crc32(bytes(value.encode(\"utf-8\")))\n\n\n<DED>def delim_count_df(\n    data: pd.DataFrame, column: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]'\n) -> pd.Series:\n    <IND>r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        The name of the column to process\n    delim_list : str, optional\n        delimiters to use. (the default is r\\'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]\\')\n\n    Returns\n    -------\n    pd.Series\n        Count of delimiters in the string in `column`.\n\n    \"\"\"\n    return data[column].str.count(delim_list)\n\n\n<DED>def char_ord_score_df(data: pd.DataFrame, column: str, scale: int = 1) -> pd.Series:\n    <IND>\"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    pd.Series\n        The sum of the ordinal values of the characters\n        in `column`.\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return data.apply(lambda x: sum(ord(char) for char in x[column]) / scale, axis=1)\n\n\n<DED>def token_count_df(data: pd.DataFrame, column: str, delimiter: str = \" \") -> pd.Series:\n    <IND>\"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    pd.Series\n        count of tokens in strings in `column`\n\n    \"\"\"\n    return data.apply(lambda x: len(x[column].split(delimiter)), axis=1)\n\n\n<DED>def crc32_hash_df(data: pd.DataFrame, column: str) -> pd.Series:\n    <IND>\"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n\n    Returns\n    -------\n    pd.Series\n        CRC32 hash of input column\n\n    \"\"\"\n    return data.apply(lambda x: crc32(bytes(x[column].encode(\"utf-8\"))), axis=1)\n\n\n# pylint: disable=too-many-arguments, too-many-statements\n<DED>@export  # noqa: C901, MC0001\ndef plot_cluster(\n    db_cluster: DBSCAN,\n    data: pd.DataFrame,\n    x_predict: np.ndarray,\n    plot_label: str = None,\n    plot_features: Tuple[int, int] = (0, 1),\n    verbose: bool = False,\n    cut_off: int = 3,\n    xlabel: str = None,\n    ylabel: str = None,\n):\n    <IND>\"\"\"\n    Plot clustered data as scatter chart.\n\n    Parameters\n    ----------\n    db_cluster : DBSCAN\n        DBScan Cluster (from SkLearn DBSCAN).\n    data : pd.DataFrame\n        Dataframe containing original data.\n    x_predict : np.ndarray\n        The DBSCAN predict numpy array\n    plot_label : str, optional\n         If set the column to use to label data points\n         (the default is None)\n    plot_features :  Tuple[int, int], optional\n        Which two features in x_predict to plot (the default is (0, 1))\n    verbose : bool, optional\n        Verbose execution with some extra info\n        (the default is False)\n    cut_off : int, optional\n        The cluster size below which items are considered outliers\n        (the default is 3)\n    xlabel : str, optional\n        x-axis label (the default is None)\n    ylabel : str, optional\n        y-axis label (the default is None)\n\n    \"\"\"\n    max_idx = x_predict.shape[1] - 1\n    if plot_features[0] >= x_predict.shape[1]:\n        <IND>raise ValueError(\n            \"plot_features[0] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    <DED>if plot_features[1] >= x_predict.shape[1]:\n        <IND>raise ValueError(\n            \"plot_features[1] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    <DED>if plot_features[0] == plot_features[1]:\n        <IND>mssg = \"plot_features indexes must be 2 different values in range 0 to\"\n        raise ValueError(mssg + f\" {max_idx}.\")\n\n    <DED>labels = db_cluster.labels_\n    core_samples_mask = np.zeros_like(labels, dtype=bool)\n\n    # pylint: disable=unsupported-assignment-operation\n    # (assignment of numpy array is valid)\n    core_samples_mask[db_cluster.core_sample_indices_] = True\n    unique_labels = set(labels)\n\n    # pylint: disable=no-member\n    # Spectral color map does exist\n    colors = [cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n    # Number of clusters in labels, ignoring noise if present.\n    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n    n_noise_ = list(labels).count(-1)\n    _, counts = np.unique(labels, return_counts=True)\n\n    if verbose:\n        <IND>print(\"Estimated number of clusters: %d\" % n_clusters_)\n        print(\"Estimated number of noise points: %d\" % n_noise_)\n        # print(\"Silhouette Coefficient: %0.3f\"\n        #       % metrics.silhouette_score(x_predict, labels))\n\n    <DED>if not isinstance(data, pd.DataFrame):\n        <IND>plot_label = None\n    <DED>elif plot_label is not None and plot_label not in data:\n        <IND>plot_label = None\n\n    <DED>p_label = None\n    for cluster_id, color in zip(unique_labels, colors):\n        <IND>if cluster_id == -1:\n            # Black used for noise.\n            <IND>color = [0, 0, 0, 1]\n        <DED>class_member_mask = labels == cluster_id\n\n        cluster_size = counts[cluster_id]\n        marker_size = cluster_size\n        marker = \"o\"\n        font_size = \"small\"\n        alpha = 0.4\n\n        if cluster_size < cut_off:\n            <IND>marker = \"+\"\n            marker_size = 10\n            font_size = \"large\"\n            alpha = 1.0\n        <DED>xy_pos = x_predict[class_member_mask & core_samples_mask]\n        plt.plot(\n            xy_pos[:, plot_features[0]],\n            xy_pos[:, plot_features[1]],\n            marker,\n            markerfacecolor=tuple(color),\n            markersize=marker_size,\n        )\n\n        if plot_label:\n            <IND>first_row = data[class_member_mask].iloc[0]\n            if not first_row.empty and plot_label in first_row:\n                <IND>p_label = first_row[plot_label]\n                try:\n                    <IND>plt.annotate(\n                        p_label,\n                        xy=(xy_pos[0, plot_features[0]], xy_pos[0, plot_features[1]]),\n                        fontsize=font_size,\n                        alpha=alpha,\n                    )\n                <DED>except IndexError:\n                    <IND>pass\n\n    <DED><DED><DED><DED>plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n    plt.show()\n    return plt\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n# flake8: noqa: F403, F401\n# pylint: disable=wildcard-import, unused-wildcard-import, unused-import\nfrom ..analysis.eventcluster import *\n\nWARN_MSSG = (\n    \"This module has moved to msticpy.analysis.eventcluster\\n\"\n    + \"Please change your import to reflect this new location.\"\n)\nwarnings.warn(WARN_MSSG, category=DeprecationWarning)\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "microsoft/msticpy",
    "commit": "28466d681e261394e18d9f4e063cebfa06ed04b4",
    "filename": "msticpy/sectools/eventcluster.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/microsoft-msticpy/msticpy/sectools/eventcluster.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "msticpy/sectools/eventcluster.py:685:8 Incompatible variable type [9]: plot_label is declared to have type `str` but is used as type `None`.",
    "message": " plot_label is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 685,
    "warning_line": "        plot_label = None",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "# --------------------------------------------------------------------------\nr\"\"\"\neventcluster module.\n\nThis module is intended to be used to summarize large numbers of events\ninto clusters of different patterns. High volume repeating events can\noften make it difficult to see unique and interesting items.\n\nThe module contains functions to generate clusterable features from\nstring data. For example, an administration command that does some\nmaintenance on thousands of servers with a commandline such as:\n``install-update -hostname {host.fqdn} -tmp:/tmp/{GUID}/rollback``\\  can\nbe collapsed into a single cluster pattern by ignoring the character\nvalues in the string and using delimiters or tokens to group the values.\n\nThis is an unsupervised learning module implemented using SciKit Learn\nDBScan.\n\nContains:\ndbcluster_events: generic clustering method using DBSCAN designed to summarize\nprocess events and other similar data by grouping on common features.\n\nadd_process_features: derives numerical features from text features such as\ncommandline and process path.\n\n\"\"\"\nfrom binascii import crc32\nfrom functools import lru_cache\nfrom math import log10, floor\nimport re\nfrom typing import List, Any, Tuple, Union\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import Normalizer\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nfrom ..common.utility import export\nfrom .._version import VERSION\n",
        "source_code_len": 1462,
        "target_code": "# --------------------------------------------------------------------------\n\"\"\"Deprecated placeholder for eventcluster.py.\"\"\"\nimport warnings\nfrom .._version import VERSION\n",
        "target_code_len": 174,
        "diff_format": "@@ -5,41 +5,4 @@\n # --------------------------------------------------------------------------\n-r\"\"\"\n-eventcluster module.\n-\n-This module is intended to be used to summarize large numbers of events\n-into clusters of different patterns. High volume repeating events can\n-often make it difficult to see unique and interesting items.\n-\n-The module contains functions to generate clusterable features from\n-string data. For example, an administration command that does some\n-maintenance on thousands of servers with a commandline such as:\n-``install-update -hostname {host.fqdn} -tmp:/tmp/{GUID}/rollback``\\  can\n-be collapsed into a single cluster pattern by ignoring the character\n-values in the string and using delimiters or tokens to group the values.\n-\n-This is an unsupervised learning module implemented using SciKit Learn\n-DBScan.\n-\n-Contains:\n-dbcluster_events: generic clustering method using DBSCAN designed to summarize\n-process events and other similar data by grouping on common features.\n-\n-add_process_features: derives numerical features from text features such as\n-commandline and process path.\n-\n-\"\"\"\n-from binascii import crc32\n-from functools import lru_cache\n-from math import log10, floor\n-import re\n-from typing import List, Any, Tuple, Union\n-\n-import numpy as np\n-import pandas as pd\n-from sklearn.cluster import DBSCAN\n-from sklearn.preprocessing import Normalizer\n-import matplotlib.pyplot as plt\n-from matplotlib import cm\n-\n-from ..common.utility import export\n+\"\"\"Deprecated placeholder for eventcluster.py.\"\"\"\n+import warnings\n from .._version import VERSION\n",
        "source_code_with_indent": "# --------------------------------------------------------------------------\nr\"\"\"\neventcluster module.\n\nThis module is intended to be used to summarize large numbers of events\ninto clusters of different patterns. High volume repeating events can\noften make it difficult to see unique and interesting items.\n\nThe module contains functions to generate clusterable features from\nstring data. For example, an administration command that does some\nmaintenance on thousands of servers with a commandline such as:\n``install-update -hostname {host.fqdn} -tmp:/tmp/{GUID}/rollback``\\  can\nbe collapsed into a single cluster pattern by ignoring the character\nvalues in the string and using delimiters or tokens to group the values.\n\nThis is an unsupervised learning module implemented using SciKit Learn\nDBScan.\n\nContains:\ndbcluster_events: generic clustering method using DBSCAN designed to summarize\nprocess events and other similar data by grouping on common features.\n\nadd_process_features: derives numerical features from text features such as\ncommandline and process path.\n\n\"\"\"\nfrom binascii import crc32\nfrom functools import lru_cache\nfrom math import log10, floor\nimport re\nfrom typing import List, Any, Tuple, Union\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import Normalizer\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nfrom ..common.utility import export\nfrom .._version import VERSION\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "# --------------------------------------------------------------------------\n\"\"\"Deprecated placeholder for eventcluster.py.\"\"\"\nimport warnings\nfrom .._version import VERSION\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n# pylint: disable=too-many-arguments, too-many-locals\n@export\ndef dbcluster_events(\n    data: Any,\n    cluster_columns: List[Any] = None,\n    verbose: bool = False,\n    normalize: bool = True,\n    time_column: str = \"TimeCreatedUtc\",\n    max_cluster_distance: float = 0.01,\n    min_cluster_samples: int = 2,\n    **kwargs,\n) -> Tuple[pd.DataFrame, DBSCAN, np.ndarray]:\n    \"\"\"\n    Cluster data set according to cluster_columns features.\n\n    Parameters\n    ----------\n    data : Any\n        Input data as a pandas DataFrame or numpy array\n    cluster_columns : List[Any], optional\n        List of columns to use for features\n        - for DataFrame this is a list of column names\n        - for numpy array this is a list of column indexes\n    verbose : bool, optional\n        Print additional information about clustering results (the default is False)\n    normalize : bool, optional\n        Normalize the input data (should probably always be True)\n    time_column : str, optional\n        If there is a time column the output data will be ordered by this\n        (the default is 'TimeCreatedUtc')\n    max_cluster_distance : float, optional\n        DBSCAN eps (max cluster member distance) (the default is 0.01)\n    min_cluster_samples : int, optional\n        DBSCAN min_samples (the minimum cluster size) (the default is 2)\n\n    Other Parameters\n    ----------------\n    kwargs: Other arguments are passed to DBSCAN constructor\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, DBSCAN, np.ndarray]\n        Output dataframe with clustered rows\n        DBSCAN model\n        Normalized data set\n\n    \"\"\"\n    allowed_types = [np.ndarray, pd.DataFrame]\n\n    x_input = None\n    if isinstance(data, pd.DataFrame):\n        if cluster_columns is None:\n            x_input = data.values\n        else:\n            x_input = data[cluster_columns].values\n    elif isinstance(data, np.ndarray):\n        x_input = data if cluster_columns is None else data[:, cluster_columns].values\n    if x_input is None:\n        mssg = \"Input data not in expected format.\\n{} is not one of allowed types {}\"\n        type_list = \", \".join([str(t) for t in allowed_types])\n        mssg = mssg.format(str(type(data)), type_list)\n        raise ValueError(mssg)\n\n    # Create DBSCAN cluster object\n    db_cluster = DBSCAN(\n        eps=max_cluster_distance, min_samples=min_cluster_samples, **kwargs\n    )\n\n    # Normalize the data (most clustering algorithms don't do well with\n    # unnormalized data)\n    x_norm = Normalizer().fit_transform(x_input) if normalize else x_input\n    # fit the data set\n    db_cluster.fit(x_norm)\n    labels = db_cluster.labels_\n    cluster_set, counts = np.unique(labels, return_counts=True)\n    if verbose:\n        print(\n            \"Clustering for set size \",\n            len(x_norm),\n            \" - \",\n            len(cluster_set),\n            \" clusters\",\n        )\n        print(\"Individual cluster sizes: \", \", \".join([str(c) for c in counts]))\n\n    clustered_events = _merge_clustered_items(\n        cluster_set, labels, data, time_column, counts\n    )\n\n    if verbose:\n        print(\"Cluster output rows: \", len(clustered_events))\n\n    return clustered_events, db_cluster, x_norm\n\n\ndef _merge_clustered_items(\n    cluster_set: np.array,\n    labels: np.array,\n    data: Union[pd.DataFrame, np.array],\n    time_column: str,\n    counts: np.array,\n) -> pd.DataFrame:\n    \"\"\"\n    Merge outliers and core clusters into single DataFrame.\n\n    Parameters\n    ----------\n    cluster_set : np.array\n        The set of clusters\n    labels : np.array\n        The cluster labels\n    data : Union[pd.DataFrame, np.array]\n        The source data\n    time_column : str\n        Name of the Time column\n    counts : np.array\n        The counts of members in each cluster\n\n    Returns\n    -------\n    pd.DataFrame\n        Merged dataframe\n\n    \"\"\"\n    tz_aware = data.iloc[0][time_column].tz\n    ts_type = \"datetime64[ns, UTC]\" if tz_aware is not None else \"datetime64[ns]\"\n\n    cluster_list = []\n    # Iterate through clusters, adding exemplar to output frame\n    # pylint: disable=consider-using-enumerate\n    # we need to know the index of the item within the loop\n    for idx in range(len(cluster_set)):\n        cluster_id = cluster_set[idx]\n        class_members = labels == cluster_id\n        if isinstance(data, pd.DataFrame):\n            time_ordered = data[class_members].sort_values(time_column, ascending=True)\n            first_event_time = time_ordered[0:][time_column].iat[0]\n            last_event_time = time_ordered[-1:][time_column].iat[0]\n        else:\n            first_event_time = None\n            last_event_time = None\n\n        if cluster_id == -1:\n            # 'Noise' events are individual items that could not be assigned\n            # to a cluster and so are unique\n            cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=False,\n                    ClusterId=cluster_id,\n                    ClusterSize=1,\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n        else:\n            # Otherwise, just choose the first example of the cluster set\n            cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=True,\n                    ClusterId=cluster_id,\n                    ClusterSize=counts[idx],\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )[0:1]\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n    # pylint: enable=consider-using-enumerate\n    return pd.concat(cluster_list)\n\n\n@export\ndef add_process_features(\n    input_frame: pd.DataFrame, path_separator: str = None, force: bool = False\n) -> pd.DataFrame:\n    r\"\"\"\n    Add numerical features based on patterns of command line and process name.\n\n    Parameters\n    ----------\n    input_frame : pd.DataFrame\n        The input dataframe\n    path_separator : str, optional\n        Path separator. If not supplied, try to determine\n        from 'NewProcessName' column of first 10 rows\n        (the default is None)\n    force : bool, optional\n        Forces re-calculation of feature columns even if they\n        already exist (the default is False)\n\n    Returns\n    -------\n    pd.DataFrame\n        Copy of the dataframe with the additional numeric features\n\n    Notes\n    -----\n    Features added:\n\n    - processNameLen: length of process file name (inc path)\n    - processNameTokens: the number of elements in the path\n    - processName: the process file name (minus path)\n    - commandlineTokens: number of space-separated tokens in the command line\n    - commandlineLen: length of the command line\n    - commandlineLogLen: log10 length of commandline\n    - isSystemSession: 1 if session Id is 0x3e7 for Windows or -1 for Linux\n    - commandlineTokensFull: counts number of token separators in commandline\n      [\\\\s\\-\\\\/\\.,\"\\'\\|&:;%$()]\n    - pathScore: sum of ord() value of characters in path\n    - pathLogScore: log10 of pathScore\n    - commandlineScore: sum of ord() value of characters in commandline\n    - commandlineLogScore: log10 of commandlineScore\n\n    \"\"\"\n    output_df = input_frame.copy()\n\n    # Set any NaN values to empty string\n    if \"NewProcessName\" in output_df and \"CommandLine\" in output_df:\n        output_df[[\"NewProcessName\", \"CommandLine\"]] = output_df[\n            [\"NewProcessName\", \"CommandLine\"]\n        ].fillna(value=\"\")\n\n    # try to determine the path separator\n    if path_separator is None:\n        sample_df = output_df.head(10)\n        lx_path = len(sample_df[sample_df[\"NewProcessName\"].str.contains(\"/\")])\n        path_separator = \"/\" if lx_path else \"\\\\\"\n    # Create features from process name and command line\n    if \"NewProcessName\" in output_df:\n        _add_processname_features(output_df, force, path_separator)\n\n    if \"CommandLine\" in output_df:\n        _add_commandline_features(output_df, force)\n\n    if \"SubjectLogonId\" in output_df and (\"isSystemSession\" not in output_df or force):\n        output_df[\"isSystemSession\"] = output_df[\"SubjectLogonId\"].isin([\"0x3e7\", \"-1\"])\n\n    return output_df\n\n\ndef _add_processname_features(\n    output_df: pd.DataFrame, force: bool, path_separator: str\n):\n    \"\"\"\n    Add process name default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n    path_separator : str\n        Path separator for OS\n\n    \"\"\"\n    if \"processName\" not in output_df or force:\n        output_df[\"processName\"] = output_df.apply(\n            lambda x: x.NewProcessName.split(path_separator)[-1], axis=1\n        )\n    if \"pathScore\" not in output_df or force:\n        output_df[\"pathScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.NewProcessName), axis=1\n        )\n    if \"pathLogScore\" not in output_df or force:\n        output_df[\"pathLogScore\"] = output_df.apply(\n            lambda x: log10(x.pathScore) if x.pathScore else 0, axis=1\n        )\n    if \"pathHash\" not in output_df or force:\n        output_df[\"pathHash\"] = output_df.apply(\n            lambda x: crc32_hash(x.NewProcessName), axis=1\n        )\n\n\ndef _add_commandline_features(output_df: pd.DataFrame, force: bool):\n    \"\"\"\n    Add commandline default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n\n    \"\"\"\n    if \"commandlineLen\" not in output_df or force:\n        output_df[\"commandlineLen\"] = output_df.apply(\n            lambda x: len(x.CommandLine), axis=1\n        )\n    if \"commandlineLogLen\" not in output_df or force:\n        output_df[\"commandlineLogLen\"] = output_df.apply(\n            lambda x: log10(x.commandlineLen) if x.commandlineLen else 0, axis=1\n        )\n    if \"commandlineTokensFull\" not in output_df or force:\n        output_df[\"commandlineTokensFull\"] = output_df[[\"CommandLine\"]].apply(\n            lambda x: delim_count(x.CommandLine), axis=1\n        )\n\n    if \"commandlineScore\" not in output_df or force:\n        output_df[\"commandlineScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.CommandLine), axis=1\n        )\n    if \"commandlineTokensHash\" not in output_df or force:\n        output_df[\"commandlineTokensHash\"] = output_df.apply(\n            lambda x: delim_hash(x.CommandLine), axis=1\n        )\n\n\n@export\n@lru_cache(maxsize=1024)\ndef delim_count(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Count of delimiters in the string.\n\n    \"\"\"\n    return len(re.findall(delim_list, value))\n\n\n@export\n@lru_cache(maxsize=1024)\ndef delim_hash(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    r\"\"\"\n    Return a hash (CRC32) of the delimiters from input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Hash of delimiter set in the string.\n\n    \"\"\"\n    return crc32(bytes(\"\".join(re.findall(delim_list, value)), \"utf-8\"))\n\n\n@export\n@lru_cache(maxsize=1024)\ndef char_ord_score(value: str, scale: int = 1) -> int:\n    \"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    int\n        [description]\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return floor(sum(ord(x) for x in value) / scale)\n\n\n@export\n@lru_cache(maxsize=1024)\ndef token_count(value: str, delimiter: str = \" \") -> int:\n    \"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    int\n        count of tokens\n\n    \"\"\"\n    return len(value.split(delimiter))\n\n\ndef _string_score(input_str):\n    \"\"\"Sum the ord(c) for characters in a string.\"\"\"\n    return sum(ord(x) for x in input_str)\n\n\n@export\n@lru_cache(maxsize=1024)\ndef crc32_hash(value: str) -> int:\n    \"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n\n    Returns\n    -------\n    int\n        CRC32 hash\n\n    \"\"\"\n    return crc32(bytes(value.encode(\"utf-8\")))\n\n\ndef delim_count_df(\n    data: pd.DataFrame, column: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]'\n) -> pd.Series:\n    r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        The name of the column to process\n    delim_list : str, optional\n        delimiters to use. (the default is r\\'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]\\')\n\n    Returns\n    -------\n    pd.Series\n        Count of delimiters in the string in `column`.\n\n    \"\"\"\n    return data[column].str.count(delim_list)\n\n\ndef char_ord_score_df(data: pd.DataFrame, column: str, scale: int = 1) -> pd.Series:\n    \"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    pd.Series\n        The sum of the ordinal values of the characters\n        in `column`.\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return data.apply(lambda x: sum(ord(char) for char in x[column]) / scale, axis=1)\n\n\ndef token_count_df(data: pd.DataFrame, column: str, delimiter: str = \" \") -> pd.Series:\n    \"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    pd.Series\n        count of tokens in strings in `column`\n\n    \"\"\"\n    return data.apply(lambda x: len(x[column].split(delimiter)), axis=1)\n\n\ndef crc32_hash_df(data: pd.DataFrame, column: str) -> pd.Series:\n    \"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n\n    Returns\n    -------\n    pd.Series\n        CRC32 hash of input column\n\n    \"\"\"\n    return data.apply(lambda x: crc32(bytes(x[column].encode(\"utf-8\"))), axis=1)\n\n\n# pylint: disable=too-many-arguments, too-many-statements\n@export  # noqa: C901, MC0001\ndef plot_cluster(\n    db_cluster: DBSCAN,\n    data: pd.DataFrame,\n    x_predict: np.ndarray,\n    plot_label: str = None,\n    plot_features: Tuple[int, int] = (0, 1),\n    verbose: bool = False,\n    cut_off: int = 3,\n    xlabel: str = None,\n    ylabel: str = None,\n):\n    \"\"\"\n    Plot clustered data as scatter chart.\n\n    Parameters\n    ----------\n    db_cluster : DBSCAN\n        DBScan Cluster (from SkLearn DBSCAN).\n    data : pd.DataFrame\n        Dataframe containing original data.\n    x_predict : np.ndarray\n        The DBSCAN predict numpy array\n    plot_label : str, optional\n         If set the column to use to label data points\n         (the default is None)\n    plot_features :  Tuple[int, int], optional\n        Which two features in x_predict to plot (the default is (0, 1))\n    verbose : bool, optional\n        Verbose execution with some extra info\n        (the default is False)\n    cut_off : int, optional\n        The cluster size below which items are considered outliers\n        (the default is 3)\n    xlabel : str, optional\n        x-axis label (the default is None)\n    ylabel : str, optional\n        y-axis label (the default is None)\n\n    \"\"\"\n    max_idx = x_predict.shape[1] - 1\n    if plot_features[0] >= x_predict.shape[1]:\n        raise ValueError(\n            \"plot_features[0] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    if plot_features[1] >= x_predict.shape[1]:\n        raise ValueError(\n            \"plot_features[1] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    if plot_features[0] == plot_features[1]:\n        mssg = \"plot_features indexes must be 2 different values in range 0 to\"\n        raise ValueError(mssg + f\" {max_idx}.\")\n\n    labels = db_cluster.labels_\n    core_samples_mask = np.zeros_like(labels, dtype=bool)\n\n    # pylint: disable=unsupported-assignment-operation\n    # (assignment of numpy array is valid)\n    core_samples_mask[db_cluster.core_sample_indices_] = True\n    unique_labels = set(labels)\n\n    # pylint: disable=no-member\n    # Spectral color map does exist\n    colors = [cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n    # Number of clusters in labels, ignoring noise if present.\n    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n    n_noise_ = list(labels).count(-1)\n    _, counts = np.unique(labels, return_counts=True)\n\n    if verbose:\n        print(\"Estimated number of clusters: %d\" % n_clusters_)\n        print(\"Estimated number of noise points: %d\" % n_noise_)\n        # print(\"Silhouette Coefficient: %0.3f\"\n        #       % metrics.silhouette_score(x_predict, labels))\n\n    if not isinstance(data, pd.DataFrame):\n        plot_label = None\n    elif plot_label is not None and plot_label not in data:\n        plot_label = None\n\n    p_label = None\n    for cluster_id, color in zip(unique_labels, colors):\n        if cluster_id == -1:\n            # Black used for noise.\n            color = [0, 0, 0, 1]\n        class_member_mask = labels == cluster_id\n\n        cluster_size = counts[cluster_id]\n        marker_size = cluster_size\n        marker = \"o\"\n        font_size = \"small\"\n        alpha = 0.4\n\n        if cluster_size < cut_off:\n            marker = \"+\"\n            marker_size = 10\n            font_size = \"large\"\n            alpha = 1.0\n        xy_pos = x_predict[class_member_mask & core_samples_mask]\n        plt.plot(\n            xy_pos[:, plot_features[0]],\n            xy_pos[:, plot_features[1]],\n            marker,\n            markerfacecolor=tuple(color),\n            markersize=marker_size,\n        )\n\n        if plot_label:\n            first_row = data[class_member_mask].iloc[0]\n            if not first_row.empty and plot_label in first_row:\n                p_label = first_row[plot_label]\n                try:\n                    plt.annotate(\n                        p_label,\n                        xy=(xy_pos[0, plot_features[0]], xy_pos[0, plot_features[1]]),\n                        fontsize=font_size,\n                        alpha=alpha,\n                    )\n                except IndexError:\n                    pass\n\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n    plt.show()\n    return plt\n",
        "source_code_len": 21140,
        "target_code": "\n# flake8: noqa: F403, F401\n# pylint: disable=wildcard-import, unused-wildcard-import, unused-import\nfrom ..analysis.eventcluster import *\n\nWARN_MSSG = (\n    \"This module has moved to msticpy.analysis.eventcluster\\n\"\n    + \"Please change your import to reflect this new location.\"\n)\nwarnings.warn(WARN_MSSG, category=DeprecationWarning)\n",
        "target_code_len": 337,
        "diff_format": "@@ -50,683 +13,10 @@\n \n-# pylint: disable=too-many-arguments, too-many-locals\n-@export\n-def dbcluster_events(\n-    data: Any,\n-    cluster_columns: List[Any] = None,\n-    verbose: bool = False,\n-    normalize: bool = True,\n-    time_column: str = \"TimeCreatedUtc\",\n-    max_cluster_distance: float = 0.01,\n-    min_cluster_samples: int = 2,\n-    **kwargs,\n-) -> Tuple[pd.DataFrame, DBSCAN, np.ndarray]:\n-    \"\"\"\n-    Cluster data set according to cluster_columns features.\n+# flake8: noqa: F403, F401\n+# pylint: disable=wildcard-import, unused-wildcard-import, unused-import\n+from ..analysis.eventcluster import *\n \n-    Parameters\n-    ----------\n-    data : Any\n-        Input data as a pandas DataFrame or numpy array\n-    cluster_columns : List[Any], optional\n-        List of columns to use for features\n-        - for DataFrame this is a list of column names\n-        - for numpy array this is a list of column indexes\n-    verbose : bool, optional\n-        Print additional information about clustering results (the default is False)\n-    normalize : bool, optional\n-        Normalize the input data (should probably always be True)\n-    time_column : str, optional\n-        If there is a time column the output data will be ordered by this\n-        (the default is 'TimeCreatedUtc')\n-    max_cluster_distance : float, optional\n-        DBSCAN eps (max cluster member distance) (the default is 0.01)\n-    min_cluster_samples : int, optional\n-        DBSCAN min_samples (the minimum cluster size) (the default is 2)\n-\n-    Other Parameters\n-    ----------------\n-    kwargs: Other arguments are passed to DBSCAN constructor\n-\n-    Returns\n-    -------\n-    Tuple[pd.DataFrame, DBSCAN, np.ndarray]\n-        Output dataframe with clustered rows\n-        DBSCAN model\n-        Normalized data set\n-\n-    \"\"\"\n-    allowed_types = [np.ndarray, pd.DataFrame]\n-\n-    x_input = None\n-    if isinstance(data, pd.DataFrame):\n-        if cluster_columns is None:\n-            x_input = data.values\n-        else:\n-            x_input = data[cluster_columns].values\n-    elif isinstance(data, np.ndarray):\n-        x_input = data if cluster_columns is None else data[:, cluster_columns].values\n-    if x_input is None:\n-        mssg = \"Input data not in expected format.\\n{} is not one of allowed types {}\"\n-        type_list = \", \".join([str(t) for t in allowed_types])\n-        mssg = mssg.format(str(type(data)), type_list)\n-        raise ValueError(mssg)\n-\n-    # Create DBSCAN cluster object\n-    db_cluster = DBSCAN(\n-        eps=max_cluster_distance, min_samples=min_cluster_samples, **kwargs\n-    )\n-\n-    # Normalize the data (most clustering algorithms don't do well with\n-    # unnormalized data)\n-    x_norm = Normalizer().fit_transform(x_input) if normalize else x_input\n-    # fit the data set\n-    db_cluster.fit(x_norm)\n-    labels = db_cluster.labels_\n-    cluster_set, counts = np.unique(labels, return_counts=True)\n-    if verbose:\n-        print(\n-            \"Clustering for set size \",\n-            len(x_norm),\n-            \" - \",\n-            len(cluster_set),\n-            \" clusters\",\n-        )\n-        print(\"Individual cluster sizes: \", \", \".join([str(c) for c in counts]))\n-\n-    clustered_events = _merge_clustered_items(\n-        cluster_set, labels, data, time_column, counts\n-    )\n-\n-    if verbose:\n-        print(\"Cluster output rows: \", len(clustered_events))\n-\n-    return clustered_events, db_cluster, x_norm\n-\n-\n-def _merge_clustered_items(\n-    cluster_set: np.array,\n-    labels: np.array,\n-    data: Union[pd.DataFrame, np.array],\n-    time_column: str,\n-    counts: np.array,\n-) -> pd.DataFrame:\n-    \"\"\"\n-    Merge outliers and core clusters into single DataFrame.\n-\n-    Parameters\n-    ----------\n-    cluster_set : np.array\n-        The set of clusters\n-    labels : np.array\n-        The cluster labels\n-    data : Union[pd.DataFrame, np.array]\n-        The source data\n-    time_column : str\n-        Name of the Time column\n-    counts : np.array\n-        The counts of members in each cluster\n-\n-    Returns\n-    -------\n-    pd.DataFrame\n-        Merged dataframe\n-\n-    \"\"\"\n-    tz_aware = data.iloc[0][time_column].tz\n-    ts_type = \"datetime64[ns, UTC]\" if tz_aware is not None else \"datetime64[ns]\"\n-\n-    cluster_list = []\n-    # Iterate through clusters, adding exemplar to output frame\n-    # pylint: disable=consider-using-enumerate\n-    # we need to know the index of the item within the loop\n-    for idx in range(len(cluster_set)):\n-        cluster_id = cluster_set[idx]\n-        class_members = labels == cluster_id\n-        if isinstance(data, pd.DataFrame):\n-            time_ordered = data[class_members].sort_values(time_column, ascending=True)\n-            first_event_time = time_ordered[0:][time_column].iat[0]\n-            last_event_time = time_ordered[-1:][time_column].iat[0]\n-        else:\n-            first_event_time = None\n-            last_event_time = None\n-\n-        if cluster_id == -1:\n-            # 'Noise' events are individual items that could not be assigned\n-            # to a cluster and so are unique\n-            cluster_list.append(\n-                data[class_members]\n-                .assign(\n-                    Clustered=False,\n-                    ClusterId=cluster_id,\n-                    ClusterSize=1,\n-                    TimeGenerated=first_event_time,\n-                    FirstEventTime=first_event_time,\n-                    LastEventTime=last_event_time,\n-                )\n-                .astype(\n-                    dtype={\n-                        \"TimeGenerated\": ts_type,\n-                        \"FirstEventTime\": ts_type,\n-                        \"LastEventTime\": ts_type,\n-                    }\n-                )\n-            )\n-        else:\n-            # Otherwise, just choose the first example of the cluster set\n-            cluster_list.append(\n-                data[class_members]\n-                .assign(\n-                    Clustered=True,\n-                    ClusterId=cluster_id,\n-                    ClusterSize=counts[idx],\n-                    TimeGenerated=first_event_time,\n-                    FirstEventTime=first_event_time,\n-                    LastEventTime=last_event_time,\n-                )[0:1]\n-                .astype(\n-                    dtype={\n-                        \"TimeGenerated\": ts_type,\n-                        \"FirstEventTime\": ts_type,\n-                        \"LastEventTime\": ts_type,\n-                    }\n-                )\n-            )\n-    # pylint: enable=consider-using-enumerate\n-    return pd.concat(cluster_list)\n-\n-\n-@export\n-def add_process_features(\n-    input_frame: pd.DataFrame, path_separator: str = None, force: bool = False\n-) -> pd.DataFrame:\n-    r\"\"\"\n-    Add numerical features based on patterns of command line and process name.\n-\n-    Parameters\n-    ----------\n-    input_frame : pd.DataFrame\n-        The input dataframe\n-    path_separator : str, optional\n-        Path separator. If not supplied, try to determine\n-        from 'NewProcessName' column of first 10 rows\n-        (the default is None)\n-    force : bool, optional\n-        Forces re-calculation of feature columns even if they\n-        already exist (the default is False)\n-\n-    Returns\n-    -------\n-    pd.DataFrame\n-        Copy of the dataframe with the additional numeric features\n-\n-    Notes\n-    -----\n-    Features added:\n-\n-    - processNameLen: length of process file name (inc path)\n-    - processNameTokens: the number of elements in the path\n-    - processName: the process file name (minus path)\n-    - commandlineTokens: number of space-separated tokens in the command line\n-    - commandlineLen: length of the command line\n-    - commandlineLogLen: log10 length of commandline\n-    - isSystemSession: 1 if session Id is 0x3e7 for Windows or -1 for Linux\n-    - commandlineTokensFull: counts number of token separators in commandline\n-      [\\\\s\\-\\\\/\\.,\"\\'\\|&:;%$()]\n-    - pathScore: sum of ord() value of characters in path\n-    - pathLogScore: log10 of pathScore\n-    - commandlineScore: sum of ord() value of characters in commandline\n-    - commandlineLogScore: log10 of commandlineScore\n-\n-    \"\"\"\n-    output_df = input_frame.copy()\n-\n-    # Set any NaN values to empty string\n-    if \"NewProcessName\" in output_df and \"CommandLine\" in output_df:\n-        output_df[[\"NewProcessName\", \"CommandLine\"]] = output_df[\n-            [\"NewProcessName\", \"CommandLine\"]\n-        ].fillna(value=\"\")\n-\n-    # try to determine the path separator\n-    if path_separator is None:\n-        sample_df = output_df.head(10)\n-        lx_path = len(sample_df[sample_df[\"NewProcessName\"].str.contains(\"/\")])\n-        path_separator = \"/\" if lx_path else \"\\\\\"\n-    # Create features from process name and command line\n-    if \"NewProcessName\" in output_df:\n-        _add_processname_features(output_df, force, path_separator)\n-\n-    if \"CommandLine\" in output_df:\n-        _add_commandline_features(output_df, force)\n-\n-    if \"SubjectLogonId\" in output_df and (\"isSystemSession\" not in output_df or force):\n-        output_df[\"isSystemSession\"] = output_df[\"SubjectLogonId\"].isin([\"0x3e7\", \"-1\"])\n-\n-    return output_df\n-\n-\n-def _add_processname_features(\n-    output_df: pd.DataFrame, force: bool, path_separator: str\n-):\n-    \"\"\"\n-    Add process name default features.\n-\n-    Parameters\n-    ----------\n-    output_df : pd.DataFrame\n-        The dataframe to add features to\n-    force : bool\n-        If True overwrite existing feature columns\n-    path_separator : str\n-        Path separator for OS\n-\n-    \"\"\"\n-    if \"processName\" not in output_df or force:\n-        output_df[\"processName\"] = output_df.apply(\n-            lambda x: x.NewProcessName.split(path_separator)[-1], axis=1\n-        )\n-    if \"pathScore\" not in output_df or force:\n-        output_df[\"pathScore\"] = output_df.apply(\n-            lambda x: char_ord_score(x.NewProcessName), axis=1\n-        )\n-    if \"pathLogScore\" not in output_df or force:\n-        output_df[\"pathLogScore\"] = output_df.apply(\n-            lambda x: log10(x.pathScore) if x.pathScore else 0, axis=1\n-        )\n-    if \"pathHash\" not in output_df or force:\n-        output_df[\"pathHash\"] = output_df.apply(\n-            lambda x: crc32_hash(x.NewProcessName), axis=1\n-        )\n-\n-\n-def _add_commandline_features(output_df: pd.DataFrame, force: bool):\n-    \"\"\"\n-    Add commandline default features.\n-\n-    Parameters\n-    ----------\n-    output_df : pd.DataFrame\n-        The dataframe to add features to\n-    force : bool\n-        If True overwrite existing feature columns\n-\n-    \"\"\"\n-    if \"commandlineLen\" not in output_df or force:\n-        output_df[\"commandlineLen\"] = output_df.apply(\n-            lambda x: len(x.CommandLine), axis=1\n-        )\n-    if \"commandlineLogLen\" not in output_df or force:\n-        output_df[\"commandlineLogLen\"] = output_df.apply(\n-            lambda x: log10(x.commandlineLen) if x.commandlineLen else 0, axis=1\n-        )\n-    if \"commandlineTokensFull\" not in output_df or force:\n-        output_df[\"commandlineTokensFull\"] = output_df[[\"CommandLine\"]].apply(\n-            lambda x: delim_count(x.CommandLine), axis=1\n-        )\n-\n-    if \"commandlineScore\" not in output_df or force:\n-        output_df[\"commandlineScore\"] = output_df.apply(\n-            lambda x: char_ord_score(x.CommandLine), axis=1\n-        )\n-    if \"commandlineTokensHash\" not in output_df or force:\n-        output_df[\"commandlineTokensHash\"] = output_df.apply(\n-            lambda x: delim_hash(x.CommandLine), axis=1\n-        )\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def delim_count(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n-    r\"\"\"\n-    Count the delimiters in input column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    delim_list : str, optional\n-        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n-\n-    Returns\n-    -------\n-    int\n-        Count of delimiters in the string.\n-\n-    \"\"\"\n-    return len(re.findall(delim_list, value))\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def delim_hash(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n-    r\"\"\"\n-    Return a hash (CRC32) of the delimiters from input column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    delim_list : str, optional\n-        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n-\n-    Returns\n-    -------\n-    int\n-        Hash of delimiter set in the string.\n-\n-    \"\"\"\n-    return crc32(bytes(\"\".join(re.findall(delim_list, value)), \"utf-8\"))\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def char_ord_score(value: str, scale: int = 1) -> int:\n-    \"\"\"\n-    Return sum of ord values of characters in string.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    scale : int, optional\n-        reduce the scale of the feature (reducing the\n-        influence of variations this feature on the clustering\n-        algorithm (the default is 1)\n-\n-    Returns\n-    -------\n-    int\n-        [description]\n-\n-    Notes\n-    -----\n-    This function sums the ordinal value of each character in the\n-    input string. Two strings with minor differences will result in\n-    a similar score. However, for strings with highly variable content\n-    (e.g. command lines or http requests containing GUIDs) this may result\n-    in too much variance to be useful when you are trying to detect\n-    similar patterns. You can use the scale parameter to reduce the\n-    influence of features using this function on clustering and anomaly\n-    algorithms.\n-\n-    \"\"\"\n-    return floor(sum(ord(x) for x in value) / scale)\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def token_count(value: str, delimiter: str = \" \") -> int:\n-    \"\"\"\n-    Return count of delimiter-separated tokens pd.Series column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-    delimiter : str, optional\n-        Delimiter used to split the column string.\n-        (the default is ' ')\n-\n-    Returns\n-    -------\n-    int\n-        count of tokens\n-\n-    \"\"\"\n-    return len(value.split(delimiter))\n-\n-\n-def _string_score(input_str):\n-    \"\"\"Sum the ord(c) for characters in a string.\"\"\"\n-    return sum(ord(x) for x in input_str)\n-\n-\n-@export\n-@lru_cache(maxsize=1024)\n-def crc32_hash(value: str) -> int:\n-    \"\"\"\n-    Return the CRC32 hash of the input column.\n-\n-    Parameters\n-    ----------\n-    value : str\n-        Data to process\n-\n-    Returns\n-    -------\n-    int\n-        CRC32 hash\n-\n-    \"\"\"\n-    return crc32(bytes(value.encode(\"utf-8\")))\n-\n-\n-def delim_count_df(\n-    data: pd.DataFrame, column: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]'\n-) -> pd.Series:\n-    r\"\"\"\n-    Count the delimiters in input column.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        The name of the column to process\n-    delim_list : str, optional\n-        delimiters to use. (the default is r\\'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]\\')\n-\n-    Returns\n-    -------\n-    pd.Series\n-        Count of delimiters in the string in `column`.\n-\n-    \"\"\"\n-    return data[column].str.count(delim_list)\n-\n-\n-def char_ord_score_df(data: pd.DataFrame, column: str, scale: int = 1) -> pd.Series:\n-    \"\"\"\n-    Return sum of ord values of characters in string.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        Column name to process\n-    scale : int, optional\n-        reduce the scale of the feature (reducing the\n-        influence of variations this feature on the clustering\n-        algorithm (the default is 1)\n-\n-    Returns\n-    -------\n-    pd.Series\n-        The sum of the ordinal values of the characters\n-        in `column`.\n-\n-    Notes\n-    -----\n-    This function sums the ordinal value of each character in the\n-    input string. Two strings with minor differences will result in\n-    a similar score. However, for strings with highly variable content\n-    (e.g. command lines or http requests containing GUIDs) this may result\n-    in too much variance to be useful when you are trying to detect\n-    similar patterns. You can use the scale parameter to reduce the\n-    influence of features using this function on clustering and anomaly\n-    algorithms.\n-\n-    \"\"\"\n-    return data.apply(lambda x: sum(ord(char) for char in x[column]) / scale, axis=1)\n-\n-\n-def token_count_df(data: pd.DataFrame, column: str, delimiter: str = \" \") -> pd.Series:\n-    \"\"\"\n-    Return count of delimiter-separated tokens pd.Series column.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        Column name to process\n-    delimiter : str, optional\n-        Delimiter used to split the column string.\n-        (the default is ' ')\n-\n-    Returns\n-    -------\n-    pd.Series\n-        count of tokens in strings in `column`\n-\n-    \"\"\"\n-    return data.apply(lambda x: len(x[column].split(delimiter)), axis=1)\n-\n-\n-def crc32_hash_df(data: pd.DataFrame, column: str) -> pd.Series:\n-    \"\"\"\n-    Return the CRC32 hash of the input column.\n-\n-    Parameters\n-    ----------\n-    data : pd.DataFrame\n-        The DataFrame to process\n-    column : str\n-        Column name to process\n-\n-    Returns\n-    -------\n-    pd.Series\n-        CRC32 hash of input column\n-\n-    \"\"\"\n-    return data.apply(lambda x: crc32(bytes(x[column].encode(\"utf-8\"))), axis=1)\n-\n-\n-# pylint: disable=too-many-arguments, too-many-statements\n-@export  # noqa: C901, MC0001\n-def plot_cluster(\n-    db_cluster: DBSCAN,\n-    data: pd.DataFrame,\n-    x_predict: np.ndarray,\n-    plot_label: str = None,\n-    plot_features: Tuple[int, int] = (0, 1),\n-    verbose: bool = False,\n-    cut_off: int = 3,\n-    xlabel: str = None,\n-    ylabel: str = None,\n-):\n-    \"\"\"\n-    Plot clustered data as scatter chart.\n-\n-    Parameters\n-    ----------\n-    db_cluster : DBSCAN\n-        DBScan Cluster (from SkLearn DBSCAN).\n-    data : pd.DataFrame\n-        Dataframe containing original data.\n-    x_predict : np.ndarray\n-        The DBSCAN predict numpy array\n-    plot_label : str, optional\n-         If set the column to use to label data points\n-         (the default is None)\n-    plot_features :  Tuple[int, int], optional\n-        Which two features in x_predict to plot (the default is (0, 1))\n-    verbose : bool, optional\n-        Verbose execution with some extra info\n-        (the default is False)\n-    cut_off : int, optional\n-        The cluster size below which items are considered outliers\n-        (the default is 3)\n-    xlabel : str, optional\n-        x-axis label (the default is None)\n-    ylabel : str, optional\n-        y-axis label (the default is None)\n-\n-    \"\"\"\n-    max_idx = x_predict.shape[1] - 1\n-    if plot_features[0] >= x_predict.shape[1]:\n-        raise ValueError(\n-            \"plot_features[0] index must be a value from 0 to {}.\".format(max_idx)\n-        )\n-    if plot_features[1] >= x_predict.shape[1]:\n-        raise ValueError(\n-            \"plot_features[1] index must be a value from 0 to {}.\".format(max_idx)\n-        )\n-    if plot_features[0] == plot_features[1]:\n-        mssg = \"plot_features indexes must be 2 different values in range 0 to\"\n-        raise ValueError(mssg + f\" {max_idx}.\")\n-\n-    labels = db_cluster.labels_\n-    core_samples_mask = np.zeros_like(labels, dtype=bool)\n-\n-    # pylint: disable=unsupported-assignment-operation\n-    # (assignment of numpy array is valid)\n-    core_samples_mask[db_cluster.core_sample_indices_] = True\n-    unique_labels = set(labels)\n-\n-    # pylint: disable=no-member\n-    # Spectral color map does exist\n-    colors = [cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n-    # Number of clusters in labels, ignoring noise if present.\n-    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n-    n_noise_ = list(labels).count(-1)\n-    _, counts = np.unique(labels, return_counts=True)\n-\n-    if verbose:\n-        print(\"Estimated number of clusters: %d\" % n_clusters_)\n-        print(\"Estimated number of noise points: %d\" % n_noise_)\n-        # print(\"Silhouette Coefficient: %0.3f\"\n-        #       % metrics.silhouette_score(x_predict, labels))\n-\n-    if not isinstance(data, pd.DataFrame):\n-        plot_label = None\n-    elif plot_label is not None and plot_label not in data:\n-        plot_label = None\n-\n-    p_label = None\n-    for cluster_id, color in zip(unique_labels, colors):\n-        if cluster_id == -1:\n-            # Black used for noise.\n-            color = [0, 0, 0, 1]\n-        class_member_mask = labels == cluster_id\n-\n-        cluster_size = counts[cluster_id]\n-        marker_size = cluster_size\n-        marker = \"o\"\n-        font_size = \"small\"\n-        alpha = 0.4\n-\n-        if cluster_size < cut_off:\n-            marker = \"+\"\n-            marker_size = 10\n-            font_size = \"large\"\n-            alpha = 1.0\n-        xy_pos = x_predict[class_member_mask & core_samples_mask]\n-        plt.plot(\n-            xy_pos[:, plot_features[0]],\n-            xy_pos[:, plot_features[1]],\n-            marker,\n-            markerfacecolor=tuple(color),\n-            markersize=marker_size,\n-        )\n-\n-        if plot_label:\n-            first_row = data[class_member_mask].iloc[0]\n-            if not first_row.empty and plot_label in first_row:\n-                p_label = first_row[plot_label]\n-                try:\n-                    plt.annotate(\n-                        p_label,\n-                        xy=(xy_pos[0, plot_features[0]], xy_pos[0, plot_features[1]]),\n-                        fontsize=font_size,\n-                        alpha=alpha,\n-                    )\n-                except IndexError:\n-                    pass\n-\n-    plt.xlabel(xlabel)\n-    plt.ylabel(ylabel)\n-    plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n-    plt.show()\n-    return plt\n+WARN_MSSG = (\n+    \"This module has moved to msticpy.analysis.eventcluster\\n\"\n+    + \"Please change your import to reflect this new location.\"\n+)\n+warnings.warn(WARN_MSSG, category=DeprecationWarning)\n",
        "source_code_with_indent": "\n# pylint: disable=too-many-arguments, too-many-locals\n@export\ndef dbcluster_events(\n    data: Any,\n    cluster_columns: List[Any] = None,\n    verbose: bool = False,\n    normalize: bool = True,\n    time_column: str = \"TimeCreatedUtc\",\n    max_cluster_distance: float = 0.01,\n    min_cluster_samples: int = 2,\n    **kwargs,\n) -> Tuple[pd.DataFrame, DBSCAN, np.ndarray]:\n    <IND>\"\"\"\n    Cluster data set according to cluster_columns features.\n\n    Parameters\n    ----------\n    data : Any\n        Input data as a pandas DataFrame or numpy array\n    cluster_columns : List[Any], optional\n        List of columns to use for features\n        - for DataFrame this is a list of column names\n        - for numpy array this is a list of column indexes\n    verbose : bool, optional\n        Print additional information about clustering results (the default is False)\n    normalize : bool, optional\n        Normalize the input data (should probably always be True)\n    time_column : str, optional\n        If there is a time column the output data will be ordered by this\n        (the default is 'TimeCreatedUtc')\n    max_cluster_distance : float, optional\n        DBSCAN eps (max cluster member distance) (the default is 0.01)\n    min_cluster_samples : int, optional\n        DBSCAN min_samples (the minimum cluster size) (the default is 2)\n\n    Other Parameters\n    ----------------\n    kwargs: Other arguments are passed to DBSCAN constructor\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, DBSCAN, np.ndarray]\n        Output dataframe with clustered rows\n        DBSCAN model\n        Normalized data set\n\n    \"\"\"\n    allowed_types = [np.ndarray, pd.DataFrame]\n\n    x_input = None\n    if isinstance(data, pd.DataFrame):\n        <IND>if cluster_columns is None:\n            <IND>x_input = data.values\n        <DED>else:\n            <IND>x_input = data[cluster_columns].values\n    <DED><DED>elif isinstance(data, np.ndarray):\n        <IND>x_input = data if cluster_columns is None else data[:, cluster_columns].values\n    <DED>if x_input is None:\n        <IND>mssg = \"Input data not in expected format.\\n{} is not one of allowed types {}\"\n        type_list = \", \".join([str(t) for t in allowed_types])\n        mssg = mssg.format(str(type(data)), type_list)\n        raise ValueError(mssg)\n\n    # Create DBSCAN cluster object\n    <DED>db_cluster = DBSCAN(\n        eps=max_cluster_distance, min_samples=min_cluster_samples, **kwargs\n    )\n\n    # Normalize the data (most clustering algorithms don't do well with\n    # unnormalized data)\n    x_norm = Normalizer().fit_transform(x_input) if normalize else x_input\n    # fit the data set\n    db_cluster.fit(x_norm)\n    labels = db_cluster.labels_\n    cluster_set, counts = np.unique(labels, return_counts=True)\n    if verbose:\n        <IND>print(\n            \"Clustering for set size \",\n            len(x_norm),\n            \" - \",\n            len(cluster_set),\n            \" clusters\",\n        )\n        print(\"Individual cluster sizes: \", \", \".join([str(c) for c in counts]))\n\n    <DED>clustered_events = _merge_clustered_items(\n        cluster_set, labels, data, time_column, counts\n    )\n\n    if verbose:\n        <IND>print(\"Cluster output rows: \", len(clustered_events))\n\n    <DED>return clustered_events, db_cluster, x_norm\n\n\n<DED>def _merge_clustered_items(\n    cluster_set: np.array,\n    labels: np.array,\n    data: Union[pd.DataFrame, np.array],\n    time_column: str,\n    counts: np.array,\n) -> pd.DataFrame:\n    <IND>\"\"\"\n    Merge outliers and core clusters into single DataFrame.\n\n    Parameters\n    ----------\n    cluster_set : np.array\n        The set of clusters\n    labels : np.array\n        The cluster labels\n    data : Union[pd.DataFrame, np.array]\n        The source data\n    time_column : str\n        Name of the Time column\n    counts : np.array\n        The counts of members in each cluster\n\n    Returns\n    -------\n    pd.DataFrame\n        Merged dataframe\n\n    \"\"\"\n    tz_aware = data.iloc[0][time_column].tz\n    ts_type = \"datetime64[ns, UTC]\" if tz_aware is not None else \"datetime64[ns]\"\n\n    cluster_list = []\n    # Iterate through clusters, adding exemplar to output frame\n    # pylint: disable=consider-using-enumerate\n    # we need to know the index of the item within the loop\n    for idx in range(len(cluster_set)):\n        <IND>cluster_id = cluster_set[idx]\n        class_members = labels == cluster_id\n        if isinstance(data, pd.DataFrame):\n            <IND>time_ordered = data[class_members].sort_values(time_column, ascending=True)\n            first_event_time = time_ordered[0:][time_column].iat[0]\n            last_event_time = time_ordered[-1:][time_column].iat[0]\n        <DED>else:\n            <IND>first_event_time = None\n            last_event_time = None\n\n        <DED>if cluster_id == -1:\n            # 'Noise' events are individual items that could not be assigned\n            # to a cluster and so are unique\n            <IND>cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=False,\n                    ClusterId=cluster_id,\n                    ClusterSize=1,\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n        <DED>else:\n            # Otherwise, just choose the first example of the cluster set\n            <IND>cluster_list.append(\n                data[class_members]\n                .assign(\n                    Clustered=True,\n                    ClusterId=cluster_id,\n                    ClusterSize=counts[idx],\n                    TimeGenerated=first_event_time,\n                    FirstEventTime=first_event_time,\n                    LastEventTime=last_event_time,\n                )[0:1]\n                .astype(\n                    dtype={\n                        \"TimeGenerated\": ts_type,\n                        \"FirstEventTime\": ts_type,\n                        \"LastEventTime\": ts_type,\n                    }\n                )\n            )\n    # pylint: enable=consider-using-enumerate\n    <DED><DED>return pd.concat(cluster_list)\n\n\n<DED>@export\ndef add_process_features(\n    input_frame: pd.DataFrame, path_separator: str = None, force: bool = False\n) -> pd.DataFrame:\n    <IND>r\"\"\"\n    Add numerical features based on patterns of command line and process name.\n\n    Parameters\n    ----------\n    input_frame : pd.DataFrame\n        The input dataframe\n    path_separator : str, optional\n        Path separator. If not supplied, try to determine\n        from 'NewProcessName' column of first 10 rows\n        (the default is None)\n    force : bool, optional\n        Forces re-calculation of feature columns even if they\n        already exist (the default is False)\n\n    Returns\n    -------\n    pd.DataFrame\n        Copy of the dataframe with the additional numeric features\n\n    Notes\n    -----\n    Features added:\n\n    - processNameLen: length of process file name (inc path)\n    - processNameTokens: the number of elements in the path\n    - processName: the process file name (minus path)\n    - commandlineTokens: number of space-separated tokens in the command line\n    - commandlineLen: length of the command line\n    - commandlineLogLen: log10 length of commandline\n    - isSystemSession: 1 if session Id is 0x3e7 for Windows or -1 for Linux\n    - commandlineTokensFull: counts number of token separators in commandline\n      [\\\\s\\-\\\\/\\.,\"\\'\\|&:;%$()]\n    - pathScore: sum of ord() value of characters in path\n    - pathLogScore: log10 of pathScore\n    - commandlineScore: sum of ord() value of characters in commandline\n    - commandlineLogScore: log10 of commandlineScore\n\n    \"\"\"\n    output_df = input_frame.copy()\n\n    # Set any NaN values to empty string\n    if \"NewProcessName\" in output_df and \"CommandLine\" in output_df:\n        <IND>output_df[[\"NewProcessName\", \"CommandLine\"]] = output_df[\n            [\"NewProcessName\", \"CommandLine\"]\n        ].fillna(value=\"\")\n\n    # try to determine the path separator\n    <DED>if path_separator is None:\n        <IND>sample_df = output_df.head(10)\n        lx_path = len(sample_df[sample_df[\"NewProcessName\"].str.contains(\"/\")])\n        path_separator = \"/\" if lx_path else \"\\\\\"\n    # Create features from process name and command line\n    <DED>if \"NewProcessName\" in output_df:\n        <IND>_add_processname_features(output_df, force, path_separator)\n\n    <DED>if \"CommandLine\" in output_df:\n        <IND>_add_commandline_features(output_df, force)\n\n    <DED>if \"SubjectLogonId\" in output_df and (\"isSystemSession\" not in output_df or force):\n        <IND>output_df[\"isSystemSession\"] = output_df[\"SubjectLogonId\"].isin([\"0x3e7\", \"-1\"])\n\n    <DED>return output_df\n\n\n<DED>def _add_processname_features(\n    output_df: pd.DataFrame, force: bool, path_separator: str\n):\n    <IND>\"\"\"\n    Add process name default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n    path_separator : str\n        Path separator for OS\n\n    \"\"\"\n    if \"processName\" not in output_df or force:\n        <IND>output_df[\"processName\"] = output_df.apply(\n            lambda x: x.NewProcessName.split(path_separator)[-1], axis=1\n        )\n    <DED>if \"pathScore\" not in output_df or force:\n        <IND>output_df[\"pathScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.NewProcessName), axis=1\n        )\n    <DED>if \"pathLogScore\" not in output_df or force:\n        <IND>output_df[\"pathLogScore\"] = output_df.apply(\n            lambda x: log10(x.pathScore) if x.pathScore else 0, axis=1\n        )\n    <DED>if \"pathHash\" not in output_df or force:\n        <IND>output_df[\"pathHash\"] = output_df.apply(\n            lambda x: crc32_hash(x.NewProcessName), axis=1\n        )\n\n\n<DED><DED>def _add_commandline_features(output_df: pd.DataFrame, force: bool):\n    <IND>\"\"\"\n    Add commandline default features.\n\n    Parameters\n    ----------\n    output_df : pd.DataFrame\n        The dataframe to add features to\n    force : bool\n        If True overwrite existing feature columns\n\n    \"\"\"\n    if \"commandlineLen\" not in output_df or force:\n        <IND>output_df[\"commandlineLen\"] = output_df.apply(\n            lambda x: len(x.CommandLine), axis=1\n        )\n    <DED>if \"commandlineLogLen\" not in output_df or force:\n        <IND>output_df[\"commandlineLogLen\"] = output_df.apply(\n            lambda x: log10(x.commandlineLen) if x.commandlineLen else 0, axis=1\n        )\n    <DED>if \"commandlineTokensFull\" not in output_df or force:\n        <IND>output_df[\"commandlineTokensFull\"] = output_df[[\"CommandLine\"]].apply(\n            lambda x: delim_count(x.CommandLine), axis=1\n        )\n\n    <DED>if \"commandlineScore\" not in output_df or force:\n        <IND>output_df[\"commandlineScore\"] = output_df.apply(\n            lambda x: char_ord_score(x.CommandLine), axis=1\n        )\n    <DED>if \"commandlineTokensHash\" not in output_df or force:\n        <IND>output_df[\"commandlineTokensHash\"] = output_df.apply(\n            lambda x: delim_hash(x.CommandLine), axis=1\n        )\n\n\n<DED><DED>@export\n@lru_cache(maxsize=1024)\ndef delim_count(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    <IND>r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Count of delimiters in the string.\n\n    \"\"\"\n    return len(re.findall(delim_list, value))\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef delim_hash(value: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]') -> int:\n    <IND>r\"\"\"\n    Return a hash (CRC32) of the delimiters from input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delim_list : str, optional\n        delimiters to use. (the default is r'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]')\n\n    Returns\n    -------\n    int\n        Hash of delimiter set in the string.\n\n    \"\"\"\n    return crc32(bytes(\"\".join(re.findall(delim_list, value)), \"utf-8\"))\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef char_ord_score(value: str, scale: int = 1) -> int:\n    <IND>\"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    int\n        [description]\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return floor(sum(ord(x) for x in value) / scale)\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef token_count(value: str, delimiter: str = \" \") -> int:\n    <IND>\"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    int\n        count of tokens\n\n    \"\"\"\n    return len(value.split(delimiter))\n\n\n<DED>def _string_score(input_str):\n    <IND>\"\"\"Sum the ord(c) for characters in a string.\"\"\"\n    return sum(ord(x) for x in input_str)\n\n\n<DED>@export\n@lru_cache(maxsize=1024)\ndef crc32_hash(value: str) -> int:\n    <IND>\"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    value : str\n        Data to process\n\n    Returns\n    -------\n    int\n        CRC32 hash\n\n    \"\"\"\n    return crc32(bytes(value.encode(\"utf-8\")))\n\n\n<DED>def delim_count_df(\n    data: pd.DataFrame, column: str, delim_list: str = r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]'\n) -> pd.Series:\n    <IND>r\"\"\"\n    Count the delimiters in input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        The name of the column to process\n    delim_list : str, optional\n        delimiters to use. (the default is r\\'[\\\\s\\\\\\\\-\\\\\\\\\\\\\\\\/\\.,\"\\\\\\\\'|&:;%$()]\\')\n\n    Returns\n    -------\n    pd.Series\n        Count of delimiters in the string in `column`.\n\n    \"\"\"\n    return data[column].str.count(delim_list)\n\n\n<DED>def char_ord_score_df(data: pd.DataFrame, column: str, scale: int = 1) -> pd.Series:\n    <IND>\"\"\"\n    Return sum of ord values of characters in string.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    scale : int, optional\n        reduce the scale of the feature (reducing the\n        influence of variations this feature on the clustering\n        algorithm (the default is 1)\n\n    Returns\n    -------\n    pd.Series\n        The sum of the ordinal values of the characters\n        in `column`.\n\n    Notes\n    -----\n    This function sums the ordinal value of each character in the\n    input string. Two strings with minor differences will result in\n    a similar score. However, for strings with highly variable content\n    (e.g. command lines or http requests containing GUIDs) this may result\n    in too much variance to be useful when you are trying to detect\n    similar patterns. You can use the scale parameter to reduce the\n    influence of features using this function on clustering and anomaly\n    algorithms.\n\n    \"\"\"\n    return data.apply(lambda x: sum(ord(char) for char in x[column]) / scale, axis=1)\n\n\n<DED>def token_count_df(data: pd.DataFrame, column: str, delimiter: str = \" \") -> pd.Series:\n    <IND>\"\"\"\n    Return count of delimiter-separated tokens pd.Series column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n    delimiter : str, optional\n        Delimiter used to split the column string.\n        (the default is ' ')\n\n    Returns\n    -------\n    pd.Series\n        count of tokens in strings in `column`\n\n    \"\"\"\n    return data.apply(lambda x: len(x[column].split(delimiter)), axis=1)\n\n\n<DED>def crc32_hash_df(data: pd.DataFrame, column: str) -> pd.Series:\n    <IND>\"\"\"\n    Return the CRC32 hash of the input column.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process\n    column : str\n        Column name to process\n\n    Returns\n    -------\n    pd.Series\n        CRC32 hash of input column\n\n    \"\"\"\n    return data.apply(lambda x: crc32(bytes(x[column].encode(\"utf-8\"))), axis=1)\n\n\n# pylint: disable=too-many-arguments, too-many-statements\n<DED>@export  # noqa: C901, MC0001\ndef plot_cluster(\n    db_cluster: DBSCAN,\n    data: pd.DataFrame,\n    x_predict: np.ndarray,\n    plot_label: str = None,\n    plot_features: Tuple[int, int] = (0, 1),\n    verbose: bool = False,\n    cut_off: int = 3,\n    xlabel: str = None,\n    ylabel: str = None,\n):\n    <IND>\"\"\"\n    Plot clustered data as scatter chart.\n\n    Parameters\n    ----------\n    db_cluster : DBSCAN\n        DBScan Cluster (from SkLearn DBSCAN).\n    data : pd.DataFrame\n        Dataframe containing original data.\n    x_predict : np.ndarray\n        The DBSCAN predict numpy array\n    plot_label : str, optional\n         If set the column to use to label data points\n         (the default is None)\n    plot_features :  Tuple[int, int], optional\n        Which two features in x_predict to plot (the default is (0, 1))\n    verbose : bool, optional\n        Verbose execution with some extra info\n        (the default is False)\n    cut_off : int, optional\n        The cluster size below which items are considered outliers\n        (the default is 3)\n    xlabel : str, optional\n        x-axis label (the default is None)\n    ylabel : str, optional\n        y-axis label (the default is None)\n\n    \"\"\"\n    max_idx = x_predict.shape[1] - 1\n    if plot_features[0] >= x_predict.shape[1]:\n        <IND>raise ValueError(\n            \"plot_features[0] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    <DED>if plot_features[1] >= x_predict.shape[1]:\n        <IND>raise ValueError(\n            \"plot_features[1] index must be a value from 0 to {}.\".format(max_idx)\n        )\n    <DED>if plot_features[0] == plot_features[1]:\n        <IND>mssg = \"plot_features indexes must be 2 different values in range 0 to\"\n        raise ValueError(mssg + f\" {max_idx}.\")\n\n    <DED>labels = db_cluster.labels_\n    core_samples_mask = np.zeros_like(labels, dtype=bool)\n\n    # pylint: disable=unsupported-assignment-operation\n    # (assignment of numpy array is valid)\n    core_samples_mask[db_cluster.core_sample_indices_] = True\n    unique_labels = set(labels)\n\n    # pylint: disable=no-member\n    # Spectral color map does exist\n    colors = [cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n    # Number of clusters in labels, ignoring noise if present.\n    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n    n_noise_ = list(labels).count(-1)\n    _, counts = np.unique(labels, return_counts=True)\n\n    if verbose:\n        <IND>print(\"Estimated number of clusters: %d\" % n_clusters_)\n        print(\"Estimated number of noise points: %d\" % n_noise_)\n        # print(\"Silhouette Coefficient: %0.3f\"\n        #       % metrics.silhouette_score(x_predict, labels))\n\n    <DED>if not isinstance(data, pd.DataFrame):\n        <IND>plot_label = None\n    <DED>elif plot_label is not None and plot_label not in data:\n        <IND>plot_label = None\n\n    <DED>p_label = None\n    for cluster_id, color in zip(unique_labels, colors):\n        <IND>if cluster_id == -1:\n            # Black used for noise.\n            <IND>color = [0, 0, 0, 1]\n        <DED>class_member_mask = labels == cluster_id\n\n        cluster_size = counts[cluster_id]\n        marker_size = cluster_size\n        marker = \"o\"\n        font_size = \"small\"\n        alpha = 0.4\n\n        if cluster_size < cut_off:\n            <IND>marker = \"+\"\n            marker_size = 10\n            font_size = \"large\"\n            alpha = 1.0\n        <DED>xy_pos = x_predict[class_member_mask & core_samples_mask]\n        plt.plot(\n            xy_pos[:, plot_features[0]],\n            xy_pos[:, plot_features[1]],\n            marker,\n            markerfacecolor=tuple(color),\n            markersize=marker_size,\n        )\n\n        if plot_label:\n            <IND>first_row = data[class_member_mask].iloc[0]\n            if not first_row.empty and plot_label in first_row:\n                <IND>p_label = first_row[plot_label]\n                try:\n                    <IND>plt.annotate(\n                        p_label,\n                        xy=(xy_pos[0, plot_features[0]], xy_pos[0, plot_features[1]]),\n                        fontsize=font_size,\n                        alpha=alpha,\n                    )\n                <DED>except IndexError:\n                    <IND>pass\n\n    <DED><DED><DED><DED>plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n    plt.show()\n    return plt\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n# flake8: noqa: F403, F401\n# pylint: disable=wildcard-import, unused-wildcard-import, unused-import\nfrom ..analysis.eventcluster import *\n\nWARN_MSSG = (\n    \"This module has moved to msticpy.analysis.eventcluster\\n\"\n    + \"Please change your import to reflect this new location.\"\n)\nwarnings.warn(WARN_MSSG, category=DeprecationWarning)\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  }
]