[
  {
    "project": "instadeepai/Mava",
    "commit": "8e4dd412ccab4fba42dfe987b81124c901e1e4c0",
    "filename": "examples/debugging_envs/run_recurrent_mad4pg.py",
    "min_patch_found": false,
    "full_warning_msg": "examples/debugging_envs/run_recurrent_mad4pg.py:183:8 Incompatible parameter type [6]: Expected `typing.Type[mad4pg.training.BaseMAD4PGTrainer]` for 4th parameter `trainer_fn` to call `mad4pg.system.MAD4PG.__init__` but got `typing.Type[DecentralisedRecurrentMAD4PGTrainer]`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "instadeepai/Mava",
    "commit": "8e4dd412ccab4fba42dfe987b81124c901e1e4c0",
    "filename": "mava/systems/tf/maddpg/training.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/instadeepai-Mava/mava/systems/tf/maddpg/training.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "mava/systems/tf/maddpg/training.py:805:4 Inconsistent override [14]: `mava.systems.tf.maddpg.training.BaseRecurrentMADDPGTrainer._transform_observations` overrides method defined in `BaseMADDPGTrainer` inconsistently. Could not find parameter `next_obs` in overriding signature.",
    "message": " `mava.systems.tf.maddpg.training.BaseRecurrentMADDPGTrainer._transform_observations` overrides method defined in `BaseMADDPGTrainer` inconsistently. Could not find parameter `next_obs` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 805,
    "warning_line": "    def _transform_observations(",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nclass BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n    \"\"\"MADDPG trainer.\n",
        "source_code_len": 77,
        "target_code": "\nclass BaseRecurrentMADDPGTrainer(mava.Trainer):\n    \"\"\"MADDPG trainer.\n",
        "target_code_len": 72,
        "diff_format": "@@ -724,3 +723,3 @@\n \n-class BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n+class BaseRecurrentMADDPGTrainer(mava.Trainer):\n     \"\"\"MADDPG trainer.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n<DED><DED>class BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n    <IND>",
        "target_code_with_indent": "\n<DED><DED>class BaseRecurrentMADDPGTrainer(mava.Trainer):\n    <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        super().__init__(\n            agents=agents,\n            agent_types=agent_types,\n            policy_networks=policy_networks,\n            critic_networks=critic_networks,\n            target_policy_networks=target_policy_networks,\n            target_critic_networks=target_critic_networks,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            discount=discount,\n            target_update_period=target_update_period,\n            dataset=dataset,\n            observation_networks=observation_networks,\n            target_observation_networks=target_observation_networks,\n            shared_weights=shared_weights,\n            max_gradient_norm=max_gradient_norm,\n            counter=counter,\n            logger=logger,\n            checkpoint=checkpoint,\n            checkpoint_subpath=checkpoint_subpath,\n        )\n\n",
        "source_code_len": 877,
        "target_code": "\n        self._agents = agents\n        self._agent_types = agent_types\n        self._shared_weights = shared_weights\n        self._checkpoint = checkpoint\n\n        # Store online and target networks.\n        self._policy_networks = policy_networks\n        self._critic_networks = critic_networks\n        self._target_policy_networks = target_policy_networks\n        self._target_critic_networks = target_critic_networks\n\n        # Ensure obs and target networks are sonnet modules\n        self._observation_networks = {\n            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n        }\n        self._target_observation_networks = {\n            k: tf2_utils.to_sonnet_module(v)\n            for k, v in target_observation_networks.items()\n        }\n\n        # General learner book-keeping and loggers.\n        self._counter = counter or counting.Counter()\n        self._logger = logger or loggers.make_default_logger(\"trainer\")\n\n        # Other learner parameters.\n        self._discount = discount\n\n        # Set up gradient clipping.\n        if max_gradient_norm is not None:\n            self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n        else:  # A very large number. Infinity results in NaNs.\n            self._max_gradient_norm = tf.convert_to_tensor(1e10)\n\n        # Necessary to track when to update target networks.\n        self._num_steps = tf.Variable(0, dtype=tf.int32)\n        self._target_update_period = target_update_period\n\n        # Create an iterator to go through the dataset.\n        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n        self._critic_optimizer = critic_optimizer\n        self._policy_optimizer = policy_optimizer\n\n        # Dictionary with network keys for each agent.\n        self.agent_net_keys = {agent: agent for agent in self._agents}\n        if self._shared_weights:\n            self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n\n        self.unique_net_keys = self._agent_types if shared_weights else self._agents\n\n        # Expose the variables.\n        policy_networks_to_expose = {}\n        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n            \"critic\": {},\n            \"policy\": {},\n        }\n        for agent_key in self.unique_net_keys:\n            policy_network_to_expose = snt.Sequential(\n                [\n                    self._target_observation_networks[agent_key],\n                    self._target_policy_networks[agent_key],\n                ]\n            )\n            policy_networks_to_expose[agent_key] = policy_network_to_expose\n            self._system_network_variables[\"critic\"][\n                agent_key\n            ] = target_critic_networks[agent_key].variables\n            self._system_network_variables[\"policy\"][\n                agent_key\n            ] = policy_network_to_expose.variables\n\n        # Create checkpointer\n        self._system_checkpointer = {}\n        if checkpoint:\n            for agent_key in self.unique_net_keys:\n                objects_to_save = {\n                    \"counter\": self._counter,\n                    \"policy\": self._policy_networks[agent_key],\n                    \"critic\": self._critic_networks[agent_key],\n                    \"observation\": self._observation_networks[agent_key],\n                    \"target_policy\": self._target_policy_networks[agent_key],\n                    \"target_critic\": self._target_critic_networks[agent_key],\n                    \"target_observation\": self._target_observation_networks[agent_key],\n                    \"policy_optimizer\": self._policy_optimizer,\n                    \"critic_optimizer\": self._critic_optimizer,\n                    \"num_steps\": self._num_steps,\n                }\n\n                subdir = os.path.join(\"trainer\", agent_key)\n                checkpointer = tf2_savers.Checkpointer(\n                    time_delta_minutes=15,\n                    directory=checkpoint_subpath,\n                    objects_to_save=objects_to_save,\n                    subdirectory=subdir,\n                )\n                self._system_checkpointer[agent_key] = checkpointer\n        # Do not record timestamps until after the first learning step is done.\n        # This is to avoid including the time it takes for actors to come online and\n        # fill the replay buffer.\n        self._timestamp = None\n\n    def _update_target_networks(self) -> None:\n        for key in self.unique_net_keys:\n            # Update target network.\n            online_variables = (\n                *self._observation_networks[key].variables,\n                *self._critic_networks[key].variables,\n                *self._policy_networks[key].variables,\n            )\n            target_variables = (\n                *self._target_observation_networks[key].variables,\n                *self._target_critic_networks[key].variables,\n                *self._target_policy_networks[key].variables,\n            )\n\n            # Make online -> target network update ops.\n            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n                for src, dest in zip(online_variables, target_variables):\n                    dest.assign(src)\n            self._num_steps.assign_add(1)\n\n",
        "target_code_len": 5252,
        "diff_format": "@@ -775,23 +774,122 @@\n \n-        super().__init__(\n-            agents=agents,\n-            agent_types=agent_types,\n-            policy_networks=policy_networks,\n-            critic_networks=critic_networks,\n-            target_policy_networks=target_policy_networks,\n-            target_critic_networks=target_critic_networks,\n-            policy_optimizer=policy_optimizer,\n-            critic_optimizer=critic_optimizer,\n-            discount=discount,\n-            target_update_period=target_update_period,\n-            dataset=dataset,\n-            observation_networks=observation_networks,\n-            target_observation_networks=target_observation_networks,\n-            shared_weights=shared_weights,\n-            max_gradient_norm=max_gradient_norm,\n-            counter=counter,\n-            logger=logger,\n-            checkpoint=checkpoint,\n-            checkpoint_subpath=checkpoint_subpath,\n-        )\n+        self._agents = agents\n+        self._agent_types = agent_types\n+        self._shared_weights = shared_weights\n+        self._checkpoint = checkpoint\n+\n+        # Store online and target networks.\n+        self._policy_networks = policy_networks\n+        self._critic_networks = critic_networks\n+        self._target_policy_networks = target_policy_networks\n+        self._target_critic_networks = target_critic_networks\n+\n+        # Ensure obs and target networks are sonnet modules\n+        self._observation_networks = {\n+            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n+        }\n+        self._target_observation_networks = {\n+            k: tf2_utils.to_sonnet_module(v)\n+            for k, v in target_observation_networks.items()\n+        }\n+\n+        # General learner book-keeping and loggers.\n+        self._counter = counter or counting.Counter()\n+        self._logger = logger or loggers.make_default_logger(\"trainer\")\n+\n+        # Other learner parameters.\n+        self._discount = discount\n+\n+        # Set up gradient clipping.\n+        if max_gradient_norm is not None:\n+            self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n+        else:  # A very large number. Infinity results in NaNs.\n+            self._max_gradient_norm = tf.convert_to_tensor(1e10)\n+\n+        # Necessary to track when to update target networks.\n+        self._num_steps = tf.Variable(0, dtype=tf.int32)\n+        self._target_update_period = target_update_period\n+\n+        # Create an iterator to go through the dataset.\n+        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n+\n+        self._critic_optimizer = critic_optimizer\n+        self._policy_optimizer = policy_optimizer\n+\n+        # Dictionary with network keys for each agent.\n+        self.agent_net_keys = {agent: agent for agent in self._agents}\n+        if self._shared_weights:\n+            self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n+\n+        self.unique_net_keys = self._agent_types if shared_weights else self._agents\n+\n+        # Expose the variables.\n+        policy_networks_to_expose = {}\n+        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n+            \"critic\": {},\n+            \"policy\": {},\n+        }\n+        for agent_key in self.unique_net_keys:\n+            policy_network_to_expose = snt.Sequential(\n+                [\n+                    self._target_observation_networks[agent_key],\n+                    self._target_policy_networks[agent_key],\n+                ]\n+            )\n+            policy_networks_to_expose[agent_key] = policy_network_to_expose\n+            self._system_network_variables[\"critic\"][\n+                agent_key\n+            ] = target_critic_networks[agent_key].variables\n+            self._system_network_variables[\"policy\"][\n+                agent_key\n+            ] = policy_network_to_expose.variables\n+\n+        # Create checkpointer\n+        self._system_checkpointer = {}\n+        if checkpoint:\n+            for agent_key in self.unique_net_keys:\n+                objects_to_save = {\n+                    \"counter\": self._counter,\n+                    \"policy\": self._policy_networks[agent_key],\n+                    \"critic\": self._critic_networks[agent_key],\n+                    \"observation\": self._observation_networks[agent_key],\n+                    \"target_policy\": self._target_policy_networks[agent_key],\n+                    \"target_critic\": self._target_critic_networks[agent_key],\n+                    \"target_observation\": self._target_observation_networks[agent_key],\n+                    \"policy_optimizer\": self._policy_optimizer,\n+                    \"critic_optimizer\": self._critic_optimizer,\n+                    \"num_steps\": self._num_steps,\n+                }\n+\n+                subdir = os.path.join(\"trainer\", agent_key)\n+                checkpointer = tf2_savers.Checkpointer(\n+                    time_delta_minutes=15,\n+                    directory=checkpoint_subpath,\n+                    objects_to_save=objects_to_save,\n+                    subdirectory=subdir,\n+                )\n+                self._system_checkpointer[agent_key] = checkpointer\n+        # Do not record timestamps until after the first learning step is done.\n+        # This is to avoid including the time it takes for actors to come online and\n+        # fill the replay buffer.\n+        self._timestamp = None\n+\n+    def _update_target_networks(self) -> None:\n+        for key in self.unique_net_keys:\n+            # Update target network.\n+            online_variables = (\n+                *self._observation_networks[key].variables,\n+                *self._critic_networks[key].variables,\n+                *self._policy_networks[key].variables,\n+            )\n+            target_variables = (\n+                *self._target_observation_networks[key].variables,\n+                *self._target_critic_networks[key].variables,\n+                *self._target_policy_networks[key].variables,\n+            )\n+\n+            # Make online -> target network update ops.\n+            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n+                for src, dest in zip(online_variables, target_variables):\n+                    dest.assign(src)\n+            self._num_steps.assign_add(1)\n \n",
        "source_code_with_indent": "\n        super().__init__(\n            agents=agents,\n            agent_types=agent_types,\n            policy_networks=policy_networks,\n            critic_networks=critic_networks,\n            target_policy_networks=target_policy_networks,\n            target_critic_networks=target_critic_networks,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            discount=discount,\n            target_update_period=target_update_period,\n            dataset=dataset,\n            observation_networks=observation_networks,\n            target_observation_networks=target_observation_networks,\n            shared_weights=shared_weights,\n            max_gradient_norm=max_gradient_norm,\n            counter=counter,\n            logger=logger,\n            checkpoint=checkpoint,\n            checkpoint_subpath=checkpoint_subpath,\n        )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        self._agents = agents\n        self._agent_types = agent_types\n        self._shared_weights = shared_weights\n        self._checkpoint = checkpoint\n\n        # Store online and target networks.\n        self._policy_networks = policy_networks\n        self._critic_networks = critic_networks\n        self._target_policy_networks = target_policy_networks\n        self._target_critic_networks = target_critic_networks\n\n        # Ensure obs and target networks are sonnet modules\n        self._observation_networks = {\n            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n        }\n        self._target_observation_networks = {\n            k: tf2_utils.to_sonnet_module(v)\n            for k, v in target_observation_networks.items()\n        }\n\n        # General learner book-keeping and loggers.\n        self._counter = counter or counting.Counter()\n        self._logger = logger or loggers.make_default_logger(\"trainer\")\n\n        # Other learner parameters.\n        self._discount = discount\n\n        # Set up gradient clipping.\n        if max_gradient_norm is not None:\n            <IND>self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n        <DED>else:  # A very large number. Infinity results in NaNs.\n            <IND>self._max_gradient_norm = tf.convert_to_tensor(1e10)\n\n        # Necessary to track when to update target networks.\n        <DED>self._num_steps = tf.Variable(0, dtype=tf.int32)\n        self._target_update_period = target_update_period\n\n        # Create an iterator to go through the dataset.\n        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n        self._critic_optimizer = critic_optimizer\n        self._policy_optimizer = policy_optimizer\n\n        # Dictionary with network keys for each agent.\n        self.agent_net_keys = {agent: agent for agent in self._agents}\n        if self._shared_weights:\n            <IND>self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n\n        <DED>self.unique_net_keys = self._agent_types if shared_weights else self._agents\n\n        # Expose the variables.\n        policy_networks_to_expose = {}\n        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n            \"critic\": {},\n            \"policy\": {},\n        }\n        for agent_key in self.unique_net_keys:\n            <IND>policy_network_to_expose = snt.Sequential(\n                [\n                    self._target_observation_networks[agent_key],\n                    self._target_policy_networks[agent_key],\n                ]\n            )\n            policy_networks_to_expose[agent_key] = policy_network_to_expose\n            self._system_network_variables[\"critic\"][\n                agent_key\n            ] = target_critic_networks[agent_key].variables\n            self._system_network_variables[\"policy\"][\n                agent_key\n            ] = policy_network_to_expose.variables\n\n        # Create checkpointer\n        <DED>self._system_checkpointer = {}\n        if checkpoint:\n            <IND>for agent_key in self.unique_net_keys:\n                <IND>objects_to_save = {\n                    \"counter\": self._counter,\n                    \"policy\": self._policy_networks[agent_key],\n                    \"critic\": self._critic_networks[agent_key],\n                    \"observation\": self._observation_networks[agent_key],\n                    \"target_policy\": self._target_policy_networks[agent_key],\n                    \"target_critic\": self._target_critic_networks[agent_key],\n                    \"target_observation\": self._target_observation_networks[agent_key],\n                    \"policy_optimizer\": self._policy_optimizer,\n                    \"critic_optimizer\": self._critic_optimizer,\n                    \"num_steps\": self._num_steps,\n                }\n\n                subdir = os.path.join(\"trainer\", agent_key)\n                checkpointer = tf2_savers.Checkpointer(\n                    time_delta_minutes=15,\n                    directory=checkpoint_subpath,\n                    objects_to_save=objects_to_save,\n                    subdirectory=subdir,\n                )\n                self._system_checkpointer[agent_key] = checkpointer\n        # Do not record timestamps until after the first learning step is done.\n        # This is to avoid including the time it takes for actors to come online and\n        # fill the replay buffer.\n        <DED><DED>self._timestamp = None\n\n    <DED>def _update_target_networks(self) -> None:\n        <IND>for key in self.unique_net_keys:\n            # Update target network.\n            <IND>online_variables = (\n                *self._observation_networks[key].variables,\n                *self._critic_networks[key].variables,\n                *self._policy_networks[key].variables,\n            )\n            target_variables = (\n                *self._target_observation_networks[key].variables,\n                *self._target_critic_networks[key].variables,\n                *self._target_policy_networks[key].variables,\n            )\n\n            # Make online -> target network update ops.\n            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n                <IND>for src, dest in zip(online_variables, target_variables):\n                    <IND>dest.assign(src)\n            <DED><DED>self._num_steps.assign_add(1)\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "instadeepai/Mava",
    "commit": "8e4dd412ccab4fba42dfe987b81124c901e1e4c0",
    "filename": "mava/systems/tf/maddpg/training.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/instadeepai-Mava/mava/systems/tf/maddpg/training.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "mava/systems/tf/maddpg/training.py:805:4 Inconsistent override [14]: `mava.systems.tf.maddpg.training.BaseRecurrentMADDPGTrainer._transform_observations` overrides method defined in `BaseMADDPGTrainer` inconsistently. Could not find parameter `obs` in overriding signature.",
    "message": " `mava.systems.tf.maddpg.training.BaseRecurrentMADDPGTrainer._transform_observations` overrides method defined in `BaseMADDPGTrainer` inconsistently. Could not find parameter `obs` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 805,
    "warning_line": "    def _transform_observations(",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nclass BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n    \"\"\"MADDPG trainer.\n",
        "source_code_len": 77,
        "target_code": "\nclass BaseRecurrentMADDPGTrainer(mava.Trainer):\n    \"\"\"MADDPG trainer.\n",
        "target_code_len": 72,
        "diff_format": "@@ -724,3 +723,3 @@\n \n-class BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n+class BaseRecurrentMADDPGTrainer(mava.Trainer):\n     \"\"\"MADDPG trainer.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n<DED><DED>class BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n    <IND>",
        "target_code_with_indent": "\n<DED><DED>class BaseRecurrentMADDPGTrainer(mava.Trainer):\n    <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        super().__init__(\n            agents=agents,\n            agent_types=agent_types,\n            policy_networks=policy_networks,\n            critic_networks=critic_networks,\n            target_policy_networks=target_policy_networks,\n            target_critic_networks=target_critic_networks,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            discount=discount,\n            target_update_period=target_update_period,\n            dataset=dataset,\n            observation_networks=observation_networks,\n            target_observation_networks=target_observation_networks,\n            shared_weights=shared_weights,\n            max_gradient_norm=max_gradient_norm,\n            counter=counter,\n            logger=logger,\n            checkpoint=checkpoint,\n            checkpoint_subpath=checkpoint_subpath,\n        )\n\n",
        "source_code_len": 877,
        "target_code": "\n        self._agents = agents\n        self._agent_types = agent_types\n        self._shared_weights = shared_weights\n        self._checkpoint = checkpoint\n\n        # Store online and target networks.\n        self._policy_networks = policy_networks\n        self._critic_networks = critic_networks\n        self._target_policy_networks = target_policy_networks\n        self._target_critic_networks = target_critic_networks\n\n        # Ensure obs and target networks are sonnet modules\n        self._observation_networks = {\n            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n        }\n        self._target_observation_networks = {\n            k: tf2_utils.to_sonnet_module(v)\n            for k, v in target_observation_networks.items()\n        }\n\n        # General learner book-keeping and loggers.\n        self._counter = counter or counting.Counter()\n        self._logger = logger or loggers.make_default_logger(\"trainer\")\n\n        # Other learner parameters.\n        self._discount = discount\n\n        # Set up gradient clipping.\n        if max_gradient_norm is not None:\n            self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n        else:  # A very large number. Infinity results in NaNs.\n            self._max_gradient_norm = tf.convert_to_tensor(1e10)\n\n        # Necessary to track when to update target networks.\n        self._num_steps = tf.Variable(0, dtype=tf.int32)\n        self._target_update_period = target_update_period\n\n        # Create an iterator to go through the dataset.\n        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n        self._critic_optimizer = critic_optimizer\n        self._policy_optimizer = policy_optimizer\n\n        # Dictionary with network keys for each agent.\n        self.agent_net_keys = {agent: agent for agent in self._agents}\n        if self._shared_weights:\n            self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n\n        self.unique_net_keys = self._agent_types if shared_weights else self._agents\n\n        # Expose the variables.\n        policy_networks_to_expose = {}\n        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n            \"critic\": {},\n            \"policy\": {},\n        }\n        for agent_key in self.unique_net_keys:\n            policy_network_to_expose = snt.Sequential(\n                [\n                    self._target_observation_networks[agent_key],\n                    self._target_policy_networks[agent_key],\n                ]\n            )\n            policy_networks_to_expose[agent_key] = policy_network_to_expose\n            self._system_network_variables[\"critic\"][\n                agent_key\n            ] = target_critic_networks[agent_key].variables\n            self._system_network_variables[\"policy\"][\n                agent_key\n            ] = policy_network_to_expose.variables\n\n        # Create checkpointer\n        self._system_checkpointer = {}\n        if checkpoint:\n            for agent_key in self.unique_net_keys:\n                objects_to_save = {\n                    \"counter\": self._counter,\n                    \"policy\": self._policy_networks[agent_key],\n                    \"critic\": self._critic_networks[agent_key],\n                    \"observation\": self._observation_networks[agent_key],\n                    \"target_policy\": self._target_policy_networks[agent_key],\n                    \"target_critic\": self._target_critic_networks[agent_key],\n                    \"target_observation\": self._target_observation_networks[agent_key],\n                    \"policy_optimizer\": self._policy_optimizer,\n                    \"critic_optimizer\": self._critic_optimizer,\n                    \"num_steps\": self._num_steps,\n                }\n\n                subdir = os.path.join(\"trainer\", agent_key)\n                checkpointer = tf2_savers.Checkpointer(\n                    time_delta_minutes=15,\n                    directory=checkpoint_subpath,\n                    objects_to_save=objects_to_save,\n                    subdirectory=subdir,\n                )\n                self._system_checkpointer[agent_key] = checkpointer\n        # Do not record timestamps until after the first learning step is done.\n        # This is to avoid including the time it takes for actors to come online and\n        # fill the replay buffer.\n        self._timestamp = None\n\n    def _update_target_networks(self) -> None:\n        for key in self.unique_net_keys:\n            # Update target network.\n            online_variables = (\n                *self._observation_networks[key].variables,\n                *self._critic_networks[key].variables,\n                *self._policy_networks[key].variables,\n            )\n            target_variables = (\n                *self._target_observation_networks[key].variables,\n                *self._target_critic_networks[key].variables,\n                *self._target_policy_networks[key].variables,\n            )\n\n            # Make online -> target network update ops.\n            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n                for src, dest in zip(online_variables, target_variables):\n                    dest.assign(src)\n            self._num_steps.assign_add(1)\n\n",
        "target_code_len": 5252,
        "diff_format": "@@ -775,23 +774,122 @@\n \n-        super().__init__(\n-            agents=agents,\n-            agent_types=agent_types,\n-            policy_networks=policy_networks,\n-            critic_networks=critic_networks,\n-            target_policy_networks=target_policy_networks,\n-            target_critic_networks=target_critic_networks,\n-            policy_optimizer=policy_optimizer,\n-            critic_optimizer=critic_optimizer,\n-            discount=discount,\n-            target_update_period=target_update_period,\n-            dataset=dataset,\n-            observation_networks=observation_networks,\n-            target_observation_networks=target_observation_networks,\n-            shared_weights=shared_weights,\n-            max_gradient_norm=max_gradient_norm,\n-            counter=counter,\n-            logger=logger,\n-            checkpoint=checkpoint,\n-            checkpoint_subpath=checkpoint_subpath,\n-        )\n+        self._agents = agents\n+        self._agent_types = agent_types\n+        self._shared_weights = shared_weights\n+        self._checkpoint = checkpoint\n+\n+        # Store online and target networks.\n+        self._policy_networks = policy_networks\n+        self._critic_networks = critic_networks\n+        self._target_policy_networks = target_policy_networks\n+        self._target_critic_networks = target_critic_networks\n+\n+        # Ensure obs and target networks are sonnet modules\n+        self._observation_networks = {\n+            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n+        }\n+        self._target_observation_networks = {\n+            k: tf2_utils.to_sonnet_module(v)\n+            for k, v in target_observation_networks.items()\n+        }\n+\n+        # General learner book-keeping and loggers.\n+        self._counter = counter or counting.Counter()\n+        self._logger = logger or loggers.make_default_logger(\"trainer\")\n+\n+        # Other learner parameters.\n+        self._discount = discount\n+\n+        # Set up gradient clipping.\n+        if max_gradient_norm is not None:\n+            self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n+        else:  # A very large number. Infinity results in NaNs.\n+            self._max_gradient_norm = tf.convert_to_tensor(1e10)\n+\n+        # Necessary to track when to update target networks.\n+        self._num_steps = tf.Variable(0, dtype=tf.int32)\n+        self._target_update_period = target_update_period\n+\n+        # Create an iterator to go through the dataset.\n+        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n+\n+        self._critic_optimizer = critic_optimizer\n+        self._policy_optimizer = policy_optimizer\n+\n+        # Dictionary with network keys for each agent.\n+        self.agent_net_keys = {agent: agent for agent in self._agents}\n+        if self._shared_weights:\n+            self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n+\n+        self.unique_net_keys = self._agent_types if shared_weights else self._agents\n+\n+        # Expose the variables.\n+        policy_networks_to_expose = {}\n+        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n+            \"critic\": {},\n+            \"policy\": {},\n+        }\n+        for agent_key in self.unique_net_keys:\n+            policy_network_to_expose = snt.Sequential(\n+                [\n+                    self._target_observation_networks[agent_key],\n+                    self._target_policy_networks[agent_key],\n+                ]\n+            )\n+            policy_networks_to_expose[agent_key] = policy_network_to_expose\n+            self._system_network_variables[\"critic\"][\n+                agent_key\n+            ] = target_critic_networks[agent_key].variables\n+            self._system_network_variables[\"policy\"][\n+                agent_key\n+            ] = policy_network_to_expose.variables\n+\n+        # Create checkpointer\n+        self._system_checkpointer = {}\n+        if checkpoint:\n+            for agent_key in self.unique_net_keys:\n+                objects_to_save = {\n+                    \"counter\": self._counter,\n+                    \"policy\": self._policy_networks[agent_key],\n+                    \"critic\": self._critic_networks[agent_key],\n+                    \"observation\": self._observation_networks[agent_key],\n+                    \"target_policy\": self._target_policy_networks[agent_key],\n+                    \"target_critic\": self._target_critic_networks[agent_key],\n+                    \"target_observation\": self._target_observation_networks[agent_key],\n+                    \"policy_optimizer\": self._policy_optimizer,\n+                    \"critic_optimizer\": self._critic_optimizer,\n+                    \"num_steps\": self._num_steps,\n+                }\n+\n+                subdir = os.path.join(\"trainer\", agent_key)\n+                checkpointer = tf2_savers.Checkpointer(\n+                    time_delta_minutes=15,\n+                    directory=checkpoint_subpath,\n+                    objects_to_save=objects_to_save,\n+                    subdirectory=subdir,\n+                )\n+                self._system_checkpointer[agent_key] = checkpointer\n+        # Do not record timestamps until after the first learning step is done.\n+        # This is to avoid including the time it takes for actors to come online and\n+        # fill the replay buffer.\n+        self._timestamp = None\n+\n+    def _update_target_networks(self) -> None:\n+        for key in self.unique_net_keys:\n+            # Update target network.\n+            online_variables = (\n+                *self._observation_networks[key].variables,\n+                *self._critic_networks[key].variables,\n+                *self._policy_networks[key].variables,\n+            )\n+            target_variables = (\n+                *self._target_observation_networks[key].variables,\n+                *self._target_critic_networks[key].variables,\n+                *self._target_policy_networks[key].variables,\n+            )\n+\n+            # Make online -> target network update ops.\n+            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n+                for src, dest in zip(online_variables, target_variables):\n+                    dest.assign(src)\n+            self._num_steps.assign_add(1)\n \n",
        "source_code_with_indent": "\n        super().__init__(\n            agents=agents,\n            agent_types=agent_types,\n            policy_networks=policy_networks,\n            critic_networks=critic_networks,\n            target_policy_networks=target_policy_networks,\n            target_critic_networks=target_critic_networks,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            discount=discount,\n            target_update_period=target_update_period,\n            dataset=dataset,\n            observation_networks=observation_networks,\n            target_observation_networks=target_observation_networks,\n            shared_weights=shared_weights,\n            max_gradient_norm=max_gradient_norm,\n            counter=counter,\n            logger=logger,\n            checkpoint=checkpoint,\n            checkpoint_subpath=checkpoint_subpath,\n        )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        self._agents = agents\n        self._agent_types = agent_types\n        self._shared_weights = shared_weights\n        self._checkpoint = checkpoint\n\n        # Store online and target networks.\n        self._policy_networks = policy_networks\n        self._critic_networks = critic_networks\n        self._target_policy_networks = target_policy_networks\n        self._target_critic_networks = target_critic_networks\n\n        # Ensure obs and target networks are sonnet modules\n        self._observation_networks = {\n            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n        }\n        self._target_observation_networks = {\n            k: tf2_utils.to_sonnet_module(v)\n            for k, v in target_observation_networks.items()\n        }\n\n        # General learner book-keeping and loggers.\n        self._counter = counter or counting.Counter()\n        self._logger = logger or loggers.make_default_logger(\"trainer\")\n\n        # Other learner parameters.\n        self._discount = discount\n\n        # Set up gradient clipping.\n        if max_gradient_norm is not None:\n            <IND>self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n        <DED>else:  # A very large number. Infinity results in NaNs.\n            <IND>self._max_gradient_norm = tf.convert_to_tensor(1e10)\n\n        # Necessary to track when to update target networks.\n        <DED>self._num_steps = tf.Variable(0, dtype=tf.int32)\n        self._target_update_period = target_update_period\n\n        # Create an iterator to go through the dataset.\n        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n        self._critic_optimizer = critic_optimizer\n        self._policy_optimizer = policy_optimizer\n\n        # Dictionary with network keys for each agent.\n        self.agent_net_keys = {agent: agent for agent in self._agents}\n        if self._shared_weights:\n            <IND>self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n\n        <DED>self.unique_net_keys = self._agent_types if shared_weights else self._agents\n\n        # Expose the variables.\n        policy_networks_to_expose = {}\n        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n            \"critic\": {},\n            \"policy\": {},\n        }\n        for agent_key in self.unique_net_keys:\n            <IND>policy_network_to_expose = snt.Sequential(\n                [\n                    self._target_observation_networks[agent_key],\n                    self._target_policy_networks[agent_key],\n                ]\n            )\n            policy_networks_to_expose[agent_key] = policy_network_to_expose\n            self._system_network_variables[\"critic\"][\n                agent_key\n            ] = target_critic_networks[agent_key].variables\n            self._system_network_variables[\"policy\"][\n                agent_key\n            ] = policy_network_to_expose.variables\n\n        # Create checkpointer\n        <DED>self._system_checkpointer = {}\n        if checkpoint:\n            <IND>for agent_key in self.unique_net_keys:\n                <IND>objects_to_save = {\n                    \"counter\": self._counter,\n                    \"policy\": self._policy_networks[agent_key],\n                    \"critic\": self._critic_networks[agent_key],\n                    \"observation\": self._observation_networks[agent_key],\n                    \"target_policy\": self._target_policy_networks[agent_key],\n                    \"target_critic\": self._target_critic_networks[agent_key],\n                    \"target_observation\": self._target_observation_networks[agent_key],\n                    \"policy_optimizer\": self._policy_optimizer,\n                    \"critic_optimizer\": self._critic_optimizer,\n                    \"num_steps\": self._num_steps,\n                }\n\n                subdir = os.path.join(\"trainer\", agent_key)\n                checkpointer = tf2_savers.Checkpointer(\n                    time_delta_minutes=15,\n                    directory=checkpoint_subpath,\n                    objects_to_save=objects_to_save,\n                    subdirectory=subdir,\n                )\n                self._system_checkpointer[agent_key] = checkpointer\n        # Do not record timestamps until after the first learning step is done.\n        # This is to avoid including the time it takes for actors to come online and\n        # fill the replay buffer.\n        <DED><DED>self._timestamp = None\n\n    <DED>def _update_target_networks(self) -> None:\n        <IND>for key in self.unique_net_keys:\n            # Update target network.\n            <IND>online_variables = (\n                *self._observation_networks[key].variables,\n                *self._critic_networks[key].variables,\n                *self._policy_networks[key].variables,\n            )\n            target_variables = (\n                *self._target_observation_networks[key].variables,\n                *self._target_critic_networks[key].variables,\n                *self._target_policy_networks[key].variables,\n            )\n\n            # Make online -> target network update ops.\n            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n                <IND>for src, dest in zip(online_variables, target_variables):\n                    <IND>dest.assign(src)\n            <DED><DED>self._num_steps.assign_add(1)\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "instadeepai/Mava",
    "commit": "8e4dd412ccab4fba42dfe987b81124c901e1e4c0",
    "filename": "mava/systems/tf/maddpg/training.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/instadeepai-Mava/mava/systems/tf/maddpg/training.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "mava/systems/tf/maddpg/training.py:835:4 Inconsistent override [14]: `mava.systems.tf.maddpg.training.BaseRecurrentMADDPGTrainer._get_critic_feed` overrides method defined in `BaseMADDPGTrainer` inconsistently. Could not find parameter `a_t` in overriding signature.",
    "message": " `mava.systems.tf.maddpg.training.BaseRecurrentMADDPGTrainer._get_critic_feed` overrides method defined in `BaseMADDPGTrainer` inconsistently. Could not find parameter `a_t` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 835,
    "warning_line": "    def _get_critic_feed(",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nclass BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n    \"\"\"MADDPG trainer.\n",
        "source_code_len": 77,
        "target_code": "\nclass BaseRecurrentMADDPGTrainer(mava.Trainer):\n    \"\"\"MADDPG trainer.\n",
        "target_code_len": 72,
        "diff_format": "@@ -724,3 +723,3 @@\n \n-class BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n+class BaseRecurrentMADDPGTrainer(mava.Trainer):\n     \"\"\"MADDPG trainer.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n<DED><DED>class BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n    <IND>",
        "target_code_with_indent": "\n<DED><DED>class BaseRecurrentMADDPGTrainer(mava.Trainer):\n    <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        super().__init__(\n            agents=agents,\n            agent_types=agent_types,\n            policy_networks=policy_networks,\n            critic_networks=critic_networks,\n            target_policy_networks=target_policy_networks,\n            target_critic_networks=target_critic_networks,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            discount=discount,\n            target_update_period=target_update_period,\n            dataset=dataset,\n            observation_networks=observation_networks,\n            target_observation_networks=target_observation_networks,\n            shared_weights=shared_weights,\n            max_gradient_norm=max_gradient_norm,\n            counter=counter,\n            logger=logger,\n            checkpoint=checkpoint,\n            checkpoint_subpath=checkpoint_subpath,\n        )\n\n",
        "source_code_len": 877,
        "target_code": "\n        self._agents = agents\n        self._agent_types = agent_types\n        self._shared_weights = shared_weights\n        self._checkpoint = checkpoint\n\n        # Store online and target networks.\n        self._policy_networks = policy_networks\n        self._critic_networks = critic_networks\n        self._target_policy_networks = target_policy_networks\n        self._target_critic_networks = target_critic_networks\n\n        # Ensure obs and target networks are sonnet modules\n        self._observation_networks = {\n            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n        }\n        self._target_observation_networks = {\n            k: tf2_utils.to_sonnet_module(v)\n            for k, v in target_observation_networks.items()\n        }\n\n        # General learner book-keeping and loggers.\n        self._counter = counter or counting.Counter()\n        self._logger = logger or loggers.make_default_logger(\"trainer\")\n\n        # Other learner parameters.\n        self._discount = discount\n\n        # Set up gradient clipping.\n        if max_gradient_norm is not None:\n            self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n        else:  # A very large number. Infinity results in NaNs.\n            self._max_gradient_norm = tf.convert_to_tensor(1e10)\n\n        # Necessary to track when to update target networks.\n        self._num_steps = tf.Variable(0, dtype=tf.int32)\n        self._target_update_period = target_update_period\n\n        # Create an iterator to go through the dataset.\n        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n        self._critic_optimizer = critic_optimizer\n        self._policy_optimizer = policy_optimizer\n\n        # Dictionary with network keys for each agent.\n        self.agent_net_keys = {agent: agent for agent in self._agents}\n        if self._shared_weights:\n            self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n\n        self.unique_net_keys = self._agent_types if shared_weights else self._agents\n\n        # Expose the variables.\n        policy_networks_to_expose = {}\n        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n            \"critic\": {},\n            \"policy\": {},\n        }\n        for agent_key in self.unique_net_keys:\n            policy_network_to_expose = snt.Sequential(\n                [\n                    self._target_observation_networks[agent_key],\n                    self._target_policy_networks[agent_key],\n                ]\n            )\n            policy_networks_to_expose[agent_key] = policy_network_to_expose\n            self._system_network_variables[\"critic\"][\n                agent_key\n            ] = target_critic_networks[agent_key].variables\n            self._system_network_variables[\"policy\"][\n                agent_key\n            ] = policy_network_to_expose.variables\n\n        # Create checkpointer\n        self._system_checkpointer = {}\n        if checkpoint:\n            for agent_key in self.unique_net_keys:\n                objects_to_save = {\n                    \"counter\": self._counter,\n                    \"policy\": self._policy_networks[agent_key],\n                    \"critic\": self._critic_networks[agent_key],\n                    \"observation\": self._observation_networks[agent_key],\n                    \"target_policy\": self._target_policy_networks[agent_key],\n                    \"target_critic\": self._target_critic_networks[agent_key],\n                    \"target_observation\": self._target_observation_networks[agent_key],\n                    \"policy_optimizer\": self._policy_optimizer,\n                    \"critic_optimizer\": self._critic_optimizer,\n                    \"num_steps\": self._num_steps,\n                }\n\n                subdir = os.path.join(\"trainer\", agent_key)\n                checkpointer = tf2_savers.Checkpointer(\n                    time_delta_minutes=15,\n                    directory=checkpoint_subpath,\n                    objects_to_save=objects_to_save,\n                    subdirectory=subdir,\n                )\n                self._system_checkpointer[agent_key] = checkpointer\n        # Do not record timestamps until after the first learning step is done.\n        # This is to avoid including the time it takes for actors to come online and\n        # fill the replay buffer.\n        self._timestamp = None\n\n    def _update_target_networks(self) -> None:\n        for key in self.unique_net_keys:\n            # Update target network.\n            online_variables = (\n                *self._observation_networks[key].variables,\n                *self._critic_networks[key].variables,\n                *self._policy_networks[key].variables,\n            )\n            target_variables = (\n                *self._target_observation_networks[key].variables,\n                *self._target_critic_networks[key].variables,\n                *self._target_policy_networks[key].variables,\n            )\n\n            # Make online -> target network update ops.\n            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n                for src, dest in zip(online_variables, target_variables):\n                    dest.assign(src)\n            self._num_steps.assign_add(1)\n\n",
        "target_code_len": 5252,
        "diff_format": "@@ -775,23 +774,122 @@\n \n-        super().__init__(\n-            agents=agents,\n-            agent_types=agent_types,\n-            policy_networks=policy_networks,\n-            critic_networks=critic_networks,\n-            target_policy_networks=target_policy_networks,\n-            target_critic_networks=target_critic_networks,\n-            policy_optimizer=policy_optimizer,\n-            critic_optimizer=critic_optimizer,\n-            discount=discount,\n-            target_update_period=target_update_period,\n-            dataset=dataset,\n-            observation_networks=observation_networks,\n-            target_observation_networks=target_observation_networks,\n-            shared_weights=shared_weights,\n-            max_gradient_norm=max_gradient_norm,\n-            counter=counter,\n-            logger=logger,\n-            checkpoint=checkpoint,\n-            checkpoint_subpath=checkpoint_subpath,\n-        )\n+        self._agents = agents\n+        self._agent_types = agent_types\n+        self._shared_weights = shared_weights\n+        self._checkpoint = checkpoint\n+\n+        # Store online and target networks.\n+        self._policy_networks = policy_networks\n+        self._critic_networks = critic_networks\n+        self._target_policy_networks = target_policy_networks\n+        self._target_critic_networks = target_critic_networks\n+\n+        # Ensure obs and target networks are sonnet modules\n+        self._observation_networks = {\n+            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n+        }\n+        self._target_observation_networks = {\n+            k: tf2_utils.to_sonnet_module(v)\n+            for k, v in target_observation_networks.items()\n+        }\n+\n+        # General learner book-keeping and loggers.\n+        self._counter = counter or counting.Counter()\n+        self._logger = logger or loggers.make_default_logger(\"trainer\")\n+\n+        # Other learner parameters.\n+        self._discount = discount\n+\n+        # Set up gradient clipping.\n+        if max_gradient_norm is not None:\n+            self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n+        else:  # A very large number. Infinity results in NaNs.\n+            self._max_gradient_norm = tf.convert_to_tensor(1e10)\n+\n+        # Necessary to track when to update target networks.\n+        self._num_steps = tf.Variable(0, dtype=tf.int32)\n+        self._target_update_period = target_update_period\n+\n+        # Create an iterator to go through the dataset.\n+        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n+\n+        self._critic_optimizer = critic_optimizer\n+        self._policy_optimizer = policy_optimizer\n+\n+        # Dictionary with network keys for each agent.\n+        self.agent_net_keys = {agent: agent for agent in self._agents}\n+        if self._shared_weights:\n+            self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n+\n+        self.unique_net_keys = self._agent_types if shared_weights else self._agents\n+\n+        # Expose the variables.\n+        policy_networks_to_expose = {}\n+        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n+            \"critic\": {},\n+            \"policy\": {},\n+        }\n+        for agent_key in self.unique_net_keys:\n+            policy_network_to_expose = snt.Sequential(\n+                [\n+                    self._target_observation_networks[agent_key],\n+                    self._target_policy_networks[agent_key],\n+                ]\n+            )\n+            policy_networks_to_expose[agent_key] = policy_network_to_expose\n+            self._system_network_variables[\"critic\"][\n+                agent_key\n+            ] = target_critic_networks[agent_key].variables\n+            self._system_network_variables[\"policy\"][\n+                agent_key\n+            ] = policy_network_to_expose.variables\n+\n+        # Create checkpointer\n+        self._system_checkpointer = {}\n+        if checkpoint:\n+            for agent_key in self.unique_net_keys:\n+                objects_to_save = {\n+                    \"counter\": self._counter,\n+                    \"policy\": self._policy_networks[agent_key],\n+                    \"critic\": self._critic_networks[agent_key],\n+                    \"observation\": self._observation_networks[agent_key],\n+                    \"target_policy\": self._target_policy_networks[agent_key],\n+                    \"target_critic\": self._target_critic_networks[agent_key],\n+                    \"target_observation\": self._target_observation_networks[agent_key],\n+                    \"policy_optimizer\": self._policy_optimizer,\n+                    \"critic_optimizer\": self._critic_optimizer,\n+                    \"num_steps\": self._num_steps,\n+                }\n+\n+                subdir = os.path.join(\"trainer\", agent_key)\n+                checkpointer = tf2_savers.Checkpointer(\n+                    time_delta_minutes=15,\n+                    directory=checkpoint_subpath,\n+                    objects_to_save=objects_to_save,\n+                    subdirectory=subdir,\n+                )\n+                self._system_checkpointer[agent_key] = checkpointer\n+        # Do not record timestamps until after the first learning step is done.\n+        # This is to avoid including the time it takes for actors to come online and\n+        # fill the replay buffer.\n+        self._timestamp = None\n+\n+    def _update_target_networks(self) -> None:\n+        for key in self.unique_net_keys:\n+            # Update target network.\n+            online_variables = (\n+                *self._observation_networks[key].variables,\n+                *self._critic_networks[key].variables,\n+                *self._policy_networks[key].variables,\n+            )\n+            target_variables = (\n+                *self._target_observation_networks[key].variables,\n+                *self._target_critic_networks[key].variables,\n+                *self._target_policy_networks[key].variables,\n+            )\n+\n+            # Make online -> target network update ops.\n+            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n+                for src, dest in zip(online_variables, target_variables):\n+                    dest.assign(src)\n+            self._num_steps.assign_add(1)\n \n",
        "source_code_with_indent": "\n        super().__init__(\n            agents=agents,\n            agent_types=agent_types,\n            policy_networks=policy_networks,\n            critic_networks=critic_networks,\n            target_policy_networks=target_policy_networks,\n            target_critic_networks=target_critic_networks,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            discount=discount,\n            target_update_period=target_update_period,\n            dataset=dataset,\n            observation_networks=observation_networks,\n            target_observation_networks=target_observation_networks,\n            shared_weights=shared_weights,\n            max_gradient_norm=max_gradient_norm,\n            counter=counter,\n            logger=logger,\n            checkpoint=checkpoint,\n            checkpoint_subpath=checkpoint_subpath,\n        )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        self._agents = agents\n        self._agent_types = agent_types\n        self._shared_weights = shared_weights\n        self._checkpoint = checkpoint\n\n        # Store online and target networks.\n        self._policy_networks = policy_networks\n        self._critic_networks = critic_networks\n        self._target_policy_networks = target_policy_networks\n        self._target_critic_networks = target_critic_networks\n\n        # Ensure obs and target networks are sonnet modules\n        self._observation_networks = {\n            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n        }\n        self._target_observation_networks = {\n            k: tf2_utils.to_sonnet_module(v)\n            for k, v in target_observation_networks.items()\n        }\n\n        # General learner book-keeping and loggers.\n        self._counter = counter or counting.Counter()\n        self._logger = logger or loggers.make_default_logger(\"trainer\")\n\n        # Other learner parameters.\n        self._discount = discount\n\n        # Set up gradient clipping.\n        if max_gradient_norm is not None:\n            <IND>self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n        <DED>else:  # A very large number. Infinity results in NaNs.\n            <IND>self._max_gradient_norm = tf.convert_to_tensor(1e10)\n\n        # Necessary to track when to update target networks.\n        <DED>self._num_steps = tf.Variable(0, dtype=tf.int32)\n        self._target_update_period = target_update_period\n\n        # Create an iterator to go through the dataset.\n        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n        self._critic_optimizer = critic_optimizer\n        self._policy_optimizer = policy_optimizer\n\n        # Dictionary with network keys for each agent.\n        self.agent_net_keys = {agent: agent for agent in self._agents}\n        if self._shared_weights:\n            <IND>self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n\n        <DED>self.unique_net_keys = self._agent_types if shared_weights else self._agents\n\n        # Expose the variables.\n        policy_networks_to_expose = {}\n        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n            \"critic\": {},\n            \"policy\": {},\n        }\n        for agent_key in self.unique_net_keys:\n            <IND>policy_network_to_expose = snt.Sequential(\n                [\n                    self._target_observation_networks[agent_key],\n                    self._target_policy_networks[agent_key],\n                ]\n            )\n            policy_networks_to_expose[agent_key] = policy_network_to_expose\n            self._system_network_variables[\"critic\"][\n                agent_key\n            ] = target_critic_networks[agent_key].variables\n            self._system_network_variables[\"policy\"][\n                agent_key\n            ] = policy_network_to_expose.variables\n\n        # Create checkpointer\n        <DED>self._system_checkpointer = {}\n        if checkpoint:\n            <IND>for agent_key in self.unique_net_keys:\n                <IND>objects_to_save = {\n                    \"counter\": self._counter,\n                    \"policy\": self._policy_networks[agent_key],\n                    \"critic\": self._critic_networks[agent_key],\n                    \"observation\": self._observation_networks[agent_key],\n                    \"target_policy\": self._target_policy_networks[agent_key],\n                    \"target_critic\": self._target_critic_networks[agent_key],\n                    \"target_observation\": self._target_observation_networks[agent_key],\n                    \"policy_optimizer\": self._policy_optimizer,\n                    \"critic_optimizer\": self._critic_optimizer,\n                    \"num_steps\": self._num_steps,\n                }\n\n                subdir = os.path.join(\"trainer\", agent_key)\n                checkpointer = tf2_savers.Checkpointer(\n                    time_delta_minutes=15,\n                    directory=checkpoint_subpath,\n                    objects_to_save=objects_to_save,\n                    subdirectory=subdir,\n                )\n                self._system_checkpointer[agent_key] = checkpointer\n        # Do not record timestamps until after the first learning step is done.\n        # This is to avoid including the time it takes for actors to come online and\n        # fill the replay buffer.\n        <DED><DED>self._timestamp = None\n\n    <DED>def _update_target_networks(self) -> None:\n        <IND>for key in self.unique_net_keys:\n            # Update target network.\n            <IND>online_variables = (\n                *self._observation_networks[key].variables,\n                *self._critic_networks[key].variables,\n                *self._policy_networks[key].variables,\n            )\n            target_variables = (\n                *self._target_observation_networks[key].variables,\n                *self._target_critic_networks[key].variables,\n                *self._target_policy_networks[key].variables,\n            )\n\n            # Make online -> target network update ops.\n            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n                <IND>for src, dest in zip(online_variables, target_variables):\n                    <IND>dest.assign(src)\n            <DED><DED>self._num_steps.assign_add(1)\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "instadeepai/Mava",
    "commit": "8e4dd412ccab4fba42dfe987b81124c901e1e4c0",
    "filename": "mava/systems/tf/maddpg/training.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/instadeepai-Mava/mava/systems/tf/maddpg/training.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "mava/systems/tf/maddpg/training.py:835:4 Inconsistent override [14]: `mava.systems.tf.maddpg.training.BaseRecurrentMADDPGTrainer._get_critic_feed` overrides method defined in `BaseMADDPGTrainer` inconsistently. Could not find parameter `a_tm1` in overriding signature.",
    "message": " `mava.systems.tf.maddpg.training.BaseRecurrentMADDPGTrainer._get_critic_feed` overrides method defined in `BaseMADDPGTrainer` inconsistently. Could not find parameter `a_tm1` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 835,
    "warning_line": "    def _get_critic_feed(",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nclass BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n    \"\"\"MADDPG trainer.\n",
        "source_code_len": 77,
        "target_code": "\nclass BaseRecurrentMADDPGTrainer(mava.Trainer):\n    \"\"\"MADDPG trainer.\n",
        "target_code_len": 72,
        "diff_format": "@@ -724,3 +723,3 @@\n \n-class BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n+class BaseRecurrentMADDPGTrainer(mava.Trainer):\n     \"\"\"MADDPG trainer.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n<DED><DED>class BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n    <IND>",
        "target_code_with_indent": "\n<DED><DED>class BaseRecurrentMADDPGTrainer(mava.Trainer):\n    <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        super().__init__(\n            agents=agents,\n            agent_types=agent_types,\n            policy_networks=policy_networks,\n            critic_networks=critic_networks,\n            target_policy_networks=target_policy_networks,\n            target_critic_networks=target_critic_networks,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            discount=discount,\n            target_update_period=target_update_period,\n            dataset=dataset,\n            observation_networks=observation_networks,\n            target_observation_networks=target_observation_networks,\n            shared_weights=shared_weights,\n            max_gradient_norm=max_gradient_norm,\n            counter=counter,\n            logger=logger,\n            checkpoint=checkpoint,\n            checkpoint_subpath=checkpoint_subpath,\n        )\n\n",
        "source_code_len": 877,
        "target_code": "\n        self._agents = agents\n        self._agent_types = agent_types\n        self._shared_weights = shared_weights\n        self._checkpoint = checkpoint\n\n        # Store online and target networks.\n        self._policy_networks = policy_networks\n        self._critic_networks = critic_networks\n        self._target_policy_networks = target_policy_networks\n        self._target_critic_networks = target_critic_networks\n\n        # Ensure obs and target networks are sonnet modules\n        self._observation_networks = {\n            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n        }\n        self._target_observation_networks = {\n            k: tf2_utils.to_sonnet_module(v)\n            for k, v in target_observation_networks.items()\n        }\n\n        # General learner book-keeping and loggers.\n        self._counter = counter or counting.Counter()\n        self._logger = logger or loggers.make_default_logger(\"trainer\")\n\n        # Other learner parameters.\n        self._discount = discount\n\n        # Set up gradient clipping.\n        if max_gradient_norm is not None:\n            self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n        else:  # A very large number. Infinity results in NaNs.\n            self._max_gradient_norm = tf.convert_to_tensor(1e10)\n\n        # Necessary to track when to update target networks.\n        self._num_steps = tf.Variable(0, dtype=tf.int32)\n        self._target_update_period = target_update_period\n\n        # Create an iterator to go through the dataset.\n        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n        self._critic_optimizer = critic_optimizer\n        self._policy_optimizer = policy_optimizer\n\n        # Dictionary with network keys for each agent.\n        self.agent_net_keys = {agent: agent for agent in self._agents}\n        if self._shared_weights:\n            self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n\n        self.unique_net_keys = self._agent_types if shared_weights else self._agents\n\n        # Expose the variables.\n        policy_networks_to_expose = {}\n        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n            \"critic\": {},\n            \"policy\": {},\n        }\n        for agent_key in self.unique_net_keys:\n            policy_network_to_expose = snt.Sequential(\n                [\n                    self._target_observation_networks[agent_key],\n                    self._target_policy_networks[agent_key],\n                ]\n            )\n            policy_networks_to_expose[agent_key] = policy_network_to_expose\n            self._system_network_variables[\"critic\"][\n                agent_key\n            ] = target_critic_networks[agent_key].variables\n            self._system_network_variables[\"policy\"][\n                agent_key\n            ] = policy_network_to_expose.variables\n\n        # Create checkpointer\n        self._system_checkpointer = {}\n        if checkpoint:\n            for agent_key in self.unique_net_keys:\n                objects_to_save = {\n                    \"counter\": self._counter,\n                    \"policy\": self._policy_networks[agent_key],\n                    \"critic\": self._critic_networks[agent_key],\n                    \"observation\": self._observation_networks[agent_key],\n                    \"target_policy\": self._target_policy_networks[agent_key],\n                    \"target_critic\": self._target_critic_networks[agent_key],\n                    \"target_observation\": self._target_observation_networks[agent_key],\n                    \"policy_optimizer\": self._policy_optimizer,\n                    \"critic_optimizer\": self._critic_optimizer,\n                    \"num_steps\": self._num_steps,\n                }\n\n                subdir = os.path.join(\"trainer\", agent_key)\n                checkpointer = tf2_savers.Checkpointer(\n                    time_delta_minutes=15,\n                    directory=checkpoint_subpath,\n                    objects_to_save=objects_to_save,\n                    subdirectory=subdir,\n                )\n                self._system_checkpointer[agent_key] = checkpointer\n        # Do not record timestamps until after the first learning step is done.\n        # This is to avoid including the time it takes for actors to come online and\n        # fill the replay buffer.\n        self._timestamp = None\n\n    def _update_target_networks(self) -> None:\n        for key in self.unique_net_keys:\n            # Update target network.\n            online_variables = (\n                *self._observation_networks[key].variables,\n                *self._critic_networks[key].variables,\n                *self._policy_networks[key].variables,\n            )\n            target_variables = (\n                *self._target_observation_networks[key].variables,\n                *self._target_critic_networks[key].variables,\n                *self._target_policy_networks[key].variables,\n            )\n\n            # Make online -> target network update ops.\n            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n                for src, dest in zip(online_variables, target_variables):\n                    dest.assign(src)\n            self._num_steps.assign_add(1)\n\n",
        "target_code_len": 5252,
        "diff_format": "@@ -775,23 +774,122 @@\n \n-        super().__init__(\n-            agents=agents,\n-            agent_types=agent_types,\n-            policy_networks=policy_networks,\n-            critic_networks=critic_networks,\n-            target_policy_networks=target_policy_networks,\n-            target_critic_networks=target_critic_networks,\n-            policy_optimizer=policy_optimizer,\n-            critic_optimizer=critic_optimizer,\n-            discount=discount,\n-            target_update_period=target_update_period,\n-            dataset=dataset,\n-            observation_networks=observation_networks,\n-            target_observation_networks=target_observation_networks,\n-            shared_weights=shared_weights,\n-            max_gradient_norm=max_gradient_norm,\n-            counter=counter,\n-            logger=logger,\n-            checkpoint=checkpoint,\n-            checkpoint_subpath=checkpoint_subpath,\n-        )\n+        self._agents = agents\n+        self._agent_types = agent_types\n+        self._shared_weights = shared_weights\n+        self._checkpoint = checkpoint\n+\n+        # Store online and target networks.\n+        self._policy_networks = policy_networks\n+        self._critic_networks = critic_networks\n+        self._target_policy_networks = target_policy_networks\n+        self._target_critic_networks = target_critic_networks\n+\n+        # Ensure obs and target networks are sonnet modules\n+        self._observation_networks = {\n+            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n+        }\n+        self._target_observation_networks = {\n+            k: tf2_utils.to_sonnet_module(v)\n+            for k, v in target_observation_networks.items()\n+        }\n+\n+        # General learner book-keeping and loggers.\n+        self._counter = counter or counting.Counter()\n+        self._logger = logger or loggers.make_default_logger(\"trainer\")\n+\n+        # Other learner parameters.\n+        self._discount = discount\n+\n+        # Set up gradient clipping.\n+        if max_gradient_norm is not None:\n+            self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n+        else:  # A very large number. Infinity results in NaNs.\n+            self._max_gradient_norm = tf.convert_to_tensor(1e10)\n+\n+        # Necessary to track when to update target networks.\n+        self._num_steps = tf.Variable(0, dtype=tf.int32)\n+        self._target_update_period = target_update_period\n+\n+        # Create an iterator to go through the dataset.\n+        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n+\n+        self._critic_optimizer = critic_optimizer\n+        self._policy_optimizer = policy_optimizer\n+\n+        # Dictionary with network keys for each agent.\n+        self.agent_net_keys = {agent: agent for agent in self._agents}\n+        if self._shared_weights:\n+            self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n+\n+        self.unique_net_keys = self._agent_types if shared_weights else self._agents\n+\n+        # Expose the variables.\n+        policy_networks_to_expose = {}\n+        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n+            \"critic\": {},\n+            \"policy\": {},\n+        }\n+        for agent_key in self.unique_net_keys:\n+            policy_network_to_expose = snt.Sequential(\n+                [\n+                    self._target_observation_networks[agent_key],\n+                    self._target_policy_networks[agent_key],\n+                ]\n+            )\n+            policy_networks_to_expose[agent_key] = policy_network_to_expose\n+            self._system_network_variables[\"critic\"][\n+                agent_key\n+            ] = target_critic_networks[agent_key].variables\n+            self._system_network_variables[\"policy\"][\n+                agent_key\n+            ] = policy_network_to_expose.variables\n+\n+        # Create checkpointer\n+        self._system_checkpointer = {}\n+        if checkpoint:\n+            for agent_key in self.unique_net_keys:\n+                objects_to_save = {\n+                    \"counter\": self._counter,\n+                    \"policy\": self._policy_networks[agent_key],\n+                    \"critic\": self._critic_networks[agent_key],\n+                    \"observation\": self._observation_networks[agent_key],\n+                    \"target_policy\": self._target_policy_networks[agent_key],\n+                    \"target_critic\": self._target_critic_networks[agent_key],\n+                    \"target_observation\": self._target_observation_networks[agent_key],\n+                    \"policy_optimizer\": self._policy_optimizer,\n+                    \"critic_optimizer\": self._critic_optimizer,\n+                    \"num_steps\": self._num_steps,\n+                }\n+\n+                subdir = os.path.join(\"trainer\", agent_key)\n+                checkpointer = tf2_savers.Checkpointer(\n+                    time_delta_minutes=15,\n+                    directory=checkpoint_subpath,\n+                    objects_to_save=objects_to_save,\n+                    subdirectory=subdir,\n+                )\n+                self._system_checkpointer[agent_key] = checkpointer\n+        # Do not record timestamps until after the first learning step is done.\n+        # This is to avoid including the time it takes for actors to come online and\n+        # fill the replay buffer.\n+        self._timestamp = None\n+\n+    def _update_target_networks(self) -> None:\n+        for key in self.unique_net_keys:\n+            # Update target network.\n+            online_variables = (\n+                *self._observation_networks[key].variables,\n+                *self._critic_networks[key].variables,\n+                *self._policy_networks[key].variables,\n+            )\n+            target_variables = (\n+                *self._target_observation_networks[key].variables,\n+                *self._target_critic_networks[key].variables,\n+                *self._target_policy_networks[key].variables,\n+            )\n+\n+            # Make online -> target network update ops.\n+            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n+                for src, dest in zip(online_variables, target_variables):\n+                    dest.assign(src)\n+            self._num_steps.assign_add(1)\n \n",
        "source_code_with_indent": "\n        super().__init__(\n            agents=agents,\n            agent_types=agent_types,\n            policy_networks=policy_networks,\n            critic_networks=critic_networks,\n            target_policy_networks=target_policy_networks,\n            target_critic_networks=target_critic_networks,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            discount=discount,\n            target_update_period=target_update_period,\n            dataset=dataset,\n            observation_networks=observation_networks,\n            target_observation_networks=target_observation_networks,\n            shared_weights=shared_weights,\n            max_gradient_norm=max_gradient_norm,\n            counter=counter,\n            logger=logger,\n            checkpoint=checkpoint,\n            checkpoint_subpath=checkpoint_subpath,\n        )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        self._agents = agents\n        self._agent_types = agent_types\n        self._shared_weights = shared_weights\n        self._checkpoint = checkpoint\n\n        # Store online and target networks.\n        self._policy_networks = policy_networks\n        self._critic_networks = critic_networks\n        self._target_policy_networks = target_policy_networks\n        self._target_critic_networks = target_critic_networks\n\n        # Ensure obs and target networks are sonnet modules\n        self._observation_networks = {\n            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n        }\n        self._target_observation_networks = {\n            k: tf2_utils.to_sonnet_module(v)\n            for k, v in target_observation_networks.items()\n        }\n\n        # General learner book-keeping and loggers.\n        self._counter = counter or counting.Counter()\n        self._logger = logger or loggers.make_default_logger(\"trainer\")\n\n        # Other learner parameters.\n        self._discount = discount\n\n        # Set up gradient clipping.\n        if max_gradient_norm is not None:\n            <IND>self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n        <DED>else:  # A very large number. Infinity results in NaNs.\n            <IND>self._max_gradient_norm = tf.convert_to_tensor(1e10)\n\n        # Necessary to track when to update target networks.\n        <DED>self._num_steps = tf.Variable(0, dtype=tf.int32)\n        self._target_update_period = target_update_period\n\n        # Create an iterator to go through the dataset.\n        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n        self._critic_optimizer = critic_optimizer\n        self._policy_optimizer = policy_optimizer\n\n        # Dictionary with network keys for each agent.\n        self.agent_net_keys = {agent: agent for agent in self._agents}\n        if self._shared_weights:\n            <IND>self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n\n        <DED>self.unique_net_keys = self._agent_types if shared_weights else self._agents\n\n        # Expose the variables.\n        policy_networks_to_expose = {}\n        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n            \"critic\": {},\n            \"policy\": {},\n        }\n        for agent_key in self.unique_net_keys:\n            <IND>policy_network_to_expose = snt.Sequential(\n                [\n                    self._target_observation_networks[agent_key],\n                    self._target_policy_networks[agent_key],\n                ]\n            )\n            policy_networks_to_expose[agent_key] = policy_network_to_expose\n            self._system_network_variables[\"critic\"][\n                agent_key\n            ] = target_critic_networks[agent_key].variables\n            self._system_network_variables[\"policy\"][\n                agent_key\n            ] = policy_network_to_expose.variables\n\n        # Create checkpointer\n        <DED>self._system_checkpointer = {}\n        if checkpoint:\n            <IND>for agent_key in self.unique_net_keys:\n                <IND>objects_to_save = {\n                    \"counter\": self._counter,\n                    \"policy\": self._policy_networks[agent_key],\n                    \"critic\": self._critic_networks[agent_key],\n                    \"observation\": self._observation_networks[agent_key],\n                    \"target_policy\": self._target_policy_networks[agent_key],\n                    \"target_critic\": self._target_critic_networks[agent_key],\n                    \"target_observation\": self._target_observation_networks[agent_key],\n                    \"policy_optimizer\": self._policy_optimizer,\n                    \"critic_optimizer\": self._critic_optimizer,\n                    \"num_steps\": self._num_steps,\n                }\n\n                subdir = os.path.join(\"trainer\", agent_key)\n                checkpointer = tf2_savers.Checkpointer(\n                    time_delta_minutes=15,\n                    directory=checkpoint_subpath,\n                    objects_to_save=objects_to_save,\n                    subdirectory=subdir,\n                )\n                self._system_checkpointer[agent_key] = checkpointer\n        # Do not record timestamps until after the first learning step is done.\n        # This is to avoid including the time it takes for actors to come online and\n        # fill the replay buffer.\n        <DED><DED>self._timestamp = None\n\n    <DED>def _update_target_networks(self) -> None:\n        <IND>for key in self.unique_net_keys:\n            # Update target network.\n            <IND>online_variables = (\n                *self._observation_networks[key].variables,\n                *self._critic_networks[key].variables,\n                *self._policy_networks[key].variables,\n            )\n            target_variables = (\n                *self._target_observation_networks[key].variables,\n                *self._target_critic_networks[key].variables,\n                *self._target_policy_networks[key].variables,\n            )\n\n            # Make online -> target network update ops.\n            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n                <IND>for src, dest in zip(online_variables, target_variables):\n                    <IND>dest.assign(src)\n            <DED><DED>self._num_steps.assign_add(1)\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "instadeepai/Mava",
    "commit": "8e4dd412ccab4fba42dfe987b81124c901e1e4c0",
    "filename": "mava/systems/tf/maddpg/training.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/instadeepai-Mava/mava/systems/tf/maddpg/training.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "mava/systems/tf/maddpg/training.py:835:4 Inconsistent override [14]: `mava.systems.tf.maddpg.training.BaseRecurrentMADDPGTrainer._get_critic_feed` overrides method defined in `BaseMADDPGTrainer` inconsistently. Could not find parameter `e_t` in overriding signature.",
    "message": " `mava.systems.tf.maddpg.training.BaseRecurrentMADDPGTrainer._get_critic_feed` overrides method defined in `BaseMADDPGTrainer` inconsistently. Could not find parameter `e_t` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 835,
    "warning_line": "    def _get_critic_feed(",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nclass BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n    \"\"\"MADDPG trainer.\n",
        "source_code_len": 77,
        "target_code": "\nclass BaseRecurrentMADDPGTrainer(mava.Trainer):\n    \"\"\"MADDPG trainer.\n",
        "target_code_len": 72,
        "diff_format": "@@ -724,3 +723,3 @@\n \n-class BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n+class BaseRecurrentMADDPGTrainer(mava.Trainer):\n     \"\"\"MADDPG trainer.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n<DED><DED>class BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n    <IND>",
        "target_code_with_indent": "\n<DED><DED>class BaseRecurrentMADDPGTrainer(mava.Trainer):\n    <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        super().__init__(\n            agents=agents,\n            agent_types=agent_types,\n            policy_networks=policy_networks,\n            critic_networks=critic_networks,\n            target_policy_networks=target_policy_networks,\n            target_critic_networks=target_critic_networks,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            discount=discount,\n            target_update_period=target_update_period,\n            dataset=dataset,\n            observation_networks=observation_networks,\n            target_observation_networks=target_observation_networks,\n            shared_weights=shared_weights,\n            max_gradient_norm=max_gradient_norm,\n            counter=counter,\n            logger=logger,\n            checkpoint=checkpoint,\n            checkpoint_subpath=checkpoint_subpath,\n        )\n\n",
        "source_code_len": 877,
        "target_code": "\n        self._agents = agents\n        self._agent_types = agent_types\n        self._shared_weights = shared_weights\n        self._checkpoint = checkpoint\n\n        # Store online and target networks.\n        self._policy_networks = policy_networks\n        self._critic_networks = critic_networks\n        self._target_policy_networks = target_policy_networks\n        self._target_critic_networks = target_critic_networks\n\n        # Ensure obs and target networks are sonnet modules\n        self._observation_networks = {\n            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n        }\n        self._target_observation_networks = {\n            k: tf2_utils.to_sonnet_module(v)\n            for k, v in target_observation_networks.items()\n        }\n\n        # General learner book-keeping and loggers.\n        self._counter = counter or counting.Counter()\n        self._logger = logger or loggers.make_default_logger(\"trainer\")\n\n        # Other learner parameters.\n        self._discount = discount\n\n        # Set up gradient clipping.\n        if max_gradient_norm is not None:\n            self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n        else:  # A very large number. Infinity results in NaNs.\n            self._max_gradient_norm = tf.convert_to_tensor(1e10)\n\n        # Necessary to track when to update target networks.\n        self._num_steps = tf.Variable(0, dtype=tf.int32)\n        self._target_update_period = target_update_period\n\n        # Create an iterator to go through the dataset.\n        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n        self._critic_optimizer = critic_optimizer\n        self._policy_optimizer = policy_optimizer\n\n        # Dictionary with network keys for each agent.\n        self.agent_net_keys = {agent: agent for agent in self._agents}\n        if self._shared_weights:\n            self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n\n        self.unique_net_keys = self._agent_types if shared_weights else self._agents\n\n        # Expose the variables.\n        policy_networks_to_expose = {}\n        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n            \"critic\": {},\n            \"policy\": {},\n        }\n        for agent_key in self.unique_net_keys:\n            policy_network_to_expose = snt.Sequential(\n                [\n                    self._target_observation_networks[agent_key],\n                    self._target_policy_networks[agent_key],\n                ]\n            )\n            policy_networks_to_expose[agent_key] = policy_network_to_expose\n            self._system_network_variables[\"critic\"][\n                agent_key\n            ] = target_critic_networks[agent_key].variables\n            self._system_network_variables[\"policy\"][\n                agent_key\n            ] = policy_network_to_expose.variables\n\n        # Create checkpointer\n        self._system_checkpointer = {}\n        if checkpoint:\n            for agent_key in self.unique_net_keys:\n                objects_to_save = {\n                    \"counter\": self._counter,\n                    \"policy\": self._policy_networks[agent_key],\n                    \"critic\": self._critic_networks[agent_key],\n                    \"observation\": self._observation_networks[agent_key],\n                    \"target_policy\": self._target_policy_networks[agent_key],\n                    \"target_critic\": self._target_critic_networks[agent_key],\n                    \"target_observation\": self._target_observation_networks[agent_key],\n                    \"policy_optimizer\": self._policy_optimizer,\n                    \"critic_optimizer\": self._critic_optimizer,\n                    \"num_steps\": self._num_steps,\n                }\n\n                subdir = os.path.join(\"trainer\", agent_key)\n                checkpointer = tf2_savers.Checkpointer(\n                    time_delta_minutes=15,\n                    directory=checkpoint_subpath,\n                    objects_to_save=objects_to_save,\n                    subdirectory=subdir,\n                )\n                self._system_checkpointer[agent_key] = checkpointer\n        # Do not record timestamps until after the first learning step is done.\n        # This is to avoid including the time it takes for actors to come online and\n        # fill the replay buffer.\n        self._timestamp = None\n\n    def _update_target_networks(self) -> None:\n        for key in self.unique_net_keys:\n            # Update target network.\n            online_variables = (\n                *self._observation_networks[key].variables,\n                *self._critic_networks[key].variables,\n                *self._policy_networks[key].variables,\n            )\n            target_variables = (\n                *self._target_observation_networks[key].variables,\n                *self._target_critic_networks[key].variables,\n                *self._target_policy_networks[key].variables,\n            )\n\n            # Make online -> target network update ops.\n            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n                for src, dest in zip(online_variables, target_variables):\n                    dest.assign(src)\n            self._num_steps.assign_add(1)\n\n",
        "target_code_len": 5252,
        "diff_format": "@@ -775,23 +774,122 @@\n \n-        super().__init__(\n-            agents=agents,\n-            agent_types=agent_types,\n-            policy_networks=policy_networks,\n-            critic_networks=critic_networks,\n-            target_policy_networks=target_policy_networks,\n-            target_critic_networks=target_critic_networks,\n-            policy_optimizer=policy_optimizer,\n-            critic_optimizer=critic_optimizer,\n-            discount=discount,\n-            target_update_period=target_update_period,\n-            dataset=dataset,\n-            observation_networks=observation_networks,\n-            target_observation_networks=target_observation_networks,\n-            shared_weights=shared_weights,\n-            max_gradient_norm=max_gradient_norm,\n-            counter=counter,\n-            logger=logger,\n-            checkpoint=checkpoint,\n-            checkpoint_subpath=checkpoint_subpath,\n-        )\n+        self._agents = agents\n+        self._agent_types = agent_types\n+        self._shared_weights = shared_weights\n+        self._checkpoint = checkpoint\n+\n+        # Store online and target networks.\n+        self._policy_networks = policy_networks\n+        self._critic_networks = critic_networks\n+        self._target_policy_networks = target_policy_networks\n+        self._target_critic_networks = target_critic_networks\n+\n+        # Ensure obs and target networks are sonnet modules\n+        self._observation_networks = {\n+            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n+        }\n+        self._target_observation_networks = {\n+            k: tf2_utils.to_sonnet_module(v)\n+            for k, v in target_observation_networks.items()\n+        }\n+\n+        # General learner book-keeping and loggers.\n+        self._counter = counter or counting.Counter()\n+        self._logger = logger or loggers.make_default_logger(\"trainer\")\n+\n+        # Other learner parameters.\n+        self._discount = discount\n+\n+        # Set up gradient clipping.\n+        if max_gradient_norm is not None:\n+            self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n+        else:  # A very large number. Infinity results in NaNs.\n+            self._max_gradient_norm = tf.convert_to_tensor(1e10)\n+\n+        # Necessary to track when to update target networks.\n+        self._num_steps = tf.Variable(0, dtype=tf.int32)\n+        self._target_update_period = target_update_period\n+\n+        # Create an iterator to go through the dataset.\n+        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n+\n+        self._critic_optimizer = critic_optimizer\n+        self._policy_optimizer = policy_optimizer\n+\n+        # Dictionary with network keys for each agent.\n+        self.agent_net_keys = {agent: agent for agent in self._agents}\n+        if self._shared_weights:\n+            self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n+\n+        self.unique_net_keys = self._agent_types if shared_weights else self._agents\n+\n+        # Expose the variables.\n+        policy_networks_to_expose = {}\n+        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n+            \"critic\": {},\n+            \"policy\": {},\n+        }\n+        for agent_key in self.unique_net_keys:\n+            policy_network_to_expose = snt.Sequential(\n+                [\n+                    self._target_observation_networks[agent_key],\n+                    self._target_policy_networks[agent_key],\n+                ]\n+            )\n+            policy_networks_to_expose[agent_key] = policy_network_to_expose\n+            self._system_network_variables[\"critic\"][\n+                agent_key\n+            ] = target_critic_networks[agent_key].variables\n+            self._system_network_variables[\"policy\"][\n+                agent_key\n+            ] = policy_network_to_expose.variables\n+\n+        # Create checkpointer\n+        self._system_checkpointer = {}\n+        if checkpoint:\n+            for agent_key in self.unique_net_keys:\n+                objects_to_save = {\n+                    \"counter\": self._counter,\n+                    \"policy\": self._policy_networks[agent_key],\n+                    \"critic\": self._critic_networks[agent_key],\n+                    \"observation\": self._observation_networks[agent_key],\n+                    \"target_policy\": self._target_policy_networks[agent_key],\n+                    \"target_critic\": self._target_critic_networks[agent_key],\n+                    \"target_observation\": self._target_observation_networks[agent_key],\n+                    \"policy_optimizer\": self._policy_optimizer,\n+                    \"critic_optimizer\": self._critic_optimizer,\n+                    \"num_steps\": self._num_steps,\n+                }\n+\n+                subdir = os.path.join(\"trainer\", agent_key)\n+                checkpointer = tf2_savers.Checkpointer(\n+                    time_delta_minutes=15,\n+                    directory=checkpoint_subpath,\n+                    objects_to_save=objects_to_save,\n+                    subdirectory=subdir,\n+                )\n+                self._system_checkpointer[agent_key] = checkpointer\n+        # Do not record timestamps until after the first learning step is done.\n+        # This is to avoid including the time it takes for actors to come online and\n+        # fill the replay buffer.\n+        self._timestamp = None\n+\n+    def _update_target_networks(self) -> None:\n+        for key in self.unique_net_keys:\n+            # Update target network.\n+            online_variables = (\n+                *self._observation_networks[key].variables,\n+                *self._critic_networks[key].variables,\n+                *self._policy_networks[key].variables,\n+            )\n+            target_variables = (\n+                *self._target_observation_networks[key].variables,\n+                *self._target_critic_networks[key].variables,\n+                *self._target_policy_networks[key].variables,\n+            )\n+\n+            # Make online -> target network update ops.\n+            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n+                for src, dest in zip(online_variables, target_variables):\n+                    dest.assign(src)\n+            self._num_steps.assign_add(1)\n \n",
        "source_code_with_indent": "\n        super().__init__(\n            agents=agents,\n            agent_types=agent_types,\n            policy_networks=policy_networks,\n            critic_networks=critic_networks,\n            target_policy_networks=target_policy_networks,\n            target_critic_networks=target_critic_networks,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            discount=discount,\n            target_update_period=target_update_period,\n            dataset=dataset,\n            observation_networks=observation_networks,\n            target_observation_networks=target_observation_networks,\n            shared_weights=shared_weights,\n            max_gradient_norm=max_gradient_norm,\n            counter=counter,\n            logger=logger,\n            checkpoint=checkpoint,\n            checkpoint_subpath=checkpoint_subpath,\n        )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        self._agents = agents\n        self._agent_types = agent_types\n        self._shared_weights = shared_weights\n        self._checkpoint = checkpoint\n\n        # Store online and target networks.\n        self._policy_networks = policy_networks\n        self._critic_networks = critic_networks\n        self._target_policy_networks = target_policy_networks\n        self._target_critic_networks = target_critic_networks\n\n        # Ensure obs and target networks are sonnet modules\n        self._observation_networks = {\n            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n        }\n        self._target_observation_networks = {\n            k: tf2_utils.to_sonnet_module(v)\n            for k, v in target_observation_networks.items()\n        }\n\n        # General learner book-keeping and loggers.\n        self._counter = counter or counting.Counter()\n        self._logger = logger or loggers.make_default_logger(\"trainer\")\n\n        # Other learner parameters.\n        self._discount = discount\n\n        # Set up gradient clipping.\n        if max_gradient_norm is not None:\n            <IND>self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n        <DED>else:  # A very large number. Infinity results in NaNs.\n            <IND>self._max_gradient_norm = tf.convert_to_tensor(1e10)\n\n        # Necessary to track when to update target networks.\n        <DED>self._num_steps = tf.Variable(0, dtype=tf.int32)\n        self._target_update_period = target_update_period\n\n        # Create an iterator to go through the dataset.\n        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n        self._critic_optimizer = critic_optimizer\n        self._policy_optimizer = policy_optimizer\n\n        # Dictionary with network keys for each agent.\n        self.agent_net_keys = {agent: agent for agent in self._agents}\n        if self._shared_weights:\n            <IND>self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n\n        <DED>self.unique_net_keys = self._agent_types if shared_weights else self._agents\n\n        # Expose the variables.\n        policy_networks_to_expose = {}\n        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n            \"critic\": {},\n            \"policy\": {},\n        }\n        for agent_key in self.unique_net_keys:\n            <IND>policy_network_to_expose = snt.Sequential(\n                [\n                    self._target_observation_networks[agent_key],\n                    self._target_policy_networks[agent_key],\n                ]\n            )\n            policy_networks_to_expose[agent_key] = policy_network_to_expose\n            self._system_network_variables[\"critic\"][\n                agent_key\n            ] = target_critic_networks[agent_key].variables\n            self._system_network_variables[\"policy\"][\n                agent_key\n            ] = policy_network_to_expose.variables\n\n        # Create checkpointer\n        <DED>self._system_checkpointer = {}\n        if checkpoint:\n            <IND>for agent_key in self.unique_net_keys:\n                <IND>objects_to_save = {\n                    \"counter\": self._counter,\n                    \"policy\": self._policy_networks[agent_key],\n                    \"critic\": self._critic_networks[agent_key],\n                    \"observation\": self._observation_networks[agent_key],\n                    \"target_policy\": self._target_policy_networks[agent_key],\n                    \"target_critic\": self._target_critic_networks[agent_key],\n                    \"target_observation\": self._target_observation_networks[agent_key],\n                    \"policy_optimizer\": self._policy_optimizer,\n                    \"critic_optimizer\": self._critic_optimizer,\n                    \"num_steps\": self._num_steps,\n                }\n\n                subdir = os.path.join(\"trainer\", agent_key)\n                checkpointer = tf2_savers.Checkpointer(\n                    time_delta_minutes=15,\n                    directory=checkpoint_subpath,\n                    objects_to_save=objects_to_save,\n                    subdirectory=subdir,\n                )\n                self._system_checkpointer[agent_key] = checkpointer\n        # Do not record timestamps until after the first learning step is done.\n        # This is to avoid including the time it takes for actors to come online and\n        # fill the replay buffer.\n        <DED><DED>self._timestamp = None\n\n    <DED>def _update_target_networks(self) -> None:\n        <IND>for key in self.unique_net_keys:\n            # Update target network.\n            <IND>online_variables = (\n                *self._observation_networks[key].variables,\n                *self._critic_networks[key].variables,\n                *self._policy_networks[key].variables,\n            )\n            target_variables = (\n                *self._target_observation_networks[key].variables,\n                *self._target_critic_networks[key].variables,\n                *self._target_policy_networks[key].variables,\n            )\n\n            # Make online -> target network update ops.\n            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n                <IND>for src, dest in zip(online_variables, target_variables):\n                    <IND>dest.assign(src)\n            <DED><DED>self._num_steps.assign_add(1)\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "instadeepai/Mava",
    "commit": "8e4dd412ccab4fba42dfe987b81124c901e1e4c0",
    "filename": "mava/systems/tf/maddpg/training.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/instadeepai-Mava/mava/systems/tf/maddpg/training.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "mava/systems/tf/maddpg/training.py:835:4 Inconsistent override [14]: `mava.systems.tf.maddpg.training.BaseRecurrentMADDPGTrainer._get_critic_feed` overrides method defined in `BaseMADDPGTrainer` inconsistently. Could not find parameter `e_tm1` in overriding signature.",
    "message": " `mava.systems.tf.maddpg.training.BaseRecurrentMADDPGTrainer._get_critic_feed` overrides method defined in `BaseMADDPGTrainer` inconsistently. Could not find parameter `e_tm1` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 835,
    "warning_line": "    def _get_critic_feed(",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nclass BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n    \"\"\"MADDPG trainer.\n",
        "source_code_len": 77,
        "target_code": "\nclass BaseRecurrentMADDPGTrainer(mava.Trainer):\n    \"\"\"MADDPG trainer.\n",
        "target_code_len": 72,
        "diff_format": "@@ -724,3 +723,3 @@\n \n-class BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n+class BaseRecurrentMADDPGTrainer(mava.Trainer):\n     \"\"\"MADDPG trainer.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n<DED><DED>class BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n    <IND>",
        "target_code_with_indent": "\n<DED><DED>class BaseRecurrentMADDPGTrainer(mava.Trainer):\n    <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        super().__init__(\n            agents=agents,\n            agent_types=agent_types,\n            policy_networks=policy_networks,\n            critic_networks=critic_networks,\n            target_policy_networks=target_policy_networks,\n            target_critic_networks=target_critic_networks,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            discount=discount,\n            target_update_period=target_update_period,\n            dataset=dataset,\n            observation_networks=observation_networks,\n            target_observation_networks=target_observation_networks,\n            shared_weights=shared_weights,\n            max_gradient_norm=max_gradient_norm,\n            counter=counter,\n            logger=logger,\n            checkpoint=checkpoint,\n            checkpoint_subpath=checkpoint_subpath,\n        )\n\n",
        "source_code_len": 877,
        "target_code": "\n        self._agents = agents\n        self._agent_types = agent_types\n        self._shared_weights = shared_weights\n        self._checkpoint = checkpoint\n\n        # Store online and target networks.\n        self._policy_networks = policy_networks\n        self._critic_networks = critic_networks\n        self._target_policy_networks = target_policy_networks\n        self._target_critic_networks = target_critic_networks\n\n        # Ensure obs and target networks are sonnet modules\n        self._observation_networks = {\n            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n        }\n        self._target_observation_networks = {\n            k: tf2_utils.to_sonnet_module(v)\n            for k, v in target_observation_networks.items()\n        }\n\n        # General learner book-keeping and loggers.\n        self._counter = counter or counting.Counter()\n        self._logger = logger or loggers.make_default_logger(\"trainer\")\n\n        # Other learner parameters.\n        self._discount = discount\n\n        # Set up gradient clipping.\n        if max_gradient_norm is not None:\n            self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n        else:  # A very large number. Infinity results in NaNs.\n            self._max_gradient_norm = tf.convert_to_tensor(1e10)\n\n        # Necessary to track when to update target networks.\n        self._num_steps = tf.Variable(0, dtype=tf.int32)\n        self._target_update_period = target_update_period\n\n        # Create an iterator to go through the dataset.\n        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n        self._critic_optimizer = critic_optimizer\n        self._policy_optimizer = policy_optimizer\n\n        # Dictionary with network keys for each agent.\n        self.agent_net_keys = {agent: agent for agent in self._agents}\n        if self._shared_weights:\n            self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n\n        self.unique_net_keys = self._agent_types if shared_weights else self._agents\n\n        # Expose the variables.\n        policy_networks_to_expose = {}\n        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n            \"critic\": {},\n            \"policy\": {},\n        }\n        for agent_key in self.unique_net_keys:\n            policy_network_to_expose = snt.Sequential(\n                [\n                    self._target_observation_networks[agent_key],\n                    self._target_policy_networks[agent_key],\n                ]\n            )\n            policy_networks_to_expose[agent_key] = policy_network_to_expose\n            self._system_network_variables[\"critic\"][\n                agent_key\n            ] = target_critic_networks[agent_key].variables\n            self._system_network_variables[\"policy\"][\n                agent_key\n            ] = policy_network_to_expose.variables\n\n        # Create checkpointer\n        self._system_checkpointer = {}\n        if checkpoint:\n            for agent_key in self.unique_net_keys:\n                objects_to_save = {\n                    \"counter\": self._counter,\n                    \"policy\": self._policy_networks[agent_key],\n                    \"critic\": self._critic_networks[agent_key],\n                    \"observation\": self._observation_networks[agent_key],\n                    \"target_policy\": self._target_policy_networks[agent_key],\n                    \"target_critic\": self._target_critic_networks[agent_key],\n                    \"target_observation\": self._target_observation_networks[agent_key],\n                    \"policy_optimizer\": self._policy_optimizer,\n                    \"critic_optimizer\": self._critic_optimizer,\n                    \"num_steps\": self._num_steps,\n                }\n\n                subdir = os.path.join(\"trainer\", agent_key)\n                checkpointer = tf2_savers.Checkpointer(\n                    time_delta_minutes=15,\n                    directory=checkpoint_subpath,\n                    objects_to_save=objects_to_save,\n                    subdirectory=subdir,\n                )\n                self._system_checkpointer[agent_key] = checkpointer\n        # Do not record timestamps until after the first learning step is done.\n        # This is to avoid including the time it takes for actors to come online and\n        # fill the replay buffer.\n        self._timestamp = None\n\n    def _update_target_networks(self) -> None:\n        for key in self.unique_net_keys:\n            # Update target network.\n            online_variables = (\n                *self._observation_networks[key].variables,\n                *self._critic_networks[key].variables,\n                *self._policy_networks[key].variables,\n            )\n            target_variables = (\n                *self._target_observation_networks[key].variables,\n                *self._target_critic_networks[key].variables,\n                *self._target_policy_networks[key].variables,\n            )\n\n            # Make online -> target network update ops.\n            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n                for src, dest in zip(online_variables, target_variables):\n                    dest.assign(src)\n            self._num_steps.assign_add(1)\n\n",
        "target_code_len": 5252,
        "diff_format": "@@ -775,23 +774,122 @@\n \n-        super().__init__(\n-            agents=agents,\n-            agent_types=agent_types,\n-            policy_networks=policy_networks,\n-            critic_networks=critic_networks,\n-            target_policy_networks=target_policy_networks,\n-            target_critic_networks=target_critic_networks,\n-            policy_optimizer=policy_optimizer,\n-            critic_optimizer=critic_optimizer,\n-            discount=discount,\n-            target_update_period=target_update_period,\n-            dataset=dataset,\n-            observation_networks=observation_networks,\n-            target_observation_networks=target_observation_networks,\n-            shared_weights=shared_weights,\n-            max_gradient_norm=max_gradient_norm,\n-            counter=counter,\n-            logger=logger,\n-            checkpoint=checkpoint,\n-            checkpoint_subpath=checkpoint_subpath,\n-        )\n+        self._agents = agents\n+        self._agent_types = agent_types\n+        self._shared_weights = shared_weights\n+        self._checkpoint = checkpoint\n+\n+        # Store online and target networks.\n+        self._policy_networks = policy_networks\n+        self._critic_networks = critic_networks\n+        self._target_policy_networks = target_policy_networks\n+        self._target_critic_networks = target_critic_networks\n+\n+        # Ensure obs and target networks are sonnet modules\n+        self._observation_networks = {\n+            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n+        }\n+        self._target_observation_networks = {\n+            k: tf2_utils.to_sonnet_module(v)\n+            for k, v in target_observation_networks.items()\n+        }\n+\n+        # General learner book-keeping and loggers.\n+        self._counter = counter or counting.Counter()\n+        self._logger = logger or loggers.make_default_logger(\"trainer\")\n+\n+        # Other learner parameters.\n+        self._discount = discount\n+\n+        # Set up gradient clipping.\n+        if max_gradient_norm is not None:\n+            self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n+        else:  # A very large number. Infinity results in NaNs.\n+            self._max_gradient_norm = tf.convert_to_tensor(1e10)\n+\n+        # Necessary to track when to update target networks.\n+        self._num_steps = tf.Variable(0, dtype=tf.int32)\n+        self._target_update_period = target_update_period\n+\n+        # Create an iterator to go through the dataset.\n+        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n+\n+        self._critic_optimizer = critic_optimizer\n+        self._policy_optimizer = policy_optimizer\n+\n+        # Dictionary with network keys for each agent.\n+        self.agent_net_keys = {agent: agent for agent in self._agents}\n+        if self._shared_weights:\n+            self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n+\n+        self.unique_net_keys = self._agent_types if shared_weights else self._agents\n+\n+        # Expose the variables.\n+        policy_networks_to_expose = {}\n+        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n+            \"critic\": {},\n+            \"policy\": {},\n+        }\n+        for agent_key in self.unique_net_keys:\n+            policy_network_to_expose = snt.Sequential(\n+                [\n+                    self._target_observation_networks[agent_key],\n+                    self._target_policy_networks[agent_key],\n+                ]\n+            )\n+            policy_networks_to_expose[agent_key] = policy_network_to_expose\n+            self._system_network_variables[\"critic\"][\n+                agent_key\n+            ] = target_critic_networks[agent_key].variables\n+            self._system_network_variables[\"policy\"][\n+                agent_key\n+            ] = policy_network_to_expose.variables\n+\n+        # Create checkpointer\n+        self._system_checkpointer = {}\n+        if checkpoint:\n+            for agent_key in self.unique_net_keys:\n+                objects_to_save = {\n+                    \"counter\": self._counter,\n+                    \"policy\": self._policy_networks[agent_key],\n+                    \"critic\": self._critic_networks[agent_key],\n+                    \"observation\": self._observation_networks[agent_key],\n+                    \"target_policy\": self._target_policy_networks[agent_key],\n+                    \"target_critic\": self._target_critic_networks[agent_key],\n+                    \"target_observation\": self._target_observation_networks[agent_key],\n+                    \"policy_optimizer\": self._policy_optimizer,\n+                    \"critic_optimizer\": self._critic_optimizer,\n+                    \"num_steps\": self._num_steps,\n+                }\n+\n+                subdir = os.path.join(\"trainer\", agent_key)\n+                checkpointer = tf2_savers.Checkpointer(\n+                    time_delta_minutes=15,\n+                    directory=checkpoint_subpath,\n+                    objects_to_save=objects_to_save,\n+                    subdirectory=subdir,\n+                )\n+                self._system_checkpointer[agent_key] = checkpointer\n+        # Do not record timestamps until after the first learning step is done.\n+        # This is to avoid including the time it takes for actors to come online and\n+        # fill the replay buffer.\n+        self._timestamp = None\n+\n+    def _update_target_networks(self) -> None:\n+        for key in self.unique_net_keys:\n+            # Update target network.\n+            online_variables = (\n+                *self._observation_networks[key].variables,\n+                *self._critic_networks[key].variables,\n+                *self._policy_networks[key].variables,\n+            )\n+            target_variables = (\n+                *self._target_observation_networks[key].variables,\n+                *self._target_critic_networks[key].variables,\n+                *self._target_policy_networks[key].variables,\n+            )\n+\n+            # Make online -> target network update ops.\n+            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n+                for src, dest in zip(online_variables, target_variables):\n+                    dest.assign(src)\n+            self._num_steps.assign_add(1)\n \n",
        "source_code_with_indent": "\n        super().__init__(\n            agents=agents,\n            agent_types=agent_types,\n            policy_networks=policy_networks,\n            critic_networks=critic_networks,\n            target_policy_networks=target_policy_networks,\n            target_critic_networks=target_critic_networks,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            discount=discount,\n            target_update_period=target_update_period,\n            dataset=dataset,\n            observation_networks=observation_networks,\n            target_observation_networks=target_observation_networks,\n            shared_weights=shared_weights,\n            max_gradient_norm=max_gradient_norm,\n            counter=counter,\n            logger=logger,\n            checkpoint=checkpoint,\n            checkpoint_subpath=checkpoint_subpath,\n        )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        self._agents = agents\n        self._agent_types = agent_types\n        self._shared_weights = shared_weights\n        self._checkpoint = checkpoint\n\n        # Store online and target networks.\n        self._policy_networks = policy_networks\n        self._critic_networks = critic_networks\n        self._target_policy_networks = target_policy_networks\n        self._target_critic_networks = target_critic_networks\n\n        # Ensure obs and target networks are sonnet modules\n        self._observation_networks = {\n            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n        }\n        self._target_observation_networks = {\n            k: tf2_utils.to_sonnet_module(v)\n            for k, v in target_observation_networks.items()\n        }\n\n        # General learner book-keeping and loggers.\n        self._counter = counter or counting.Counter()\n        self._logger = logger or loggers.make_default_logger(\"trainer\")\n\n        # Other learner parameters.\n        self._discount = discount\n\n        # Set up gradient clipping.\n        if max_gradient_norm is not None:\n            <IND>self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n        <DED>else:  # A very large number. Infinity results in NaNs.\n            <IND>self._max_gradient_norm = tf.convert_to_tensor(1e10)\n\n        # Necessary to track when to update target networks.\n        <DED>self._num_steps = tf.Variable(0, dtype=tf.int32)\n        self._target_update_period = target_update_period\n\n        # Create an iterator to go through the dataset.\n        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n        self._critic_optimizer = critic_optimizer\n        self._policy_optimizer = policy_optimizer\n\n        # Dictionary with network keys for each agent.\n        self.agent_net_keys = {agent: agent for agent in self._agents}\n        if self._shared_weights:\n            <IND>self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n\n        <DED>self.unique_net_keys = self._agent_types if shared_weights else self._agents\n\n        # Expose the variables.\n        policy_networks_to_expose = {}\n        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n            \"critic\": {},\n            \"policy\": {},\n        }\n        for agent_key in self.unique_net_keys:\n            <IND>policy_network_to_expose = snt.Sequential(\n                [\n                    self._target_observation_networks[agent_key],\n                    self._target_policy_networks[agent_key],\n                ]\n            )\n            policy_networks_to_expose[agent_key] = policy_network_to_expose\n            self._system_network_variables[\"critic\"][\n                agent_key\n            ] = target_critic_networks[agent_key].variables\n            self._system_network_variables[\"policy\"][\n                agent_key\n            ] = policy_network_to_expose.variables\n\n        # Create checkpointer\n        <DED>self._system_checkpointer = {}\n        if checkpoint:\n            <IND>for agent_key in self.unique_net_keys:\n                <IND>objects_to_save = {\n                    \"counter\": self._counter,\n                    \"policy\": self._policy_networks[agent_key],\n                    \"critic\": self._critic_networks[agent_key],\n                    \"observation\": self._observation_networks[agent_key],\n                    \"target_policy\": self._target_policy_networks[agent_key],\n                    \"target_critic\": self._target_critic_networks[agent_key],\n                    \"target_observation\": self._target_observation_networks[agent_key],\n                    \"policy_optimizer\": self._policy_optimizer,\n                    \"critic_optimizer\": self._critic_optimizer,\n                    \"num_steps\": self._num_steps,\n                }\n\n                subdir = os.path.join(\"trainer\", agent_key)\n                checkpointer = tf2_savers.Checkpointer(\n                    time_delta_minutes=15,\n                    directory=checkpoint_subpath,\n                    objects_to_save=objects_to_save,\n                    subdirectory=subdir,\n                )\n                self._system_checkpointer[agent_key] = checkpointer\n        # Do not record timestamps until after the first learning step is done.\n        # This is to avoid including the time it takes for actors to come online and\n        # fill the replay buffer.\n        <DED><DED>self._timestamp = None\n\n    <DED>def _update_target_networks(self) -> None:\n        <IND>for key in self.unique_net_keys:\n            # Update target network.\n            <IND>online_variables = (\n                *self._observation_networks[key].variables,\n                *self._critic_networks[key].variables,\n                *self._policy_networks[key].variables,\n            )\n            target_variables = (\n                *self._target_observation_networks[key].variables,\n                *self._target_critic_networks[key].variables,\n                *self._target_policy_networks[key].variables,\n            )\n\n            # Make online -> target network update ops.\n            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n                <IND>for src, dest in zip(online_variables, target_variables):\n                    <IND>dest.assign(src)\n            <DED><DED>self._num_steps.assign_add(1)\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "instadeepai/Mava",
    "commit": "8e4dd412ccab4fba42dfe987b81124c901e1e4c0",
    "filename": "mava/systems/tf/maddpg/training.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/instadeepai-Mava/mava/systems/tf/maddpg/training.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "mava/systems/tf/maddpg/training.py:835:4 Inconsistent override [14]: `mava.systems.tf.maddpg.training.BaseRecurrentMADDPGTrainer._get_critic_feed` overrides method defined in `BaseMADDPGTrainer` inconsistently. Could not find parameter `o_t_trans` in overriding signature.",
    "message": " `mava.systems.tf.maddpg.training.BaseRecurrentMADDPGTrainer._get_critic_feed` overrides method defined in `BaseMADDPGTrainer` inconsistently. Could not find parameter `o_t_trans` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 835,
    "warning_line": "    def _get_critic_feed(",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nclass BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n    \"\"\"MADDPG trainer.\n",
        "source_code_len": 77,
        "target_code": "\nclass BaseRecurrentMADDPGTrainer(mava.Trainer):\n    \"\"\"MADDPG trainer.\n",
        "target_code_len": 72,
        "diff_format": "@@ -724,3 +723,3 @@\n \n-class BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n+class BaseRecurrentMADDPGTrainer(mava.Trainer):\n     \"\"\"MADDPG trainer.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n<DED><DED>class BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n    <IND>",
        "target_code_with_indent": "\n<DED><DED>class BaseRecurrentMADDPGTrainer(mava.Trainer):\n    <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        super().__init__(\n            agents=agents,\n            agent_types=agent_types,\n            policy_networks=policy_networks,\n            critic_networks=critic_networks,\n            target_policy_networks=target_policy_networks,\n            target_critic_networks=target_critic_networks,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            discount=discount,\n            target_update_period=target_update_period,\n            dataset=dataset,\n            observation_networks=observation_networks,\n            target_observation_networks=target_observation_networks,\n            shared_weights=shared_weights,\n            max_gradient_norm=max_gradient_norm,\n            counter=counter,\n            logger=logger,\n            checkpoint=checkpoint,\n            checkpoint_subpath=checkpoint_subpath,\n        )\n\n",
        "source_code_len": 877,
        "target_code": "\n        self._agents = agents\n        self._agent_types = agent_types\n        self._shared_weights = shared_weights\n        self._checkpoint = checkpoint\n\n        # Store online and target networks.\n        self._policy_networks = policy_networks\n        self._critic_networks = critic_networks\n        self._target_policy_networks = target_policy_networks\n        self._target_critic_networks = target_critic_networks\n\n        # Ensure obs and target networks are sonnet modules\n        self._observation_networks = {\n            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n        }\n        self._target_observation_networks = {\n            k: tf2_utils.to_sonnet_module(v)\n            for k, v in target_observation_networks.items()\n        }\n\n        # General learner book-keeping and loggers.\n        self._counter = counter or counting.Counter()\n        self._logger = logger or loggers.make_default_logger(\"trainer\")\n\n        # Other learner parameters.\n        self._discount = discount\n\n        # Set up gradient clipping.\n        if max_gradient_norm is not None:\n            self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n        else:  # A very large number. Infinity results in NaNs.\n            self._max_gradient_norm = tf.convert_to_tensor(1e10)\n\n        # Necessary to track when to update target networks.\n        self._num_steps = tf.Variable(0, dtype=tf.int32)\n        self._target_update_period = target_update_period\n\n        # Create an iterator to go through the dataset.\n        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n        self._critic_optimizer = critic_optimizer\n        self._policy_optimizer = policy_optimizer\n\n        # Dictionary with network keys for each agent.\n        self.agent_net_keys = {agent: agent for agent in self._agents}\n        if self._shared_weights:\n            self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n\n        self.unique_net_keys = self._agent_types if shared_weights else self._agents\n\n        # Expose the variables.\n        policy_networks_to_expose = {}\n        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n            \"critic\": {},\n            \"policy\": {},\n        }\n        for agent_key in self.unique_net_keys:\n            policy_network_to_expose = snt.Sequential(\n                [\n                    self._target_observation_networks[agent_key],\n                    self._target_policy_networks[agent_key],\n                ]\n            )\n            policy_networks_to_expose[agent_key] = policy_network_to_expose\n            self._system_network_variables[\"critic\"][\n                agent_key\n            ] = target_critic_networks[agent_key].variables\n            self._system_network_variables[\"policy\"][\n                agent_key\n            ] = policy_network_to_expose.variables\n\n        # Create checkpointer\n        self._system_checkpointer = {}\n        if checkpoint:\n            for agent_key in self.unique_net_keys:\n                objects_to_save = {\n                    \"counter\": self._counter,\n                    \"policy\": self._policy_networks[agent_key],\n                    \"critic\": self._critic_networks[agent_key],\n                    \"observation\": self._observation_networks[agent_key],\n                    \"target_policy\": self._target_policy_networks[agent_key],\n                    \"target_critic\": self._target_critic_networks[agent_key],\n                    \"target_observation\": self._target_observation_networks[agent_key],\n                    \"policy_optimizer\": self._policy_optimizer,\n                    \"critic_optimizer\": self._critic_optimizer,\n                    \"num_steps\": self._num_steps,\n                }\n\n                subdir = os.path.join(\"trainer\", agent_key)\n                checkpointer = tf2_savers.Checkpointer(\n                    time_delta_minutes=15,\n                    directory=checkpoint_subpath,\n                    objects_to_save=objects_to_save,\n                    subdirectory=subdir,\n                )\n                self._system_checkpointer[agent_key] = checkpointer\n        # Do not record timestamps until after the first learning step is done.\n        # This is to avoid including the time it takes for actors to come online and\n        # fill the replay buffer.\n        self._timestamp = None\n\n    def _update_target_networks(self) -> None:\n        for key in self.unique_net_keys:\n            # Update target network.\n            online_variables = (\n                *self._observation_networks[key].variables,\n                *self._critic_networks[key].variables,\n                *self._policy_networks[key].variables,\n            )\n            target_variables = (\n                *self._target_observation_networks[key].variables,\n                *self._target_critic_networks[key].variables,\n                *self._target_policy_networks[key].variables,\n            )\n\n            # Make online -> target network update ops.\n            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n                for src, dest in zip(online_variables, target_variables):\n                    dest.assign(src)\n            self._num_steps.assign_add(1)\n\n",
        "target_code_len": 5252,
        "diff_format": "@@ -775,23 +774,122 @@\n \n-        super().__init__(\n-            agents=agents,\n-            agent_types=agent_types,\n-            policy_networks=policy_networks,\n-            critic_networks=critic_networks,\n-            target_policy_networks=target_policy_networks,\n-            target_critic_networks=target_critic_networks,\n-            policy_optimizer=policy_optimizer,\n-            critic_optimizer=critic_optimizer,\n-            discount=discount,\n-            target_update_period=target_update_period,\n-            dataset=dataset,\n-            observation_networks=observation_networks,\n-            target_observation_networks=target_observation_networks,\n-            shared_weights=shared_weights,\n-            max_gradient_norm=max_gradient_norm,\n-            counter=counter,\n-            logger=logger,\n-            checkpoint=checkpoint,\n-            checkpoint_subpath=checkpoint_subpath,\n-        )\n+        self._agents = agents\n+        self._agent_types = agent_types\n+        self._shared_weights = shared_weights\n+        self._checkpoint = checkpoint\n+\n+        # Store online and target networks.\n+        self._policy_networks = policy_networks\n+        self._critic_networks = critic_networks\n+        self._target_policy_networks = target_policy_networks\n+        self._target_critic_networks = target_critic_networks\n+\n+        # Ensure obs and target networks are sonnet modules\n+        self._observation_networks = {\n+            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n+        }\n+        self._target_observation_networks = {\n+            k: tf2_utils.to_sonnet_module(v)\n+            for k, v in target_observation_networks.items()\n+        }\n+\n+        # General learner book-keeping and loggers.\n+        self._counter = counter or counting.Counter()\n+        self._logger = logger or loggers.make_default_logger(\"trainer\")\n+\n+        # Other learner parameters.\n+        self._discount = discount\n+\n+        # Set up gradient clipping.\n+        if max_gradient_norm is not None:\n+            self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n+        else:  # A very large number. Infinity results in NaNs.\n+            self._max_gradient_norm = tf.convert_to_tensor(1e10)\n+\n+        # Necessary to track when to update target networks.\n+        self._num_steps = tf.Variable(0, dtype=tf.int32)\n+        self._target_update_period = target_update_period\n+\n+        # Create an iterator to go through the dataset.\n+        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n+\n+        self._critic_optimizer = critic_optimizer\n+        self._policy_optimizer = policy_optimizer\n+\n+        # Dictionary with network keys for each agent.\n+        self.agent_net_keys = {agent: agent for agent in self._agents}\n+        if self._shared_weights:\n+            self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n+\n+        self.unique_net_keys = self._agent_types if shared_weights else self._agents\n+\n+        # Expose the variables.\n+        policy_networks_to_expose = {}\n+        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n+            \"critic\": {},\n+            \"policy\": {},\n+        }\n+        for agent_key in self.unique_net_keys:\n+            policy_network_to_expose = snt.Sequential(\n+                [\n+                    self._target_observation_networks[agent_key],\n+                    self._target_policy_networks[agent_key],\n+                ]\n+            )\n+            policy_networks_to_expose[agent_key] = policy_network_to_expose\n+            self._system_network_variables[\"critic\"][\n+                agent_key\n+            ] = target_critic_networks[agent_key].variables\n+            self._system_network_variables[\"policy\"][\n+                agent_key\n+            ] = policy_network_to_expose.variables\n+\n+        # Create checkpointer\n+        self._system_checkpointer = {}\n+        if checkpoint:\n+            for agent_key in self.unique_net_keys:\n+                objects_to_save = {\n+                    \"counter\": self._counter,\n+                    \"policy\": self._policy_networks[agent_key],\n+                    \"critic\": self._critic_networks[agent_key],\n+                    \"observation\": self._observation_networks[agent_key],\n+                    \"target_policy\": self._target_policy_networks[agent_key],\n+                    \"target_critic\": self._target_critic_networks[agent_key],\n+                    \"target_observation\": self._target_observation_networks[agent_key],\n+                    \"policy_optimizer\": self._policy_optimizer,\n+                    \"critic_optimizer\": self._critic_optimizer,\n+                    \"num_steps\": self._num_steps,\n+                }\n+\n+                subdir = os.path.join(\"trainer\", agent_key)\n+                checkpointer = tf2_savers.Checkpointer(\n+                    time_delta_minutes=15,\n+                    directory=checkpoint_subpath,\n+                    objects_to_save=objects_to_save,\n+                    subdirectory=subdir,\n+                )\n+                self._system_checkpointer[agent_key] = checkpointer\n+        # Do not record timestamps until after the first learning step is done.\n+        # This is to avoid including the time it takes for actors to come online and\n+        # fill the replay buffer.\n+        self._timestamp = None\n+\n+    def _update_target_networks(self) -> None:\n+        for key in self.unique_net_keys:\n+            # Update target network.\n+            online_variables = (\n+                *self._observation_networks[key].variables,\n+                *self._critic_networks[key].variables,\n+                *self._policy_networks[key].variables,\n+            )\n+            target_variables = (\n+                *self._target_observation_networks[key].variables,\n+                *self._target_critic_networks[key].variables,\n+                *self._target_policy_networks[key].variables,\n+            )\n+\n+            # Make online -> target network update ops.\n+            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n+                for src, dest in zip(online_variables, target_variables):\n+                    dest.assign(src)\n+            self._num_steps.assign_add(1)\n \n",
        "source_code_with_indent": "\n        super().__init__(\n            agents=agents,\n            agent_types=agent_types,\n            policy_networks=policy_networks,\n            critic_networks=critic_networks,\n            target_policy_networks=target_policy_networks,\n            target_critic_networks=target_critic_networks,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            discount=discount,\n            target_update_period=target_update_period,\n            dataset=dataset,\n            observation_networks=observation_networks,\n            target_observation_networks=target_observation_networks,\n            shared_weights=shared_weights,\n            max_gradient_norm=max_gradient_norm,\n            counter=counter,\n            logger=logger,\n            checkpoint=checkpoint,\n            checkpoint_subpath=checkpoint_subpath,\n        )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        self._agents = agents\n        self._agent_types = agent_types\n        self._shared_weights = shared_weights\n        self._checkpoint = checkpoint\n\n        # Store online and target networks.\n        self._policy_networks = policy_networks\n        self._critic_networks = critic_networks\n        self._target_policy_networks = target_policy_networks\n        self._target_critic_networks = target_critic_networks\n\n        # Ensure obs and target networks are sonnet modules\n        self._observation_networks = {\n            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n        }\n        self._target_observation_networks = {\n            k: tf2_utils.to_sonnet_module(v)\n            for k, v in target_observation_networks.items()\n        }\n\n        # General learner book-keeping and loggers.\n        self._counter = counter or counting.Counter()\n        self._logger = logger or loggers.make_default_logger(\"trainer\")\n\n        # Other learner parameters.\n        self._discount = discount\n\n        # Set up gradient clipping.\n        if max_gradient_norm is not None:\n            <IND>self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n        <DED>else:  # A very large number. Infinity results in NaNs.\n            <IND>self._max_gradient_norm = tf.convert_to_tensor(1e10)\n\n        # Necessary to track when to update target networks.\n        <DED>self._num_steps = tf.Variable(0, dtype=tf.int32)\n        self._target_update_period = target_update_period\n\n        # Create an iterator to go through the dataset.\n        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n        self._critic_optimizer = critic_optimizer\n        self._policy_optimizer = policy_optimizer\n\n        # Dictionary with network keys for each agent.\n        self.agent_net_keys = {agent: agent for agent in self._agents}\n        if self._shared_weights:\n            <IND>self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n\n        <DED>self.unique_net_keys = self._agent_types if shared_weights else self._agents\n\n        # Expose the variables.\n        policy_networks_to_expose = {}\n        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n            \"critic\": {},\n            \"policy\": {},\n        }\n        for agent_key in self.unique_net_keys:\n            <IND>policy_network_to_expose = snt.Sequential(\n                [\n                    self._target_observation_networks[agent_key],\n                    self._target_policy_networks[agent_key],\n                ]\n            )\n            policy_networks_to_expose[agent_key] = policy_network_to_expose\n            self._system_network_variables[\"critic\"][\n                agent_key\n            ] = target_critic_networks[agent_key].variables\n            self._system_network_variables[\"policy\"][\n                agent_key\n            ] = policy_network_to_expose.variables\n\n        # Create checkpointer\n        <DED>self._system_checkpointer = {}\n        if checkpoint:\n            <IND>for agent_key in self.unique_net_keys:\n                <IND>objects_to_save = {\n                    \"counter\": self._counter,\n                    \"policy\": self._policy_networks[agent_key],\n                    \"critic\": self._critic_networks[agent_key],\n                    \"observation\": self._observation_networks[agent_key],\n                    \"target_policy\": self._target_policy_networks[agent_key],\n                    \"target_critic\": self._target_critic_networks[agent_key],\n                    \"target_observation\": self._target_observation_networks[agent_key],\n                    \"policy_optimizer\": self._policy_optimizer,\n                    \"critic_optimizer\": self._critic_optimizer,\n                    \"num_steps\": self._num_steps,\n                }\n\n                subdir = os.path.join(\"trainer\", agent_key)\n                checkpointer = tf2_savers.Checkpointer(\n                    time_delta_minutes=15,\n                    directory=checkpoint_subpath,\n                    objects_to_save=objects_to_save,\n                    subdirectory=subdir,\n                )\n                self._system_checkpointer[agent_key] = checkpointer\n        # Do not record timestamps until after the first learning step is done.\n        # This is to avoid including the time it takes for actors to come online and\n        # fill the replay buffer.\n        <DED><DED>self._timestamp = None\n\n    <DED>def _update_target_networks(self) -> None:\n        <IND>for key in self.unique_net_keys:\n            # Update target network.\n            <IND>online_variables = (\n                *self._observation_networks[key].variables,\n                *self._critic_networks[key].variables,\n                *self._policy_networks[key].variables,\n            )\n            target_variables = (\n                *self._target_observation_networks[key].variables,\n                *self._target_critic_networks[key].variables,\n                *self._target_policy_networks[key].variables,\n            )\n\n            # Make online -> target network update ops.\n            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n                <IND>for src, dest in zip(online_variables, target_variables):\n                    <IND>dest.assign(src)\n            <DED><DED>self._num_steps.assign_add(1)\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "instadeepai/Mava",
    "commit": "8e4dd412ccab4fba42dfe987b81124c901e1e4c0",
    "filename": "mava/systems/tf/maddpg/training.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/instadeepai-Mava/mava/systems/tf/maddpg/training.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "mava/systems/tf/maddpg/training.py:835:4 Inconsistent override [14]: `mava.systems.tf.maddpg.training.BaseRecurrentMADDPGTrainer._get_critic_feed` overrides method defined in `BaseMADDPGTrainer` inconsistently. Could not find parameter `o_tm1_trans` in overriding signature.",
    "message": " `mava.systems.tf.maddpg.training.BaseRecurrentMADDPGTrainer._get_critic_feed` overrides method defined in `BaseMADDPGTrainer` inconsistently. Could not find parameter `o_tm1_trans` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 835,
    "warning_line": "    def _get_critic_feed(",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nclass BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n    \"\"\"MADDPG trainer.\n",
        "source_code_len": 77,
        "target_code": "\nclass BaseRecurrentMADDPGTrainer(mava.Trainer):\n    \"\"\"MADDPG trainer.\n",
        "target_code_len": 72,
        "diff_format": "@@ -724,3 +723,3 @@\n \n-class BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n+class BaseRecurrentMADDPGTrainer(mava.Trainer):\n     \"\"\"MADDPG trainer.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n<DED><DED>class BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n    <IND>",
        "target_code_with_indent": "\n<DED><DED>class BaseRecurrentMADDPGTrainer(mava.Trainer):\n    <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        super().__init__(\n            agents=agents,\n            agent_types=agent_types,\n            policy_networks=policy_networks,\n            critic_networks=critic_networks,\n            target_policy_networks=target_policy_networks,\n            target_critic_networks=target_critic_networks,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            discount=discount,\n            target_update_period=target_update_period,\n            dataset=dataset,\n            observation_networks=observation_networks,\n            target_observation_networks=target_observation_networks,\n            shared_weights=shared_weights,\n            max_gradient_norm=max_gradient_norm,\n            counter=counter,\n            logger=logger,\n            checkpoint=checkpoint,\n            checkpoint_subpath=checkpoint_subpath,\n        )\n\n",
        "source_code_len": 877,
        "target_code": "\n        self._agents = agents\n        self._agent_types = agent_types\n        self._shared_weights = shared_weights\n        self._checkpoint = checkpoint\n\n        # Store online and target networks.\n        self._policy_networks = policy_networks\n        self._critic_networks = critic_networks\n        self._target_policy_networks = target_policy_networks\n        self._target_critic_networks = target_critic_networks\n\n        # Ensure obs and target networks are sonnet modules\n        self._observation_networks = {\n            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n        }\n        self._target_observation_networks = {\n            k: tf2_utils.to_sonnet_module(v)\n            for k, v in target_observation_networks.items()\n        }\n\n        # General learner book-keeping and loggers.\n        self._counter = counter or counting.Counter()\n        self._logger = logger or loggers.make_default_logger(\"trainer\")\n\n        # Other learner parameters.\n        self._discount = discount\n\n        # Set up gradient clipping.\n        if max_gradient_norm is not None:\n            self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n        else:  # A very large number. Infinity results in NaNs.\n            self._max_gradient_norm = tf.convert_to_tensor(1e10)\n\n        # Necessary to track when to update target networks.\n        self._num_steps = tf.Variable(0, dtype=tf.int32)\n        self._target_update_period = target_update_period\n\n        # Create an iterator to go through the dataset.\n        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n        self._critic_optimizer = critic_optimizer\n        self._policy_optimizer = policy_optimizer\n\n        # Dictionary with network keys for each agent.\n        self.agent_net_keys = {agent: agent for agent in self._agents}\n        if self._shared_weights:\n            self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n\n        self.unique_net_keys = self._agent_types if shared_weights else self._agents\n\n        # Expose the variables.\n        policy_networks_to_expose = {}\n        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n            \"critic\": {},\n            \"policy\": {},\n        }\n        for agent_key in self.unique_net_keys:\n            policy_network_to_expose = snt.Sequential(\n                [\n                    self._target_observation_networks[agent_key],\n                    self._target_policy_networks[agent_key],\n                ]\n            )\n            policy_networks_to_expose[agent_key] = policy_network_to_expose\n            self._system_network_variables[\"critic\"][\n                agent_key\n            ] = target_critic_networks[agent_key].variables\n            self._system_network_variables[\"policy\"][\n                agent_key\n            ] = policy_network_to_expose.variables\n\n        # Create checkpointer\n        self._system_checkpointer = {}\n        if checkpoint:\n            for agent_key in self.unique_net_keys:\n                objects_to_save = {\n                    \"counter\": self._counter,\n                    \"policy\": self._policy_networks[agent_key],\n                    \"critic\": self._critic_networks[agent_key],\n                    \"observation\": self._observation_networks[agent_key],\n                    \"target_policy\": self._target_policy_networks[agent_key],\n                    \"target_critic\": self._target_critic_networks[agent_key],\n                    \"target_observation\": self._target_observation_networks[agent_key],\n                    \"policy_optimizer\": self._policy_optimizer,\n                    \"critic_optimizer\": self._critic_optimizer,\n                    \"num_steps\": self._num_steps,\n                }\n\n                subdir = os.path.join(\"trainer\", agent_key)\n                checkpointer = tf2_savers.Checkpointer(\n                    time_delta_minutes=15,\n                    directory=checkpoint_subpath,\n                    objects_to_save=objects_to_save,\n                    subdirectory=subdir,\n                )\n                self._system_checkpointer[agent_key] = checkpointer\n        # Do not record timestamps until after the first learning step is done.\n        # This is to avoid including the time it takes for actors to come online and\n        # fill the replay buffer.\n        self._timestamp = None\n\n    def _update_target_networks(self) -> None:\n        for key in self.unique_net_keys:\n            # Update target network.\n            online_variables = (\n                *self._observation_networks[key].variables,\n                *self._critic_networks[key].variables,\n                *self._policy_networks[key].variables,\n            )\n            target_variables = (\n                *self._target_observation_networks[key].variables,\n                *self._target_critic_networks[key].variables,\n                *self._target_policy_networks[key].variables,\n            )\n\n            # Make online -> target network update ops.\n            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n                for src, dest in zip(online_variables, target_variables):\n                    dest.assign(src)\n            self._num_steps.assign_add(1)\n\n",
        "target_code_len": 5252,
        "diff_format": "@@ -775,23 +774,122 @@\n \n-        super().__init__(\n-            agents=agents,\n-            agent_types=agent_types,\n-            policy_networks=policy_networks,\n-            critic_networks=critic_networks,\n-            target_policy_networks=target_policy_networks,\n-            target_critic_networks=target_critic_networks,\n-            policy_optimizer=policy_optimizer,\n-            critic_optimizer=critic_optimizer,\n-            discount=discount,\n-            target_update_period=target_update_period,\n-            dataset=dataset,\n-            observation_networks=observation_networks,\n-            target_observation_networks=target_observation_networks,\n-            shared_weights=shared_weights,\n-            max_gradient_norm=max_gradient_norm,\n-            counter=counter,\n-            logger=logger,\n-            checkpoint=checkpoint,\n-            checkpoint_subpath=checkpoint_subpath,\n-        )\n+        self._agents = agents\n+        self._agent_types = agent_types\n+        self._shared_weights = shared_weights\n+        self._checkpoint = checkpoint\n+\n+        # Store online and target networks.\n+        self._policy_networks = policy_networks\n+        self._critic_networks = critic_networks\n+        self._target_policy_networks = target_policy_networks\n+        self._target_critic_networks = target_critic_networks\n+\n+        # Ensure obs and target networks are sonnet modules\n+        self._observation_networks = {\n+            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n+        }\n+        self._target_observation_networks = {\n+            k: tf2_utils.to_sonnet_module(v)\n+            for k, v in target_observation_networks.items()\n+        }\n+\n+        # General learner book-keeping and loggers.\n+        self._counter = counter or counting.Counter()\n+        self._logger = logger or loggers.make_default_logger(\"trainer\")\n+\n+        # Other learner parameters.\n+        self._discount = discount\n+\n+        # Set up gradient clipping.\n+        if max_gradient_norm is not None:\n+            self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n+        else:  # A very large number. Infinity results in NaNs.\n+            self._max_gradient_norm = tf.convert_to_tensor(1e10)\n+\n+        # Necessary to track when to update target networks.\n+        self._num_steps = tf.Variable(0, dtype=tf.int32)\n+        self._target_update_period = target_update_period\n+\n+        # Create an iterator to go through the dataset.\n+        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n+\n+        self._critic_optimizer = critic_optimizer\n+        self._policy_optimizer = policy_optimizer\n+\n+        # Dictionary with network keys for each agent.\n+        self.agent_net_keys = {agent: agent for agent in self._agents}\n+        if self._shared_weights:\n+            self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n+\n+        self.unique_net_keys = self._agent_types if shared_weights else self._agents\n+\n+        # Expose the variables.\n+        policy_networks_to_expose = {}\n+        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n+            \"critic\": {},\n+            \"policy\": {},\n+        }\n+        for agent_key in self.unique_net_keys:\n+            policy_network_to_expose = snt.Sequential(\n+                [\n+                    self._target_observation_networks[agent_key],\n+                    self._target_policy_networks[agent_key],\n+                ]\n+            )\n+            policy_networks_to_expose[agent_key] = policy_network_to_expose\n+            self._system_network_variables[\"critic\"][\n+                agent_key\n+            ] = target_critic_networks[agent_key].variables\n+            self._system_network_variables[\"policy\"][\n+                agent_key\n+            ] = policy_network_to_expose.variables\n+\n+        # Create checkpointer\n+        self._system_checkpointer = {}\n+        if checkpoint:\n+            for agent_key in self.unique_net_keys:\n+                objects_to_save = {\n+                    \"counter\": self._counter,\n+                    \"policy\": self._policy_networks[agent_key],\n+                    \"critic\": self._critic_networks[agent_key],\n+                    \"observation\": self._observation_networks[agent_key],\n+                    \"target_policy\": self._target_policy_networks[agent_key],\n+                    \"target_critic\": self._target_critic_networks[agent_key],\n+                    \"target_observation\": self._target_observation_networks[agent_key],\n+                    \"policy_optimizer\": self._policy_optimizer,\n+                    \"critic_optimizer\": self._critic_optimizer,\n+                    \"num_steps\": self._num_steps,\n+                }\n+\n+                subdir = os.path.join(\"trainer\", agent_key)\n+                checkpointer = tf2_savers.Checkpointer(\n+                    time_delta_minutes=15,\n+                    directory=checkpoint_subpath,\n+                    objects_to_save=objects_to_save,\n+                    subdirectory=subdir,\n+                )\n+                self._system_checkpointer[agent_key] = checkpointer\n+        # Do not record timestamps until after the first learning step is done.\n+        # This is to avoid including the time it takes for actors to come online and\n+        # fill the replay buffer.\n+        self._timestamp = None\n+\n+    def _update_target_networks(self) -> None:\n+        for key in self.unique_net_keys:\n+            # Update target network.\n+            online_variables = (\n+                *self._observation_networks[key].variables,\n+                *self._critic_networks[key].variables,\n+                *self._policy_networks[key].variables,\n+            )\n+            target_variables = (\n+                *self._target_observation_networks[key].variables,\n+                *self._target_critic_networks[key].variables,\n+                *self._target_policy_networks[key].variables,\n+            )\n+\n+            # Make online -> target network update ops.\n+            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n+                for src, dest in zip(online_variables, target_variables):\n+                    dest.assign(src)\n+            self._num_steps.assign_add(1)\n \n",
        "source_code_with_indent": "\n        super().__init__(\n            agents=agents,\n            agent_types=agent_types,\n            policy_networks=policy_networks,\n            critic_networks=critic_networks,\n            target_policy_networks=target_policy_networks,\n            target_critic_networks=target_critic_networks,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            discount=discount,\n            target_update_period=target_update_period,\n            dataset=dataset,\n            observation_networks=observation_networks,\n            target_observation_networks=target_observation_networks,\n            shared_weights=shared_weights,\n            max_gradient_norm=max_gradient_norm,\n            counter=counter,\n            logger=logger,\n            checkpoint=checkpoint,\n            checkpoint_subpath=checkpoint_subpath,\n        )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        self._agents = agents\n        self._agent_types = agent_types\n        self._shared_weights = shared_weights\n        self._checkpoint = checkpoint\n\n        # Store online and target networks.\n        self._policy_networks = policy_networks\n        self._critic_networks = critic_networks\n        self._target_policy_networks = target_policy_networks\n        self._target_critic_networks = target_critic_networks\n\n        # Ensure obs and target networks are sonnet modules\n        self._observation_networks = {\n            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n        }\n        self._target_observation_networks = {\n            k: tf2_utils.to_sonnet_module(v)\n            for k, v in target_observation_networks.items()\n        }\n\n        # General learner book-keeping and loggers.\n        self._counter = counter or counting.Counter()\n        self._logger = logger or loggers.make_default_logger(\"trainer\")\n\n        # Other learner parameters.\n        self._discount = discount\n\n        # Set up gradient clipping.\n        if max_gradient_norm is not None:\n            <IND>self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n        <DED>else:  # A very large number. Infinity results in NaNs.\n            <IND>self._max_gradient_norm = tf.convert_to_tensor(1e10)\n\n        # Necessary to track when to update target networks.\n        <DED>self._num_steps = tf.Variable(0, dtype=tf.int32)\n        self._target_update_period = target_update_period\n\n        # Create an iterator to go through the dataset.\n        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n        self._critic_optimizer = critic_optimizer\n        self._policy_optimizer = policy_optimizer\n\n        # Dictionary with network keys for each agent.\n        self.agent_net_keys = {agent: agent for agent in self._agents}\n        if self._shared_weights:\n            <IND>self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n\n        <DED>self.unique_net_keys = self._agent_types if shared_weights else self._agents\n\n        # Expose the variables.\n        policy_networks_to_expose = {}\n        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n            \"critic\": {},\n            \"policy\": {},\n        }\n        for agent_key in self.unique_net_keys:\n            <IND>policy_network_to_expose = snt.Sequential(\n                [\n                    self._target_observation_networks[agent_key],\n                    self._target_policy_networks[agent_key],\n                ]\n            )\n            policy_networks_to_expose[agent_key] = policy_network_to_expose\n            self._system_network_variables[\"critic\"][\n                agent_key\n            ] = target_critic_networks[agent_key].variables\n            self._system_network_variables[\"policy\"][\n                agent_key\n            ] = policy_network_to_expose.variables\n\n        # Create checkpointer\n        <DED>self._system_checkpointer = {}\n        if checkpoint:\n            <IND>for agent_key in self.unique_net_keys:\n                <IND>objects_to_save = {\n                    \"counter\": self._counter,\n                    \"policy\": self._policy_networks[agent_key],\n                    \"critic\": self._critic_networks[agent_key],\n                    \"observation\": self._observation_networks[agent_key],\n                    \"target_policy\": self._target_policy_networks[agent_key],\n                    \"target_critic\": self._target_critic_networks[agent_key],\n                    \"target_observation\": self._target_observation_networks[agent_key],\n                    \"policy_optimizer\": self._policy_optimizer,\n                    \"critic_optimizer\": self._critic_optimizer,\n                    \"num_steps\": self._num_steps,\n                }\n\n                subdir = os.path.join(\"trainer\", agent_key)\n                checkpointer = tf2_savers.Checkpointer(\n                    time_delta_minutes=15,\n                    directory=checkpoint_subpath,\n                    objects_to_save=objects_to_save,\n                    subdirectory=subdir,\n                )\n                self._system_checkpointer[agent_key] = checkpointer\n        # Do not record timestamps until after the first learning step is done.\n        # This is to avoid including the time it takes for actors to come online and\n        # fill the replay buffer.\n        <DED><DED>self._timestamp = None\n\n    <DED>def _update_target_networks(self) -> None:\n        <IND>for key in self.unique_net_keys:\n            # Update target network.\n            <IND>online_variables = (\n                *self._observation_networks[key].variables,\n                *self._critic_networks[key].variables,\n                *self._policy_networks[key].variables,\n            )\n            target_variables = (\n                *self._target_observation_networks[key].variables,\n                *self._target_critic_networks[key].variables,\n                *self._target_policy_networks[key].variables,\n            )\n\n            # Make online -> target network update ops.\n            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n                <IND>for src, dest in zip(online_variables, target_variables):\n                    <IND>dest.assign(src)\n            <DED><DED>self._num_steps.assign_add(1)\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "instadeepai/Mava",
    "commit": "8e4dd412ccab4fba42dfe987b81124c901e1e4c0",
    "filename": "mava/systems/tf/maddpg/training.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/instadeepai-Mava/mava/systems/tf/maddpg/training.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "mava/systems/tf/maddpg/training.py:852:4 Inconsistent override [14]: `mava.systems.tf.maddpg.training.BaseRecurrentMADDPGTrainer._get_dpg_feed` overrides method defined in `BaseMADDPGTrainer` inconsistently. Could not find parameter `a_t` in overriding signature.",
    "message": " `mava.systems.tf.maddpg.training.BaseRecurrentMADDPGTrainer._get_dpg_feed` overrides method defined in `BaseMADDPGTrainer` inconsistently. Could not find parameter `a_t` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 852,
    "warning_line": "    def _get_dpg_feed(",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nclass BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n    \"\"\"MADDPG trainer.\n",
        "source_code_len": 77,
        "target_code": "\nclass BaseRecurrentMADDPGTrainer(mava.Trainer):\n    \"\"\"MADDPG trainer.\n",
        "target_code_len": 72,
        "diff_format": "@@ -724,3 +723,3 @@\n \n-class BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n+class BaseRecurrentMADDPGTrainer(mava.Trainer):\n     \"\"\"MADDPG trainer.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n<DED><DED>class BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n    <IND>",
        "target_code_with_indent": "\n<DED><DED>class BaseRecurrentMADDPGTrainer(mava.Trainer):\n    <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        super().__init__(\n            agents=agents,\n            agent_types=agent_types,\n            policy_networks=policy_networks,\n            critic_networks=critic_networks,\n            target_policy_networks=target_policy_networks,\n            target_critic_networks=target_critic_networks,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            discount=discount,\n            target_update_period=target_update_period,\n            dataset=dataset,\n            observation_networks=observation_networks,\n            target_observation_networks=target_observation_networks,\n            shared_weights=shared_weights,\n            max_gradient_norm=max_gradient_norm,\n            counter=counter,\n            logger=logger,\n            checkpoint=checkpoint,\n            checkpoint_subpath=checkpoint_subpath,\n        )\n\n",
        "source_code_len": 877,
        "target_code": "\n        self._agents = agents\n        self._agent_types = agent_types\n        self._shared_weights = shared_weights\n        self._checkpoint = checkpoint\n\n        # Store online and target networks.\n        self._policy_networks = policy_networks\n        self._critic_networks = critic_networks\n        self._target_policy_networks = target_policy_networks\n        self._target_critic_networks = target_critic_networks\n\n        # Ensure obs and target networks are sonnet modules\n        self._observation_networks = {\n            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n        }\n        self._target_observation_networks = {\n            k: tf2_utils.to_sonnet_module(v)\n            for k, v in target_observation_networks.items()\n        }\n\n        # General learner book-keeping and loggers.\n        self._counter = counter or counting.Counter()\n        self._logger = logger or loggers.make_default_logger(\"trainer\")\n\n        # Other learner parameters.\n        self._discount = discount\n\n        # Set up gradient clipping.\n        if max_gradient_norm is not None:\n            self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n        else:  # A very large number. Infinity results in NaNs.\n            self._max_gradient_norm = tf.convert_to_tensor(1e10)\n\n        # Necessary to track when to update target networks.\n        self._num_steps = tf.Variable(0, dtype=tf.int32)\n        self._target_update_period = target_update_period\n\n        # Create an iterator to go through the dataset.\n        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n        self._critic_optimizer = critic_optimizer\n        self._policy_optimizer = policy_optimizer\n\n        # Dictionary with network keys for each agent.\n        self.agent_net_keys = {agent: agent for agent in self._agents}\n        if self._shared_weights:\n            self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n\n        self.unique_net_keys = self._agent_types if shared_weights else self._agents\n\n        # Expose the variables.\n        policy_networks_to_expose = {}\n        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n            \"critic\": {},\n            \"policy\": {},\n        }\n        for agent_key in self.unique_net_keys:\n            policy_network_to_expose = snt.Sequential(\n                [\n                    self._target_observation_networks[agent_key],\n                    self._target_policy_networks[agent_key],\n                ]\n            )\n            policy_networks_to_expose[agent_key] = policy_network_to_expose\n            self._system_network_variables[\"critic\"][\n                agent_key\n            ] = target_critic_networks[agent_key].variables\n            self._system_network_variables[\"policy\"][\n                agent_key\n            ] = policy_network_to_expose.variables\n\n        # Create checkpointer\n        self._system_checkpointer = {}\n        if checkpoint:\n            for agent_key in self.unique_net_keys:\n                objects_to_save = {\n                    \"counter\": self._counter,\n                    \"policy\": self._policy_networks[agent_key],\n                    \"critic\": self._critic_networks[agent_key],\n                    \"observation\": self._observation_networks[agent_key],\n                    \"target_policy\": self._target_policy_networks[agent_key],\n                    \"target_critic\": self._target_critic_networks[agent_key],\n                    \"target_observation\": self._target_observation_networks[agent_key],\n                    \"policy_optimizer\": self._policy_optimizer,\n                    \"critic_optimizer\": self._critic_optimizer,\n                    \"num_steps\": self._num_steps,\n                }\n\n                subdir = os.path.join(\"trainer\", agent_key)\n                checkpointer = tf2_savers.Checkpointer(\n                    time_delta_minutes=15,\n                    directory=checkpoint_subpath,\n                    objects_to_save=objects_to_save,\n                    subdirectory=subdir,\n                )\n                self._system_checkpointer[agent_key] = checkpointer\n        # Do not record timestamps until after the first learning step is done.\n        # This is to avoid including the time it takes for actors to come online and\n        # fill the replay buffer.\n        self._timestamp = None\n\n    def _update_target_networks(self) -> None:\n        for key in self.unique_net_keys:\n            # Update target network.\n            online_variables = (\n                *self._observation_networks[key].variables,\n                *self._critic_networks[key].variables,\n                *self._policy_networks[key].variables,\n            )\n            target_variables = (\n                *self._target_observation_networks[key].variables,\n                *self._target_critic_networks[key].variables,\n                *self._target_policy_networks[key].variables,\n            )\n\n            # Make online -> target network update ops.\n            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n                for src, dest in zip(online_variables, target_variables):\n                    dest.assign(src)\n            self._num_steps.assign_add(1)\n\n",
        "target_code_len": 5252,
        "diff_format": "@@ -775,23 +774,122 @@\n \n-        super().__init__(\n-            agents=agents,\n-            agent_types=agent_types,\n-            policy_networks=policy_networks,\n-            critic_networks=critic_networks,\n-            target_policy_networks=target_policy_networks,\n-            target_critic_networks=target_critic_networks,\n-            policy_optimizer=policy_optimizer,\n-            critic_optimizer=critic_optimizer,\n-            discount=discount,\n-            target_update_period=target_update_period,\n-            dataset=dataset,\n-            observation_networks=observation_networks,\n-            target_observation_networks=target_observation_networks,\n-            shared_weights=shared_weights,\n-            max_gradient_norm=max_gradient_norm,\n-            counter=counter,\n-            logger=logger,\n-            checkpoint=checkpoint,\n-            checkpoint_subpath=checkpoint_subpath,\n-        )\n+        self._agents = agents\n+        self._agent_types = agent_types\n+        self._shared_weights = shared_weights\n+        self._checkpoint = checkpoint\n+\n+        # Store online and target networks.\n+        self._policy_networks = policy_networks\n+        self._critic_networks = critic_networks\n+        self._target_policy_networks = target_policy_networks\n+        self._target_critic_networks = target_critic_networks\n+\n+        # Ensure obs and target networks are sonnet modules\n+        self._observation_networks = {\n+            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n+        }\n+        self._target_observation_networks = {\n+            k: tf2_utils.to_sonnet_module(v)\n+            for k, v in target_observation_networks.items()\n+        }\n+\n+        # General learner book-keeping and loggers.\n+        self._counter = counter or counting.Counter()\n+        self._logger = logger or loggers.make_default_logger(\"trainer\")\n+\n+        # Other learner parameters.\n+        self._discount = discount\n+\n+        # Set up gradient clipping.\n+        if max_gradient_norm is not None:\n+            self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n+        else:  # A very large number. Infinity results in NaNs.\n+            self._max_gradient_norm = tf.convert_to_tensor(1e10)\n+\n+        # Necessary to track when to update target networks.\n+        self._num_steps = tf.Variable(0, dtype=tf.int32)\n+        self._target_update_period = target_update_period\n+\n+        # Create an iterator to go through the dataset.\n+        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n+\n+        self._critic_optimizer = critic_optimizer\n+        self._policy_optimizer = policy_optimizer\n+\n+        # Dictionary with network keys for each agent.\n+        self.agent_net_keys = {agent: agent for agent in self._agents}\n+        if self._shared_weights:\n+            self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n+\n+        self.unique_net_keys = self._agent_types if shared_weights else self._agents\n+\n+        # Expose the variables.\n+        policy_networks_to_expose = {}\n+        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n+            \"critic\": {},\n+            \"policy\": {},\n+        }\n+        for agent_key in self.unique_net_keys:\n+            policy_network_to_expose = snt.Sequential(\n+                [\n+                    self._target_observation_networks[agent_key],\n+                    self._target_policy_networks[agent_key],\n+                ]\n+            )\n+            policy_networks_to_expose[agent_key] = policy_network_to_expose\n+            self._system_network_variables[\"critic\"][\n+                agent_key\n+            ] = target_critic_networks[agent_key].variables\n+            self._system_network_variables[\"policy\"][\n+                agent_key\n+            ] = policy_network_to_expose.variables\n+\n+        # Create checkpointer\n+        self._system_checkpointer = {}\n+        if checkpoint:\n+            for agent_key in self.unique_net_keys:\n+                objects_to_save = {\n+                    \"counter\": self._counter,\n+                    \"policy\": self._policy_networks[agent_key],\n+                    \"critic\": self._critic_networks[agent_key],\n+                    \"observation\": self._observation_networks[agent_key],\n+                    \"target_policy\": self._target_policy_networks[agent_key],\n+                    \"target_critic\": self._target_critic_networks[agent_key],\n+                    \"target_observation\": self._target_observation_networks[agent_key],\n+                    \"policy_optimizer\": self._policy_optimizer,\n+                    \"critic_optimizer\": self._critic_optimizer,\n+                    \"num_steps\": self._num_steps,\n+                }\n+\n+                subdir = os.path.join(\"trainer\", agent_key)\n+                checkpointer = tf2_savers.Checkpointer(\n+                    time_delta_minutes=15,\n+                    directory=checkpoint_subpath,\n+                    objects_to_save=objects_to_save,\n+                    subdirectory=subdir,\n+                )\n+                self._system_checkpointer[agent_key] = checkpointer\n+        # Do not record timestamps until after the first learning step is done.\n+        # This is to avoid including the time it takes for actors to come online and\n+        # fill the replay buffer.\n+        self._timestamp = None\n+\n+    def _update_target_networks(self) -> None:\n+        for key in self.unique_net_keys:\n+            # Update target network.\n+            online_variables = (\n+                *self._observation_networks[key].variables,\n+                *self._critic_networks[key].variables,\n+                *self._policy_networks[key].variables,\n+            )\n+            target_variables = (\n+                *self._target_observation_networks[key].variables,\n+                *self._target_critic_networks[key].variables,\n+                *self._target_policy_networks[key].variables,\n+            )\n+\n+            # Make online -> target network update ops.\n+            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n+                for src, dest in zip(online_variables, target_variables):\n+                    dest.assign(src)\n+            self._num_steps.assign_add(1)\n \n",
        "source_code_with_indent": "\n        super().__init__(\n            agents=agents,\n            agent_types=agent_types,\n            policy_networks=policy_networks,\n            critic_networks=critic_networks,\n            target_policy_networks=target_policy_networks,\n            target_critic_networks=target_critic_networks,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            discount=discount,\n            target_update_period=target_update_period,\n            dataset=dataset,\n            observation_networks=observation_networks,\n            target_observation_networks=target_observation_networks,\n            shared_weights=shared_weights,\n            max_gradient_norm=max_gradient_norm,\n            counter=counter,\n            logger=logger,\n            checkpoint=checkpoint,\n            checkpoint_subpath=checkpoint_subpath,\n        )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        self._agents = agents\n        self._agent_types = agent_types\n        self._shared_weights = shared_weights\n        self._checkpoint = checkpoint\n\n        # Store online and target networks.\n        self._policy_networks = policy_networks\n        self._critic_networks = critic_networks\n        self._target_policy_networks = target_policy_networks\n        self._target_critic_networks = target_critic_networks\n\n        # Ensure obs and target networks are sonnet modules\n        self._observation_networks = {\n            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n        }\n        self._target_observation_networks = {\n            k: tf2_utils.to_sonnet_module(v)\n            for k, v in target_observation_networks.items()\n        }\n\n        # General learner book-keeping and loggers.\n        self._counter = counter or counting.Counter()\n        self._logger = logger or loggers.make_default_logger(\"trainer\")\n\n        # Other learner parameters.\n        self._discount = discount\n\n        # Set up gradient clipping.\n        if max_gradient_norm is not None:\n            <IND>self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n        <DED>else:  # A very large number. Infinity results in NaNs.\n            <IND>self._max_gradient_norm = tf.convert_to_tensor(1e10)\n\n        # Necessary to track when to update target networks.\n        <DED>self._num_steps = tf.Variable(0, dtype=tf.int32)\n        self._target_update_period = target_update_period\n\n        # Create an iterator to go through the dataset.\n        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n        self._critic_optimizer = critic_optimizer\n        self._policy_optimizer = policy_optimizer\n\n        # Dictionary with network keys for each agent.\n        self.agent_net_keys = {agent: agent for agent in self._agents}\n        if self._shared_weights:\n            <IND>self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n\n        <DED>self.unique_net_keys = self._agent_types if shared_weights else self._agents\n\n        # Expose the variables.\n        policy_networks_to_expose = {}\n        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n            \"critic\": {},\n            \"policy\": {},\n        }\n        for agent_key in self.unique_net_keys:\n            <IND>policy_network_to_expose = snt.Sequential(\n                [\n                    self._target_observation_networks[agent_key],\n                    self._target_policy_networks[agent_key],\n                ]\n            )\n            policy_networks_to_expose[agent_key] = policy_network_to_expose\n            self._system_network_variables[\"critic\"][\n                agent_key\n            ] = target_critic_networks[agent_key].variables\n            self._system_network_variables[\"policy\"][\n                agent_key\n            ] = policy_network_to_expose.variables\n\n        # Create checkpointer\n        <DED>self._system_checkpointer = {}\n        if checkpoint:\n            <IND>for agent_key in self.unique_net_keys:\n                <IND>objects_to_save = {\n                    \"counter\": self._counter,\n                    \"policy\": self._policy_networks[agent_key],\n                    \"critic\": self._critic_networks[agent_key],\n                    \"observation\": self._observation_networks[agent_key],\n                    \"target_policy\": self._target_policy_networks[agent_key],\n                    \"target_critic\": self._target_critic_networks[agent_key],\n                    \"target_observation\": self._target_observation_networks[agent_key],\n                    \"policy_optimizer\": self._policy_optimizer,\n                    \"critic_optimizer\": self._critic_optimizer,\n                    \"num_steps\": self._num_steps,\n                }\n\n                subdir = os.path.join(\"trainer\", agent_key)\n                checkpointer = tf2_savers.Checkpointer(\n                    time_delta_minutes=15,\n                    directory=checkpoint_subpath,\n                    objects_to_save=objects_to_save,\n                    subdirectory=subdir,\n                )\n                self._system_checkpointer[agent_key] = checkpointer\n        # Do not record timestamps until after the first learning step is done.\n        # This is to avoid including the time it takes for actors to come online and\n        # fill the replay buffer.\n        <DED><DED>self._timestamp = None\n\n    <DED>def _update_target_networks(self) -> None:\n        <IND>for key in self.unique_net_keys:\n            # Update target network.\n            <IND>online_variables = (\n                *self._observation_networks[key].variables,\n                *self._critic_networks[key].variables,\n                *self._policy_networks[key].variables,\n            )\n            target_variables = (\n                *self._target_observation_networks[key].variables,\n                *self._target_critic_networks[key].variables,\n                *self._target_policy_networks[key].variables,\n            )\n\n            # Make online -> target network update ops.\n            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n                <IND>for src, dest in zip(online_variables, target_variables):\n                    <IND>dest.assign(src)\n            <DED><DED>self._num_steps.assign_add(1)\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "instadeepai/Mava",
    "commit": "8e4dd412ccab4fba42dfe987b81124c901e1e4c0",
    "filename": "mava/systems/tf/maddpg/training.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/instadeepai-Mava/mava/systems/tf/maddpg/training.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "mava/systems/tf/maddpg/training.py:852:4 Inconsistent override [14]: `mava.systems.tf.maddpg.training.BaseRecurrentMADDPGTrainer._get_dpg_feed` overrides method defined in `BaseMADDPGTrainer` inconsistently. Could not find parameter `dpg_a_t` in overriding signature.",
    "message": " `mava.systems.tf.maddpg.training.BaseRecurrentMADDPGTrainer._get_dpg_feed` overrides method defined in `BaseMADDPGTrainer` inconsistently. Could not find parameter `dpg_a_t` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 852,
    "warning_line": "    def _get_dpg_feed(",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nclass BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n    \"\"\"MADDPG trainer.\n",
        "source_code_len": 77,
        "target_code": "\nclass BaseRecurrentMADDPGTrainer(mava.Trainer):\n    \"\"\"MADDPG trainer.\n",
        "target_code_len": 72,
        "diff_format": "@@ -724,3 +723,3 @@\n \n-class BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n+class BaseRecurrentMADDPGTrainer(mava.Trainer):\n     \"\"\"MADDPG trainer.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n<DED><DED>class BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n    <IND>",
        "target_code_with_indent": "\n<DED><DED>class BaseRecurrentMADDPGTrainer(mava.Trainer):\n    <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        super().__init__(\n            agents=agents,\n            agent_types=agent_types,\n            policy_networks=policy_networks,\n            critic_networks=critic_networks,\n            target_policy_networks=target_policy_networks,\n            target_critic_networks=target_critic_networks,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            discount=discount,\n            target_update_period=target_update_period,\n            dataset=dataset,\n            observation_networks=observation_networks,\n            target_observation_networks=target_observation_networks,\n            shared_weights=shared_weights,\n            max_gradient_norm=max_gradient_norm,\n            counter=counter,\n            logger=logger,\n            checkpoint=checkpoint,\n            checkpoint_subpath=checkpoint_subpath,\n        )\n\n",
        "source_code_len": 877,
        "target_code": "\n        self._agents = agents\n        self._agent_types = agent_types\n        self._shared_weights = shared_weights\n        self._checkpoint = checkpoint\n\n        # Store online and target networks.\n        self._policy_networks = policy_networks\n        self._critic_networks = critic_networks\n        self._target_policy_networks = target_policy_networks\n        self._target_critic_networks = target_critic_networks\n\n        # Ensure obs and target networks are sonnet modules\n        self._observation_networks = {\n            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n        }\n        self._target_observation_networks = {\n            k: tf2_utils.to_sonnet_module(v)\n            for k, v in target_observation_networks.items()\n        }\n\n        # General learner book-keeping and loggers.\n        self._counter = counter or counting.Counter()\n        self._logger = logger or loggers.make_default_logger(\"trainer\")\n\n        # Other learner parameters.\n        self._discount = discount\n\n        # Set up gradient clipping.\n        if max_gradient_norm is not None:\n            self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n        else:  # A very large number. Infinity results in NaNs.\n            self._max_gradient_norm = tf.convert_to_tensor(1e10)\n\n        # Necessary to track when to update target networks.\n        self._num_steps = tf.Variable(0, dtype=tf.int32)\n        self._target_update_period = target_update_period\n\n        # Create an iterator to go through the dataset.\n        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n        self._critic_optimizer = critic_optimizer\n        self._policy_optimizer = policy_optimizer\n\n        # Dictionary with network keys for each agent.\n        self.agent_net_keys = {agent: agent for agent in self._agents}\n        if self._shared_weights:\n            self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n\n        self.unique_net_keys = self._agent_types if shared_weights else self._agents\n\n        # Expose the variables.\n        policy_networks_to_expose = {}\n        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n            \"critic\": {},\n            \"policy\": {},\n        }\n        for agent_key in self.unique_net_keys:\n            policy_network_to_expose = snt.Sequential(\n                [\n                    self._target_observation_networks[agent_key],\n                    self._target_policy_networks[agent_key],\n                ]\n            )\n            policy_networks_to_expose[agent_key] = policy_network_to_expose\n            self._system_network_variables[\"critic\"][\n                agent_key\n            ] = target_critic_networks[agent_key].variables\n            self._system_network_variables[\"policy\"][\n                agent_key\n            ] = policy_network_to_expose.variables\n\n        # Create checkpointer\n        self._system_checkpointer = {}\n        if checkpoint:\n            for agent_key in self.unique_net_keys:\n                objects_to_save = {\n                    \"counter\": self._counter,\n                    \"policy\": self._policy_networks[agent_key],\n                    \"critic\": self._critic_networks[agent_key],\n                    \"observation\": self._observation_networks[agent_key],\n                    \"target_policy\": self._target_policy_networks[agent_key],\n                    \"target_critic\": self._target_critic_networks[agent_key],\n                    \"target_observation\": self._target_observation_networks[agent_key],\n                    \"policy_optimizer\": self._policy_optimizer,\n                    \"critic_optimizer\": self._critic_optimizer,\n                    \"num_steps\": self._num_steps,\n                }\n\n                subdir = os.path.join(\"trainer\", agent_key)\n                checkpointer = tf2_savers.Checkpointer(\n                    time_delta_minutes=15,\n                    directory=checkpoint_subpath,\n                    objects_to_save=objects_to_save,\n                    subdirectory=subdir,\n                )\n                self._system_checkpointer[agent_key] = checkpointer\n        # Do not record timestamps until after the first learning step is done.\n        # This is to avoid including the time it takes for actors to come online and\n        # fill the replay buffer.\n        self._timestamp = None\n\n    def _update_target_networks(self) -> None:\n        for key in self.unique_net_keys:\n            # Update target network.\n            online_variables = (\n                *self._observation_networks[key].variables,\n                *self._critic_networks[key].variables,\n                *self._policy_networks[key].variables,\n            )\n            target_variables = (\n                *self._target_observation_networks[key].variables,\n                *self._target_critic_networks[key].variables,\n                *self._target_policy_networks[key].variables,\n            )\n\n            # Make online -> target network update ops.\n            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n                for src, dest in zip(online_variables, target_variables):\n                    dest.assign(src)\n            self._num_steps.assign_add(1)\n\n",
        "target_code_len": 5252,
        "diff_format": "@@ -775,23 +774,122 @@\n \n-        super().__init__(\n-            agents=agents,\n-            agent_types=agent_types,\n-            policy_networks=policy_networks,\n-            critic_networks=critic_networks,\n-            target_policy_networks=target_policy_networks,\n-            target_critic_networks=target_critic_networks,\n-            policy_optimizer=policy_optimizer,\n-            critic_optimizer=critic_optimizer,\n-            discount=discount,\n-            target_update_period=target_update_period,\n-            dataset=dataset,\n-            observation_networks=observation_networks,\n-            target_observation_networks=target_observation_networks,\n-            shared_weights=shared_weights,\n-            max_gradient_norm=max_gradient_norm,\n-            counter=counter,\n-            logger=logger,\n-            checkpoint=checkpoint,\n-            checkpoint_subpath=checkpoint_subpath,\n-        )\n+        self._agents = agents\n+        self._agent_types = agent_types\n+        self._shared_weights = shared_weights\n+        self._checkpoint = checkpoint\n+\n+        # Store online and target networks.\n+        self._policy_networks = policy_networks\n+        self._critic_networks = critic_networks\n+        self._target_policy_networks = target_policy_networks\n+        self._target_critic_networks = target_critic_networks\n+\n+        # Ensure obs and target networks are sonnet modules\n+        self._observation_networks = {\n+            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n+        }\n+        self._target_observation_networks = {\n+            k: tf2_utils.to_sonnet_module(v)\n+            for k, v in target_observation_networks.items()\n+        }\n+\n+        # General learner book-keeping and loggers.\n+        self._counter = counter or counting.Counter()\n+        self._logger = logger or loggers.make_default_logger(\"trainer\")\n+\n+        # Other learner parameters.\n+        self._discount = discount\n+\n+        # Set up gradient clipping.\n+        if max_gradient_norm is not None:\n+            self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n+        else:  # A very large number. Infinity results in NaNs.\n+            self._max_gradient_norm = tf.convert_to_tensor(1e10)\n+\n+        # Necessary to track when to update target networks.\n+        self._num_steps = tf.Variable(0, dtype=tf.int32)\n+        self._target_update_period = target_update_period\n+\n+        # Create an iterator to go through the dataset.\n+        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n+\n+        self._critic_optimizer = critic_optimizer\n+        self._policy_optimizer = policy_optimizer\n+\n+        # Dictionary with network keys for each agent.\n+        self.agent_net_keys = {agent: agent for agent in self._agents}\n+        if self._shared_weights:\n+            self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n+\n+        self.unique_net_keys = self._agent_types if shared_weights else self._agents\n+\n+        # Expose the variables.\n+        policy_networks_to_expose = {}\n+        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n+            \"critic\": {},\n+            \"policy\": {},\n+        }\n+        for agent_key in self.unique_net_keys:\n+            policy_network_to_expose = snt.Sequential(\n+                [\n+                    self._target_observation_networks[agent_key],\n+                    self._target_policy_networks[agent_key],\n+                ]\n+            )\n+            policy_networks_to_expose[agent_key] = policy_network_to_expose\n+            self._system_network_variables[\"critic\"][\n+                agent_key\n+            ] = target_critic_networks[agent_key].variables\n+            self._system_network_variables[\"policy\"][\n+                agent_key\n+            ] = policy_network_to_expose.variables\n+\n+        # Create checkpointer\n+        self._system_checkpointer = {}\n+        if checkpoint:\n+            for agent_key in self.unique_net_keys:\n+                objects_to_save = {\n+                    \"counter\": self._counter,\n+                    \"policy\": self._policy_networks[agent_key],\n+                    \"critic\": self._critic_networks[agent_key],\n+                    \"observation\": self._observation_networks[agent_key],\n+                    \"target_policy\": self._target_policy_networks[agent_key],\n+                    \"target_critic\": self._target_critic_networks[agent_key],\n+                    \"target_observation\": self._target_observation_networks[agent_key],\n+                    \"policy_optimizer\": self._policy_optimizer,\n+                    \"critic_optimizer\": self._critic_optimizer,\n+                    \"num_steps\": self._num_steps,\n+                }\n+\n+                subdir = os.path.join(\"trainer\", agent_key)\n+                checkpointer = tf2_savers.Checkpointer(\n+                    time_delta_minutes=15,\n+                    directory=checkpoint_subpath,\n+                    objects_to_save=objects_to_save,\n+                    subdirectory=subdir,\n+                )\n+                self._system_checkpointer[agent_key] = checkpointer\n+        # Do not record timestamps until after the first learning step is done.\n+        # This is to avoid including the time it takes for actors to come online and\n+        # fill the replay buffer.\n+        self._timestamp = None\n+\n+    def _update_target_networks(self) -> None:\n+        for key in self.unique_net_keys:\n+            # Update target network.\n+            online_variables = (\n+                *self._observation_networks[key].variables,\n+                *self._critic_networks[key].variables,\n+                *self._policy_networks[key].variables,\n+            )\n+            target_variables = (\n+                *self._target_observation_networks[key].variables,\n+                *self._target_critic_networks[key].variables,\n+                *self._target_policy_networks[key].variables,\n+            )\n+\n+            # Make online -> target network update ops.\n+            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n+                for src, dest in zip(online_variables, target_variables):\n+                    dest.assign(src)\n+            self._num_steps.assign_add(1)\n \n",
        "source_code_with_indent": "\n        super().__init__(\n            agents=agents,\n            agent_types=agent_types,\n            policy_networks=policy_networks,\n            critic_networks=critic_networks,\n            target_policy_networks=target_policy_networks,\n            target_critic_networks=target_critic_networks,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            discount=discount,\n            target_update_period=target_update_period,\n            dataset=dataset,\n            observation_networks=observation_networks,\n            target_observation_networks=target_observation_networks,\n            shared_weights=shared_weights,\n            max_gradient_norm=max_gradient_norm,\n            counter=counter,\n            logger=logger,\n            checkpoint=checkpoint,\n            checkpoint_subpath=checkpoint_subpath,\n        )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        self._agents = agents\n        self._agent_types = agent_types\n        self._shared_weights = shared_weights\n        self._checkpoint = checkpoint\n\n        # Store online and target networks.\n        self._policy_networks = policy_networks\n        self._critic_networks = critic_networks\n        self._target_policy_networks = target_policy_networks\n        self._target_critic_networks = target_critic_networks\n\n        # Ensure obs and target networks are sonnet modules\n        self._observation_networks = {\n            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n        }\n        self._target_observation_networks = {\n            k: tf2_utils.to_sonnet_module(v)\n            for k, v in target_observation_networks.items()\n        }\n\n        # General learner book-keeping and loggers.\n        self._counter = counter or counting.Counter()\n        self._logger = logger or loggers.make_default_logger(\"trainer\")\n\n        # Other learner parameters.\n        self._discount = discount\n\n        # Set up gradient clipping.\n        if max_gradient_norm is not None:\n            <IND>self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n        <DED>else:  # A very large number. Infinity results in NaNs.\n            <IND>self._max_gradient_norm = tf.convert_to_tensor(1e10)\n\n        # Necessary to track when to update target networks.\n        <DED>self._num_steps = tf.Variable(0, dtype=tf.int32)\n        self._target_update_period = target_update_period\n\n        # Create an iterator to go through the dataset.\n        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n        self._critic_optimizer = critic_optimizer\n        self._policy_optimizer = policy_optimizer\n\n        # Dictionary with network keys for each agent.\n        self.agent_net_keys = {agent: agent for agent in self._agents}\n        if self._shared_weights:\n            <IND>self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n\n        <DED>self.unique_net_keys = self._agent_types if shared_weights else self._agents\n\n        # Expose the variables.\n        policy_networks_to_expose = {}\n        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n            \"critic\": {},\n            \"policy\": {},\n        }\n        for agent_key in self.unique_net_keys:\n            <IND>policy_network_to_expose = snt.Sequential(\n                [\n                    self._target_observation_networks[agent_key],\n                    self._target_policy_networks[agent_key],\n                ]\n            )\n            policy_networks_to_expose[agent_key] = policy_network_to_expose\n            self._system_network_variables[\"critic\"][\n                agent_key\n            ] = target_critic_networks[agent_key].variables\n            self._system_network_variables[\"policy\"][\n                agent_key\n            ] = policy_network_to_expose.variables\n\n        # Create checkpointer\n        <DED>self._system_checkpointer = {}\n        if checkpoint:\n            <IND>for agent_key in self.unique_net_keys:\n                <IND>objects_to_save = {\n                    \"counter\": self._counter,\n                    \"policy\": self._policy_networks[agent_key],\n                    \"critic\": self._critic_networks[agent_key],\n                    \"observation\": self._observation_networks[agent_key],\n                    \"target_policy\": self._target_policy_networks[agent_key],\n                    \"target_critic\": self._target_critic_networks[agent_key],\n                    \"target_observation\": self._target_observation_networks[agent_key],\n                    \"policy_optimizer\": self._policy_optimizer,\n                    \"critic_optimizer\": self._critic_optimizer,\n                    \"num_steps\": self._num_steps,\n                }\n\n                subdir = os.path.join(\"trainer\", agent_key)\n                checkpointer = tf2_savers.Checkpointer(\n                    time_delta_minutes=15,\n                    directory=checkpoint_subpath,\n                    objects_to_save=objects_to_save,\n                    subdirectory=subdir,\n                )\n                self._system_checkpointer[agent_key] = checkpointer\n        # Do not record timestamps until after the first learning step is done.\n        # This is to avoid including the time it takes for actors to come online and\n        # fill the replay buffer.\n        <DED><DED>self._timestamp = None\n\n    <DED>def _update_target_networks(self) -> None:\n        <IND>for key in self.unique_net_keys:\n            # Update target network.\n            <IND>online_variables = (\n                *self._observation_networks[key].variables,\n                *self._critic_networks[key].variables,\n                *self._policy_networks[key].variables,\n            )\n            target_variables = (\n                *self._target_observation_networks[key].variables,\n                *self._target_critic_networks[key].variables,\n                *self._target_policy_networks[key].variables,\n            )\n\n            # Make online -> target network update ops.\n            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n                <IND>for src, dest in zip(online_variables, target_variables):\n                    <IND>dest.assign(src)\n            <DED><DED>self._num_steps.assign_add(1)\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "instadeepai/Mava",
    "commit": "8e4dd412ccab4fba42dfe987b81124c901e1e4c0",
    "filename": "mava/systems/tf/maddpg/training.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/instadeepai-Mava/mava/systems/tf/maddpg/training.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "mava/systems/tf/maddpg/training.py:862:4 Inconsistent override [14]: `mava.systems.tf.maddpg.training.BaseRecurrentMADDPGTrainer._target_policy_actions` overrides method defined in `BaseMADDPGTrainer` inconsistently. Could not find parameter `next_obs` in overriding signature.",
    "message": " `mava.systems.tf.maddpg.training.BaseRecurrentMADDPGTrainer._target_policy_actions` overrides method defined in `BaseMADDPGTrainer` inconsistently. Could not find parameter `next_obs` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 862,
    "warning_line": "    def _target_policy_actions(",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nclass BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n    \"\"\"MADDPG trainer.\n",
        "source_code_len": 77,
        "target_code": "\nclass BaseRecurrentMADDPGTrainer(mava.Trainer):\n    \"\"\"MADDPG trainer.\n",
        "target_code_len": 72,
        "diff_format": "@@ -724,3 +723,3 @@\n \n-class BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n+class BaseRecurrentMADDPGTrainer(mava.Trainer):\n     \"\"\"MADDPG trainer.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n<DED><DED>class BaseRecurrentMADDPGTrainer(BaseMADDPGTrainer):\n    <IND>",
        "target_code_with_indent": "\n<DED><DED>class BaseRecurrentMADDPGTrainer(mava.Trainer):\n    <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        super().__init__(\n            agents=agents,\n            agent_types=agent_types,\n            policy_networks=policy_networks,\n            critic_networks=critic_networks,\n            target_policy_networks=target_policy_networks,\n            target_critic_networks=target_critic_networks,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            discount=discount,\n            target_update_period=target_update_period,\n            dataset=dataset,\n            observation_networks=observation_networks,\n            target_observation_networks=target_observation_networks,\n            shared_weights=shared_weights,\n            max_gradient_norm=max_gradient_norm,\n            counter=counter,\n            logger=logger,\n            checkpoint=checkpoint,\n            checkpoint_subpath=checkpoint_subpath,\n        )\n\n",
        "source_code_len": 877,
        "target_code": "\n        self._agents = agents\n        self._agent_types = agent_types\n        self._shared_weights = shared_weights\n        self._checkpoint = checkpoint\n\n        # Store online and target networks.\n        self._policy_networks = policy_networks\n        self._critic_networks = critic_networks\n        self._target_policy_networks = target_policy_networks\n        self._target_critic_networks = target_critic_networks\n\n        # Ensure obs and target networks are sonnet modules\n        self._observation_networks = {\n            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n        }\n        self._target_observation_networks = {\n            k: tf2_utils.to_sonnet_module(v)\n            for k, v in target_observation_networks.items()\n        }\n\n        # General learner book-keeping and loggers.\n        self._counter = counter or counting.Counter()\n        self._logger = logger or loggers.make_default_logger(\"trainer\")\n\n        # Other learner parameters.\n        self._discount = discount\n\n        # Set up gradient clipping.\n        if max_gradient_norm is not None:\n            self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n        else:  # A very large number. Infinity results in NaNs.\n            self._max_gradient_norm = tf.convert_to_tensor(1e10)\n\n        # Necessary to track when to update target networks.\n        self._num_steps = tf.Variable(0, dtype=tf.int32)\n        self._target_update_period = target_update_period\n\n        # Create an iterator to go through the dataset.\n        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n        self._critic_optimizer = critic_optimizer\n        self._policy_optimizer = policy_optimizer\n\n        # Dictionary with network keys for each agent.\n        self.agent_net_keys = {agent: agent for agent in self._agents}\n        if self._shared_weights:\n            self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n\n        self.unique_net_keys = self._agent_types if shared_weights else self._agents\n\n        # Expose the variables.\n        policy_networks_to_expose = {}\n        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n            \"critic\": {},\n            \"policy\": {},\n        }\n        for agent_key in self.unique_net_keys:\n            policy_network_to_expose = snt.Sequential(\n                [\n                    self._target_observation_networks[agent_key],\n                    self._target_policy_networks[agent_key],\n                ]\n            )\n            policy_networks_to_expose[agent_key] = policy_network_to_expose\n            self._system_network_variables[\"critic\"][\n                agent_key\n            ] = target_critic_networks[agent_key].variables\n            self._system_network_variables[\"policy\"][\n                agent_key\n            ] = policy_network_to_expose.variables\n\n        # Create checkpointer\n        self._system_checkpointer = {}\n        if checkpoint:\n            for agent_key in self.unique_net_keys:\n                objects_to_save = {\n                    \"counter\": self._counter,\n                    \"policy\": self._policy_networks[agent_key],\n                    \"critic\": self._critic_networks[agent_key],\n                    \"observation\": self._observation_networks[agent_key],\n                    \"target_policy\": self._target_policy_networks[agent_key],\n                    \"target_critic\": self._target_critic_networks[agent_key],\n                    \"target_observation\": self._target_observation_networks[agent_key],\n                    \"policy_optimizer\": self._policy_optimizer,\n                    \"critic_optimizer\": self._critic_optimizer,\n                    \"num_steps\": self._num_steps,\n                }\n\n                subdir = os.path.join(\"trainer\", agent_key)\n                checkpointer = tf2_savers.Checkpointer(\n                    time_delta_minutes=15,\n                    directory=checkpoint_subpath,\n                    objects_to_save=objects_to_save,\n                    subdirectory=subdir,\n                )\n                self._system_checkpointer[agent_key] = checkpointer\n        # Do not record timestamps until after the first learning step is done.\n        # This is to avoid including the time it takes for actors to come online and\n        # fill the replay buffer.\n        self._timestamp = None\n\n    def _update_target_networks(self) -> None:\n        for key in self.unique_net_keys:\n            # Update target network.\n            online_variables = (\n                *self._observation_networks[key].variables,\n                *self._critic_networks[key].variables,\n                *self._policy_networks[key].variables,\n            )\n            target_variables = (\n                *self._target_observation_networks[key].variables,\n                *self._target_critic_networks[key].variables,\n                *self._target_policy_networks[key].variables,\n            )\n\n            # Make online -> target network update ops.\n            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n                for src, dest in zip(online_variables, target_variables):\n                    dest.assign(src)\n            self._num_steps.assign_add(1)\n\n",
        "target_code_len": 5252,
        "diff_format": "@@ -775,23 +774,122 @@\n \n-        super().__init__(\n-            agents=agents,\n-            agent_types=agent_types,\n-            policy_networks=policy_networks,\n-            critic_networks=critic_networks,\n-            target_policy_networks=target_policy_networks,\n-            target_critic_networks=target_critic_networks,\n-            policy_optimizer=policy_optimizer,\n-            critic_optimizer=critic_optimizer,\n-            discount=discount,\n-            target_update_period=target_update_period,\n-            dataset=dataset,\n-            observation_networks=observation_networks,\n-            target_observation_networks=target_observation_networks,\n-            shared_weights=shared_weights,\n-            max_gradient_norm=max_gradient_norm,\n-            counter=counter,\n-            logger=logger,\n-            checkpoint=checkpoint,\n-            checkpoint_subpath=checkpoint_subpath,\n-        )\n+        self._agents = agents\n+        self._agent_types = agent_types\n+        self._shared_weights = shared_weights\n+        self._checkpoint = checkpoint\n+\n+        # Store online and target networks.\n+        self._policy_networks = policy_networks\n+        self._critic_networks = critic_networks\n+        self._target_policy_networks = target_policy_networks\n+        self._target_critic_networks = target_critic_networks\n+\n+        # Ensure obs and target networks are sonnet modules\n+        self._observation_networks = {\n+            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n+        }\n+        self._target_observation_networks = {\n+            k: tf2_utils.to_sonnet_module(v)\n+            for k, v in target_observation_networks.items()\n+        }\n+\n+        # General learner book-keeping and loggers.\n+        self._counter = counter or counting.Counter()\n+        self._logger = logger or loggers.make_default_logger(\"trainer\")\n+\n+        # Other learner parameters.\n+        self._discount = discount\n+\n+        # Set up gradient clipping.\n+        if max_gradient_norm is not None:\n+            self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n+        else:  # A very large number. Infinity results in NaNs.\n+            self._max_gradient_norm = tf.convert_to_tensor(1e10)\n+\n+        # Necessary to track when to update target networks.\n+        self._num_steps = tf.Variable(0, dtype=tf.int32)\n+        self._target_update_period = target_update_period\n+\n+        # Create an iterator to go through the dataset.\n+        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n+\n+        self._critic_optimizer = critic_optimizer\n+        self._policy_optimizer = policy_optimizer\n+\n+        # Dictionary with network keys for each agent.\n+        self.agent_net_keys = {agent: agent for agent in self._agents}\n+        if self._shared_weights:\n+            self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n+\n+        self.unique_net_keys = self._agent_types if shared_weights else self._agents\n+\n+        # Expose the variables.\n+        policy_networks_to_expose = {}\n+        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n+            \"critic\": {},\n+            \"policy\": {},\n+        }\n+        for agent_key in self.unique_net_keys:\n+            policy_network_to_expose = snt.Sequential(\n+                [\n+                    self._target_observation_networks[agent_key],\n+                    self._target_policy_networks[agent_key],\n+                ]\n+            )\n+            policy_networks_to_expose[agent_key] = policy_network_to_expose\n+            self._system_network_variables[\"critic\"][\n+                agent_key\n+            ] = target_critic_networks[agent_key].variables\n+            self._system_network_variables[\"policy\"][\n+                agent_key\n+            ] = policy_network_to_expose.variables\n+\n+        # Create checkpointer\n+        self._system_checkpointer = {}\n+        if checkpoint:\n+            for agent_key in self.unique_net_keys:\n+                objects_to_save = {\n+                    \"counter\": self._counter,\n+                    \"policy\": self._policy_networks[agent_key],\n+                    \"critic\": self._critic_networks[agent_key],\n+                    \"observation\": self._observation_networks[agent_key],\n+                    \"target_policy\": self._target_policy_networks[agent_key],\n+                    \"target_critic\": self._target_critic_networks[agent_key],\n+                    \"target_observation\": self._target_observation_networks[agent_key],\n+                    \"policy_optimizer\": self._policy_optimizer,\n+                    \"critic_optimizer\": self._critic_optimizer,\n+                    \"num_steps\": self._num_steps,\n+                }\n+\n+                subdir = os.path.join(\"trainer\", agent_key)\n+                checkpointer = tf2_savers.Checkpointer(\n+                    time_delta_minutes=15,\n+                    directory=checkpoint_subpath,\n+                    objects_to_save=objects_to_save,\n+                    subdirectory=subdir,\n+                )\n+                self._system_checkpointer[agent_key] = checkpointer\n+        # Do not record timestamps until after the first learning step is done.\n+        # This is to avoid including the time it takes for actors to come online and\n+        # fill the replay buffer.\n+        self._timestamp = None\n+\n+    def _update_target_networks(self) -> None:\n+        for key in self.unique_net_keys:\n+            # Update target network.\n+            online_variables = (\n+                *self._observation_networks[key].variables,\n+                *self._critic_networks[key].variables,\n+                *self._policy_networks[key].variables,\n+            )\n+            target_variables = (\n+                *self._target_observation_networks[key].variables,\n+                *self._target_critic_networks[key].variables,\n+                *self._target_policy_networks[key].variables,\n+            )\n+\n+            # Make online -> target network update ops.\n+            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n+                for src, dest in zip(online_variables, target_variables):\n+                    dest.assign(src)\n+            self._num_steps.assign_add(1)\n \n",
        "source_code_with_indent": "\n        super().__init__(\n            agents=agents,\n            agent_types=agent_types,\n            policy_networks=policy_networks,\n            critic_networks=critic_networks,\n            target_policy_networks=target_policy_networks,\n            target_critic_networks=target_critic_networks,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            discount=discount,\n            target_update_period=target_update_period,\n            dataset=dataset,\n            observation_networks=observation_networks,\n            target_observation_networks=target_observation_networks,\n            shared_weights=shared_weights,\n            max_gradient_norm=max_gradient_norm,\n            counter=counter,\n            logger=logger,\n            checkpoint=checkpoint,\n            checkpoint_subpath=checkpoint_subpath,\n        )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        self._agents = agents\n        self._agent_types = agent_types\n        self._shared_weights = shared_weights\n        self._checkpoint = checkpoint\n\n        # Store online and target networks.\n        self._policy_networks = policy_networks\n        self._critic_networks = critic_networks\n        self._target_policy_networks = target_policy_networks\n        self._target_critic_networks = target_critic_networks\n\n        # Ensure obs and target networks are sonnet modules\n        self._observation_networks = {\n            k: tf2_utils.to_sonnet_module(v) for k, v in observation_networks.items()\n        }\n        self._target_observation_networks = {\n            k: tf2_utils.to_sonnet_module(v)\n            for k, v in target_observation_networks.items()\n        }\n\n        # General learner book-keeping and loggers.\n        self._counter = counter or counting.Counter()\n        self._logger = logger or loggers.make_default_logger(\"trainer\")\n\n        # Other learner parameters.\n        self._discount = discount\n\n        # Set up gradient clipping.\n        if max_gradient_norm is not None:\n            <IND>self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n        <DED>else:  # A very large number. Infinity results in NaNs.\n            <IND>self._max_gradient_norm = tf.convert_to_tensor(1e10)\n\n        # Necessary to track when to update target networks.\n        <DED>self._num_steps = tf.Variable(0, dtype=tf.int32)\n        self._target_update_period = target_update_period\n\n        # Create an iterator to go through the dataset.\n        self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n        self._critic_optimizer = critic_optimizer\n        self._policy_optimizer = policy_optimizer\n\n        # Dictionary with network keys for each agent.\n        self.agent_net_keys = {agent: agent for agent in self._agents}\n        if self._shared_weights:\n            <IND>self.agent_net_keys = {agent: agent.split(\"_\")[0] for agent in self._agents}\n\n        <DED>self.unique_net_keys = self._agent_types if shared_weights else self._agents\n\n        # Expose the variables.\n        policy_networks_to_expose = {}\n        self._system_network_variables: Dict[str, Dict[str, snt.Module]] = {\n            \"critic\": {},\n            \"policy\": {},\n        }\n        for agent_key in self.unique_net_keys:\n            <IND>policy_network_to_expose = snt.Sequential(\n                [\n                    self._target_observation_networks[agent_key],\n                    self._target_policy_networks[agent_key],\n                ]\n            )\n            policy_networks_to_expose[agent_key] = policy_network_to_expose\n            self._system_network_variables[\"critic\"][\n                agent_key\n            ] = target_critic_networks[agent_key].variables\n            self._system_network_variables[\"policy\"][\n                agent_key\n            ] = policy_network_to_expose.variables\n\n        # Create checkpointer\n        <DED>self._system_checkpointer = {}\n        if checkpoint:\n            <IND>for agent_key in self.unique_net_keys:\n                <IND>objects_to_save = {\n                    \"counter\": self._counter,\n                    \"policy\": self._policy_networks[agent_key],\n                    \"critic\": self._critic_networks[agent_key],\n                    \"observation\": self._observation_networks[agent_key],\n                    \"target_policy\": self._target_policy_networks[agent_key],\n                    \"target_critic\": self._target_critic_networks[agent_key],\n                    \"target_observation\": self._target_observation_networks[agent_key],\n                    \"policy_optimizer\": self._policy_optimizer,\n                    \"critic_optimizer\": self._critic_optimizer,\n                    \"num_steps\": self._num_steps,\n                }\n\n                subdir = os.path.join(\"trainer\", agent_key)\n                checkpointer = tf2_savers.Checkpointer(\n                    time_delta_minutes=15,\n                    directory=checkpoint_subpath,\n                    objects_to_save=objects_to_save,\n                    subdirectory=subdir,\n                )\n                self._system_checkpointer[agent_key] = checkpointer\n        # Do not record timestamps until after the first learning step is done.\n        # This is to avoid including the time it takes for actors to come online and\n        # fill the replay buffer.\n        <DED><DED>self._timestamp = None\n\n    <DED>def _update_target_networks(self) -> None:\n        <IND>for key in self.unique_net_keys:\n            # Update target network.\n            <IND>online_variables = (\n                *self._observation_networks[key].variables,\n                *self._critic_networks[key].variables,\n                *self._policy_networks[key].variables,\n            )\n            target_variables = (\n                *self._target_observation_networks[key].variables,\n                *self._target_critic_networks[key].variables,\n                *self._target_policy_networks[key].variables,\n            )\n\n            # Make online -> target network update ops.\n            if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n                <IND>for src, dest in zip(online_variables, target_variables):\n                    <IND>dest.assign(src)\n            <DED><DED>self._num_steps.assign_add(1)\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  }
]