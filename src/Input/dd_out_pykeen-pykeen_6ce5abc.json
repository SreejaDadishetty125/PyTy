[
  {
    "project": "pykeen/pykeen",
    "commit": "6ce5abc75bc90241bd29275343a999a4488de958",
    "filename": "src/pykeen/ablation/ablation.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/pykeen-pykeen/src/pykeen/ablation/ablation.py",
    "file_hunks_size": 24,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "src/pykeen/ablation/ablation.py:391:51 Incompatible parameter type [6]: Expected `str` for 2nd parameter `value` to anonymous call but got `Union[str, Mapping[str, Mapping[str, typing.Any]]]`.",
    "message": " Expected `str` for 2nd parameter `value` to anonymous call but got `Union[str, Mapping[str, Mapping[str, typing.Any]]]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 391,
    "warning_line": "            _set_arguments(key='negative_sampler', value=negative_sampler)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "import time\nfrom copy import deepcopy\nfrom typing import Any, List, Mapping, Optional, Tuple, Union\nfrom uuid import uuid4\n",
        "source_code_len": 123,
        "target_code": "import time\nfrom typing import Any, Dict, List, Mapping, Optional, Tuple, Union\nfrom uuid import uuid4\n",
        "target_code_len": 103,
        "diff_format": "@@ -9,4 +9,3 @@\n import time\n-from copy import deepcopy\n-from typing import Any, List, Mapping, Optional, Tuple, Union\n+from typing import Any, Dict, List, Mapping, Optional, Tuple, Union\n from uuid import uuid4\n",
        "source_code_with_indent": "import time\nfrom copy import deepcopy\nfrom typing import Any, List, Mapping, Optional, Tuple, Union\nfrom uuid import uuid4\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "import time\nfrom typing import Any, Dict, List, Mapping, Optional, Tuple, Union\nfrom uuid import uuid4\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    datasets: Union[str, List[str]],\n    models: Union[str, List[str]],\n",
        "source_code_len": 72,
        "target_code": "    datasets: Union[str, List[str]],\n    directory: str,\n    models: Union[str, List[str]],\n",
        "target_code_len": 92,
        "diff_format": "@@ -32,2 +31,3 @@\n     datasets: Union[str, List[str]],\n+    directory: str,\n     models: Union[str, List[str]],\n",
        "source_code_with_indent": "    datasets: Union[str, List[str]],\n    models: Union[str, List[str]],\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    datasets: Union[str, List[str]],\n    directory: str,\n    models: Union[str, List[str]],\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    *,\n    create_inverse_triples: Union[bool, List[bool]] = False,\n    regularizers: Union[None, str, List[str]] = None,\n    model_to_model_kwargs: Optional[Mapping2D] = None,\n    model_to_model_kwargs_ranges: Optional[Mapping2D] = None,\n    model_to_trainer_to_training_kwargs: Optional[Mapping3D] = None,\n    model_to_trainer_to_training_kwargs_ranges: Optional[Mapping3D] = None,\n    ablation_config: Optional[Mapping3D] = None,\n    evaluator: Optional[str] = None,\n    optuna_config: Optional[Mapping[str, Any]] = None,\n    evaluator_kwargs: Optional[Mapping[str, Any]] = None,\n    evaluation_kwargs: Optional[Mapping[str, Any]] = None,\n    directory: Optional[str] = None,\n    dry_run: bool = False,\n    best_replicates: Optional[int] = None,\n    save_artifacts: bool = True,\n    move_to_cpu: bool = True,\n    discard_replicates: bool = False,\n) -> None:\n    \"\"\"Generate a set of HPO configurations.\n\n    A sample file can be run with``pykeen experiment ablation tests/resources/hpo_complex_nations.json``.\n\n    :param datasets: A dataset name or list of dataset names\n    :param models: A model name or list of model names\n    :param losses: A loss function name or list of loss function names\n    :param optimizers: An optimizer name or list of optimizer names\n    :param training_loops: A training loop name or list of training loop names\n    :param create_inverse_triples: Either a boolean for a single entry or a list of booleans\n    :param regularizers: A regularizer name, list of regularizer names, or None if no regularizer is desired.\n        Defaults to None.\n\n    :param evaluator: The name of the evaluator to be used. Defaults to rank-based evaluator.\n    :param evaluator_kwargs: The keyword arguments passed to the evaluator (in the pipeline)\n    :param evaluation_kwargs: The keyword arguments passed during evaluation (in the pipeline)\n\n    :param model_to_model_kwargs: A mapping from model name to dictionaries of default keyword arguments for\n        the instantiation of that model\n    :param model_to_model_kwargs_ranges: A mapping from model name to dictionaries of keyword argument\n        ranges for that model to be used in HPO.\n    :param model_to_trainer_to_training_kwargs: A mapping from model name to a mapping of trainer name to a mapping\n        of default keyword arguments for the instantiation of that trainer. This is useful becuase for some models,\n        you might want to set the number of epochs differently.\n    :param model_to_trainer_to_training_kwargs_ranges: A mapping from model name to a mapping of trainer name\n        to a mapping of keyword arguments for that trainer to be used in HPO.\n    :param ablation_config: Additional third-order and fourth-order ablation configuration for all other ablation\n        keys to models to either kwargs or kwarg ranges\n\n    :param optuna_config: Configuration passed to optuna for HPO over all ablation studies\n    :param directory: The directory in which the experimental artifacts will be saved.\n    :param dry_run: Defines whether only the configurations for the single experiments should be created without\n     running them.\n    :param best_replicates: Defines how often the final model should be re-trained and evaluated based on the best\n     hyper-parameters enabling to measure the variance in performance.\n    :param save_artifacts: Defines, whether each trained model sampled during HPO should be saved.\n    :param move_to_cpu: Defines, whether a replicate of the best model should be moved to CPU.\n     We recommend to set this flag to 'True' to avoid unnecessary GPU usage.\n    :param discard_replicates: Defines, whether the best model should be discarded after training and evaluation.\n    \"\"\"\n    datetime = time.strftime('%Y-%m-%d-%H-%M')\n    directory = os.path.join(directory, f'{datetime}_{uuid4()}')\n\n",
        "source_code_len": 3819,
        "target_code": "    *,\n    epochs: Optional[int] = None,\n    create_inverse_triples: Union[bool, List[bool]] = False,\n    regularizers: Union[None, str, List[str]] = None,\n    negative_sampler: Union[str, None] = None,\n    evaluator: Optional[str] = None,\n    stopper: Optional[str] = 'NopStopper',\n    model_to_model_kwargs: Optional[Mapping2D] = None,\n    model_to_model_kwargs_ranges: Optional[Mapping2D] = None,\n    model_to_loss_to_loss_kwargs: Optional[Mapping3D] = None,\n    model_to_loss_to_loss_kwargs_ranges: Optional[Mapping3D] = None,\n    model_to_optimizer_to_optimizer_kwargs: Optional[Mapping3D] = None,\n    model_to_optimizer_to_optimizer_kwargs_ranges: Optional[Mapping3D] = None,\n    model_to_negative_sampler_to_negative_sampler_kwargs: Optional[Mapping3D] = None,\n    model_to_negative_sampler_to_negative_sampler_kwargs_ranges: Optional[Mapping3D] = None,\n    model_to_training_loop_to_training_loop_kwargs: Optional[Mapping3D] = None,\n    model_to_training_loop_to_training_kwargs: Optional[Mapping3D] = None,\n    model_to_training_loop_to_training_kwargs_ranges: Optional[Mapping3D] = None,\n    model_to_regularizer_to_regularizer_kwargs: Optional[Mapping3D] = None,\n    model_to_regularizer_to_regularizer_kwargs_ranges: Optional[Mapping3D] = None,\n    evaluator_kwargs: Optional[Mapping[str, Any]] = None,\n    evaluation_kwargs: Optional[Mapping[str, Any]] = None,\n    stopper_kwargs: Optional[Mapping[str, Any]] = None,\n    n_trials: Optional[int] = 5,\n    timeout: Optional[int] = 3600,\n    metric: Optional[str] = 'hits@10',\n    direction: Optional[str] = 'maximize',\n    sampler: Optional[str] = 'random',\n    pruner: Optional[str] = 'nop',\n    metadata: Optional[Mapping] = None,\n    save_artifacts: bool = True,\n    move_to_cpu: bool = True,\n    dry_run: bool = False,\n    best_replicates: Optional[int] = None,\n    discard_replicates: bool = False,\n    create_unique_subdir: bool = False,\n):\n    \"\"\"Run ablation study.\n\n    :param datasets: A dataset name or list of dataset names.\n    :param directory: The directory in which the experimental artifacts will be saved.\n    :param models: A model name or list of model names.\n    :param losses: A loss function name or list of loss function names.\n    :param optimizers: An optimizer name or list of optimizer names.\n    :param training_loops: A training loop name or list of training loop names.\n    :param epochs: A quick way to set the ``num_epochs`` in the training kwargs.\n    :param create_inverse_triples: Either a boolean for a single entry or a list of booleans.\n    :param regularizers: A regularizer name, list of regularizer names, or None if no regularizer is desired.\n    :param negative_sampler: A negative sampler name, list of regularizer names, or None if no negative sampler\n        is desired. Negative sampling is used only in combination with :class:`pykeen.training.SLCWATrainingLoop`.\n    :param evaluator: The name of the evaluator to be used. Defaults to rank-based evaluator.\n    :param stopper: The name of the stopper to be used. Defaults to NopStopper which doesn't define a\n        stopping criterion.\n    :param model_to_model_kwargs: A mapping from model name to dictionaries of default keyword arguments for\n        the instantiation of that model.\n    :param model_to_model_kwargs_ranges: A mapping from model name to dictionaries of keyword argument\n        ranges for that model to be used in HPO.\n    :param model_to_loss_to_loss_kwargs: A mapping from model name to a mapping of loss name to a mapping\n        of default keyword arguments for the instantiation of that loss function. This is useful because for some\n        losses, have hyper-parameters such as :class:`pykeen.losses.MarginRankingLoss`.\n    :param model_to_loss_to_loss_kwargs_ranges: A mapping from model name to a mapping of loss name\n        to a mapping of keyword argument ranges for that loss to be used in HPO.\n    :param model_to_optimizer_to_optimizer_kwargs: A mapping from model name to a mapping of optimizer name to a mapping\n        of default keyword arguments for the instantiation of that optimizer. This is useful because the optimizers,\n        have hyper-parameters such as the learning rate.\n    :param model_to_optimizer_to_optimizer_kwargs_ranges: A mapping from model name to a mapping of optimizer name\n        to a mapping of keyword argument ranges for that optimizer to be used in HPO.\n    :param model_to_regularizer_to_regularizer_kwargs: A mapping from model name to a mapping of regularizer name to a\n        mapping of default keyword arguments for the instantiation of that regularizer. This is useful because the\n        optimizers, have hyper-parameters such as the regularization weight.\n    :param model_to_regularizer_to_regularizer_kwargs_ranges: A mapping from model name to a mapping of regularizer name\n        to a mapping of keyword argument ranges for that regularizer to be used in HPO.\n    :param model_to_negative_sampler_to_negative_sampler_kwargs: A mapping from model name to a mapping of\n        negative sampler name to a mapping of default keyword arguments for the instantiation of that negative sampler.\n        This is useful because the negative samplers, have hyper-parameters such as the number of negatives that should\n        get generated for each positive training example.\n    :param model_to_negative_sampler_to_negative_sampler_kwargs_ranges: A mapping from model name to a mapping of\n        negative sampler name to a mapping of keyword argument ranges for that negative sampler to be used in HPO.\n    :param model_to_training_loop_to_training_loop_kwargs: A mapping from model name to a mapping of training loop name\n        to a mapping of default keyword arguments for the training loop.\n    :param model_to_training_loop_to_training_kwargs: A mapping from model name to a mapping of trainer name to a\n        mapping of default keyword arguments for the training procedure. This is useful because you can set the\n        hyper-parameters such as the number of training epochs and the batch size.\n    :param model_to_training_loop_to_training_kwargs_ranges:  A mapping from model name to a mapping of\n        trainer name to a mapping of keyword argument ranges for that trainer to be used in HPO.\n    :param evaluator_kwargs: The keyword arguments passed to the evaluator.\n    :param evaluation_kwargs: The keyword arguments passed during evaluation.\n    :param stopper_kwargs: The keyword arguments passed to the stopper.\n    :param n_trials: Number of HPO trials.\n    :param timeout: The time (seconds) after which the ablation study will be terminated.\n    :param metric: The metric to optimize during HPO.\n    :param direction: Defines, whether to 'maximize' or 'minimize' the metric during HPO.\n    :param sampler: The HPO sampler, it defaults to random search.\n    :param pruner: Defines approach for pruning trials. Per default no pruning is used, i.e., pruner is\n        set to 'Nopruner'.\n    :param metadata: A mapping of meta data arguments such as name of the ablation study.\n    :param save_artifacts: Defines, whether each trained model sampled during HPO should be saved.\n    :param move_to_cpu: Defines, whether a replicate of the best model should be moved to CPU.\n    :param dry_run: Defines whether only the configurations for the single experiments should be created without\n        running them.\n    :param best_replicates: Defines how often the final model should be re-trained and evaluated based on the best\n        hyper-parameters enabling to measure the variance in performance.\n    :param discard_replicates: Defines, whether the best model should be discarded after training and evaluation.\n    :param create_unique_subdir: Defines, whether a unique sub-directory for the experimental artifacts should\n        be created. The sub-directory name is defined  by the  current  data + a unique id.\n    \"\"\"\n    if create_unique_subdir:\n        directory = _create_path_with_id(directory=directory)\n\n",
        "target_code_len": 7968,
        "diff_format": "@@ -37,62 +37,109 @@\n     *,\n+    epochs: Optional[int] = None,\n     create_inverse_triples: Union[bool, List[bool]] = False,\n     regularizers: Union[None, str, List[str]] = None,\n+    negative_sampler: Union[str, None] = None,\n+    evaluator: Optional[str] = None,\n+    stopper: Optional[str] = 'NopStopper',\n     model_to_model_kwargs: Optional[Mapping2D] = None,\n     model_to_model_kwargs_ranges: Optional[Mapping2D] = None,\n-    model_to_trainer_to_training_kwargs: Optional[Mapping3D] = None,\n-    model_to_trainer_to_training_kwargs_ranges: Optional[Mapping3D] = None,\n-    ablation_config: Optional[Mapping3D] = None,\n-    evaluator: Optional[str] = None,\n-    optuna_config: Optional[Mapping[str, Any]] = None,\n+    model_to_loss_to_loss_kwargs: Optional[Mapping3D] = None,\n+    model_to_loss_to_loss_kwargs_ranges: Optional[Mapping3D] = None,\n+    model_to_optimizer_to_optimizer_kwargs: Optional[Mapping3D] = None,\n+    model_to_optimizer_to_optimizer_kwargs_ranges: Optional[Mapping3D] = None,\n+    model_to_negative_sampler_to_negative_sampler_kwargs: Optional[Mapping3D] = None,\n+    model_to_negative_sampler_to_negative_sampler_kwargs_ranges: Optional[Mapping3D] = None,\n+    model_to_training_loop_to_training_loop_kwargs: Optional[Mapping3D] = None,\n+    model_to_training_loop_to_training_kwargs: Optional[Mapping3D] = None,\n+    model_to_training_loop_to_training_kwargs_ranges: Optional[Mapping3D] = None,\n+    model_to_regularizer_to_regularizer_kwargs: Optional[Mapping3D] = None,\n+    model_to_regularizer_to_regularizer_kwargs_ranges: Optional[Mapping3D] = None,\n     evaluator_kwargs: Optional[Mapping[str, Any]] = None,\n     evaluation_kwargs: Optional[Mapping[str, Any]] = None,\n-    directory: Optional[str] = None,\n+    stopper_kwargs: Optional[Mapping[str, Any]] = None,\n+    n_trials: Optional[int] = 5,\n+    timeout: Optional[int] = 3600,\n+    metric: Optional[str] = 'hits@10',\n+    direction: Optional[str] = 'maximize',\n+    sampler: Optional[str] = 'random',\n+    pruner: Optional[str] = 'nop',\n+    metadata: Optional[Mapping] = None,\n+    save_artifacts: bool = True,\n+    move_to_cpu: bool = True,\n     dry_run: bool = False,\n     best_replicates: Optional[int] = None,\n-    save_artifacts: bool = True,\n-    move_to_cpu: bool = True,\n     discard_replicates: bool = False,\n-) -> None:\n-    \"\"\"Generate a set of HPO configurations.\n-\n-    A sample file can be run with``pykeen experiment ablation tests/resources/hpo_complex_nations.json``.\n-\n-    :param datasets: A dataset name or list of dataset names\n-    :param models: A model name or list of model names\n-    :param losses: A loss function name or list of loss function names\n-    :param optimizers: An optimizer name or list of optimizer names\n-    :param training_loops: A training loop name or list of training loop names\n-    :param create_inverse_triples: Either a boolean for a single entry or a list of booleans\n+    create_unique_subdir: bool = False,\n+):\n+    \"\"\"Run ablation study.\n+\n+    :param datasets: A dataset name or list of dataset names.\n+    :param directory: The directory in which the experimental artifacts will be saved.\n+    :param models: A model name or list of model names.\n+    :param losses: A loss function name or list of loss function names.\n+    :param optimizers: An optimizer name or list of optimizer names.\n+    :param training_loops: A training loop name or list of training loop names.\n+    :param epochs: A quick way to set the ``num_epochs`` in the training kwargs.\n+    :param create_inverse_triples: Either a boolean for a single entry or a list of booleans.\n     :param regularizers: A regularizer name, list of regularizer names, or None if no regularizer is desired.\n-        Defaults to None.\n-\n+    :param negative_sampler: A negative sampler name, list of regularizer names, or None if no negative sampler\n+        is desired. Negative sampling is used only in combination with :class:`pykeen.training.SLCWATrainingLoop`.\n     :param evaluator: The name of the evaluator to be used. Defaults to rank-based evaluator.\n-    :param evaluator_kwargs: The keyword arguments passed to the evaluator (in the pipeline)\n-    :param evaluation_kwargs: The keyword arguments passed during evaluation (in the pipeline)\n-\n+    :param stopper: The name of the stopper to be used. Defaults to NopStopper which doesn't define a\n+        stopping criterion.\n     :param model_to_model_kwargs: A mapping from model name to dictionaries of default keyword arguments for\n-        the instantiation of that model\n+        the instantiation of that model.\n     :param model_to_model_kwargs_ranges: A mapping from model name to dictionaries of keyword argument\n         ranges for that model to be used in HPO.\n-    :param model_to_trainer_to_training_kwargs: A mapping from model name to a mapping of trainer name to a mapping\n-        of default keyword arguments for the instantiation of that trainer. This is useful becuase for some models,\n-        you might want to set the number of epochs differently.\n-    :param model_to_trainer_to_training_kwargs_ranges: A mapping from model name to a mapping of trainer name\n-        to a mapping of keyword arguments for that trainer to be used in HPO.\n-    :param ablation_config: Additional third-order and fourth-order ablation configuration for all other ablation\n-        keys to models to either kwargs or kwarg ranges\n-\n-    :param optuna_config: Configuration passed to optuna for HPO over all ablation studies\n-    :param directory: The directory in which the experimental artifacts will be saved.\n-    :param dry_run: Defines whether only the configurations for the single experiments should be created without\n-     running them.\n-    :param best_replicates: Defines how often the final model should be re-trained and evaluated based on the best\n-     hyper-parameters enabling to measure the variance in performance.\n+    :param model_to_loss_to_loss_kwargs: A mapping from model name to a mapping of loss name to a mapping\n+        of default keyword arguments for the instantiation of that loss function. This is useful because for some\n+        losses, have hyper-parameters such as :class:`pykeen.losses.MarginRankingLoss`.\n+    :param model_to_loss_to_loss_kwargs_ranges: A mapping from model name to a mapping of loss name\n+        to a mapping of keyword argument ranges for that loss to be used in HPO.\n+    :param model_to_optimizer_to_optimizer_kwargs: A mapping from model name to a mapping of optimizer name to a mapping\n+        of default keyword arguments for the instantiation of that optimizer. This is useful because the optimizers,\n+        have hyper-parameters such as the learning rate.\n+    :param model_to_optimizer_to_optimizer_kwargs_ranges: A mapping from model name to a mapping of optimizer name\n+        to a mapping of keyword argument ranges for that optimizer to be used in HPO.\n+    :param model_to_regularizer_to_regularizer_kwargs: A mapping from model name to a mapping of regularizer name to a\n+        mapping of default keyword arguments for the instantiation of that regularizer. This is useful because the\n+        optimizers, have hyper-parameters such as the regularization weight.\n+    :param model_to_regularizer_to_regularizer_kwargs_ranges: A mapping from model name to a mapping of regularizer name\n+        to a mapping of keyword argument ranges for that regularizer to be used in HPO.\n+    :param model_to_negative_sampler_to_negative_sampler_kwargs: A mapping from model name to a mapping of\n+        negative sampler name to a mapping of default keyword arguments for the instantiation of that negative sampler.\n+        This is useful because the negative samplers, have hyper-parameters such as the number of negatives that should\n+        get generated for each positive training example.\n+    :param model_to_negative_sampler_to_negative_sampler_kwargs_ranges: A mapping from model name to a mapping of\n+        negative sampler name to a mapping of keyword argument ranges for that negative sampler to be used in HPO.\n+    :param model_to_training_loop_to_training_loop_kwargs: A mapping from model name to a mapping of training loop name\n+        to a mapping of default keyword arguments for the training loop.\n+    :param model_to_training_loop_to_training_kwargs: A mapping from model name to a mapping of trainer name to a\n+        mapping of default keyword arguments for the training procedure. This is useful because you can set the\n+        hyper-parameters such as the number of training epochs and the batch size.\n+    :param model_to_training_loop_to_training_kwargs_ranges:  A mapping from model name to a mapping of\n+        trainer name to a mapping of keyword argument ranges for that trainer to be used in HPO.\n+    :param evaluator_kwargs: The keyword arguments passed to the evaluator.\n+    :param evaluation_kwargs: The keyword arguments passed during evaluation.\n+    :param stopper_kwargs: The keyword arguments passed to the stopper.\n+    :param n_trials: Number of HPO trials.\n+    :param timeout: The time (seconds) after which the ablation study will be terminated.\n+    :param metric: The metric to optimize during HPO.\n+    :param direction: Defines, whether to 'maximize' or 'minimize' the metric during HPO.\n+    :param sampler: The HPO sampler, it defaults to random search.\n+    :param pruner: Defines approach for pruning trials. Per default no pruning is used, i.e., pruner is\n+        set to 'Nopruner'.\n+    :param metadata: A mapping of meta data arguments such as name of the ablation study.\n     :param save_artifacts: Defines, whether each trained model sampled during HPO should be saved.\n     :param move_to_cpu: Defines, whether a replicate of the best model should be moved to CPU.\n-     We recommend to set this flag to 'True' to avoid unnecessary GPU usage.\n+    :param dry_run: Defines whether only the configurations for the single experiments should be created without\n+        running them.\n+    :param best_replicates: Defines how often the final model should be re-trained and evaluated based on the best\n+        hyper-parameters enabling to measure the variance in performance.\n     :param discard_replicates: Defines, whether the best model should be discarded after training and evaluation.\n+    :param create_unique_subdir: Defines, whether a unique sub-directory for the experimental artifacts should\n+        be created. The sub-directory name is defined  by the  current  data + a unique id.\n     \"\"\"\n-    datetime = time.strftime('%Y-%m-%d-%H-%M')\n-    directory = os.path.join(directory, f'{datetime}_{uuid4()}')\n+    if create_unique_subdir:\n+        directory = _create_path_with_id(directory=directory)\n \n",
        "source_code_with_indent": "    *,\n    create_inverse_triples: Union[bool, List[bool]] = False,\n    regularizers: Union[None, str, List[str]] = None,\n    model_to_model_kwargs: Optional[Mapping2D] = None,\n    model_to_model_kwargs_ranges: Optional[Mapping2D] = None,\n    model_to_trainer_to_training_kwargs: Optional[Mapping3D] = None,\n    model_to_trainer_to_training_kwargs_ranges: Optional[Mapping3D] = None,\n    ablation_config: Optional[Mapping3D] = None,\n    evaluator: Optional[str] = None,\n    optuna_config: Optional[Mapping[str, Any]] = None,\n    evaluator_kwargs: Optional[Mapping[str, Any]] = None,\n    evaluation_kwargs: Optional[Mapping[str, Any]] = None,\n    directory: Optional[str] = None,\n    dry_run: bool = False,\n    best_replicates: Optional[int] = None,\n    save_artifacts: bool = True,\n    move_to_cpu: bool = True,\n    discard_replicates: bool = False,\n) -> None:\n    <IND>\"\"\"Generate a set of HPO configurations.\n\n    A sample file can be run with``pykeen experiment ablation tests/resources/hpo_complex_nations.json``.\n\n    :param datasets: A dataset name or list of dataset names\n    :param models: A model name or list of model names\n    :param losses: A loss function name or list of loss function names\n    :param optimizers: An optimizer name or list of optimizer names\n    :param training_loops: A training loop name or list of training loop names\n    :param create_inverse_triples: Either a boolean for a single entry or a list of booleans\n    :param regularizers: A regularizer name, list of regularizer names, or None if no regularizer is desired.\n        Defaults to None.\n\n    :param evaluator: The name of the evaluator to be used. Defaults to rank-based evaluator.\n    :param evaluator_kwargs: The keyword arguments passed to the evaluator (in the pipeline)\n    :param evaluation_kwargs: The keyword arguments passed during evaluation (in the pipeline)\n\n    :param model_to_model_kwargs: A mapping from model name to dictionaries of default keyword arguments for\n        the instantiation of that model\n    :param model_to_model_kwargs_ranges: A mapping from model name to dictionaries of keyword argument\n        ranges for that model to be used in HPO.\n    :param model_to_trainer_to_training_kwargs: A mapping from model name to a mapping of trainer name to a mapping\n        of default keyword arguments for the instantiation of that trainer. This is useful becuase for some models,\n        you might want to set the number of epochs differently.\n    :param model_to_trainer_to_training_kwargs_ranges: A mapping from model name to a mapping of trainer name\n        to a mapping of keyword arguments for that trainer to be used in HPO.\n    :param ablation_config: Additional third-order and fourth-order ablation configuration for all other ablation\n        keys to models to either kwargs or kwarg ranges\n\n    :param optuna_config: Configuration passed to optuna for HPO over all ablation studies\n    :param directory: The directory in which the experimental artifacts will be saved.\n    :param dry_run: Defines whether only the configurations for the single experiments should be created without\n     running them.\n    :param best_replicates: Defines how often the final model should be re-trained and evaluated based on the best\n     hyper-parameters enabling to measure the variance in performance.\n    :param save_artifacts: Defines, whether each trained model sampled during HPO should be saved.\n    :param move_to_cpu: Defines, whether a replicate of the best model should be moved to CPU.\n     We recommend to set this flag to 'True' to avoid unnecessary GPU usage.\n    :param discard_replicates: Defines, whether the best model should be discarded after training and evaluation.\n    \"\"\"\n    datetime = time.strftime('%Y-%m-%d-%H-%M')\n    directory = os.path.join(directory, f'{datetime}_{uuid4()}')\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    *,\n    epochs: Optional[int] = None,\n    create_inverse_triples: Union[bool, List[bool]] = False,\n    regularizers: Union[None, str, List[str]] = None,\n    negative_sampler: Union[str, None] = None,\n    evaluator: Optional[str] = None,\n    stopper: Optional[str] = 'NopStopper',\n    model_to_model_kwargs: Optional[Mapping2D] = None,\n    model_to_model_kwargs_ranges: Optional[Mapping2D] = None,\n    model_to_loss_to_loss_kwargs: Optional[Mapping3D] = None,\n    model_to_loss_to_loss_kwargs_ranges: Optional[Mapping3D] = None,\n    model_to_optimizer_to_optimizer_kwargs: Optional[Mapping3D] = None,\n    model_to_optimizer_to_optimizer_kwargs_ranges: Optional[Mapping3D] = None,\n    model_to_negative_sampler_to_negative_sampler_kwargs: Optional[Mapping3D] = None,\n    model_to_negative_sampler_to_negative_sampler_kwargs_ranges: Optional[Mapping3D] = None,\n    model_to_training_loop_to_training_loop_kwargs: Optional[Mapping3D] = None,\n    model_to_training_loop_to_training_kwargs: Optional[Mapping3D] = None,\n    model_to_training_loop_to_training_kwargs_ranges: Optional[Mapping3D] = None,\n    model_to_regularizer_to_regularizer_kwargs: Optional[Mapping3D] = None,\n    model_to_regularizer_to_regularizer_kwargs_ranges: Optional[Mapping3D] = None,\n    evaluator_kwargs: Optional[Mapping[str, Any]] = None,\n    evaluation_kwargs: Optional[Mapping[str, Any]] = None,\n    stopper_kwargs: Optional[Mapping[str, Any]] = None,\n    n_trials: Optional[int] = 5,\n    timeout: Optional[int] = 3600,\n    metric: Optional[str] = 'hits@10',\n    direction: Optional[str] = 'maximize',\n    sampler: Optional[str] = 'random',\n    pruner: Optional[str] = 'nop',\n    metadata: Optional[Mapping] = None,\n    save_artifacts: bool = True,\n    move_to_cpu: bool = True,\n    dry_run: bool = False,\n    best_replicates: Optional[int] = None,\n    discard_replicates: bool = False,\n    create_unique_subdir: bool = False,\n):\n    <IND>\"\"\"Run ablation study.\n\n    :param datasets: A dataset name or list of dataset names.\n    :param directory: The directory in which the experimental artifacts will be saved.\n    :param models: A model name or list of model names.\n    :param losses: A loss function name or list of loss function names.\n    :param optimizers: An optimizer name or list of optimizer names.\n    :param training_loops: A training loop name or list of training loop names.\n    :param epochs: A quick way to set the ``num_epochs`` in the training kwargs.\n    :param create_inverse_triples: Either a boolean for a single entry or a list of booleans.\n    :param regularizers: A regularizer name, list of regularizer names, or None if no regularizer is desired.\n    :param negative_sampler: A negative sampler name, list of regularizer names, or None if no negative sampler\n        is desired. Negative sampling is used only in combination with :class:`pykeen.training.SLCWATrainingLoop`.\n    :param evaluator: The name of the evaluator to be used. Defaults to rank-based evaluator.\n    :param stopper: The name of the stopper to be used. Defaults to NopStopper which doesn't define a\n        stopping criterion.\n    :param model_to_model_kwargs: A mapping from model name to dictionaries of default keyword arguments for\n        the instantiation of that model.\n    :param model_to_model_kwargs_ranges: A mapping from model name to dictionaries of keyword argument\n        ranges for that model to be used in HPO.\n    :param model_to_loss_to_loss_kwargs: A mapping from model name to a mapping of loss name to a mapping\n        of default keyword arguments for the instantiation of that loss function. This is useful because for some\n        losses, have hyper-parameters such as :class:`pykeen.losses.MarginRankingLoss`.\n    :param model_to_loss_to_loss_kwargs_ranges: A mapping from model name to a mapping of loss name\n        to a mapping of keyword argument ranges for that loss to be used in HPO.\n    :param model_to_optimizer_to_optimizer_kwargs: A mapping from model name to a mapping of optimizer name to a mapping\n        of default keyword arguments for the instantiation of that optimizer. This is useful because the optimizers,\n        have hyper-parameters such as the learning rate.\n    :param model_to_optimizer_to_optimizer_kwargs_ranges: A mapping from model name to a mapping of optimizer name\n        to a mapping of keyword argument ranges for that optimizer to be used in HPO.\n    :param model_to_regularizer_to_regularizer_kwargs: A mapping from model name to a mapping of regularizer name to a\n        mapping of default keyword arguments for the instantiation of that regularizer. This is useful because the\n        optimizers, have hyper-parameters such as the regularization weight.\n    :param model_to_regularizer_to_regularizer_kwargs_ranges: A mapping from model name to a mapping of regularizer name\n        to a mapping of keyword argument ranges for that regularizer to be used in HPO.\n    :param model_to_negative_sampler_to_negative_sampler_kwargs: A mapping from model name to a mapping of\n        negative sampler name to a mapping of default keyword arguments for the instantiation of that negative sampler.\n        This is useful because the negative samplers, have hyper-parameters such as the number of negatives that should\n        get generated for each positive training example.\n    :param model_to_negative_sampler_to_negative_sampler_kwargs_ranges: A mapping from model name to a mapping of\n        negative sampler name to a mapping of keyword argument ranges for that negative sampler to be used in HPO.\n    :param model_to_training_loop_to_training_loop_kwargs: A mapping from model name to a mapping of training loop name\n        to a mapping of default keyword arguments for the training loop.\n    :param model_to_training_loop_to_training_kwargs: A mapping from model name to a mapping of trainer name to a\n        mapping of default keyword arguments for the training procedure. This is useful because you can set the\n        hyper-parameters such as the number of training epochs and the batch size.\n    :param model_to_training_loop_to_training_kwargs_ranges:  A mapping from model name to a mapping of\n        trainer name to a mapping of keyword argument ranges for that trainer to be used in HPO.\n    :param evaluator_kwargs: The keyword arguments passed to the evaluator.\n    :param evaluation_kwargs: The keyword arguments passed during evaluation.\n    :param stopper_kwargs: The keyword arguments passed to the stopper.\n    :param n_trials: Number of HPO trials.\n    :param timeout: The time (seconds) after which the ablation study will be terminated.\n    :param metric: The metric to optimize during HPO.\n    :param direction: Defines, whether to 'maximize' or 'minimize' the metric during HPO.\n    :param sampler: The HPO sampler, it defaults to random search.\n    :param pruner: Defines approach for pruning trials. Per default no pruning is used, i.e., pruner is\n        set to 'Nopruner'.\n    :param metadata: A mapping of meta data arguments such as name of the ablation study.\n    :param save_artifacts: Defines, whether each trained model sampled during HPO should be saved.\n    :param move_to_cpu: Defines, whether a replicate of the best model should be moved to CPU.\n    :param dry_run: Defines whether only the configurations for the single experiments should be created without\n        running them.\n    :param best_replicates: Defines how often the final model should be re-trained and evaluated based on the best\n        hyper-parameters enabling to measure the variance in performance.\n    :param discard_replicates: Defines, whether the best model should be discarded after training and evaluation.\n    :param create_unique_subdir: Defines, whether a unique sub-directory for the experimental artifacts should\n        be created. The sub-directory name is defined  by the  current  data + a unique id.\n    \"\"\"\n    if create_unique_subdir:\n        <IND>directory = _create_path_with_id(directory=directory)\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        datasets=datasets,\n        create_inverse_triples=create_inverse_triples,\n        models=models,\n        model_to_model_kwargs=model_to_model_kwargs,\n        model_to_model_kwargs_ranges=model_to_model_kwargs_ranges,\n        model_to_trainer_to_training_kwargs=model_to_trainer_to_training_kwargs,\n        model_to_trainer_to_training_kwargs_ranges=model_to_trainer_to_training_kwargs_ranges,\n        losses=losses,\n        regularizers=regularizers,\n        optimizers=optimizers,\n        training_loops=training_loops,\n        evaluator=evaluator,\n        optuna_config=optuna_config,\n        ablation_config=ablation_config,\n        evaluator_kwargs=evaluator_kwargs,\n        evaluation_kwargs=evaluation_kwargs,\n        directory=directory,\n",
        "source_code_len": 753,
        "target_code": "        datasets=datasets,\n        models=models,\n        losses=losses,\n        optimizers=optimizers,\n        training_loops=training_loops,\n        epochs=epochs,\n        create_inverse_triples=create_inverse_triples,\n        regularizers=regularizers,\n        model_to_model_kwargs=model_to_model_kwargs,\n        model_to_model_kwargs_ranges=model_to_model_kwargs_ranges,\n        model_to_loss_to_loss_kwargs=model_to_loss_to_loss_kwargs,\n        model_to_loss_to_loss_kwargs_ranges=model_to_loss_to_loss_kwargs_ranges,\n        model_to_optimizer_to_optimizer_kwargs=model_to_optimizer_to_optimizer_kwargs,\n        model_to_optimizer_to_optimizer_kwargs_ranges=model_to_optimizer_to_optimizer_kwargs_ranges,\n        negative_sampler=negative_sampler,\n        model_to_neg_sampler_to_neg_sampler_kwargs=model_to_negative_sampler_to_negative_sampler_kwargs,\n        model_to_neg_sampler_to_neg_sampler_kwargs_ranges=model_to_negative_sampler_to_negative_sampler_kwargs_ranges,\n        model_to_training_loop_to_training_loop_kwargs=model_to_training_loop_to_training_loop_kwargs,\n        model_to_training_loop_to_training_kwargs=model_to_training_loop_to_training_kwargs,\n        model_to_training_loop_to_training_kwargs_ranges=model_to_training_loop_to_training_kwargs_ranges,\n        model_to_regularizer_to_regularizer_kwargs=model_to_regularizer_to_regularizer_kwargs,\n        model_to_regularizer_to_regularizer_kwargs_ranges=model_to_regularizer_to_regularizer_kwargs_ranges,\n        evaluator=evaluator,\n        n_trials=n_trials,\n        timeout=timeout,\n        metric=metric,\n        direction=direction,\n        sampler=sampler,\n        pruner=pruner,\n        evaluator_kwargs=evaluator_kwargs,\n        evaluation_kwargs=evaluation_kwargs,\n        stopper=stopper,\n        stopper_kwargs=stopper_kwargs,\n        metadata=metadata,\n        directory=directory,\n",
        "target_code_len": 1875,
        "diff_format": "@@ -100,17 +147,35 @@\n         datasets=datasets,\n+        models=models,\n+        losses=losses,\n+        optimizers=optimizers,\n+        training_loops=training_loops,\n+        epochs=epochs,\n         create_inverse_triples=create_inverse_triples,\n-        models=models,\n+        regularizers=regularizers,\n         model_to_model_kwargs=model_to_model_kwargs,\n         model_to_model_kwargs_ranges=model_to_model_kwargs_ranges,\n-        model_to_trainer_to_training_kwargs=model_to_trainer_to_training_kwargs,\n-        model_to_trainer_to_training_kwargs_ranges=model_to_trainer_to_training_kwargs_ranges,\n-        losses=losses,\n-        regularizers=regularizers,\n-        optimizers=optimizers,\n-        training_loops=training_loops,\n+        model_to_loss_to_loss_kwargs=model_to_loss_to_loss_kwargs,\n+        model_to_loss_to_loss_kwargs_ranges=model_to_loss_to_loss_kwargs_ranges,\n+        model_to_optimizer_to_optimizer_kwargs=model_to_optimizer_to_optimizer_kwargs,\n+        model_to_optimizer_to_optimizer_kwargs_ranges=model_to_optimizer_to_optimizer_kwargs_ranges,\n+        negative_sampler=negative_sampler,\n+        model_to_neg_sampler_to_neg_sampler_kwargs=model_to_negative_sampler_to_negative_sampler_kwargs,\n+        model_to_neg_sampler_to_neg_sampler_kwargs_ranges=model_to_negative_sampler_to_negative_sampler_kwargs_ranges,\n+        model_to_training_loop_to_training_loop_kwargs=model_to_training_loop_to_training_loop_kwargs,\n+        model_to_training_loop_to_training_kwargs=model_to_training_loop_to_training_kwargs,\n+        model_to_training_loop_to_training_kwargs_ranges=model_to_training_loop_to_training_kwargs_ranges,\n+        model_to_regularizer_to_regularizer_kwargs=model_to_regularizer_to_regularizer_kwargs,\n+        model_to_regularizer_to_regularizer_kwargs_ranges=model_to_regularizer_to_regularizer_kwargs_ranges,\n         evaluator=evaluator,\n-        optuna_config=optuna_config,\n-        ablation_config=ablation_config,\n+        n_trials=n_trials,\n+        timeout=timeout,\n+        metric=metric,\n+        direction=direction,\n+        sampler=sampler,\n+        pruner=pruner,\n         evaluator_kwargs=evaluator_kwargs,\n         evaluation_kwargs=evaluation_kwargs,\n+        stopper=stopper,\n+        stopper_kwargs=stopper_kwargs,\n+        metadata=metadata,\n         directory=directory,\n",
        "source_code_with_indent": "        datasets=datasets,\n        create_inverse_triples=create_inverse_triples,\n        models=models,\n        model_to_model_kwargs=model_to_model_kwargs,\n        model_to_model_kwargs_ranges=model_to_model_kwargs_ranges,\n        model_to_trainer_to_training_kwargs=model_to_trainer_to_training_kwargs,\n        model_to_trainer_to_training_kwargs_ranges=model_to_trainer_to_training_kwargs_ranges,\n        losses=losses,\n        regularizers=regularizers,\n        optimizers=optimizers,\n        training_loops=training_loops,\n        evaluator=evaluator,\n        optuna_config=optuna_config,\n        ablation_config=ablation_config,\n        evaluator_kwargs=evaluator_kwargs,\n        evaluation_kwargs=evaluation_kwargs,\n        directory=directory,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        datasets=datasets,\n        models=models,\n        losses=losses,\n        optimizers=optimizers,\n        training_loops=training_loops,\n        epochs=epochs,\n        create_inverse_triples=create_inverse_triples,\n        regularizers=regularizers,\n        model_to_model_kwargs=model_to_model_kwargs,\n        model_to_model_kwargs_ranges=model_to_model_kwargs_ranges,\n        model_to_loss_to_loss_kwargs=model_to_loss_to_loss_kwargs,\n        model_to_loss_to_loss_kwargs_ranges=model_to_loss_to_loss_kwargs_ranges,\n        model_to_optimizer_to_optimizer_kwargs=model_to_optimizer_to_optimizer_kwargs,\n        model_to_optimizer_to_optimizer_kwargs_ranges=model_to_optimizer_to_optimizer_kwargs_ranges,\n        negative_sampler=negative_sampler,\n        model_to_neg_sampler_to_neg_sampler_kwargs=model_to_negative_sampler_to_negative_sampler_kwargs,\n        model_to_neg_sampler_to_neg_sampler_kwargs_ranges=model_to_negative_sampler_to_negative_sampler_kwargs_ranges,\n        model_to_training_loop_to_training_loop_kwargs=model_to_training_loop_to_training_loop_kwargs,\n        model_to_training_loop_to_training_kwargs=model_to_training_loop_to_training_kwargs,\n        model_to_training_loop_to_training_kwargs_ranges=model_to_training_loop_to_training_kwargs_ranges,\n        model_to_regularizer_to_regularizer_kwargs=model_to_regularizer_to_regularizer_kwargs,\n        model_to_regularizer_to_regularizer_kwargs_ranges=model_to_regularizer_to_regularizer_kwargs_ranges,\n        evaluator=evaluator,\n        n_trials=n_trials,\n        timeout=timeout,\n        metric=metric,\n        direction=direction,\n        sampler=sampler,\n        pruner=pruner,\n        evaluator_kwargs=evaluator_kwargs,\n        evaluation_kwargs=evaluation_kwargs,\n        stopper=stopper,\n        stopper_kwargs=stopper_kwargs,\n        metadata=metadata,\n        directory=directory,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\ndef ablation_pipeline_from_config(\n    config: Mapping[str, Any],\n    *,\n    directory: Optional[str] = None,\n    dry_run: bool = False,\n",
        "source_code_len": 138,
        "target_code": "\ndef _create_path_with_id(directory: str) -> str:\n    \"\"\"Add unique id to path.\"\"\"\n    datetime = time.strftime('%Y-%m-%d-%H-%M')\n    return os.path.join(directory, f'{datetime}_{uuid4()}')\n\n\ndef ablation_pipeline_from_config(\n    config: Mapping[str, Any],\n    directory: str,\n    *,\n    dry_run: bool = False,\n",
        "target_code_len": 312,
        "diff_format": "@@ -141,6 +224,12 @@\n \n+def _create_path_with_id(directory: str) -> str:\n+    \"\"\"Add unique id to path.\"\"\"\n+    datetime = time.strftime('%Y-%m-%d-%H-%M')\n+    return os.path.join(directory, f'{datetime}_{uuid4()}')\n+\n+\n def ablation_pipeline_from_config(\n     config: Mapping[str, Any],\n+    directory: str,\n     *,\n-    directory: Optional[str] = None,\n     dry_run: bool = False,\n",
        "source_code_with_indent": "\n<DED><DED>def ablation_pipeline_from_config(\n    config: Mapping[str, Any],\n    *,\n    directory: Optional[str] = None,\n    dry_run: bool = False,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n<DED><DED>def _create_path_with_id(directory: str) -> str:\n    <IND>\"\"\"Add unique id to path.\"\"\"\n    datetime = time.strftime('%Y-%m-%d-%H-%M')\n    return os.path.join(directory, f'{datetime}_{uuid4()}')\n\n\n<DED>def ablation_pipeline_from_config(\n    config: Mapping[str, Any],\n    directory: str,\n    *,\n    dry_run: bool = False,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    evaluator = ablation_config['evaluator']\n    evaluator_kwargs = ablation_config['evaluator_kwargs']\n    evaluation_kwargs = ablation_config['evaluation_kwargs']\n\n    datasets = ablation_config['datasets']\n    create_inverse_triples = ablation_config['create_inverse_triples']\n    models = ablation_config['models']\n    losses = ablation_config['loss_functions'] if 'loss_functions' in ablation_config else ablation_config['losses']\n    regularizers = ablation_config['regularizers']\n    optimizers = ablation_config['optimizers']\n    training_loops = ablation_config['training_loops']\n    return prepare_ablation(\n        datasets=datasets,\n        create_inverse_triples=create_inverse_triples,\n        models=models,\n        losses=losses,\n        regularizers=regularizers,\n        optimizers=optimizers,\n        training_loops=training_loops,\n        evaluator=evaluator,\n        optuna_config=optuna_config,\n        ablation_config=ablation_config,\n        evaluator_kwargs=evaluator_kwargs,\n        evaluation_kwargs=evaluation_kwargs,\n        metadata=metadata,\n",
        "source_code_len": 1074,
        "target_code": "\n    return prepare_ablation(\n        **ablation_config,\n        **optuna_config,\n        metadata=metadata,\n",
        "target_code_len": 109,
        "diff_format": "@@ -210,26 +302,5 @@\n \n-    evaluator = ablation_config['evaluator']\n-    evaluator_kwargs = ablation_config['evaluator_kwargs']\n-    evaluation_kwargs = ablation_config['evaluation_kwargs']\n-\n-    datasets = ablation_config['datasets']\n-    create_inverse_triples = ablation_config['create_inverse_triples']\n-    models = ablation_config['models']\n-    losses = ablation_config['loss_functions'] if 'loss_functions' in ablation_config else ablation_config['losses']\n-    regularizers = ablation_config['regularizers']\n-    optimizers = ablation_config['optimizers']\n-    training_loops = ablation_config['training_loops']\n     return prepare_ablation(\n-        datasets=datasets,\n-        create_inverse_triples=create_inverse_triples,\n-        models=models,\n-        losses=losses,\n-        regularizers=regularizers,\n-        optimizers=optimizers,\n-        training_loops=training_loops,\n-        evaluator=evaluator,\n-        optuna_config=optuna_config,\n-        ablation_config=ablation_config,\n-        evaluator_kwargs=evaluator_kwargs,\n-        evaluation_kwargs=evaluation_kwargs,\n+        **ablation_config,\n+        **optuna_config,\n         metadata=metadata,\n",
        "source_code_with_indent": "\n    evaluator = ablation_config['evaluator']\n    evaluator_kwargs = ablation_config['evaluator_kwargs']\n    evaluation_kwargs = ablation_config['evaluation_kwargs']\n\n    datasets = ablation_config['datasets']\n    create_inverse_triples = ablation_config['create_inverse_triples']\n    models = ablation_config['models']\n    losses = ablation_config['loss_functions'] if 'loss_functions' in ablation_config else ablation_config['losses']\n    regularizers = ablation_config['regularizers']\n    optimizers = ablation_config['optimizers']\n    training_loops = ablation_config['training_loops']\n    return prepare_ablation(\n        datasets=datasets,\n        create_inverse_triples=create_inverse_triples,\n        models=models,\n        losses=losses,\n        regularizers=regularizers,\n        optimizers=optimizers,\n        training_loops=training_loops,\n        evaluator=evaluator,\n        optuna_config=optuna_config,\n        ablation_config=ablation_config,\n        evaluator_kwargs=evaluator_kwargs,\n        evaluation_kwargs=evaluation_kwargs,\n        metadata=metadata,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    return prepare_ablation(\n        **ablation_config,\n        **optuna_config,\n        metadata=metadata,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    training_loops: Union[str, List[str]],\n    *,\n    ablation_config: Optional[Mapping3D] = None,\n    create_inverse_triples: Union[bool, List[bool]] = False,\n    regularizers: Union[None, str, List[str]] = None,\n    model_to_model_kwargs: Optional[Mapping2D] = None,\n    model_to_model_kwargs_ranges: Optional[Mapping2D] = None,\n    model_to_trainer_to_training_kwargs: Optional[Mapping3D] = None,\n    model_to_trainer_to_training_kwargs_ranges: Optional[Mapping3D] = None,\n    evaluator: Optional[str] = None,\n    optuna_config: Optional[Mapping[str, Any]] = None,\n    evaluator_kwargs: Optional[Mapping[str, Any]] = None,\n    evaluation_kwargs: Optional[Mapping[str, Any]] = None,\n    metadata=None,\n    directory: Optional[str] = None,\n    save_artifacts: bool = True,\n",
        "source_code_len": 774,
        "target_code": "    training_loops: Union[str, List[str]],\n    directory: str,\n    *,\n    epochs: Optional[int] = None,\n    create_inverse_triples: Union[bool, List[bool]] = False,\n    regularizers: Union[None, str, List[str]] = None,\n    negative_sampler: Optional[str] = None,\n    evaluator: Optional[str] = None,\n    model_to_model_kwargs: Optional[Mapping2D] = None,\n    model_to_model_kwargs_ranges: Optional[Mapping2D] = None,\n    model_to_loss_to_loss_kwargs: Optional[Mapping3D] = None,\n    model_to_loss_to_loss_kwargs_ranges: Optional[Mapping3D] = None,\n    model_to_optimizer_to_optimizer_kwargs: Optional[Mapping3D] = None,\n    model_to_optimizer_to_optimizer_kwargs_ranges: Optional[Mapping3D] = None,\n    model_to_training_loop_to_training_loop_kwargs: Optional[Mapping3D] = None,\n    model_to_neg_sampler_to_neg_sampler_kwargs: Optional[Mapping3D] = None,\n    model_to_neg_sampler_to_neg_sampler_kwargs_ranges: Optional[Mapping3D] = None,\n    model_to_training_loop_to_training_kwargs: Optional[Mapping3D] = None,\n    model_to_training_loop_to_training_kwargs_ranges: Optional[Mapping3D] = None,\n    model_to_regularizer_to_regularizer_kwargs: Optional[Mapping3D] = None,\n    model_to_regularizer_to_regularizer_kwargs_ranges: Optional[Mapping3D] = None,\n    n_trials: Optional[int] = 5,\n    timeout: Optional[int] = 3600,\n    metric: Optional[str] = 'hits@10',\n    direction: Optional[str] = 'maximize',\n    sampler: Optional[str] = 'random',\n    pruner: Optional[str] = 'nop',\n    evaluator_kwargs: Optional[Mapping[str, Any]] = None,\n    evaluation_kwargs: Optional[Mapping[str, Any]] = None,\n    stopper: Optional[str] = 'NopStopper',\n    stopper_kwargs: Optional[Mapping[str, Any]] = None,\n    metadata: Optional[Mapping] = None,\n    save_artifacts: bool = True,\n",
        "target_code_len": 1767,
        "diff_format": "@@ -246,16 +317,33 @@\n     training_loops: Union[str, List[str]],\n+    directory: str,\n     *,\n-    ablation_config: Optional[Mapping3D] = None,\n+    epochs: Optional[int] = None,\n     create_inverse_triples: Union[bool, List[bool]] = False,\n     regularizers: Union[None, str, List[str]] = None,\n+    negative_sampler: Optional[str] = None,\n+    evaluator: Optional[str] = None,\n     model_to_model_kwargs: Optional[Mapping2D] = None,\n     model_to_model_kwargs_ranges: Optional[Mapping2D] = None,\n-    model_to_trainer_to_training_kwargs: Optional[Mapping3D] = None,\n-    model_to_trainer_to_training_kwargs_ranges: Optional[Mapping3D] = None,\n-    evaluator: Optional[str] = None,\n-    optuna_config: Optional[Mapping[str, Any]] = None,\n+    model_to_loss_to_loss_kwargs: Optional[Mapping3D] = None,\n+    model_to_loss_to_loss_kwargs_ranges: Optional[Mapping3D] = None,\n+    model_to_optimizer_to_optimizer_kwargs: Optional[Mapping3D] = None,\n+    model_to_optimizer_to_optimizer_kwargs_ranges: Optional[Mapping3D] = None,\n+    model_to_training_loop_to_training_loop_kwargs: Optional[Mapping3D] = None,\n+    model_to_neg_sampler_to_neg_sampler_kwargs: Optional[Mapping3D] = None,\n+    model_to_neg_sampler_to_neg_sampler_kwargs_ranges: Optional[Mapping3D] = None,\n+    model_to_training_loop_to_training_kwargs: Optional[Mapping3D] = None,\n+    model_to_training_loop_to_training_kwargs_ranges: Optional[Mapping3D] = None,\n+    model_to_regularizer_to_regularizer_kwargs: Optional[Mapping3D] = None,\n+    model_to_regularizer_to_regularizer_kwargs_ranges: Optional[Mapping3D] = None,\n+    n_trials: Optional[int] = 5,\n+    timeout: Optional[int] = 3600,\n+    metric: Optional[str] = 'hits@10',\n+    direction: Optional[str] = 'maximize',\n+    sampler: Optional[str] = 'random',\n+    pruner: Optional[str] = 'nop',\n     evaluator_kwargs: Optional[Mapping[str, Any]] = None,\n     evaluation_kwargs: Optional[Mapping[str, Any]] = None,\n-    metadata=None,\n-    directory: Optional[str] = None,\n+    stopper: Optional[str] = 'NopStopper',\n+    stopper_kwargs: Optional[Mapping[str, Any]] = None,\n+    metadata: Optional[Mapping] = None,\n     save_artifacts: bool = True,\n",
        "source_code_with_indent": "    training_loops: Union[str, List[str]],\n    *,\n    ablation_config: Optional[Mapping3D] = None,\n    create_inverse_triples: Union[bool, List[bool]] = False,\n    regularizers: Union[None, str, List[str]] = None,\n    model_to_model_kwargs: Optional[Mapping2D] = None,\n    model_to_model_kwargs_ranges: Optional[Mapping2D] = None,\n    model_to_trainer_to_training_kwargs: Optional[Mapping3D] = None,\n    model_to_trainer_to_training_kwargs_ranges: Optional[Mapping3D] = None,\n    evaluator: Optional[str] = None,\n    optuna_config: Optional[Mapping[str, Any]] = None,\n    evaluator_kwargs: Optional[Mapping[str, Any]] = None,\n    evaluation_kwargs: Optional[Mapping[str, Any]] = None,\n    metadata=None,\n    directory: Optional[str] = None,\n    save_artifacts: bool = True,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    training_loops: Union[str, List[str]],\n    directory: str,\n    *,\n    epochs: Optional[int] = None,\n    create_inverse_triples: Union[bool, List[bool]] = False,\n    regularizers: Union[None, str, List[str]] = None,\n    negative_sampler: Optional[str] = None,\n    evaluator: Optional[str] = None,\n    model_to_model_kwargs: Optional[Mapping2D] = None,\n    model_to_model_kwargs_ranges: Optional[Mapping2D] = None,\n    model_to_loss_to_loss_kwargs: Optional[Mapping3D] = None,\n    model_to_loss_to_loss_kwargs_ranges: Optional[Mapping3D] = None,\n    model_to_optimizer_to_optimizer_kwargs: Optional[Mapping3D] = None,\n    model_to_optimizer_to_optimizer_kwargs_ranges: Optional[Mapping3D] = None,\n    model_to_training_loop_to_training_loop_kwargs: Optional[Mapping3D] = None,\n    model_to_neg_sampler_to_neg_sampler_kwargs: Optional[Mapping3D] = None,\n    model_to_neg_sampler_to_neg_sampler_kwargs_ranges: Optional[Mapping3D] = None,\n    model_to_training_loop_to_training_kwargs: Optional[Mapping3D] = None,\n    model_to_training_loop_to_training_kwargs_ranges: Optional[Mapping3D] = None,\n    model_to_regularizer_to_regularizer_kwargs: Optional[Mapping3D] = None,\n    model_to_regularizer_to_regularizer_kwargs_ranges: Optional[Mapping3D] = None,\n    n_trials: Optional[int] = 5,\n    timeout: Optional[int] = 3600,\n    metric: Optional[str] = 'hits@10',\n    direction: Optional[str] = 'maximize',\n    sampler: Optional[str] = 'random',\n    pruner: Optional[str] = 'nop',\n    evaluator_kwargs: Optional[Mapping[str, Any]] = None,\n    evaluation_kwargs: Optional[Mapping[str, Any]] = None,\n    stopper: Optional[str] = 'NopStopper',\n    stopper_kwargs: Optional[Mapping[str, Any]] = None,\n    metadata: Optional[Mapping] = None,\n    save_artifacts: bool = True,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        model_to_model_kwargs_ranges = {}\n    if not model_to_trainer_to_training_kwargs:\n        model_to_trainer_to_training_kwargs = {}\n    if not model_to_trainer_to_training_kwargs_ranges:\n        model_to_trainer_to_training_kwargs_ranges = {}\n    if not ablation_config:\n        ablation_config = {}\n\n",
        "source_code_len": 308,
        "target_code": "        model_to_model_kwargs_ranges = {}\n\n",
        "target_code_len": 43,
        "diff_format": "@@ -295,8 +447,2 @@\n         model_to_model_kwargs_ranges = {}\n-    if not model_to_trainer_to_training_kwargs:\n-        model_to_trainer_to_training_kwargs = {}\n-    if not model_to_trainer_to_training_kwargs_ranges:\n-        model_to_trainer_to_training_kwargs_ranges = {}\n-    if not ablation_config:\n-        ablation_config = {}\n \n",
        "source_code_with_indent": "        <IND>model_to_model_kwargs_ranges = {}\n    <DED>if not model_to_trainer_to_training_kwargs:\n        <IND>model_to_trainer_to_training_kwargs = {}\n    <DED>if not model_to_trainer_to_training_kwargs_ranges:\n        <IND>model_to_trainer_to_training_kwargs_ranges = {}\n    <DED>if not ablation_config:\n        <IND>ablation_config = {}\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <IND>model_to_model_kwargs_ranges = {}\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        _experiment_optuna_config = optuna_config.copy() if optuna_config else {}\n        _experiment_optuna_config['storage'] = f'sqlite:///{output_directory}/optuna_results.db'\n",
        "source_code_len": 180,
        "target_code": "\n        _experiment_optuna_config = {\n            'n_trials': n_trials,\n            'timeout': timeout,\n            'metric': metric,\n            'direction': direction,\n            'sampler': sampler,\n            'pruner': pruner,\n        }\n        _experiment_optuna_config['storage'] = f'sqlite:///{output_directory}/optuna_results.db'\n",
        "target_code_len": 340,
        "diff_format": "@@ -318,3 +464,10 @@\n \n-        _experiment_optuna_config = optuna_config.copy() if optuna_config else {}\n+        _experiment_optuna_config = {\n+            'n_trials': n_trials,\n+            'timeout': timeout,\n+            'metric': metric,\n+            'direction': direction,\n+            'sampler': sampler,\n+            'pruner': pruner,\n+        }\n         _experiment_optuna_config['storage'] = f'sqlite:///{output_directory}/optuna_results.db'\n",
        "source_code_with_indent": "\n        _experiment_optuna_config = optuna_config.copy() if optuna_config else {}\n        _experiment_optuna_config['storage'] = f'sqlite:///{output_directory}/optuna_results.db'\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        _experiment_optuna_config = {\n            'n_trials': n_trials,\n            'timeout': timeout,\n            'metric': metric,\n            'direction': direction,\n            'sampler': sampler,\n            'pruner': pruner,\n        }\n        _experiment_optuna_config['storage'] = f'sqlite:///{output_directory}/optuna_results.db'\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        hpo_config = dict()\n        for retain_key in ('stopper', 'stopper_kwargs'):\n            if retain_key in ablation_config:\n                logger.info(f'Retaining {retain_key} configuration in HPO')\n                hpo_config[retain_key] = deepcopy(ablation_config[retain_key])\n\n        for error_key in ('early_stopping', 'early_stopping_kwargs'):\n            if error_key in ablation_config:\n                raise ValueError(f'Outdated key: {error_key}. Please update')\n\n",
        "source_code_len": 482,
        "target_code": "\n        hpo_config: Dict[str, Any] = dict()\n        hpo_config['stopper'] = stopper\n\n        if stopper_kwargs is not None:\n            hpo_config['stopper_kwargs'] = stopper_kwargs\n\n",
        "target_code_len": 184,
        "diff_format": "@@ -325,11 +478,7 @@\n \n-        hpo_config = dict()\n-        for retain_key in ('stopper', 'stopper_kwargs'):\n-            if retain_key in ablation_config:\n-                logger.info(f'Retaining {retain_key} configuration in HPO')\n-                hpo_config[retain_key] = deepcopy(ablation_config[retain_key])\n-\n-        for error_key in ('early_stopping', 'early_stopping_kwargs'):\n-            if error_key in ablation_config:\n-                raise ValueError(f'Outdated key: {error_key}. Please update')\n+        hpo_config: Dict[str, Any] = dict()\n+        hpo_config['stopper'] = stopper\n+\n+        if stopper_kwargs is not None:\n+            hpo_config['stopper_kwargs'] = stopper_kwargs\n \n",
        "source_code_with_indent": "\n        <DED>hpo_config = dict()\n        for retain_key in ('stopper', 'stopper_kwargs'):\n            <IND>if retain_key in ablation_config:\n                <IND>logger.info(f'Retaining {retain_key} configuration in HPO')\n                hpo_config[retain_key] = deepcopy(ablation_config[retain_key])\n\n        <DED><DED>for error_key in ('early_stopping', 'early_stopping_kwargs'):\n            <IND>if error_key in ablation_config:\n                <IND>raise ValueError(f'Outdated key: {error_key}. Please update')\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        <DED>hpo_config: Dict[str, Any] = dict()\n        hpo_config['stopper'] = stopper\n\n        if stopper_kwargs is not None:\n            <IND>hpo_config['stopper_kwargs'] = stopper_kwargs\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        def _set_arguments(key: str, value: str) -> None:\n            \"\"\"Set argument and its values.\"\"\"\n            d = {key: value}\n            kwargs = ablation_config.get(f'{key}_kwargs', {}).get(model, {}).get(value, {})\n            if kwargs:\n                d[f'{key}_kwargs'] = kwargs\n            kwargs_ranges = ablation_config.get(f'{key}_kwargs_ranges', {}).get(model, {}).get(value, {})\n            if kwargs_ranges:\n                d[f'{key}_kwargs_ranges'] = kwargs_ranges\n\n            hpo_config.update(d)\n\n",
        "source_code_len": 523,
        "target_code": "\n        def _set_arguments(config: Optional[Mapping3D], key: str, value: str) -> None:\n            \"\"\"Set argument and its values.\"\"\"\n            d = {}\n            d[key] = {} if config is None else config.get(model, {}).get(value, {})\n            if d[key]:\n                hpo_config.update(d)\n\n",
        "target_code_len": 299,
        "diff_format": "@@ -340,13 +489,8 @@\n \n-        def _set_arguments(key: str, value: str) -> None:\n+        def _set_arguments(config: Optional[Mapping3D], key: str, value: str) -> None:\n             \"\"\"Set argument and its values.\"\"\"\n-            d = {key: value}\n-            kwargs = ablation_config.get(f'{key}_kwargs', {}).get(model, {}).get(value, {})\n-            if kwargs:\n-                d[f'{key}_kwargs'] = kwargs\n-            kwargs_ranges = ablation_config.get(f'{key}_kwargs_ranges', {}).get(model, {}).get(value, {})\n-            if kwargs_ranges:\n-                d[f'{key}_kwargs_ranges'] = kwargs_ranges\n-\n-            hpo_config.update(d)\n+            d = {}\n+            d[key] = {} if config is None else config.get(model, {}).get(value, {})\n+            if d[key]:\n+                hpo_config.update(d)\n \n",
        "source_code_with_indent": "\n        <DED><DED>def _set_arguments(key: str, value: str) -> None:\n            <IND>\"\"\"Set argument and its values.\"\"\"\n            d = {key: value}\n            kwargs = ablation_config.get(f'{key}_kwargs', {}).get(model, {}).get(value, {})\n            if kwargs:\n                <IND>d[f'{key}_kwargs'] = kwargs\n            <DED>kwargs_ranges = ablation_config.get(f'{key}_kwargs_ranges', {}).get(model, {}).get(value, {})\n            if kwargs_ranges:\n                <IND>d[f'{key}_kwargs_ranges'] = kwargs_ranges\n\n            <DED>hpo_config.update(d)\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        <DED>def _set_arguments(config: Optional[Mapping3D], key: str, value: str) -> None:\n            <IND>\"\"\"Set argument and its values.\"\"\"\n            d = {}\n            d[key] = {} if config is None else config.get(model, {}).get(value, {})\n            if d[key]:\n                <IND>hpo_config.update(d)\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        # Add loss function to current_pipeline\n        _set_arguments(key='loss', value=loss)\n        logger.info(f\"Loss function: {loss}\")\n\n        # Add regularizer to current_pipeline\n        _set_arguments(key='regularizer', value=regularizer)\n        logger.info(f\"Regularizer: {regularizer}\")\n",
        "source_code_len": 300,
        "target_code": "        # Add loss function to current_pipeline\n        hpo_config['loss'] = loss\n        _set_arguments(config=model_to_loss_to_loss_kwargs, key='loss_kwargs', value=loss)\n        _set_arguments(config=model_to_loss_to_loss_kwargs_ranges, key='loss_kwargs_ranges', value=loss)\n        logger.info(f\"Loss functions: {loss}\")\n\n        # Add regularizer to current_pipeline\n        hpo_config['regularizer'] = regularizer\n        _set_arguments(config=model_to_regularizer_to_regularizer_kwargs, key='regularizer_kwargs', value=regularizer)\n        _set_arguments(\n            config=model_to_regularizer_to_regularizer_kwargs_ranges,\n            key='regularizer_kwargs_ranges',\n            value=regularizer,\n        )\n        logger.info(f\"Regularizer: {regularizer}\")\n",
        "target_code_len": 770,
        "diff_format": "@@ -373,7 +519,15 @@\n         # Add loss function to current_pipeline\n-        _set_arguments(key='loss', value=loss)\n-        logger.info(f\"Loss function: {loss}\")\n+        hpo_config['loss'] = loss\n+        _set_arguments(config=model_to_loss_to_loss_kwargs, key='loss_kwargs', value=loss)\n+        _set_arguments(config=model_to_loss_to_loss_kwargs_ranges, key='loss_kwargs_ranges', value=loss)\n+        logger.info(f\"Loss functions: {loss}\")\n \n         # Add regularizer to current_pipeline\n-        _set_arguments(key='regularizer', value=regularizer)\n+        hpo_config['regularizer'] = regularizer\n+        _set_arguments(config=model_to_regularizer_to_regularizer_kwargs, key='regularizer_kwargs', value=regularizer)\n+        _set_arguments(\n+            config=model_to_regularizer_to_regularizer_kwargs_ranges,\n+            key='regularizer_kwargs_ranges',\n+            value=regularizer,\n+        )\n         logger.info(f\"Regularizer: {regularizer}\")\n",
        "source_code_with_indent": "        # Add loss function to current_pipeline\n        _set_arguments(key='loss', value=loss)\n        logger.info(f\"Loss function: {loss}\")\n\n        # Add regularizer to current_pipeline\n        _set_arguments(key='regularizer', value=regularizer)\n        logger.info(f\"Regularizer: {regularizer}\")\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        # Add loss function to current_pipeline\n        hpo_config['loss'] = loss\n        _set_arguments(config=model_to_loss_to_loss_kwargs, key='loss_kwargs', value=loss)\n        _set_arguments(config=model_to_loss_to_loss_kwargs_ranges, key='loss_kwargs_ranges', value=loss)\n        logger.info(f\"Loss functions: {loss}\")\n\n        # Add regularizer to current_pipeline\n        hpo_config['regularizer'] = regularizer\n        _set_arguments(config=model_to_regularizer_to_regularizer_kwargs, key='regularizer_kwargs', value=regularizer)\n        _set_arguments(\n            config=model_to_regularizer_to_regularizer_kwargs_ranges,\n            key='regularizer_kwargs_ranges',\n            value=regularizer,\n        )\n        logger.info(f\"Regularizer: {regularizer}\")\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        # Add optimizer to current_pipeline\n        _set_arguments(key='optimizer', value=optimizer)\n        logger.info(f\"Optimizer: {optimizer}\")\n",
        "source_code_len": 148,
        "target_code": "        # Add optimizer to current_pipeline\n        hpo_config['optimizer'] = optimizer\n        _set_arguments(config=model_to_optimizer_to_optimizer_kwargs, key='optimizer_kwargs', value=optimizer)\n        _set_arguments(\n            config=model_to_optimizer_to_optimizer_kwargs_ranges,\n            key='optimizer_kwargs_ranges',\n            value=optimizer,\n        )\n        logger.info(f\"Optimizer: {optimizer}\")\n",
        "target_code_len": 418,
        "diff_format": "@@ -381,3 +535,9 @@\n         # Add optimizer to current_pipeline\n-        _set_arguments(key='optimizer', value=optimizer)\n+        hpo_config['optimizer'] = optimizer\n+        _set_arguments(config=model_to_optimizer_to_optimizer_kwargs, key='optimizer_kwargs', value=optimizer)\n+        _set_arguments(\n+            config=model_to_optimizer_to_optimizer_kwargs_ranges,\n+            key='optimizer_kwargs_ranges',\n+            value=optimizer,\n+        )\n         logger.info(f\"Optimizer: {optimizer}\")\n",
        "source_code_with_indent": "        # Add optimizer to current_pipeline\n        _set_arguments(key='optimizer', value=optimizer)\n        logger.info(f\"Optimizer: {optimizer}\")\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        # Add optimizer to current_pipeline\n        hpo_config['optimizer'] = optimizer\n        _set_arguments(config=model_to_optimizer_to_optimizer_kwargs, key='optimizer_kwargs', value=optimizer)\n        _set_arguments(\n            config=model_to_optimizer_to_optimizer_kwargs_ranges,\n            key='optimizer_kwargs_ranges',\n            value=optimizer,\n        )\n        logger.info(f\"Optimizer: {optimizer}\")\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        if normalize_string(training_loop, suffix=_TRAINING_LOOP_SUFFIX) == 'slcwa':\n            negative_sampler = ablation_config.get('negative_sampler', 'basic')  # default to basic\n            _set_arguments(key='negative_sampler', value=negative_sampler)\n            logger.info(f\"Negative sampler: {negative_sampler}\")\n\n        # Add training kwargs and kwargs_ranges\n        hpo_config['training_kwargs'] = model_to_trainer_to_training_kwargs.get(model, {}).get(training_loop, {})\n        hpo_config['training_kwargs_ranges'] = model_to_trainer_to_training_kwargs_ranges.get(model, {}).get(\n            training_loop, {})\n\n",
        "source_code_len": 630,
        "target_code": "        if normalize_string(training_loop, suffix=_TRAINING_LOOP_SUFFIX) == 'slcwa':\n            negative_sampler = negative_sampler or 'basic'  # default to basic\n            _set_arguments(\n                config=model_to_neg_sampler_to_neg_sampler_kwargs,\n                key='negative_sampler_kwargs',\n                value=negative_sampler,\n            )\n            _set_arguments(\n                config=model_to_neg_sampler_to_neg_sampler_kwargs_ranges,\n                key='negative_sampler_kwargs_ranges',\n                value=negative_sampler,\n            )\n            logger.info(f\"Negative sampler: {negative_sampler}\")\n\n",
        "target_code_len": 636,
        "diff_format": "@@ -389,10 +560,14 @@\n         if normalize_string(training_loop, suffix=_TRAINING_LOOP_SUFFIX) == 'slcwa':\n-            negative_sampler = ablation_config.get('negative_sampler', 'basic')  # default to basic\n-            _set_arguments(key='negative_sampler', value=negative_sampler)\n+            negative_sampler = negative_sampler or 'basic'  # default to basic\n+            _set_arguments(\n+                config=model_to_neg_sampler_to_neg_sampler_kwargs,\n+                key='negative_sampler_kwargs',\n+                value=negative_sampler,\n+            )\n+            _set_arguments(\n+                config=model_to_neg_sampler_to_neg_sampler_kwargs_ranges,\n+                key='negative_sampler_kwargs_ranges',\n+                value=negative_sampler,\n+            )\n             logger.info(f\"Negative sampler: {negative_sampler}\")\n-\n-        # Add training kwargs and kwargs_ranges\n-        hpo_config['training_kwargs'] = model_to_trainer_to_training_kwargs.get(model, {}).get(training_loop, {})\n-        hpo_config['training_kwargs_ranges'] = model_to_trainer_to_training_kwargs_ranges.get(model, {}).get(\n-            training_loop, {})\n \n",
        "source_code_with_indent": "        if normalize_string(training_loop, suffix=_TRAINING_LOOP_SUFFIX) == 'slcwa':\n            <IND>negative_sampler = ablation_config.get('negative_sampler', 'basic')  # default to basic\n            _set_arguments(key='negative_sampler', value=negative_sampler)\n            logger.info(f\"Negative sampler: {negative_sampler}\")\n\n        # Add training kwargs and kwargs_ranges\n        <DED>hpo_config['training_kwargs'] = model_to_trainer_to_training_kwargs.get(model, {}).get(training_loop, {})\n        hpo_config['training_kwargs_ranges'] = model_to_trainer_to_training_kwargs_ranges.get(model, {}).get(\n            training_loop, {})\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        if normalize_string(training_loop, suffix=_TRAINING_LOOP_SUFFIX) == 'slcwa':\n            <IND>negative_sampler = negative_sampler or 'basic'  # default to basic\n            _set_arguments(\n                config=model_to_neg_sampler_to_neg_sampler_kwargs,\n                key='negative_sampler_kwargs',\n                value=negative_sampler,\n            )\n            _set_arguments(\n                config=model_to_neg_sampler_to_neg_sampler_kwargs_ranges,\n                key='negative_sampler_kwargs_ranges',\n                value=negative_sampler,\n            )\n            logger.info(f\"Negative sampler: {negative_sampler}\")\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  }
]