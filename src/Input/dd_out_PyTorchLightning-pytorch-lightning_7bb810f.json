[
  {
    "project": "PyTorchLightning/pytorch-lightning",
    "commit": "7bb810f1438baf7acd81aae170c3105af5fd8269",
    "filename": "pytorch_lightning/loops/fit_loop.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-pytorch-lightning/pytorch_lightning/loops/fit_loop.py",
    "file_hunks_size": 10,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pytorch_lightning/loops/fit_loop.py:290:4 Inconsistent override [14]: `pytorch_lightning.loops.fit_loop.FitLoop.state_dict` overrides method defined in `pl.loops.base.Loop` inconsistently. Could not find parameter `destination` in overriding signature.",
    "message": " `pytorch_lightning.loops.fit_loop.FitLoop.state_dict` overrides method defined in `pl.loops.base.Loop` inconsistently. Could not find parameter `destination` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 290,
    "warning_line": "    def state_dict(self) -> Dict:",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\n    def state_dict(self) -> Dict:\n        return {\"epoch_loop\": self.epoch_loop.state_dict()}\n\n    def load_state_dict(self, state_dict: Dict) -> None:\n        self.epoch_loop.load_state_dict(state_dict[\"epoch_loop\"])\n\n    def teardown(self) -> None:\n",
        "source_code_len": 252,
        "target_code": "\n    def teardown(self) -> None:\n",
        "target_code_len": 33,
        "diff_format": "@@ -289,8 +287,2 @@\n \n-    def state_dict(self) -> Dict:\n-        return {\"epoch_loop\": self.epoch_loop.state_dict()}\n-\n-    def load_state_dict(self, state_dict: Dict) -> None:\n-        self.epoch_loop.load_state_dict(state_dict[\"epoch_loop\"])\n-\n     def teardown(self) -> None:\n",
        "source_code_with_indent": "\n    <DED><DED><DED>def state_dict(self) -> Dict:\n        <IND>return {\"epoch_loop\": self.epoch_loop.state_dict()}\n\n    <DED>def load_state_dict(self, state_dict: Dict) -> None:\n        <IND>self.epoch_loop.load_state_dict(state_dict[\"epoch_loop\"])\n\n    <DED>def teardown(self) -> None:\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED><DED><DED>def teardown(self) -> None:\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "PyTorchLightning/pytorch-lightning",
    "commit": "7bb810f1438baf7acd81aae170c3105af5fd8269",
    "filename": "pytorch_lightning/loops/fit_loop.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-pytorch-lightning/pytorch_lightning/loops/fit_loop.py",
    "file_hunks_size": 10,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pytorch_lightning/loops/fit_loop.py:290:4 Inconsistent override [14]: `pytorch_lightning.loops.fit_loop.FitLoop.state_dict` overrides method defined in `pl.loops.base.Loop` inconsistently. Could not find parameter `prefix` in overriding signature.",
    "message": " `pytorch_lightning.loops.fit_loop.FitLoop.state_dict` overrides method defined in `pl.loops.base.Loop` inconsistently. Could not find parameter `prefix` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 290,
    "warning_line": "    def state_dict(self) -> Dict:",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\n    def state_dict(self) -> Dict:\n        return {\"epoch_loop\": self.epoch_loop.state_dict()}\n\n    def load_state_dict(self, state_dict: Dict) -> None:\n        self.epoch_loop.load_state_dict(state_dict[\"epoch_loop\"])\n\n    def teardown(self) -> None:\n",
        "source_code_len": 252,
        "target_code": "\n    def teardown(self) -> None:\n",
        "target_code_len": 33,
        "diff_format": "@@ -289,8 +287,2 @@\n \n-    def state_dict(self) -> Dict:\n-        return {\"epoch_loop\": self.epoch_loop.state_dict()}\n-\n-    def load_state_dict(self, state_dict: Dict) -> None:\n-        self.epoch_loop.load_state_dict(state_dict[\"epoch_loop\"])\n-\n     def teardown(self) -> None:\n",
        "source_code_with_indent": "\n    <DED><DED><DED>def state_dict(self) -> Dict:\n        <IND>return {\"epoch_loop\": self.epoch_loop.state_dict()}\n\n    <DED>def load_state_dict(self, state_dict: Dict) -> None:\n        <IND>self.epoch_loop.load_state_dict(state_dict[\"epoch_loop\"])\n\n    <DED>def teardown(self) -> None:\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED><DED><DED>def teardown(self) -> None:\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "PyTorchLightning/pytorch-lightning",
    "commit": "7bb810f1438baf7acd81aae170c3105af5fd8269",
    "filename": "pytorch_lightning/loops/fit_loop.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-pytorch-lightning/pytorch_lightning/loops/fit_loop.py",
    "file_hunks_size": 10,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pytorch_lightning/loops/fit_loop.py:293:4 Inconsistent override [14]: `pytorch_lightning.loops.fit_loop.FitLoop.load_state_dict` overrides method defined in `pl.loops.base.Loop` inconsistently. Could not find parameter `prefix` in overriding signature.",
    "message": " `pytorch_lightning.loops.fit_loop.FitLoop.load_state_dict` overrides method defined in `pl.loops.base.Loop` inconsistently. Could not find parameter `prefix` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 293,
    "warning_line": "    def load_state_dict(self, state_dict: Dict) -> None:",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\n    def state_dict(self) -> Dict:\n        return {\"epoch_loop\": self.epoch_loop.state_dict()}\n\n    def load_state_dict(self, state_dict: Dict) -> None:\n        self.epoch_loop.load_state_dict(state_dict[\"epoch_loop\"])\n\n    def teardown(self) -> None:\n",
        "source_code_len": 252,
        "target_code": "\n    def teardown(self) -> None:\n",
        "target_code_len": 33,
        "diff_format": "@@ -289,8 +287,2 @@\n \n-    def state_dict(self) -> Dict:\n-        return {\"epoch_loop\": self.epoch_loop.state_dict()}\n-\n-    def load_state_dict(self, state_dict: Dict) -> None:\n-        self.epoch_loop.load_state_dict(state_dict[\"epoch_loop\"])\n-\n     def teardown(self) -> None:\n",
        "source_code_with_indent": "\n    <DED><DED><DED>def state_dict(self) -> Dict:\n        <IND>return {\"epoch_loop\": self.epoch_loop.state_dict()}\n\n    <DED>def load_state_dict(self, state_dict: Dict) -> None:\n        <IND>self.epoch_loop.load_state_dict(state_dict[\"epoch_loop\"])\n\n    <DED>def teardown(self) -> None:\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED><DED><DED>def teardown(self) -> None:\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "PyTorchLightning/pytorch-lightning",
    "commit": "7bb810f1438baf7acd81aae170c3105af5fd8269",
    "filename": "pytorch_lightning/loops/fit_loop.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-pytorch-lightning/pytorch_lightning/loops/fit_loop.py",
    "file_hunks_size": 10,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pytorch_lightning/loops/fit_loop.py:293:4 Inconsistent override [14]: `pytorch_lightning.loops.fit_loop.FitLoop.load_state_dict` overrides method defined in `pl.loops.base.Loop` inconsistently. Could not find parameter `restart_progress` in overriding signature.",
    "message": " `pytorch_lightning.loops.fit_loop.FitLoop.load_state_dict` overrides method defined in `pl.loops.base.Loop` inconsistently. Could not find parameter `restart_progress` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 293,
    "warning_line": "    def load_state_dict(self, state_dict: Dict) -> None:",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\n    def state_dict(self) -> Dict:\n        return {\"epoch_loop\": self.epoch_loop.state_dict()}\n\n    def load_state_dict(self, state_dict: Dict) -> None:\n        self.epoch_loop.load_state_dict(state_dict[\"epoch_loop\"])\n\n    def teardown(self) -> None:\n",
        "source_code_len": 252,
        "target_code": "\n    def teardown(self) -> None:\n",
        "target_code_len": 33,
        "diff_format": "@@ -289,8 +287,2 @@\n \n-    def state_dict(self) -> Dict:\n-        return {\"epoch_loop\": self.epoch_loop.state_dict()}\n-\n-    def load_state_dict(self, state_dict: Dict) -> None:\n-        self.epoch_loop.load_state_dict(state_dict[\"epoch_loop\"])\n-\n     def teardown(self) -> None:\n",
        "source_code_with_indent": "\n    <DED><DED><DED>def state_dict(self) -> Dict:\n        <IND>return {\"epoch_loop\": self.epoch_loop.state_dict()}\n\n    <DED>def load_state_dict(self, state_dict: Dict) -> None:\n        <IND>self.epoch_loop.load_state_dict(state_dict[\"epoch_loop\"])\n\n    <DED>def teardown(self) -> None:\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED><DED><DED>def teardown(self) -> None:\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "PyTorchLightning/pytorch-lightning",
    "commit": "7bb810f1438baf7acd81aae170c3105af5fd8269",
    "filename": "pytorch_lightning/trainer/progress.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-pytorch-lightning/pytorch_lightning/trainer/progress.py",
    "file_hunks_size": 11,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pytorch_lightning/trainer/progress.py:207:8 Incompatible return type [7]: Expected `int` but got `Optional[int]`.",
    "message": " Expected `int` but got `Optional[int]`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 207,
    "warning_line": "        return self.scheduler.total.completed",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    optimizer: OptimizerProgress = field(default_factory=OptimizerProgress)\n    scheduler: Progress = field(default_factory=lambda: Progress.from_defaults(started=None, processed=None))\n\n",
        "source_code_len": 187,
        "target_code": "    optimizer: OptimizerProgress = field(default_factory=OptimizerProgress)\n    optimizer_idx: int = 0\n\n",
        "target_code_len": 104,
        "diff_format": "@@ -198,3 +188,3 @@\n     optimizer: OptimizerProgress = field(default_factory=OptimizerProgress)\n-    scheduler: Progress = field(default_factory=lambda: Progress.from_defaults(started=None, processed=None))\n+    optimizer_idx: int = 0\n \n",
        "source_code_with_indent": "    optimizer: OptimizerProgress = field(default_factory=OptimizerProgress)\n    scheduler: Progress = field(default_factory=lambda: Progress.from_defaults(started=None, processed=None))\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    optimizer: OptimizerProgress = field(default_factory=OptimizerProgress)\n    optimizer_idx: int = 0\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    @property\n    def scheduler_steps(self) -> int:\n        return self.scheduler.total.completed\n\n    def reset_on_epoch(self) -> None:\n        self.optimizer.reset_on_epoch()\n        self.scheduler.current.reset()\n\n",
        "source_code_len": 218,
        "target_code": "\n    def reset_on_epoch(self) -> None:\n        self.optimizer.reset_on_epoch()\n        self.optimizer_idx = 0\n\n",
        "target_code_len": 111,
        "diff_format": "@@ -204,9 +194,5 @@\n \n-    @property\n-    def scheduler_steps(self) -> int:\n-        return self.scheduler.total.completed\n-\n     def reset_on_epoch(self) -> None:\n         self.optimizer.reset_on_epoch()\n-        self.scheduler.current.reset()\n+        self.optimizer_idx = 0\n \n",
        "source_code_with_indent": "\n    <DED>@property\n    def scheduler_steps(self) -> int:\n        <IND>return self.scheduler.total.completed\n\n    <DED>def reset_on_epoch(self) -> None:\n        <IND>self.optimizer.reset_on_epoch()\n        self.scheduler.current.reset()\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def reset_on_epoch(self) -> None:\n        <IND>self.optimizer.reset_on_epoch()\n        self.optimizer_idx = 0\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        self.optimizer.load_state_dict(state_dict[\"optimizer\"])\n        self.scheduler.load_state_dict(state_dict[\"scheduler\"])\n\n\n@dataclass\nclass EpochLoopProgress(BaseProgress):\n    \"\"\"\n    Tracks epoch loop progress.\n    These counters are local to a trainer rank. By default, they are not globally synced across all ranks.\n\n    Args:\n        epoch: Tracks epochs progress.\n    \"\"\"\n\n    epoch: EpochProgress = field(default_factory=EpochProgress)\n\n    def increment_epoch_completed(self) -> None:\n        self.epoch.increment_completed()\n        self.reset_on_epoch()\n\n    def reset_on_epoch(self) -> None:\n        self.epoch.reset_on_epoch()\n        self.epoch.current.reset()\n\n    def load_state_dict(self, state_dict: dict) -> None:\n        self.epoch.load_state_dict(state_dict[\"epoch\"])\n\n\n@dataclass\nclass TrainingEpochProgress(EpochProgress):\n    \"\"\"\n    Extends ``EpochProgress`` with training specific attributes\n\n    Args:\n        total: Tracks the total epoch progress.\n        current: Tracks the current epoch progress.\n        batch: Tracks batch progress.\n        optim: Tracks optimization progress.\n        val: Tracks val_loop progress.\n    \"\"\"\n\n    optim: OptimizationProgress = field(default_factory=OptimizationProgress)\n    val: EpochLoopProgress = field(default_factory=EpochLoopProgress)\n\n    def load_state_dict(self, state_dict: dict) -> None:\n        super().load_state_dict(state_dict)\n        self.optim.load_state_dict(state_dict[\"optim\"])\n        self.val.load_state_dict(state_dict[\"val\"])\n\n\n@dataclass\nclass FitLoopProgress(EpochLoopProgress):\n    \"\"\"\n    Extends ``EpochLoopProgress`` with fit specific attributes\n\n    Args:\n        epoch: Tracks epochs progress.\n    \"\"\"\n\n    epoch: TrainingEpochProgress = field(default_factory=TrainingEpochProgress)\n\n    def reset_on_epoch(self) -> None:\n        # do not reset `epoch.current` as it should track the number of epochs this `fit` call\n        self.epoch.reset_on_epoch()\n        self.epoch.optim.reset_on_epoch()\n",
        "source_code_len": 2001,
        "target_code": "        self.optimizer.load_state_dict(state_dict[\"optimizer\"])\n        self.optimizer_idx = state_dict[\"optimizer_idx\"]\n",
        "target_code_len": 121,
        "diff_format": "@@ -214,65 +200,2 @@\n         self.optimizer.load_state_dict(state_dict[\"optimizer\"])\n-        self.scheduler.load_state_dict(state_dict[\"scheduler\"])\n-\n-\n-@dataclass\n-class EpochLoopProgress(BaseProgress):\n-    \"\"\"\n-    Tracks epoch loop progress.\n-    These counters are local to a trainer rank. By default, they are not globally synced across all ranks.\n-\n-    Args:\n-        epoch: Tracks epochs progress.\n-    \"\"\"\n-\n-    epoch: EpochProgress = field(default_factory=EpochProgress)\n-\n-    def increment_epoch_completed(self) -> None:\n-        self.epoch.increment_completed()\n-        self.reset_on_epoch()\n-\n-    def reset_on_epoch(self) -> None:\n-        self.epoch.reset_on_epoch()\n-        self.epoch.current.reset()\n-\n-    def load_state_dict(self, state_dict: dict) -> None:\n-        self.epoch.load_state_dict(state_dict[\"epoch\"])\n-\n-\n-@dataclass\n-class TrainingEpochProgress(EpochProgress):\n-    \"\"\"\n-    Extends ``EpochProgress`` with training specific attributes\n-\n-    Args:\n-        total: Tracks the total epoch progress.\n-        current: Tracks the current epoch progress.\n-        batch: Tracks batch progress.\n-        optim: Tracks optimization progress.\n-        val: Tracks val_loop progress.\n-    \"\"\"\n-\n-    optim: OptimizationProgress = field(default_factory=OptimizationProgress)\n-    val: EpochLoopProgress = field(default_factory=EpochLoopProgress)\n-\n-    def load_state_dict(self, state_dict: dict) -> None:\n-        super().load_state_dict(state_dict)\n-        self.optim.load_state_dict(state_dict[\"optim\"])\n-        self.val.load_state_dict(state_dict[\"val\"])\n-\n-\n-@dataclass\n-class FitLoopProgress(EpochLoopProgress):\n-    \"\"\"\n-    Extends ``EpochLoopProgress`` with fit specific attributes\n-\n-    Args:\n-        epoch: Tracks epochs progress.\n-    \"\"\"\n-\n-    epoch: TrainingEpochProgress = field(default_factory=TrainingEpochProgress)\n-\n-    def reset_on_epoch(self) -> None:\n-        # do not reset `epoch.current` as it should track the number of epochs this `fit` call\n-        self.epoch.reset_on_epoch()\n-        self.epoch.optim.reset_on_epoch()\n+        self.optimizer_idx = state_dict[\"optimizer_idx\"]\n",
        "source_code_with_indent": "        <IND>self.optimizer.load_state_dict(state_dict[\"optimizer\"])\n        self.scheduler.load_state_dict(state_dict[\"scheduler\"])\n\n\n<DED><DED>@dataclass\nclass EpochLoopProgress(BaseProgress):\n    <IND>\"\"\"\n    Tracks epoch loop progress.\n    These counters are local to a trainer rank. By default, they are not globally synced across all ranks.\n\n    Args:\n        epoch: Tracks epochs progress.\n    \"\"\"\n\n    epoch: EpochProgress = field(default_factory=EpochProgress)\n\n    def increment_epoch_completed(self) -> None:\n        <IND>self.epoch.increment_completed()\n        self.reset_on_epoch()\n\n    <DED>def reset_on_epoch(self) -> None:\n        <IND>self.epoch.reset_on_epoch()\n        self.epoch.current.reset()\n\n    <DED>def load_state_dict(self, state_dict: dict) -> None:\n        <IND>self.epoch.load_state_dict(state_dict[\"epoch\"])\n\n\n<DED><DED>@dataclass\nclass TrainingEpochProgress(EpochProgress):\n    <IND>\"\"\"\n    Extends ``EpochProgress`` with training specific attributes\n\n    Args:\n        total: Tracks the total epoch progress.\n        current: Tracks the current epoch progress.\n        batch: Tracks batch progress.\n        optim: Tracks optimization progress.\n        val: Tracks val_loop progress.\n    \"\"\"\n\n    optim: OptimizationProgress = field(default_factory=OptimizationProgress)\n    val: EpochLoopProgress = field(default_factory=EpochLoopProgress)\n\n    def load_state_dict(self, state_dict: dict) -> None:\n        <IND>super().load_state_dict(state_dict)\n        self.optim.load_state_dict(state_dict[\"optim\"])\n        self.val.load_state_dict(state_dict[\"val\"])\n\n\n<DED><DED>@dataclass\nclass FitLoopProgress(EpochLoopProgress):\n    <IND>\"\"\"\n    Extends ``EpochLoopProgress`` with fit specific attributes\n\n    Args:\n        epoch: Tracks epochs progress.\n    \"\"\"\n\n    epoch: TrainingEpochProgress = field(default_factory=TrainingEpochProgress)\n\n    def reset_on_epoch(self) -> None:\n        # do not reset `epoch.current` as it should track the number of epochs this `fit` call\n        <IND>self.epoch.reset_on_epoch()\n        self.epoch.optim.reset_on_epoch()\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <IND>self.optimizer.load_state_dict(state_dict[\"optimizer\"])\n        self.optimizer_idx = state_dict[\"optimizer_idx\"]\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  }
]