[
  {
    "project": "medipixel/rl_algorithms",
    "commit": "509a777b6fb0b5fa5c4478ccbd112e162dc4f104",
    "filename": "algorithms/fd/ddpg_agent.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/medipixel-rl_algorithms/algorithms/fd/ddpg_agent.py",
    "file_hunks_size": 9,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "algorithms/fd/ddpg_agent.py:134:4 Inconsistent override [14]: `algorithms.fd.ddpg_agent.Agent.update_model` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `algorithms.fd.ddpg_agent.Agent.update_model` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 134,
    "warning_line": "    def update_model("
  },
  {
    "project": "medipixel/rl_algorithms",
    "commit": "509a777b6fb0b5fa5c4478ccbd112e162dc4f104",
    "filename": "algorithms/fd/ddpg_agent.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/medipixel-rl_algorithms/algorithms/fd/ddpg_agent.py",
    "file_hunks_size": 9,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "algorithms/fd/ddpg_agent.py:181:4 Inconsistent override [14]: `algorithms.fd.ddpg_agent.Agent.load_params` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `algorithms.fd.ddpg_agent.Agent.load_params` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 181,
    "warning_line": "    def load_params(self, path: str):"
  },
  {
    "project": "medipixel/rl_algorithms",
    "commit": "509a777b6fb0b5fa5c4478ccbd112e162dc4f104",
    "filename": "algorithms/fd/ddpg_agent.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/medipixel-rl_algorithms/algorithms/fd/ddpg_agent.py",
    "file_hunks_size": 9,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "algorithms/fd/ddpg_agent.py:196:4 Inconsistent override [14]: `algorithms.fd.ddpg_agent.Agent.save_params` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `params` in overriding signature.",
    "message": " `algorithms.fd.ddpg_agent.Agent.save_params` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `params` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 196,
    "warning_line": "    def save_params(self, n_episode: int):"
  },
  {
    "project": "medipixel/rl_algorithms",
    "commit": "509a777b6fb0b5fa5c4478ccbd112e162dc4f104",
    "filename": "algorithms/fd/ddpg_agent.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/medipixel-rl_algorithms/algorithms/fd/ddpg_agent.py",
    "file_hunks_size": 9,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "algorithms/fd/ddpg_agent.py:209:4 Inconsistent override [14]: `algorithms.fd.ddpg_agent.Agent.write_log` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `algorithms.fd.ddpg_agent.Agent.write_log` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 209,
    "warning_line": "    def write_log(self, i: int, loss: np.ndarray, score: int = 0):"
  },
  {
    "project": "medipixel/rl_algorithms",
    "commit": "509a777b6fb0b5fa5c4478ccbd112e162dc4f104",
    "filename": "algorithms/fd/dqn_agent.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/medipixel-rl_algorithms/algorithms/fd/dqn_agent.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "algorithms/fd/dqn_agent.py:139:4 Inconsistent override [14]: `algorithms.fd.dqn_agent.Agent.update_model` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `algorithms.fd.dqn_agent.Agent.update_model` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 139,
    "warning_line": "    def update_model("
  },
  {
    "project": "medipixel/rl_algorithms",
    "commit": "509a777b6fb0b5fa5c4478ccbd112e162dc4f104",
    "filename": "algorithms/fd/dqn_agent.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/medipixel-rl_algorithms/algorithms/fd/dqn_agent.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "algorithms/fd/dqn_agent.py:207:4 Inconsistent override [14]: `algorithms.fd.dqn_agent.Agent.load_params` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `algorithms.fd.dqn_agent.Agent.load_params` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 207,
    "warning_line": "    def load_params(self, path: str):"
  },
  {
    "project": "medipixel/rl_algorithms",
    "commit": "509a777b6fb0b5fa5c4478ccbd112e162dc4f104",
    "filename": "algorithms/fd/dqn_agent.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/medipixel-rl_algorithms/algorithms/fd/dqn_agent.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "algorithms/fd/dqn_agent.py:219:4 Inconsistent override [14]: `algorithms.fd.dqn_agent.Agent.save_params` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `params` in overriding signature.",
    "message": " `algorithms.fd.dqn_agent.Agent.save_params` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `params` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 219,
    "warning_line": "    def save_params(self, n_episode: int):"
  },
  {
    "project": "medipixel/rl_algorithms",
    "commit": "509a777b6fb0b5fa5c4478ccbd112e162dc4f104",
    "filename": "algorithms/fd/dqn_agent.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/medipixel-rl_algorithms/algorithms/fd/dqn_agent.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "algorithms/fd/dqn_agent.py:229:4 Inconsistent override [14]: `algorithms.fd.dqn_agent.Agent.write_log` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `algorithms.fd.dqn_agent.Agent.write_log` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 229,
    "warning_line": "    def write_log(self, i: int, avg_loss: dict, score: int = 0):"
  },
  {
    "project": "medipixel/rl_algorithms",
    "commit": "509a777b6fb0b5fa5c4478ccbd112e162dc4f104",
    "filename": "algorithms/fd/sac_agent.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/medipixel-rl_algorithms/algorithms/fd/sac_agent.py",
    "file_hunks_size": 11,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "algorithms/fd/sac_agent.py:146:4 Inconsistent override [14]: `algorithms.fd.sac_agent.Agent.update_model` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `algorithms.fd.sac_agent.Agent.update_model` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 146,
    "warning_line": "    def update_model(",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nimport gym\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport wandb\n\nfrom algorithms.common.abstract.agent import AbstractAgent\nfrom algorithms.common.buffer.priortized_replay_buffer import PrioritizedReplayBufferfD\nimport algorithms.common.helper_functions as common_utils\n\n",
        "source_code_len": 292,
        "target_code": "\nimport numpy as np\nimport torch\n\nfrom algorithms.common.buffer.priortized_replay_buffer import PrioritizedReplayBufferfD\nimport algorithms.common.helper_functions as common_utils\nfrom algorithms.sac.agent import Agent as SACAgent\n\n",
        "target_code_len": 232,
        "diff_format": "@@ -16,11 +14,8 @@\n \n-import gym\n import numpy as np\n import torch\n-import torch.optim as optim\n-import wandb\n \n-from algorithms.common.abstract.agent import AbstractAgent\n from algorithms.common.buffer.priortized_replay_buffer import PrioritizedReplayBufferfD\n import algorithms.common.helper_functions as common_utils\n+from algorithms.sac.agent import Agent as SACAgent\n \n",
        "source_code_with_indent": "\nimport gym\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport wandb\n\nfrom algorithms.common.abstract.agent import AbstractAgent\nfrom algorithms.common.buffer.priortized_replay_buffer import PrioritizedReplayBufferfD\nimport algorithms.common.helper_functions as common_utils\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\nimport numpy as np\nimport torch\n\nfrom algorithms.common.buffer.priortized_replay_buffer import PrioritizedReplayBufferfD\nimport algorithms.common.helper_functions as common_utils\nfrom algorithms.sac.agent import Agent as SACAgent\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nclass Agent(AbstractAgent):\n    \"\"\"SAC agent interacting with environment.\n",
        "source_code_len": 76,
        "target_code": "\nclass Agent(SACAgent):\n    \"\"\"SAC agent interacting with environment.\n",
        "target_code_len": 71,
        "diff_format": "@@ -29,3 +24,3 @@\n \n-class Agent(AbstractAgent):\n+class Agent(SACAgent):\n     \"\"\"SAC agent interacting with environment.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\nclass Agent(AbstractAgent):\n    <IND>",
        "target_code_with_indent": "\nclass Agent(SACAgent):\n    <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    def __init__(\n        self,\n        env: gym.Env,\n        args: argparse.Namespace,\n        hyper_params: dict,\n        models: tuple,\n        optims: tuple,\n        target_entropy: float,\n    ):\n        \"\"\"Initialization.\n\n        Args:\n            env (gym.Env): openAI Gym environment\n            args (argparse.Namespace): arguments including hyperparameters and training settings\n            hyper_params (dict): hyper-parameters\n            models (tuple): models including actor and critic\n            optims (tuple): optimizers for actor and critic\n            target_entropy (float): target entropy for the inequality constraint\n\n        \"\"\"\n        AbstractAgent.__init__(self, env, args)\n\n        self.actor, self.vf, self.vf_target, self.qf_1, self.qf_2 = models\n        self.actor_optimizer, self.vf_optimizer = optims[0:2]\n        self.qf_1_optimizer, self.qf_2_optimizer = optims[2:4]\n        self.hyper_params = hyper_params\n        self.curr_state = np.zeros((1,))\n        self.total_step = 0\n        self.episode_step = 0\n\n        # automatic entropy tuning\n        if hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n            self.target_entropy = target_entropy\n            self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\n            self.alpha_optimizer = optim.Adam(\n                [self.log_alpha], lr=hyper_params[\"LR_ENTROPY\"]\n            )\n\n        # load the optimizer and model parameters\n        if args.load_from is not None and os.path.exists(args.load_from):\n            self.load_params(args.load_from)\n\n        if not self.args.test:\n",
        "source_code_len": 1584,
        "target_code": "\n    # pylint: disable=attribute-defined-outside-init\n    def _initialize(self):\n        \"\"\"Initialize non-common things.\"\"\"\n        if not self.args.test:\n",
        "target_code_len": 156,
        "diff_format": "@@ -53,44 +33,5 @@\n \n-    def __init__(\n-        self,\n-        env: gym.Env,\n-        args: argparse.Namespace,\n-        hyper_params: dict,\n-        models: tuple,\n-        optims: tuple,\n-        target_entropy: float,\n-    ):\n-        \"\"\"Initialization.\n-\n-        Args:\n-            env (gym.Env): openAI Gym environment\n-            args (argparse.Namespace): arguments including hyperparameters and training settings\n-            hyper_params (dict): hyper-parameters\n-            models (tuple): models including actor and critic\n-            optims (tuple): optimizers for actor and critic\n-            target_entropy (float): target entropy for the inequality constraint\n-\n-        \"\"\"\n-        AbstractAgent.__init__(self, env, args)\n-\n-        self.actor, self.vf, self.vf_target, self.qf_1, self.qf_2 = models\n-        self.actor_optimizer, self.vf_optimizer = optims[0:2]\n-        self.qf_1_optimizer, self.qf_2_optimizer = optims[2:4]\n-        self.hyper_params = hyper_params\n-        self.curr_state = np.zeros((1,))\n-        self.total_step = 0\n-        self.episode_step = 0\n-\n-        # automatic entropy tuning\n-        if hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n-            self.target_entropy = target_entropy\n-            self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\n-            self.alpha_optimizer = optim.Adam(\n-                [self.log_alpha], lr=hyper_params[\"LR_ENTROPY\"]\n-            )\n-\n-        # load the optimizer and model parameters\n-        if args.load_from is not None and os.path.exists(args.load_from):\n-            self.load_params(args.load_from)\n-\n+    # pylint: disable=attribute-defined-outside-init\n+    def _initialize(self):\n+        \"\"\"Initialize non-common things.\"\"\"\n         if not self.args.test:\n",
        "source_code_with_indent": "\n    def __init__(\n        self,\n        env: gym.Env,\n        args: argparse.Namespace,\n        hyper_params: dict,\n        models: tuple,\n        optims: tuple,\n        target_entropy: float,\n    ):\n        <IND>\"\"\"Initialization.\n\n        Args:\n            env (gym.Env): openAI Gym environment\n            args (argparse.Namespace): arguments including hyperparameters and training settings\n            hyper_params (dict): hyper-parameters\n            models (tuple): models including actor and critic\n            optims (tuple): optimizers for actor and critic\n            target_entropy (float): target entropy for the inequality constraint\n\n        \"\"\"\n        AbstractAgent.__init__(self, env, args)\n\n        self.actor, self.vf, self.vf_target, self.qf_1, self.qf_2 = models\n        self.actor_optimizer, self.vf_optimizer = optims[0:2]\n        self.qf_1_optimizer, self.qf_2_optimizer = optims[2:4]\n        self.hyper_params = hyper_params\n        self.curr_state = np.zeros((1,))\n        self.total_step = 0\n        self.episode_step = 0\n\n        # automatic entropy tuning\n        if hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n            <IND>self.target_entropy = target_entropy\n            self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\n            self.alpha_optimizer = optim.Adam(\n                [self.log_alpha], lr=hyper_params[\"LR_ENTROPY\"]\n            )\n\n        # load the optimizer and model parameters\n        <DED>if args.load_from is not None and os.path.exists(args.load_from):\n            <IND>self.load_params(args.load_from)\n\n        <DED>if not self.args.test:\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    # pylint: disable=attribute-defined-outside-init\n    def _initialize(self):\n        <IND>\"\"\"Initialize non-common things.\"\"\"\n        if not self.args.test:\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\n    def load_params(self, path: str):\n        \"\"\"Load model and optimizer parameters.\"\"\"\n        if not os.path.exists(path):\n            print(\"[ERROR] the input path does not exist. ->\", path)\n            return\n\n        params = torch.load(path)\n        self.actor.load_state_dict(params[\"actor\"])\n        self.qf_1.load_state_dict(params[\"qf_1\"])\n        self.qf_2.load_state_dict(params[\"qf_2\"])\n        self.vf.load_state_dict(params[\"vf\"])\n        self.vf_target.load_state_dict(params[\"vf_target\"])\n        self.actor_optimizer.load_state_dict(params[\"actor_optim\"])\n        self.qf_1_optimizer.load_state_dict(params[\"qf_1_optim\"])\n        self.qf_2_optimizer.load_state_dict(params[\"qf_2_optim\"])\n        self.vf_optimizer.load_state_dict(params[\"vf_optim\"])\n\n        if self.hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n            self.alpha_optimizer.load_state_dict(params[\"alpha_optim\"])\n\n        print(\"[INFO] loaded the model and optimizer from\", path)\n\n    def save_params(self, n_episode: int):\n        \"\"\"Save model and optimizer parameters.\"\"\"\n        params = {\n            \"actor\": self.actor.state_dict(),\n            \"qf_1\": self.qf_1.state_dict(),\n            \"qf_2\": self.qf_2.state_dict(),\n            \"vf\": self.vf.state_dict(),\n            \"vf_target\": self.vf_target.state_dict(),\n            \"actor_optim\": self.actor_optimizer.state_dict(),\n            \"qf_1_optim\": self.qf_1_optimizer.state_dict(),\n            \"qf_2_optim\": self.qf_2_optimizer.state_dict(),\n            \"vf_optim\": self.vf_optimizer.state_dict(),\n        }\n\n        if self.hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n            params[\"alpha_optim\"] = self.alpha_optimizer.state_dict()\n\n        AbstractAgent.save_params(self, params, n_episode)\n\n    def write_log(\n        self, i: int, loss: np.ndarray, score: int = 0, delayed_update: int = 1\n    ):\n        \"\"\"Write log about loss and score\"\"\"\n        total_loss = loss.sum()\n\n        print(\n            \"[INFO] episode %d, episode_step %d, total step %d, total score: %d\\n\"\n            \"total loss: %.3f actor_loss: %.3f qf_1_loss: %.3f qf_2_loss: %.3f \"\n            \"vf_loss: %.3f alpha_loss: %.3f\\n\"\n            % (\n                i,\n                self.episode_step,\n                self.total_step,\n                score,\n                total_loss,\n                loss[0] * delayed_update,  # actor loss\n                loss[1],  # qf_1 loss\n                loss[2],  # qf_2 loss\n                loss[3],  # vf loss\n                loss[4],  # alpha loss\n            )\n        )\n\n        if self.args.log:\n            wandb.log(\n                {\n                    \"score\": score,\n                    \"total loss\": total_loss,\n                    \"actor loss\": loss[0] * delayed_update,\n                    \"qf_1 loss\": loss[1],\n                    \"qf_2 loss\": loss[2],\n                    \"vf loss\": loss[3],\n                    \"alpha loss\": loss[4],\n                }\n            )\n\n        if self.args.log:\n            wandb.log(\n                {\n                    \"score\": score,\n                    \"total loss\": total_loss,\n                    \"actor loss\": loss[0] * delayed_update,\n                    \"qf_1 loss\": loss[1],\n                    \"qf_2 loss\": loss[2],\n                    \"vf loss\": loss[3],\n                    \"alpha loss\": loss[4],\n                }\n            )\n\n    def pretrain(self):\n",
        "source_code_len": 3376,
        "target_code": "\n    def pretrain(self):\n",
        "target_code_len": 25,
        "diff_format": "@@ -247,93 +156,2 @@\n \n-    def load_params(self, path: str):\n-        \"\"\"Load model and optimizer parameters.\"\"\"\n-        if not os.path.exists(path):\n-            print(\"[ERROR] the input path does not exist. ->\", path)\n-            return\n-\n-        params = torch.load(path)\n-        self.actor.load_state_dict(params[\"actor\"])\n-        self.qf_1.load_state_dict(params[\"qf_1\"])\n-        self.qf_2.load_state_dict(params[\"qf_2\"])\n-        self.vf.load_state_dict(params[\"vf\"])\n-        self.vf_target.load_state_dict(params[\"vf_target\"])\n-        self.actor_optimizer.load_state_dict(params[\"actor_optim\"])\n-        self.qf_1_optimizer.load_state_dict(params[\"qf_1_optim\"])\n-        self.qf_2_optimizer.load_state_dict(params[\"qf_2_optim\"])\n-        self.vf_optimizer.load_state_dict(params[\"vf_optim\"])\n-\n-        if self.hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n-            self.alpha_optimizer.load_state_dict(params[\"alpha_optim\"])\n-\n-        print(\"[INFO] loaded the model and optimizer from\", path)\n-\n-    def save_params(self, n_episode: int):\n-        \"\"\"Save model and optimizer parameters.\"\"\"\n-        params = {\n-            \"actor\": self.actor.state_dict(),\n-            \"qf_1\": self.qf_1.state_dict(),\n-            \"qf_2\": self.qf_2.state_dict(),\n-            \"vf\": self.vf.state_dict(),\n-            \"vf_target\": self.vf_target.state_dict(),\n-            \"actor_optim\": self.actor_optimizer.state_dict(),\n-            \"qf_1_optim\": self.qf_1_optimizer.state_dict(),\n-            \"qf_2_optim\": self.qf_2_optimizer.state_dict(),\n-            \"vf_optim\": self.vf_optimizer.state_dict(),\n-        }\n-\n-        if self.hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n-            params[\"alpha_optim\"] = self.alpha_optimizer.state_dict()\n-\n-        AbstractAgent.save_params(self, params, n_episode)\n-\n-    def write_log(\n-        self, i: int, loss: np.ndarray, score: int = 0, delayed_update: int = 1\n-    ):\n-        \"\"\"Write log about loss and score\"\"\"\n-        total_loss = loss.sum()\n-\n-        print(\n-            \"[INFO] episode %d, episode_step %d, total step %d, total score: %d\\n\"\n-            \"total loss: %.3f actor_loss: %.3f qf_1_loss: %.3f qf_2_loss: %.3f \"\n-            \"vf_loss: %.3f alpha_loss: %.3f\\n\"\n-            % (\n-                i,\n-                self.episode_step,\n-                self.total_step,\n-                score,\n-                total_loss,\n-                loss[0] * delayed_update,  # actor loss\n-                loss[1],  # qf_1 loss\n-                loss[2],  # qf_2 loss\n-                loss[3],  # vf loss\n-                loss[4],  # alpha loss\n-            )\n-        )\n-\n-        if self.args.log:\n-            wandb.log(\n-                {\n-                    \"score\": score,\n-                    \"total loss\": total_loss,\n-                    \"actor loss\": loss[0] * delayed_update,\n-                    \"qf_1 loss\": loss[1],\n-                    \"qf_2 loss\": loss[2],\n-                    \"vf loss\": loss[3],\n-                    \"alpha loss\": loss[4],\n-                }\n-            )\n-\n-        if self.args.log:\n-            wandb.log(\n-                {\n-                    \"score\": score,\n-                    \"total loss\": total_loss,\n-                    \"actor loss\": loss[0] * delayed_update,\n-                    \"qf_1 loss\": loss[1],\n-                    \"qf_2 loss\": loss[2],\n-                    \"vf loss\": loss[3],\n-                    \"alpha loss\": loss[4],\n-                }\n-            )\n-\n     def pretrain(self):\n",
        "source_code_with_indent": "\n    <DED>def load_params(self, path: str):\n        <IND>\"\"\"Load model and optimizer parameters.\"\"\"\n        if not os.path.exists(path):\n            <IND>print(\"[ERROR] the input path does not exist. ->\", path)\n            return\n\n        <DED>params = torch.load(path)\n        self.actor.load_state_dict(params[\"actor\"])\n        self.qf_1.load_state_dict(params[\"qf_1\"])\n        self.qf_2.load_state_dict(params[\"qf_2\"])\n        self.vf.load_state_dict(params[\"vf\"])\n        self.vf_target.load_state_dict(params[\"vf_target\"])\n        self.actor_optimizer.load_state_dict(params[\"actor_optim\"])\n        self.qf_1_optimizer.load_state_dict(params[\"qf_1_optim\"])\n        self.qf_2_optimizer.load_state_dict(params[\"qf_2_optim\"])\n        self.vf_optimizer.load_state_dict(params[\"vf_optim\"])\n\n        if self.hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n            <IND>self.alpha_optimizer.load_state_dict(params[\"alpha_optim\"])\n\n        <DED>print(\"[INFO] loaded the model and optimizer from\", path)\n\n    <DED>def save_params(self, n_episode: int):\n        <IND>\"\"\"Save model and optimizer parameters.\"\"\"\n        params = {\n            \"actor\": self.actor.state_dict(),\n            \"qf_1\": self.qf_1.state_dict(),\n            \"qf_2\": self.qf_2.state_dict(),\n            \"vf\": self.vf.state_dict(),\n            \"vf_target\": self.vf_target.state_dict(),\n            \"actor_optim\": self.actor_optimizer.state_dict(),\n            \"qf_1_optim\": self.qf_1_optimizer.state_dict(),\n            \"qf_2_optim\": self.qf_2_optimizer.state_dict(),\n            \"vf_optim\": self.vf_optimizer.state_dict(),\n        }\n\n        if self.hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n            <IND>params[\"alpha_optim\"] = self.alpha_optimizer.state_dict()\n\n        <DED>AbstractAgent.save_params(self, params, n_episode)\n\n    <DED>def write_log(\n        self, i: int, loss: np.ndarray, score: int = 0, delayed_update: int = 1\n    ):\n        <IND>\"\"\"Write log about loss and score\"\"\"\n        total_loss = loss.sum()\n\n        print(\n            \"[INFO] episode %d, episode_step %d, total step %d, total score: %d\\n\"\n            \"total loss: %.3f actor_loss: %.3f qf_1_loss: %.3f qf_2_loss: %.3f \"\n            \"vf_loss: %.3f alpha_loss: %.3f\\n\"\n            % (\n                i,\n                self.episode_step,\n                self.total_step,\n                score,\n                total_loss,\n                loss[0] * delayed_update,  # actor loss\n                loss[1],  # qf_1 loss\n                loss[2],  # qf_2 loss\n                loss[3],  # vf loss\n                loss[4],  # alpha loss\n            )\n        )\n\n        if self.args.log:\n            <IND>wandb.log(\n                {\n                    \"score\": score,\n                    \"total loss\": total_loss,\n                    \"actor loss\": loss[0] * delayed_update,\n                    \"qf_1 loss\": loss[1],\n                    \"qf_2 loss\": loss[2],\n                    \"vf loss\": loss[3],\n                    \"alpha loss\": loss[4],\n                }\n            )\n\n        <DED>if self.args.log:\n            <IND>wandb.log(\n                {\n                    \"score\": score,\n                    \"total loss\": total_loss,\n                    \"actor loss\": loss[0] * delayed_update,\n                    \"qf_1 loss\": loss[1],\n                    \"qf_2 loss\": loss[2],\n                    \"vf loss\": loss[3],\n                    \"alpha loss\": loss[4],\n                }\n            )\n\n    <DED><DED>def pretrain(self):\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def pretrain(self):\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "medipixel/rl_algorithms",
    "commit": "509a777b6fb0b5fa5c4478ccbd112e162dc4f104",
    "filename": "algorithms/fd/sac_agent.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/medipixel-rl_algorithms/algorithms/fd/sac_agent.py",
    "file_hunks_size": 11,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "algorithms/fd/sac_agent.py:248:4 Inconsistent override [14]: `algorithms.fd.sac_agent.Agent.load_params` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `algorithms.fd.sac_agent.Agent.load_params` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 248,
    "warning_line": "    def load_params(self, path: str):",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\n    def load_params(self, path: str):\n        \"\"\"Load model and optimizer parameters.\"\"\"\n        if not os.path.exists(path):\n            print(\"[ERROR] the input path does not exist. ->\", path)\n            return\n\n        params = torch.load(path)\n        self.actor.load_state_dict(params[\"actor\"])\n        self.qf_1.load_state_dict(params[\"qf_1\"])\n        self.qf_2.load_state_dict(params[\"qf_2\"])\n        self.vf.load_state_dict(params[\"vf\"])\n        self.vf_target.load_state_dict(params[\"vf_target\"])\n        self.actor_optimizer.load_state_dict(params[\"actor_optim\"])\n        self.qf_1_optimizer.load_state_dict(params[\"qf_1_optim\"])\n        self.qf_2_optimizer.load_state_dict(params[\"qf_2_optim\"])\n        self.vf_optimizer.load_state_dict(params[\"vf_optim\"])\n\n        if self.hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n            self.alpha_optimizer.load_state_dict(params[\"alpha_optim\"])\n\n        print(\"[INFO] loaded the model and optimizer from\", path)\n\n    def save_params(self, n_episode: int):\n        \"\"\"Save model and optimizer parameters.\"\"\"\n        params = {\n            \"actor\": self.actor.state_dict(),\n            \"qf_1\": self.qf_1.state_dict(),\n            \"qf_2\": self.qf_2.state_dict(),\n            \"vf\": self.vf.state_dict(),\n            \"vf_target\": self.vf_target.state_dict(),\n            \"actor_optim\": self.actor_optimizer.state_dict(),\n            \"qf_1_optim\": self.qf_1_optimizer.state_dict(),\n            \"qf_2_optim\": self.qf_2_optimizer.state_dict(),\n            \"vf_optim\": self.vf_optimizer.state_dict(),\n        }\n\n        if self.hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n            params[\"alpha_optim\"] = self.alpha_optimizer.state_dict()\n\n        AbstractAgent.save_params(self, params, n_episode)\n\n    def write_log(\n        self, i: int, loss: np.ndarray, score: int = 0, delayed_update: int = 1\n    ):\n        \"\"\"Write log about loss and score\"\"\"\n        total_loss = loss.sum()\n\n        print(\n            \"[INFO] episode %d, episode_step %d, total step %d, total score: %d\\n\"\n            \"total loss: %.3f actor_loss: %.3f qf_1_loss: %.3f qf_2_loss: %.3f \"\n            \"vf_loss: %.3f alpha_loss: %.3f\\n\"\n            % (\n                i,\n                self.episode_step,\n                self.total_step,\n                score,\n                total_loss,\n                loss[0] * delayed_update,  # actor loss\n                loss[1],  # qf_1 loss\n                loss[2],  # qf_2 loss\n                loss[3],  # vf loss\n                loss[4],  # alpha loss\n            )\n        )\n\n        if self.args.log:\n            wandb.log(\n                {\n                    \"score\": score,\n                    \"total loss\": total_loss,\n                    \"actor loss\": loss[0] * delayed_update,\n                    \"qf_1 loss\": loss[1],\n                    \"qf_2 loss\": loss[2],\n                    \"vf loss\": loss[3],\n                    \"alpha loss\": loss[4],\n                }\n            )\n\n        if self.args.log:\n            wandb.log(\n                {\n                    \"score\": score,\n                    \"total loss\": total_loss,\n                    \"actor loss\": loss[0] * delayed_update,\n                    \"qf_1 loss\": loss[1],\n                    \"qf_2 loss\": loss[2],\n                    \"vf loss\": loss[3],\n                    \"alpha loss\": loss[4],\n                }\n            )\n\n    def pretrain(self):\n",
        "source_code_len": 3376,
        "target_code": "\n    def pretrain(self):\n",
        "target_code_len": 25,
        "diff_format": "@@ -247,93 +156,2 @@\n \n-    def load_params(self, path: str):\n-        \"\"\"Load model and optimizer parameters.\"\"\"\n-        if not os.path.exists(path):\n-            print(\"[ERROR] the input path does not exist. ->\", path)\n-            return\n-\n-        params = torch.load(path)\n-        self.actor.load_state_dict(params[\"actor\"])\n-        self.qf_1.load_state_dict(params[\"qf_1\"])\n-        self.qf_2.load_state_dict(params[\"qf_2\"])\n-        self.vf.load_state_dict(params[\"vf\"])\n-        self.vf_target.load_state_dict(params[\"vf_target\"])\n-        self.actor_optimizer.load_state_dict(params[\"actor_optim\"])\n-        self.qf_1_optimizer.load_state_dict(params[\"qf_1_optim\"])\n-        self.qf_2_optimizer.load_state_dict(params[\"qf_2_optim\"])\n-        self.vf_optimizer.load_state_dict(params[\"vf_optim\"])\n-\n-        if self.hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n-            self.alpha_optimizer.load_state_dict(params[\"alpha_optim\"])\n-\n-        print(\"[INFO] loaded the model and optimizer from\", path)\n-\n-    def save_params(self, n_episode: int):\n-        \"\"\"Save model and optimizer parameters.\"\"\"\n-        params = {\n-            \"actor\": self.actor.state_dict(),\n-            \"qf_1\": self.qf_1.state_dict(),\n-            \"qf_2\": self.qf_2.state_dict(),\n-            \"vf\": self.vf.state_dict(),\n-            \"vf_target\": self.vf_target.state_dict(),\n-            \"actor_optim\": self.actor_optimizer.state_dict(),\n-            \"qf_1_optim\": self.qf_1_optimizer.state_dict(),\n-            \"qf_2_optim\": self.qf_2_optimizer.state_dict(),\n-            \"vf_optim\": self.vf_optimizer.state_dict(),\n-        }\n-\n-        if self.hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n-            params[\"alpha_optim\"] = self.alpha_optimizer.state_dict()\n-\n-        AbstractAgent.save_params(self, params, n_episode)\n-\n-    def write_log(\n-        self, i: int, loss: np.ndarray, score: int = 0, delayed_update: int = 1\n-    ):\n-        \"\"\"Write log about loss and score\"\"\"\n-        total_loss = loss.sum()\n-\n-        print(\n-            \"[INFO] episode %d, episode_step %d, total step %d, total score: %d\\n\"\n-            \"total loss: %.3f actor_loss: %.3f qf_1_loss: %.3f qf_2_loss: %.3f \"\n-            \"vf_loss: %.3f alpha_loss: %.3f\\n\"\n-            % (\n-                i,\n-                self.episode_step,\n-                self.total_step,\n-                score,\n-                total_loss,\n-                loss[0] * delayed_update,  # actor loss\n-                loss[1],  # qf_1 loss\n-                loss[2],  # qf_2 loss\n-                loss[3],  # vf loss\n-                loss[4],  # alpha loss\n-            )\n-        )\n-\n-        if self.args.log:\n-            wandb.log(\n-                {\n-                    \"score\": score,\n-                    \"total loss\": total_loss,\n-                    \"actor loss\": loss[0] * delayed_update,\n-                    \"qf_1 loss\": loss[1],\n-                    \"qf_2 loss\": loss[2],\n-                    \"vf loss\": loss[3],\n-                    \"alpha loss\": loss[4],\n-                }\n-            )\n-\n-        if self.args.log:\n-            wandb.log(\n-                {\n-                    \"score\": score,\n-                    \"total loss\": total_loss,\n-                    \"actor loss\": loss[0] * delayed_update,\n-                    \"qf_1 loss\": loss[1],\n-                    \"qf_2 loss\": loss[2],\n-                    \"vf loss\": loss[3],\n-                    \"alpha loss\": loss[4],\n-                }\n-            )\n-\n     def pretrain(self):\n",
        "source_code_with_indent": "\n    <DED>def load_params(self, path: str):\n        <IND>\"\"\"Load model and optimizer parameters.\"\"\"\n        if not os.path.exists(path):\n            <IND>print(\"[ERROR] the input path does not exist. ->\", path)\n            return\n\n        <DED>params = torch.load(path)\n        self.actor.load_state_dict(params[\"actor\"])\n        self.qf_1.load_state_dict(params[\"qf_1\"])\n        self.qf_2.load_state_dict(params[\"qf_2\"])\n        self.vf.load_state_dict(params[\"vf\"])\n        self.vf_target.load_state_dict(params[\"vf_target\"])\n        self.actor_optimizer.load_state_dict(params[\"actor_optim\"])\n        self.qf_1_optimizer.load_state_dict(params[\"qf_1_optim\"])\n        self.qf_2_optimizer.load_state_dict(params[\"qf_2_optim\"])\n        self.vf_optimizer.load_state_dict(params[\"vf_optim\"])\n\n        if self.hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n            <IND>self.alpha_optimizer.load_state_dict(params[\"alpha_optim\"])\n\n        <DED>print(\"[INFO] loaded the model and optimizer from\", path)\n\n    <DED>def save_params(self, n_episode: int):\n        <IND>\"\"\"Save model and optimizer parameters.\"\"\"\n        params = {\n            \"actor\": self.actor.state_dict(),\n            \"qf_1\": self.qf_1.state_dict(),\n            \"qf_2\": self.qf_2.state_dict(),\n            \"vf\": self.vf.state_dict(),\n            \"vf_target\": self.vf_target.state_dict(),\n            \"actor_optim\": self.actor_optimizer.state_dict(),\n            \"qf_1_optim\": self.qf_1_optimizer.state_dict(),\n            \"qf_2_optim\": self.qf_2_optimizer.state_dict(),\n            \"vf_optim\": self.vf_optimizer.state_dict(),\n        }\n\n        if self.hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n            <IND>params[\"alpha_optim\"] = self.alpha_optimizer.state_dict()\n\n        <DED>AbstractAgent.save_params(self, params, n_episode)\n\n    <DED>def write_log(\n        self, i: int, loss: np.ndarray, score: int = 0, delayed_update: int = 1\n    ):\n        <IND>\"\"\"Write log about loss and score\"\"\"\n        total_loss = loss.sum()\n\n        print(\n            \"[INFO] episode %d, episode_step %d, total step %d, total score: %d\\n\"\n            \"total loss: %.3f actor_loss: %.3f qf_1_loss: %.3f qf_2_loss: %.3f \"\n            \"vf_loss: %.3f alpha_loss: %.3f\\n\"\n            % (\n                i,\n                self.episode_step,\n                self.total_step,\n                score,\n                total_loss,\n                loss[0] * delayed_update,  # actor loss\n                loss[1],  # qf_1 loss\n                loss[2],  # qf_2 loss\n                loss[3],  # vf loss\n                loss[4],  # alpha loss\n            )\n        )\n\n        if self.args.log:\n            <IND>wandb.log(\n                {\n                    \"score\": score,\n                    \"total loss\": total_loss,\n                    \"actor loss\": loss[0] * delayed_update,\n                    \"qf_1 loss\": loss[1],\n                    \"qf_2 loss\": loss[2],\n                    \"vf loss\": loss[3],\n                    \"alpha loss\": loss[4],\n                }\n            )\n\n        <DED>if self.args.log:\n            <IND>wandb.log(\n                {\n                    \"score\": score,\n                    \"total loss\": total_loss,\n                    \"actor loss\": loss[0] * delayed_update,\n                    \"qf_1 loss\": loss[1],\n                    \"qf_2 loss\": loss[2],\n                    \"vf loss\": loss[3],\n                    \"alpha loss\": loss[4],\n                }\n            )\n\n    <DED><DED>def pretrain(self):\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def pretrain(self):\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "medipixel/rl_algorithms",
    "commit": "509a777b6fb0b5fa5c4478ccbd112e162dc4f104",
    "filename": "algorithms/fd/sac_agent.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/medipixel-rl_algorithms/algorithms/fd/sac_agent.py",
    "file_hunks_size": 11,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "algorithms/fd/sac_agent.py:270:4 Inconsistent override [14]: `algorithms.fd.sac_agent.Agent.save_params` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `params` in overriding signature.",
    "message": " `algorithms.fd.sac_agent.Agent.save_params` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `params` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 270,
    "warning_line": "    def save_params(self, n_episode: int):",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\n    def load_params(self, path: str):\n        \"\"\"Load model and optimizer parameters.\"\"\"\n        if not os.path.exists(path):\n            print(\"[ERROR] the input path does not exist. ->\", path)\n            return\n\n        params = torch.load(path)\n        self.actor.load_state_dict(params[\"actor\"])\n        self.qf_1.load_state_dict(params[\"qf_1\"])\n        self.qf_2.load_state_dict(params[\"qf_2\"])\n        self.vf.load_state_dict(params[\"vf\"])\n        self.vf_target.load_state_dict(params[\"vf_target\"])\n        self.actor_optimizer.load_state_dict(params[\"actor_optim\"])\n        self.qf_1_optimizer.load_state_dict(params[\"qf_1_optim\"])\n        self.qf_2_optimizer.load_state_dict(params[\"qf_2_optim\"])\n        self.vf_optimizer.load_state_dict(params[\"vf_optim\"])\n\n        if self.hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n            self.alpha_optimizer.load_state_dict(params[\"alpha_optim\"])\n\n        print(\"[INFO] loaded the model and optimizer from\", path)\n\n    def save_params(self, n_episode: int):\n        \"\"\"Save model and optimizer parameters.\"\"\"\n        params = {\n            \"actor\": self.actor.state_dict(),\n            \"qf_1\": self.qf_1.state_dict(),\n            \"qf_2\": self.qf_2.state_dict(),\n            \"vf\": self.vf.state_dict(),\n            \"vf_target\": self.vf_target.state_dict(),\n            \"actor_optim\": self.actor_optimizer.state_dict(),\n            \"qf_1_optim\": self.qf_1_optimizer.state_dict(),\n            \"qf_2_optim\": self.qf_2_optimizer.state_dict(),\n            \"vf_optim\": self.vf_optimizer.state_dict(),\n        }\n\n        if self.hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n            params[\"alpha_optim\"] = self.alpha_optimizer.state_dict()\n\n        AbstractAgent.save_params(self, params, n_episode)\n\n    def write_log(\n        self, i: int, loss: np.ndarray, score: int = 0, delayed_update: int = 1\n    ):\n        \"\"\"Write log about loss and score\"\"\"\n        total_loss = loss.sum()\n\n        print(\n            \"[INFO] episode %d, episode_step %d, total step %d, total score: %d\\n\"\n            \"total loss: %.3f actor_loss: %.3f qf_1_loss: %.3f qf_2_loss: %.3f \"\n            \"vf_loss: %.3f alpha_loss: %.3f\\n\"\n            % (\n                i,\n                self.episode_step,\n                self.total_step,\n                score,\n                total_loss,\n                loss[0] * delayed_update,  # actor loss\n                loss[1],  # qf_1 loss\n                loss[2],  # qf_2 loss\n                loss[3],  # vf loss\n                loss[4],  # alpha loss\n            )\n        )\n\n        if self.args.log:\n            wandb.log(\n                {\n                    \"score\": score,\n                    \"total loss\": total_loss,\n                    \"actor loss\": loss[0] * delayed_update,\n                    \"qf_1 loss\": loss[1],\n                    \"qf_2 loss\": loss[2],\n                    \"vf loss\": loss[3],\n                    \"alpha loss\": loss[4],\n                }\n            )\n\n        if self.args.log:\n            wandb.log(\n                {\n                    \"score\": score,\n                    \"total loss\": total_loss,\n                    \"actor loss\": loss[0] * delayed_update,\n                    \"qf_1 loss\": loss[1],\n                    \"qf_2 loss\": loss[2],\n                    \"vf loss\": loss[3],\n                    \"alpha loss\": loss[4],\n                }\n            )\n\n    def pretrain(self):\n",
        "source_code_len": 3376,
        "target_code": "\n    def pretrain(self):\n",
        "target_code_len": 25,
        "diff_format": "@@ -247,93 +156,2 @@\n \n-    def load_params(self, path: str):\n-        \"\"\"Load model and optimizer parameters.\"\"\"\n-        if not os.path.exists(path):\n-            print(\"[ERROR] the input path does not exist. ->\", path)\n-            return\n-\n-        params = torch.load(path)\n-        self.actor.load_state_dict(params[\"actor\"])\n-        self.qf_1.load_state_dict(params[\"qf_1\"])\n-        self.qf_2.load_state_dict(params[\"qf_2\"])\n-        self.vf.load_state_dict(params[\"vf\"])\n-        self.vf_target.load_state_dict(params[\"vf_target\"])\n-        self.actor_optimizer.load_state_dict(params[\"actor_optim\"])\n-        self.qf_1_optimizer.load_state_dict(params[\"qf_1_optim\"])\n-        self.qf_2_optimizer.load_state_dict(params[\"qf_2_optim\"])\n-        self.vf_optimizer.load_state_dict(params[\"vf_optim\"])\n-\n-        if self.hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n-            self.alpha_optimizer.load_state_dict(params[\"alpha_optim\"])\n-\n-        print(\"[INFO] loaded the model and optimizer from\", path)\n-\n-    def save_params(self, n_episode: int):\n-        \"\"\"Save model and optimizer parameters.\"\"\"\n-        params = {\n-            \"actor\": self.actor.state_dict(),\n-            \"qf_1\": self.qf_1.state_dict(),\n-            \"qf_2\": self.qf_2.state_dict(),\n-            \"vf\": self.vf.state_dict(),\n-            \"vf_target\": self.vf_target.state_dict(),\n-            \"actor_optim\": self.actor_optimizer.state_dict(),\n-            \"qf_1_optim\": self.qf_1_optimizer.state_dict(),\n-            \"qf_2_optim\": self.qf_2_optimizer.state_dict(),\n-            \"vf_optim\": self.vf_optimizer.state_dict(),\n-        }\n-\n-        if self.hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n-            params[\"alpha_optim\"] = self.alpha_optimizer.state_dict()\n-\n-        AbstractAgent.save_params(self, params, n_episode)\n-\n-    def write_log(\n-        self, i: int, loss: np.ndarray, score: int = 0, delayed_update: int = 1\n-    ):\n-        \"\"\"Write log about loss and score\"\"\"\n-        total_loss = loss.sum()\n-\n-        print(\n-            \"[INFO] episode %d, episode_step %d, total step %d, total score: %d\\n\"\n-            \"total loss: %.3f actor_loss: %.3f qf_1_loss: %.3f qf_2_loss: %.3f \"\n-            \"vf_loss: %.3f alpha_loss: %.3f\\n\"\n-            % (\n-                i,\n-                self.episode_step,\n-                self.total_step,\n-                score,\n-                total_loss,\n-                loss[0] * delayed_update,  # actor loss\n-                loss[1],  # qf_1 loss\n-                loss[2],  # qf_2 loss\n-                loss[3],  # vf loss\n-                loss[4],  # alpha loss\n-            )\n-        )\n-\n-        if self.args.log:\n-            wandb.log(\n-                {\n-                    \"score\": score,\n-                    \"total loss\": total_loss,\n-                    \"actor loss\": loss[0] * delayed_update,\n-                    \"qf_1 loss\": loss[1],\n-                    \"qf_2 loss\": loss[2],\n-                    \"vf loss\": loss[3],\n-                    \"alpha loss\": loss[4],\n-                }\n-            )\n-\n-        if self.args.log:\n-            wandb.log(\n-                {\n-                    \"score\": score,\n-                    \"total loss\": total_loss,\n-                    \"actor loss\": loss[0] * delayed_update,\n-                    \"qf_1 loss\": loss[1],\n-                    \"qf_2 loss\": loss[2],\n-                    \"vf loss\": loss[3],\n-                    \"alpha loss\": loss[4],\n-                }\n-            )\n-\n     def pretrain(self):\n",
        "source_code_with_indent": "\n    <DED>def load_params(self, path: str):\n        <IND>\"\"\"Load model and optimizer parameters.\"\"\"\n        if not os.path.exists(path):\n            <IND>print(\"[ERROR] the input path does not exist. ->\", path)\n            return\n\n        <DED>params = torch.load(path)\n        self.actor.load_state_dict(params[\"actor\"])\n        self.qf_1.load_state_dict(params[\"qf_1\"])\n        self.qf_2.load_state_dict(params[\"qf_2\"])\n        self.vf.load_state_dict(params[\"vf\"])\n        self.vf_target.load_state_dict(params[\"vf_target\"])\n        self.actor_optimizer.load_state_dict(params[\"actor_optim\"])\n        self.qf_1_optimizer.load_state_dict(params[\"qf_1_optim\"])\n        self.qf_2_optimizer.load_state_dict(params[\"qf_2_optim\"])\n        self.vf_optimizer.load_state_dict(params[\"vf_optim\"])\n\n        if self.hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n            <IND>self.alpha_optimizer.load_state_dict(params[\"alpha_optim\"])\n\n        <DED>print(\"[INFO] loaded the model and optimizer from\", path)\n\n    <DED>def save_params(self, n_episode: int):\n        <IND>\"\"\"Save model and optimizer parameters.\"\"\"\n        params = {\n            \"actor\": self.actor.state_dict(),\n            \"qf_1\": self.qf_1.state_dict(),\n            \"qf_2\": self.qf_2.state_dict(),\n            \"vf\": self.vf.state_dict(),\n            \"vf_target\": self.vf_target.state_dict(),\n            \"actor_optim\": self.actor_optimizer.state_dict(),\n            \"qf_1_optim\": self.qf_1_optimizer.state_dict(),\n            \"qf_2_optim\": self.qf_2_optimizer.state_dict(),\n            \"vf_optim\": self.vf_optimizer.state_dict(),\n        }\n\n        if self.hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n            <IND>params[\"alpha_optim\"] = self.alpha_optimizer.state_dict()\n\n        <DED>AbstractAgent.save_params(self, params, n_episode)\n\n    <DED>def write_log(\n        self, i: int, loss: np.ndarray, score: int = 0, delayed_update: int = 1\n    ):\n        <IND>\"\"\"Write log about loss and score\"\"\"\n        total_loss = loss.sum()\n\n        print(\n            \"[INFO] episode %d, episode_step %d, total step %d, total score: %d\\n\"\n            \"total loss: %.3f actor_loss: %.3f qf_1_loss: %.3f qf_2_loss: %.3f \"\n            \"vf_loss: %.3f alpha_loss: %.3f\\n\"\n            % (\n                i,\n                self.episode_step,\n                self.total_step,\n                score,\n                total_loss,\n                loss[0] * delayed_update,  # actor loss\n                loss[1],  # qf_1 loss\n                loss[2],  # qf_2 loss\n                loss[3],  # vf loss\n                loss[4],  # alpha loss\n            )\n        )\n\n        if self.args.log:\n            <IND>wandb.log(\n                {\n                    \"score\": score,\n                    \"total loss\": total_loss,\n                    \"actor loss\": loss[0] * delayed_update,\n                    \"qf_1 loss\": loss[1],\n                    \"qf_2 loss\": loss[2],\n                    \"vf loss\": loss[3],\n                    \"alpha loss\": loss[4],\n                }\n            )\n\n        <DED>if self.args.log:\n            <IND>wandb.log(\n                {\n                    \"score\": score,\n                    \"total loss\": total_loss,\n                    \"actor loss\": loss[0] * delayed_update,\n                    \"qf_1 loss\": loss[1],\n                    \"qf_2 loss\": loss[2],\n                    \"vf loss\": loss[3],\n                    \"alpha loss\": loss[4],\n                }\n            )\n\n    <DED><DED>def pretrain(self):\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def pretrain(self):\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "medipixel/rl_algorithms",
    "commit": "509a777b6fb0b5fa5c4478ccbd112e162dc4f104",
    "filename": "algorithms/fd/sac_agent.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/medipixel-rl_algorithms/algorithms/fd/sac_agent.py",
    "file_hunks_size": 11,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "algorithms/fd/sac_agent.py:289:4 Inconsistent override [14]: `algorithms.fd.sac_agent.Agent.write_log` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `algorithms.fd.sac_agent.Agent.write_log` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 289,
    "warning_line": "    def write_log(",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\n    def load_params(self, path: str):\n        \"\"\"Load model and optimizer parameters.\"\"\"\n        if not os.path.exists(path):\n            print(\"[ERROR] the input path does not exist. ->\", path)\n            return\n\n        params = torch.load(path)\n        self.actor.load_state_dict(params[\"actor\"])\n        self.qf_1.load_state_dict(params[\"qf_1\"])\n        self.qf_2.load_state_dict(params[\"qf_2\"])\n        self.vf.load_state_dict(params[\"vf\"])\n        self.vf_target.load_state_dict(params[\"vf_target\"])\n        self.actor_optimizer.load_state_dict(params[\"actor_optim\"])\n        self.qf_1_optimizer.load_state_dict(params[\"qf_1_optim\"])\n        self.qf_2_optimizer.load_state_dict(params[\"qf_2_optim\"])\n        self.vf_optimizer.load_state_dict(params[\"vf_optim\"])\n\n        if self.hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n            self.alpha_optimizer.load_state_dict(params[\"alpha_optim\"])\n\n        print(\"[INFO] loaded the model and optimizer from\", path)\n\n    def save_params(self, n_episode: int):\n        \"\"\"Save model and optimizer parameters.\"\"\"\n        params = {\n            \"actor\": self.actor.state_dict(),\n            \"qf_1\": self.qf_1.state_dict(),\n            \"qf_2\": self.qf_2.state_dict(),\n            \"vf\": self.vf.state_dict(),\n            \"vf_target\": self.vf_target.state_dict(),\n            \"actor_optim\": self.actor_optimizer.state_dict(),\n            \"qf_1_optim\": self.qf_1_optimizer.state_dict(),\n            \"qf_2_optim\": self.qf_2_optimizer.state_dict(),\n            \"vf_optim\": self.vf_optimizer.state_dict(),\n        }\n\n        if self.hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n            params[\"alpha_optim\"] = self.alpha_optimizer.state_dict()\n\n        AbstractAgent.save_params(self, params, n_episode)\n\n    def write_log(\n        self, i: int, loss: np.ndarray, score: int = 0, delayed_update: int = 1\n    ):\n        \"\"\"Write log about loss and score\"\"\"\n        total_loss = loss.sum()\n\n        print(\n            \"[INFO] episode %d, episode_step %d, total step %d, total score: %d\\n\"\n            \"total loss: %.3f actor_loss: %.3f qf_1_loss: %.3f qf_2_loss: %.3f \"\n            \"vf_loss: %.3f alpha_loss: %.3f\\n\"\n            % (\n                i,\n                self.episode_step,\n                self.total_step,\n                score,\n                total_loss,\n                loss[0] * delayed_update,  # actor loss\n                loss[1],  # qf_1 loss\n                loss[2],  # qf_2 loss\n                loss[3],  # vf loss\n                loss[4],  # alpha loss\n            )\n        )\n\n        if self.args.log:\n            wandb.log(\n                {\n                    \"score\": score,\n                    \"total loss\": total_loss,\n                    \"actor loss\": loss[0] * delayed_update,\n                    \"qf_1 loss\": loss[1],\n                    \"qf_2 loss\": loss[2],\n                    \"vf loss\": loss[3],\n                    \"alpha loss\": loss[4],\n                }\n            )\n\n        if self.args.log:\n            wandb.log(\n                {\n                    \"score\": score,\n                    \"total loss\": total_loss,\n                    \"actor loss\": loss[0] * delayed_update,\n                    \"qf_1 loss\": loss[1],\n                    \"qf_2 loss\": loss[2],\n                    \"vf loss\": loss[3],\n                    \"alpha loss\": loss[4],\n                }\n            )\n\n    def pretrain(self):\n",
        "source_code_len": 3376,
        "target_code": "\n    def pretrain(self):\n",
        "target_code_len": 25,
        "diff_format": "@@ -247,93 +156,2 @@\n \n-    def load_params(self, path: str):\n-        \"\"\"Load model and optimizer parameters.\"\"\"\n-        if not os.path.exists(path):\n-            print(\"[ERROR] the input path does not exist. ->\", path)\n-            return\n-\n-        params = torch.load(path)\n-        self.actor.load_state_dict(params[\"actor\"])\n-        self.qf_1.load_state_dict(params[\"qf_1\"])\n-        self.qf_2.load_state_dict(params[\"qf_2\"])\n-        self.vf.load_state_dict(params[\"vf\"])\n-        self.vf_target.load_state_dict(params[\"vf_target\"])\n-        self.actor_optimizer.load_state_dict(params[\"actor_optim\"])\n-        self.qf_1_optimizer.load_state_dict(params[\"qf_1_optim\"])\n-        self.qf_2_optimizer.load_state_dict(params[\"qf_2_optim\"])\n-        self.vf_optimizer.load_state_dict(params[\"vf_optim\"])\n-\n-        if self.hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n-            self.alpha_optimizer.load_state_dict(params[\"alpha_optim\"])\n-\n-        print(\"[INFO] loaded the model and optimizer from\", path)\n-\n-    def save_params(self, n_episode: int):\n-        \"\"\"Save model and optimizer parameters.\"\"\"\n-        params = {\n-            \"actor\": self.actor.state_dict(),\n-            \"qf_1\": self.qf_1.state_dict(),\n-            \"qf_2\": self.qf_2.state_dict(),\n-            \"vf\": self.vf.state_dict(),\n-            \"vf_target\": self.vf_target.state_dict(),\n-            \"actor_optim\": self.actor_optimizer.state_dict(),\n-            \"qf_1_optim\": self.qf_1_optimizer.state_dict(),\n-            \"qf_2_optim\": self.qf_2_optimizer.state_dict(),\n-            \"vf_optim\": self.vf_optimizer.state_dict(),\n-        }\n-\n-        if self.hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n-            params[\"alpha_optim\"] = self.alpha_optimizer.state_dict()\n-\n-        AbstractAgent.save_params(self, params, n_episode)\n-\n-    def write_log(\n-        self, i: int, loss: np.ndarray, score: int = 0, delayed_update: int = 1\n-    ):\n-        \"\"\"Write log about loss and score\"\"\"\n-        total_loss = loss.sum()\n-\n-        print(\n-            \"[INFO] episode %d, episode_step %d, total step %d, total score: %d\\n\"\n-            \"total loss: %.3f actor_loss: %.3f qf_1_loss: %.3f qf_2_loss: %.3f \"\n-            \"vf_loss: %.3f alpha_loss: %.3f\\n\"\n-            % (\n-                i,\n-                self.episode_step,\n-                self.total_step,\n-                score,\n-                total_loss,\n-                loss[0] * delayed_update,  # actor loss\n-                loss[1],  # qf_1 loss\n-                loss[2],  # qf_2 loss\n-                loss[3],  # vf loss\n-                loss[4],  # alpha loss\n-            )\n-        )\n-\n-        if self.args.log:\n-            wandb.log(\n-                {\n-                    \"score\": score,\n-                    \"total loss\": total_loss,\n-                    \"actor loss\": loss[0] * delayed_update,\n-                    \"qf_1 loss\": loss[1],\n-                    \"qf_2 loss\": loss[2],\n-                    \"vf loss\": loss[3],\n-                    \"alpha loss\": loss[4],\n-                }\n-            )\n-\n-        if self.args.log:\n-            wandb.log(\n-                {\n-                    \"score\": score,\n-                    \"total loss\": total_loss,\n-                    \"actor loss\": loss[0] * delayed_update,\n-                    \"qf_1 loss\": loss[1],\n-                    \"qf_2 loss\": loss[2],\n-                    \"vf loss\": loss[3],\n-                    \"alpha loss\": loss[4],\n-                }\n-            )\n-\n     def pretrain(self):\n",
        "source_code_with_indent": "\n    <DED>def load_params(self, path: str):\n        <IND>\"\"\"Load model and optimizer parameters.\"\"\"\n        if not os.path.exists(path):\n            <IND>print(\"[ERROR] the input path does not exist. ->\", path)\n            return\n\n        <DED>params = torch.load(path)\n        self.actor.load_state_dict(params[\"actor\"])\n        self.qf_1.load_state_dict(params[\"qf_1\"])\n        self.qf_2.load_state_dict(params[\"qf_2\"])\n        self.vf.load_state_dict(params[\"vf\"])\n        self.vf_target.load_state_dict(params[\"vf_target\"])\n        self.actor_optimizer.load_state_dict(params[\"actor_optim\"])\n        self.qf_1_optimizer.load_state_dict(params[\"qf_1_optim\"])\n        self.qf_2_optimizer.load_state_dict(params[\"qf_2_optim\"])\n        self.vf_optimizer.load_state_dict(params[\"vf_optim\"])\n\n        if self.hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n            <IND>self.alpha_optimizer.load_state_dict(params[\"alpha_optim\"])\n\n        <DED>print(\"[INFO] loaded the model and optimizer from\", path)\n\n    <DED>def save_params(self, n_episode: int):\n        <IND>\"\"\"Save model and optimizer parameters.\"\"\"\n        params = {\n            \"actor\": self.actor.state_dict(),\n            \"qf_1\": self.qf_1.state_dict(),\n            \"qf_2\": self.qf_2.state_dict(),\n            \"vf\": self.vf.state_dict(),\n            \"vf_target\": self.vf_target.state_dict(),\n            \"actor_optim\": self.actor_optimizer.state_dict(),\n            \"qf_1_optim\": self.qf_1_optimizer.state_dict(),\n            \"qf_2_optim\": self.qf_2_optimizer.state_dict(),\n            \"vf_optim\": self.vf_optimizer.state_dict(),\n        }\n\n        if self.hyper_params[\"AUTO_ENTROPY_TUNING\"]:\n            <IND>params[\"alpha_optim\"] = self.alpha_optimizer.state_dict()\n\n        <DED>AbstractAgent.save_params(self, params, n_episode)\n\n    <DED>def write_log(\n        self, i: int, loss: np.ndarray, score: int = 0, delayed_update: int = 1\n    ):\n        <IND>\"\"\"Write log about loss and score\"\"\"\n        total_loss = loss.sum()\n\n        print(\n            \"[INFO] episode %d, episode_step %d, total step %d, total score: %d\\n\"\n            \"total loss: %.3f actor_loss: %.3f qf_1_loss: %.3f qf_2_loss: %.3f \"\n            \"vf_loss: %.3f alpha_loss: %.3f\\n\"\n            % (\n                i,\n                self.episode_step,\n                self.total_step,\n                score,\n                total_loss,\n                loss[0] * delayed_update,  # actor loss\n                loss[1],  # qf_1 loss\n                loss[2],  # qf_2 loss\n                loss[3],  # vf loss\n                loss[4],  # alpha loss\n            )\n        )\n\n        if self.args.log:\n            <IND>wandb.log(\n                {\n                    \"score\": score,\n                    \"total loss\": total_loss,\n                    \"actor loss\": loss[0] * delayed_update,\n                    \"qf_1 loss\": loss[1],\n                    \"qf_2 loss\": loss[2],\n                    \"vf loss\": loss[3],\n                    \"alpha loss\": loss[4],\n                }\n            )\n\n        <DED>if self.args.log:\n            <IND>wandb.log(\n                {\n                    \"score\": score,\n                    \"total loss\": total_loss,\n                    \"actor loss\": loss[0] * delayed_update,\n                    \"qf_1 loss\": loss[1],\n                    \"qf_2 loss\": loss[2],\n                    \"vf loss\": loss[3],\n                    \"alpha loss\": loss[4],\n                }\n            )\n\n    <DED><DED>def pretrain(self):\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def pretrain(self):\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "medipixel/rl_algorithms",
    "commit": "509a777b6fb0b5fa5c4478ccbd112e162dc4f104",
    "filename": "algorithms/per/ddpg_agent.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/medipixel-rl_algorithms/algorithms/per/ddpg_agent.py",
    "file_hunks_size": 8,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "algorithms/per/ddpg_agent.py:126:4 Inconsistent override [14]: `algorithms.per.ddpg_agent.Agent.update_model` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `algorithms.per.ddpg_agent.Agent.update_model` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 126,
    "warning_line": "    def update_model("
  },
  {
    "project": "medipixel/rl_algorithms",
    "commit": "509a777b6fb0b5fa5c4478ccbd112e162dc4f104",
    "filename": "algorithms/per/ddpg_agent.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/medipixel-rl_algorithms/algorithms/per/ddpg_agent.py",
    "file_hunks_size": 8,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "algorithms/per/ddpg_agent.py:179:4 Inconsistent override [14]: `algorithms.per.ddpg_agent.Agent.load_params` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `algorithms.per.ddpg_agent.Agent.load_params` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 179,
    "warning_line": "    def load_params(self, path: str):"
  },
  {
    "project": "medipixel/rl_algorithms",
    "commit": "509a777b6fb0b5fa5c4478ccbd112e162dc4f104",
    "filename": "algorithms/per/ddpg_agent.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/medipixel-rl_algorithms/algorithms/per/ddpg_agent.py",
    "file_hunks_size": 8,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "algorithms/per/ddpg_agent.py:194:4 Inconsistent override [14]: `algorithms.per.ddpg_agent.Agent.save_params` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `params` in overriding signature.",
    "message": " `algorithms.per.ddpg_agent.Agent.save_params` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `params` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 194,
    "warning_line": "    def save_params(self, n_episode: int):"
  },
  {
    "project": "medipixel/rl_algorithms",
    "commit": "509a777b6fb0b5fa5c4478ccbd112e162dc4f104",
    "filename": "algorithms/per/ddpg_agent.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/medipixel-rl_algorithms/algorithms/per/ddpg_agent.py",
    "file_hunks_size": 8,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "algorithms/per/ddpg_agent.py:207:4 Inconsistent override [14]: `algorithms.per.ddpg_agent.Agent.write_log` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `algorithms.per.ddpg_agent.Agent.write_log` overrides method defined in `AbstractAgent` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 207,
    "warning_line": "    def write_log(self, i: int, loss: np.ndarray, score: int):"
  }
]