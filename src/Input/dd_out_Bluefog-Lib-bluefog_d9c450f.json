[
  {
    "project": "Bluefog-Lib/bluefog",
    "commit": "d9c450f2bdac11ff719b7e5f5434ee5ff56499b4",
    "filename": "bluefog/torch/mpi_ops.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Bluefog-Lib-bluefog/bluefog/torch/mpi_ops.py",
    "file_hunks_size": 19,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "bluefog/torch/mpi_ops.py:650:36 Incompatible variable type [9]: neighbor_machine_weights is declared to have type `Dict[int, float]` but is used as type `None`.",
    "message": " neighbor_machine_weights is declared to have type `Dict[int, float]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 650,
    "warning_line": "                                    neighbor_machine_weights: Dict[int, float] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n# TODO(hanbinhu) #81 Add dst_weight for hierarchical neighbor allreduce.\ndef hierarchical_neighbor_allreduce(tensor: torch.Tensor,\n                                    self_weight: float = None,\n                                    neighbor_machine_weights: Dict[int, float] = None,\n                                    send_neighbor_machines: List[int] = None,\n                                    enable_topo_check: bool = False,\n                                    name: Optional[str] = None) -> torch.Tensor:\n    \"\"\"\n    A function that performs weighted averaging of the input tensor over the negihbor machines and\n    itself in the Bluefog processes. It is similar to neighbor_allreduce. But each machine runs\n",
        "source_code_len": 713,
        "target_code": "\n@deprecated_function_arg(arg_name=\"neighbor_machine_weights\", fix=\"Use src_machine_weights instead\")\n@deprecated_function_arg(arg_name=\"send_neighbor_machines\", fix=\"Use dst_machine_weights instead\")\ndef hierarchical_neighbor_allreduce(tensor: torch.Tensor, *,\n                                    self_weight: Optional[float] = None,\n                                    src_machine_weights: Optional[Dict[int, float]] = None,\n                                    dst_machine_weights: Optional[Union[Dict[int, float],\n                                                                        List[int]]] = None,\n                                    enable_topo_check: Optional[bool] = False,\n                                    name: Optional[str] = None) -> torch.Tensor:\n    \"\"\"\n    A function that performs weighted averaging of the input tensor over the neighbor machines and\n    itself in the Bluefog processes. It is similar to neighbor_allreduce. But each machine runs\n",
        "target_code_len": 972,
        "diff_format": "@@ -646,11 +646,13 @@\n \n-# TODO(hanbinhu) #81 Add dst_weight for hierarchical neighbor allreduce.\n-def hierarchical_neighbor_allreduce(tensor: torch.Tensor,\n-                                    self_weight: float = None,\n-                                    neighbor_machine_weights: Dict[int, float] = None,\n-                                    send_neighbor_machines: List[int] = None,\n-                                    enable_topo_check: bool = False,\n+@deprecated_function_arg(arg_name=\"neighbor_machine_weights\", fix=\"Use src_machine_weights instead\")\n+@deprecated_function_arg(arg_name=\"send_neighbor_machines\", fix=\"Use dst_machine_weights instead\")\n+def hierarchical_neighbor_allreduce(tensor: torch.Tensor, *,\n+                                    self_weight: Optional[float] = None,\n+                                    src_machine_weights: Optional[Dict[int, float]] = None,\n+                                    dst_machine_weights: Optional[Union[Dict[int, float],\n+                                                                        List[int]]] = None,\n+                                    enable_topo_check: Optional[bool] = False,\n                                     name: Optional[str] = None) -> torch.Tensor:\n     \"\"\"\n-    A function that performs weighted averaging of the input tensor over the negihbor machines and\n+    A function that performs weighted averaging of the input tensor over the neighbor machines and\n     itself in the Bluefog processes. It is similar to neighbor_allreduce. But each machine runs\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n# TODO(hanbinhu) #81 Add dst_weight for hierarchical neighbor allreduce.\n<DED>def hierarchical_neighbor_allreduce(tensor: torch.Tensor,\n                                    self_weight: float = None,\n                                    neighbor_machine_weights: Dict[int, float] = None,\n                                    send_neighbor_machines: List[int] = None,\n                                    enable_topo_check: bool = False,\n                                    name: Optional[str] = None) -> torch.Tensor:\n    <IND>",
        "target_code_with_indent": "\n<DED>@deprecated_function_arg(arg_name=\"neighbor_machine_weights\", fix=\"Use src_machine_weights instead\")\n@deprecated_function_arg(arg_name=\"send_neighbor_machines\", fix=\"Use dst_machine_weights instead\")\ndef hierarchical_neighbor_allreduce(tensor: torch.Tensor, *,\n                                    self_weight: Optional[float] = None,\n                                    src_machine_weights: Optional[Dict[int, float]] = None,\n                                    dst_machine_weights: Optional[Union[Dict[int, float],\n                                                                        List[int]]] = None,\n                                    enable_topo_check: Optional[bool] = False,\n                                    name: Optional[str] = None) -> torch.Tensor:\n    <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    \"\"\"\n    # TODO(hhb) Implement the logics for topo check for hierarchical_neighbor_allreduce.\n\n    # TODO(ybc) add check for self_weight and neighbor_machine_weights.\n    if (self_weight is None and neighbor_machine_weights is not None) or \\\n       (self_weight is not None and send_neighbor_machines is None):\n        raise ValueError(\"Arguments self_weight and neighbor_machine_weights have to \"\n                         \"be presented at the same time\")\n    handle = hierarchical_neighbor_allreduce_nonblocking(\n        tensor, self_weight, neighbor_machine_weights, send_neighbor_machines,\n        enable_topo_check, name)\n    return synchronize(handle)\n\n\ndef hierarchical_neighbor_allreduce_nonblocking(tensor: torch.Tensor,\n                                                self_weight: float = None,\n                                                neighbor_machine_weights: Dict[int, float] = None,\n                                                send_neighbor_machines: List[int] = None,\n                                                enable_topo_check: bool = False,\n                                                name: Optional[str] = None) -> int:\n    \"\"\"\n",
        "source_code_len": 1169,
        "target_code": "    \"\"\"\n    # TODO(ybc) add check for self_weight and neighbor_machine_weights.\n    if (self_weight is None and src_machine_weights is not None) or \\\n       (self_weight is not None and src_machine_weights is None):\n        raise ValueError(\"Arguments self_weight and src_machine_weights have to be presented at \"\n                         \"the same time\")\n    handle = hierarchical_neighbor_allreduce_nonblocking(tensor,\n                                                         self_weight=self_weight,\n                                                         src_machine_weights=src_machine_weights,\n                                                         dst_machine_weights=dst_machine_weights,\n                                                         enable_topo_check=enable_topo_check,\n                                                         name=name)\n    return synchronize(handle)\n\n@deprecated_function_arg(arg_name=\"neighbor_machine_weights\", fix=\"Use src_machine_weights instead\")\n@deprecated_function_arg(arg_name=\"send_neighbor_machines\", fix=\"Use dst_machine_weights instead\")\ndef hierarchical_neighbor_allreduce_nonblocking(\n    tensor: torch.Tensor, *,\n    self_weight: Optional[float] = None,\n    src_machine_weights: Optional[Dict[int, float]] = None,\n    dst_machine_weights: Optional[Union[List[int],\n                                        Dict[int, float]]] = None,\n    enable_topo_check: Optional[bool] = False,\n    name: Optional[str] = None) -> int:\n    \"\"\"\n",
        "target_code_len": 1485,
        "diff_format": "@@ -685,21 +697,25 @@\n     \"\"\"\n-    # TODO(hhb) Implement the logics for topo check for hierarchical_neighbor_allreduce.\n-\n     # TODO(ybc) add check for self_weight and neighbor_machine_weights.\n-    if (self_weight is None and neighbor_machine_weights is not None) or \\\n-       (self_weight is not None and send_neighbor_machines is None):\n-        raise ValueError(\"Arguments self_weight and neighbor_machine_weights have to \"\n-                         \"be presented at the same time\")\n-    handle = hierarchical_neighbor_allreduce_nonblocking(\n-        tensor, self_weight, neighbor_machine_weights, send_neighbor_machines,\n-        enable_topo_check, name)\n+    if (self_weight is None and src_machine_weights is not None) or \\\n+       (self_weight is not None and src_machine_weights is None):\n+        raise ValueError(\"Arguments self_weight and src_machine_weights have to be presented at \"\n+                         \"the same time\")\n+    handle = hierarchical_neighbor_allreduce_nonblocking(tensor,\n+                                                         self_weight=self_weight,\n+                                                         src_machine_weights=src_machine_weights,\n+                                                         dst_machine_weights=dst_machine_weights,\n+                                                         enable_topo_check=enable_topo_check,\n+                                                         name=name)\n     return synchronize(handle)\n \n-\n-def hierarchical_neighbor_allreduce_nonblocking(tensor: torch.Tensor,\n-                                                self_weight: float = None,\n-                                                neighbor_machine_weights: Dict[int, float] = None,\n-                                                send_neighbor_machines: List[int] = None,\n-                                                enable_topo_check: bool = False,\n-                                                name: Optional[str] = None) -> int:\n+@deprecated_function_arg(arg_name=\"neighbor_machine_weights\", fix=\"Use src_machine_weights instead\")\n+@deprecated_function_arg(arg_name=\"send_neighbor_machines\", fix=\"Use dst_machine_weights instead\")\n+def hierarchical_neighbor_allreduce_nonblocking(\n+    tensor: torch.Tensor, *,\n+    self_weight: Optional[float] = None,\n+    src_machine_weights: Optional[Dict[int, float]] = None,\n+    dst_machine_weights: Optional[Union[List[int],\n+                                        Dict[int, float]]] = None,\n+    enable_topo_check: Optional[bool] = False,\n+    name: Optional[str] = None) -> int:\n     \"\"\"\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n    # TODO(hhb) Implement the logics for topo check for hierarchical_neighbor_allreduce.\n\n    # TODO(ybc) add check for self_weight and neighbor_machine_weights.\n    if (self_weight is None and neighbor_machine_weights is not None) or       (self_weight is not None and send_neighbor_machines is None):\n        <IND>raise ValueError(\"Arguments self_weight and neighbor_machine_weights have to \"\n                         \"be presented at the same time\")\n    <DED>handle = hierarchical_neighbor_allreduce_nonblocking(\n        tensor, self_weight, neighbor_machine_weights, send_neighbor_machines,\n        enable_topo_check, name)\n    return synchronize(handle)\n\n\n<DED>def hierarchical_neighbor_allreduce_nonblocking(tensor: torch.Tensor,\n                                                self_weight: float = None,\n                                                neighbor_machine_weights: Dict[int, float] = None,\n                                                send_neighbor_machines: List[int] = None,\n                                                enable_topo_check: bool = False,\n                                                name: Optional[str] = None) -> int:\n    <IND>",
        "target_code_with_indent": "\n    # TODO(ybc) add check for self_weight and neighbor_machine_weights.\n    if (self_weight is None and src_machine_weights is not None) or       (self_weight is not None and src_machine_weights is None):\n        <IND>raise ValueError(\"Arguments self_weight and src_machine_weights have to be presented at \"\n                         \"the same time\")\n    <DED>handle = hierarchical_neighbor_allreduce_nonblocking(tensor,\n                                                         self_weight=self_weight,\n                                                         src_machine_weights=src_machine_weights,\n                                                         dst_machine_weights=dst_machine_weights,\n                                                         enable_topo_check=enable_topo_check,\n                                                         name=name)\n    return synchronize(handle)\n\n<DED>@deprecated_function_arg(arg_name=\"neighbor_machine_weights\", fix=\"Use src_machine_weights instead\")\n@deprecated_function_arg(arg_name=\"send_neighbor_machines\", fix=\"Use dst_machine_weights instead\")\ndef hierarchical_neighbor_allreduce_nonblocking(\n    tensor: torch.Tensor, *,\n    self_weight: Optional[float] = None,\n    src_machine_weights: Optional[Dict[int, float]] = None,\n    dst_machine_weights: Optional[Union[List[int],\n                                        Dict[int, float]]] = None,\n    enable_topo_check: Optional[bool] = False,\n    name: Optional[str] = None) -> int:\n    <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    \"\"\"\n    if (self_weight is None and neighbor_machine_weights is not None) or \\\n       (self_weight is not None and neighbor_machine_weights is None):\n        raise ValueError(\"Arguments self_weight and neighbor_weights have to be presented at \"\n                         \"the same time\")\n\n    if send_neighbor_machines is None:\n        first_dim = tensor.shape[0] * len(in_neighbor_machine_ranks())\n    else:\n        first_dim = tensor.shape[0] * len(neighbor_machine_weights)\n    new_shape = torch.Size([first_dim] + list(tensor.shape[1:]))\n",
        "source_code_len": 545,
        "target_code": "    \"\"\"\n    # TODO(hanbinhu) #82 Symmetrical argument for self_weight, src_weights, dst_weights\n    if (self_weight is None and src_machine_weights is not None) or \\\n       (self_weight is not None and src_machine_weights is None):\n        raise ValueError(\"Arguments self_weight and src_machine_weights have to be presented at \"\n                         \"the same time\")\n\n    if dst_machine_weights is None:\n        first_dim = tensor.shape[0] * len(in_neighbor_machine_ranks())\n    else:\n        first_dim = tensor.shape[0] * len(dst_machine_weights)\n    new_shape = torch.Size([first_dim] + list(tensor.shape[1:]))\n",
        "target_code_len": 618,
        "diff_format": "@@ -737,11 +763,12 @@\n     \"\"\"\n-    if (self_weight is None and neighbor_machine_weights is not None) or \\\n-       (self_weight is not None and neighbor_machine_weights is None):\n-        raise ValueError(\"Arguments self_weight and neighbor_weights have to be presented at \"\n+    # TODO(hanbinhu) #82 Symmetrical argument for self_weight, src_weights, dst_weights\n+    if (self_weight is None and src_machine_weights is not None) or \\\n+       (self_weight is not None and src_machine_weights is None):\n+        raise ValueError(\"Arguments self_weight and src_machine_weights have to be presented at \"\n                          \"the same time\")\n \n-    if send_neighbor_machines is None:\n+    if dst_machine_weights is None:\n         first_dim = tensor.shape[0] * len(in_neighbor_machine_ranks())\n     else:\n-        first_dim = tensor.shape[0] * len(neighbor_machine_weights)\n+        first_dim = tensor.shape[0] * len(dst_machine_weights)\n     new_shape = torch.Size([first_dim] + list(tensor.shape[1:]))\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n    if (self_weight is None and neighbor_machine_weights is not None) or       (self_weight is not None and neighbor_machine_weights is None):\n        <IND>raise ValueError(\"Arguments self_weight and neighbor_weights have to be presented at \"\n                         \"the same time\")\n\n    <DED>if send_neighbor_machines is None:\n        <IND>first_dim = tensor.shape[0] * len(in_neighbor_machine_ranks())\n    <DED>else:\n        <IND>first_dim = tensor.shape[0] * len(neighbor_machine_weights)\n    <DED>new_shape = torch.Size([first_dim] + list(tensor.shape[1:]))\n",
        "target_code_with_indent": "\n    # TODO(hanbinhu) #82 Symmetrical argument for self_weight, src_weights, dst_weights\n    if (self_weight is None and src_machine_weights is not None) or       (self_weight is not None and src_machine_weights is None):\n        <IND>raise ValueError(\"Arguments self_weight and src_machine_weights have to be presented at \"\n                         \"the same time\")\n\n    <DED>if dst_machine_weights is None:\n        <IND>first_dim = tensor.shape[0] * len(in_neighbor_machine_ranks())\n    <DED>else:\n        <IND>first_dim = tensor.shape[0] * len(dst_machine_weights)\n    <DED>new_shape = torch.Size([first_dim] + list(tensor.shape[1:]))\n"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    return _hierarchical_neighbor_allreduce_nonblocking(\n        tensor, output, self_weight, neighbor_machine_weights,\n        send_neighbor_machines, enable_topo_check, name=name)\n\n",
        "source_code_len": 183,
        "target_code": "    return _hierarchical_neighbor_allreduce_nonblocking(\n        tensor, output, self_weight, src_machine_weights,\n        dst_machine_weights, enable_topo_check, name=name)\n\n",
        "target_code_len": 175,
        "diff_format": "@@ -750,4 +777,4 @@\n     return _hierarchical_neighbor_allreduce_nonblocking(\n-        tensor, output, self_weight, neighbor_machine_weights,\n-        send_neighbor_machines, enable_topo_check, name=name)\n+        tensor, output, self_weight, src_machine_weights,\n+        dst_machine_weights, enable_topo_check, name=name)\n \n",
        "source_code_with_indent": "    return _hierarchical_neighbor_allreduce_nonblocking(\n        tensor, output, self_weight, neighbor_machine_weights,\n        send_neighbor_machines, enable_topo_check, name=name)\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    return _hierarchical_neighbor_allreduce_nonblocking(\n        tensor, output, self_weight, src_machine_weights,\n        dst_machine_weights, enable_topo_check, name=name)\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "Bluefog-Lib/bluefog",
    "commit": "d9c450f2bdac11ff719b7e5f5434ee5ff56499b4",
    "filename": "bluefog/torch/mpi_ops.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Bluefog-Lib-bluefog/bluefog/torch/mpi_ops.py",
    "file_hunks_size": 19,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "bluefog/torch/mpi_ops.py:651:36 Incompatible variable type [9]: send_neighbor_machines is declared to have type `List[int]` but is used as type `None`.",
    "message": " send_neighbor_machines is declared to have type `List[int]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 651,
    "warning_line": "                                    send_neighbor_machines: List[int] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n# TODO(hanbinhu) #81 Add dst_weight for hierarchical neighbor allreduce.\ndef hierarchical_neighbor_allreduce(tensor: torch.Tensor,\n                                    self_weight: float = None,\n                                    neighbor_machine_weights: Dict[int, float] = None,\n                                    send_neighbor_machines: List[int] = None,\n                                    enable_topo_check: bool = False,\n                                    name: Optional[str] = None) -> torch.Tensor:\n    \"\"\"\n    A function that performs weighted averaging of the input tensor over the negihbor machines and\n    itself in the Bluefog processes. It is similar to neighbor_allreduce. But each machine runs\n",
        "source_code_len": 713,
        "target_code": "\n@deprecated_function_arg(arg_name=\"neighbor_machine_weights\", fix=\"Use src_machine_weights instead\")\n@deprecated_function_arg(arg_name=\"send_neighbor_machines\", fix=\"Use dst_machine_weights instead\")\ndef hierarchical_neighbor_allreduce(tensor: torch.Tensor, *,\n                                    self_weight: Optional[float] = None,\n                                    src_machine_weights: Optional[Dict[int, float]] = None,\n                                    dst_machine_weights: Optional[Union[Dict[int, float],\n                                                                        List[int]]] = None,\n                                    enable_topo_check: Optional[bool] = False,\n                                    name: Optional[str] = None) -> torch.Tensor:\n    \"\"\"\n    A function that performs weighted averaging of the input tensor over the neighbor machines and\n    itself in the Bluefog processes. It is similar to neighbor_allreduce. But each machine runs\n",
        "target_code_len": 972,
        "diff_format": "@@ -646,11 +646,13 @@\n \n-# TODO(hanbinhu) #81 Add dst_weight for hierarchical neighbor allreduce.\n-def hierarchical_neighbor_allreduce(tensor: torch.Tensor,\n-                                    self_weight: float = None,\n-                                    neighbor_machine_weights: Dict[int, float] = None,\n-                                    send_neighbor_machines: List[int] = None,\n-                                    enable_topo_check: bool = False,\n+@deprecated_function_arg(arg_name=\"neighbor_machine_weights\", fix=\"Use src_machine_weights instead\")\n+@deprecated_function_arg(arg_name=\"send_neighbor_machines\", fix=\"Use dst_machine_weights instead\")\n+def hierarchical_neighbor_allreduce(tensor: torch.Tensor, *,\n+                                    self_weight: Optional[float] = None,\n+                                    src_machine_weights: Optional[Dict[int, float]] = None,\n+                                    dst_machine_weights: Optional[Union[Dict[int, float],\n+                                                                        List[int]]] = None,\n+                                    enable_topo_check: Optional[bool] = False,\n                                     name: Optional[str] = None) -> torch.Tensor:\n     \"\"\"\n-    A function that performs weighted averaging of the input tensor over the negihbor machines and\n+    A function that performs weighted averaging of the input tensor over the neighbor machines and\n     itself in the Bluefog processes. It is similar to neighbor_allreduce. But each machine runs\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n# TODO(hanbinhu) #81 Add dst_weight for hierarchical neighbor allreduce.\n<DED>def hierarchical_neighbor_allreduce(tensor: torch.Tensor,\n                                    self_weight: float = None,\n                                    neighbor_machine_weights: Dict[int, float] = None,\n                                    send_neighbor_machines: List[int] = None,\n                                    enable_topo_check: bool = False,\n                                    name: Optional[str] = None) -> torch.Tensor:\n    <IND>",
        "target_code_with_indent": "\n<DED>@deprecated_function_arg(arg_name=\"neighbor_machine_weights\", fix=\"Use src_machine_weights instead\")\n@deprecated_function_arg(arg_name=\"send_neighbor_machines\", fix=\"Use dst_machine_weights instead\")\ndef hierarchical_neighbor_allreduce(tensor: torch.Tensor, *,\n                                    self_weight: Optional[float] = None,\n                                    src_machine_weights: Optional[Dict[int, float]] = None,\n                                    dst_machine_weights: Optional[Union[Dict[int, float],\n                                                                        List[int]]] = None,\n                                    enable_topo_check: Optional[bool] = False,\n                                    name: Optional[str] = None) -> torch.Tensor:\n    <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    \"\"\"\n    # TODO(hhb) Implement the logics for topo check for hierarchical_neighbor_allreduce.\n\n    # TODO(ybc) add check for self_weight and neighbor_machine_weights.\n    if (self_weight is None and neighbor_machine_weights is not None) or \\\n       (self_weight is not None and send_neighbor_machines is None):\n        raise ValueError(\"Arguments self_weight and neighbor_machine_weights have to \"\n                         \"be presented at the same time\")\n    handle = hierarchical_neighbor_allreduce_nonblocking(\n        tensor, self_weight, neighbor_machine_weights, send_neighbor_machines,\n        enable_topo_check, name)\n    return synchronize(handle)\n\n\ndef hierarchical_neighbor_allreduce_nonblocking(tensor: torch.Tensor,\n                                                self_weight: float = None,\n                                                neighbor_machine_weights: Dict[int, float] = None,\n                                                send_neighbor_machines: List[int] = None,\n                                                enable_topo_check: bool = False,\n                                                name: Optional[str] = None) -> int:\n    \"\"\"\n",
        "source_code_len": 1169,
        "target_code": "    \"\"\"\n    # TODO(ybc) add check for self_weight and neighbor_machine_weights.\n    if (self_weight is None and src_machine_weights is not None) or \\\n       (self_weight is not None and src_machine_weights is None):\n        raise ValueError(\"Arguments self_weight and src_machine_weights have to be presented at \"\n                         \"the same time\")\n    handle = hierarchical_neighbor_allreduce_nonblocking(tensor,\n                                                         self_weight=self_weight,\n                                                         src_machine_weights=src_machine_weights,\n                                                         dst_machine_weights=dst_machine_weights,\n                                                         enable_topo_check=enable_topo_check,\n                                                         name=name)\n    return synchronize(handle)\n\n@deprecated_function_arg(arg_name=\"neighbor_machine_weights\", fix=\"Use src_machine_weights instead\")\n@deprecated_function_arg(arg_name=\"send_neighbor_machines\", fix=\"Use dst_machine_weights instead\")\ndef hierarchical_neighbor_allreduce_nonblocking(\n    tensor: torch.Tensor, *,\n    self_weight: Optional[float] = None,\n    src_machine_weights: Optional[Dict[int, float]] = None,\n    dst_machine_weights: Optional[Union[List[int],\n                                        Dict[int, float]]] = None,\n    enable_topo_check: Optional[bool] = False,\n    name: Optional[str] = None) -> int:\n    \"\"\"\n",
        "target_code_len": 1485,
        "diff_format": "@@ -685,21 +697,25 @@\n     \"\"\"\n-    # TODO(hhb) Implement the logics for topo check for hierarchical_neighbor_allreduce.\n-\n     # TODO(ybc) add check for self_weight and neighbor_machine_weights.\n-    if (self_weight is None and neighbor_machine_weights is not None) or \\\n-       (self_weight is not None and send_neighbor_machines is None):\n-        raise ValueError(\"Arguments self_weight and neighbor_machine_weights have to \"\n-                         \"be presented at the same time\")\n-    handle = hierarchical_neighbor_allreduce_nonblocking(\n-        tensor, self_weight, neighbor_machine_weights, send_neighbor_machines,\n-        enable_topo_check, name)\n+    if (self_weight is None and src_machine_weights is not None) or \\\n+       (self_weight is not None and src_machine_weights is None):\n+        raise ValueError(\"Arguments self_weight and src_machine_weights have to be presented at \"\n+                         \"the same time\")\n+    handle = hierarchical_neighbor_allreduce_nonblocking(tensor,\n+                                                         self_weight=self_weight,\n+                                                         src_machine_weights=src_machine_weights,\n+                                                         dst_machine_weights=dst_machine_weights,\n+                                                         enable_topo_check=enable_topo_check,\n+                                                         name=name)\n     return synchronize(handle)\n \n-\n-def hierarchical_neighbor_allreduce_nonblocking(tensor: torch.Tensor,\n-                                                self_weight: float = None,\n-                                                neighbor_machine_weights: Dict[int, float] = None,\n-                                                send_neighbor_machines: List[int] = None,\n-                                                enable_topo_check: bool = False,\n-                                                name: Optional[str] = None) -> int:\n+@deprecated_function_arg(arg_name=\"neighbor_machine_weights\", fix=\"Use src_machine_weights instead\")\n+@deprecated_function_arg(arg_name=\"send_neighbor_machines\", fix=\"Use dst_machine_weights instead\")\n+def hierarchical_neighbor_allreduce_nonblocking(\n+    tensor: torch.Tensor, *,\n+    self_weight: Optional[float] = None,\n+    src_machine_weights: Optional[Dict[int, float]] = None,\n+    dst_machine_weights: Optional[Union[List[int],\n+                                        Dict[int, float]]] = None,\n+    enable_topo_check: Optional[bool] = False,\n+    name: Optional[str] = None) -> int:\n     \"\"\"\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n    # TODO(hhb) Implement the logics for topo check for hierarchical_neighbor_allreduce.\n\n    # TODO(ybc) add check for self_weight and neighbor_machine_weights.\n    if (self_weight is None and neighbor_machine_weights is not None) or       (self_weight is not None and send_neighbor_machines is None):\n        <IND>raise ValueError(\"Arguments self_weight and neighbor_machine_weights have to \"\n                         \"be presented at the same time\")\n    <DED>handle = hierarchical_neighbor_allreduce_nonblocking(\n        tensor, self_weight, neighbor_machine_weights, send_neighbor_machines,\n        enable_topo_check, name)\n    return synchronize(handle)\n\n\n<DED>def hierarchical_neighbor_allreduce_nonblocking(tensor: torch.Tensor,\n                                                self_weight: float = None,\n                                                neighbor_machine_weights: Dict[int, float] = None,\n                                                send_neighbor_machines: List[int] = None,\n                                                enable_topo_check: bool = False,\n                                                name: Optional[str] = None) -> int:\n    <IND>",
        "target_code_with_indent": "\n    # TODO(ybc) add check for self_weight and neighbor_machine_weights.\n    if (self_weight is None and src_machine_weights is not None) or       (self_weight is not None and src_machine_weights is None):\n        <IND>raise ValueError(\"Arguments self_weight and src_machine_weights have to be presented at \"\n                         \"the same time\")\n    <DED>handle = hierarchical_neighbor_allreduce_nonblocking(tensor,\n                                                         self_weight=self_weight,\n                                                         src_machine_weights=src_machine_weights,\n                                                         dst_machine_weights=dst_machine_weights,\n                                                         enable_topo_check=enable_topo_check,\n                                                         name=name)\n    return synchronize(handle)\n\n<DED>@deprecated_function_arg(arg_name=\"neighbor_machine_weights\", fix=\"Use src_machine_weights instead\")\n@deprecated_function_arg(arg_name=\"send_neighbor_machines\", fix=\"Use dst_machine_weights instead\")\ndef hierarchical_neighbor_allreduce_nonblocking(\n    tensor: torch.Tensor, *,\n    self_weight: Optional[float] = None,\n    src_machine_weights: Optional[Dict[int, float]] = None,\n    dst_machine_weights: Optional[Union[List[int],\n                                        Dict[int, float]]] = None,\n    enable_topo_check: Optional[bool] = False,\n    name: Optional[str] = None) -> int:\n    <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    \"\"\"\n    if (self_weight is None and neighbor_machine_weights is not None) or \\\n       (self_weight is not None and neighbor_machine_weights is None):\n        raise ValueError(\"Arguments self_weight and neighbor_weights have to be presented at \"\n                         \"the same time\")\n\n    if send_neighbor_machines is None:\n        first_dim = tensor.shape[0] * len(in_neighbor_machine_ranks())\n    else:\n        first_dim = tensor.shape[0] * len(neighbor_machine_weights)\n    new_shape = torch.Size([first_dim] + list(tensor.shape[1:]))\n",
        "source_code_len": 545,
        "target_code": "    \"\"\"\n    # TODO(hanbinhu) #82 Symmetrical argument for self_weight, src_weights, dst_weights\n    if (self_weight is None and src_machine_weights is not None) or \\\n       (self_weight is not None and src_machine_weights is None):\n        raise ValueError(\"Arguments self_weight and src_machine_weights have to be presented at \"\n                         \"the same time\")\n\n    if dst_machine_weights is None:\n        first_dim = tensor.shape[0] * len(in_neighbor_machine_ranks())\n    else:\n        first_dim = tensor.shape[0] * len(dst_machine_weights)\n    new_shape = torch.Size([first_dim] + list(tensor.shape[1:]))\n",
        "target_code_len": 618,
        "diff_format": "@@ -737,11 +763,12 @@\n     \"\"\"\n-    if (self_weight is None and neighbor_machine_weights is not None) or \\\n-       (self_weight is not None and neighbor_machine_weights is None):\n-        raise ValueError(\"Arguments self_weight and neighbor_weights have to be presented at \"\n+    # TODO(hanbinhu) #82 Symmetrical argument for self_weight, src_weights, dst_weights\n+    if (self_weight is None and src_machine_weights is not None) or \\\n+       (self_weight is not None and src_machine_weights is None):\n+        raise ValueError(\"Arguments self_weight and src_machine_weights have to be presented at \"\n                          \"the same time\")\n \n-    if send_neighbor_machines is None:\n+    if dst_machine_weights is None:\n         first_dim = tensor.shape[0] * len(in_neighbor_machine_ranks())\n     else:\n-        first_dim = tensor.shape[0] * len(neighbor_machine_weights)\n+        first_dim = tensor.shape[0] * len(dst_machine_weights)\n     new_shape = torch.Size([first_dim] + list(tensor.shape[1:]))\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n    if (self_weight is None and neighbor_machine_weights is not None) or       (self_weight is not None and neighbor_machine_weights is None):\n        <IND>raise ValueError(\"Arguments self_weight and neighbor_weights have to be presented at \"\n                         \"the same time\")\n\n    <DED>if send_neighbor_machines is None:\n        <IND>first_dim = tensor.shape[0] * len(in_neighbor_machine_ranks())\n    <DED>else:\n        <IND>first_dim = tensor.shape[0] * len(neighbor_machine_weights)\n    <DED>new_shape = torch.Size([first_dim] + list(tensor.shape[1:]))\n",
        "target_code_with_indent": "\n    # TODO(hanbinhu) #82 Symmetrical argument for self_weight, src_weights, dst_weights\n    if (self_weight is None and src_machine_weights is not None) or       (self_weight is not None and src_machine_weights is None):\n        <IND>raise ValueError(\"Arguments self_weight and src_machine_weights have to be presented at \"\n                         \"the same time\")\n\n    <DED>if dst_machine_weights is None:\n        <IND>first_dim = tensor.shape[0] * len(in_neighbor_machine_ranks())\n    <DED>else:\n        <IND>first_dim = tensor.shape[0] * len(dst_machine_weights)\n    <DED>new_shape = torch.Size([first_dim] + list(tensor.shape[1:]))\n"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    return _hierarchical_neighbor_allreduce_nonblocking(\n        tensor, output, self_weight, neighbor_machine_weights,\n        send_neighbor_machines, enable_topo_check, name=name)\n\n",
        "source_code_len": 183,
        "target_code": "    return _hierarchical_neighbor_allreduce_nonblocking(\n        tensor, output, self_weight, src_machine_weights,\n        dst_machine_weights, enable_topo_check, name=name)\n\n",
        "target_code_len": 175,
        "diff_format": "@@ -750,4 +777,4 @@\n     return _hierarchical_neighbor_allreduce_nonblocking(\n-        tensor, output, self_weight, neighbor_machine_weights,\n-        send_neighbor_machines, enable_topo_check, name=name)\n+        tensor, output, self_weight, src_machine_weights,\n+        dst_machine_weights, enable_topo_check, name=name)\n \n",
        "source_code_with_indent": "    return _hierarchical_neighbor_allreduce_nonblocking(\n        tensor, output, self_weight, neighbor_machine_weights,\n        send_neighbor_machines, enable_topo_check, name=name)\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    return _hierarchical_neighbor_allreduce_nonblocking(\n        tensor, output, self_weight, src_machine_weights,\n        dst_machine_weights, enable_topo_check, name=name)\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "Bluefog-Lib/bluefog",
    "commit": "d9c450f2bdac11ff719b7e5f5434ee5ff56499b4",
    "filename": "bluefog/torch/mpi_ops.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Bluefog-Lib-bluefog/bluefog/torch/mpi_ops.py",
    "file_hunks_size": 19,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "bluefog/torch/mpi_ops.py:701:48 Incompatible variable type [9]: neighbor_machine_weights is declared to have type `Dict[int, float]` but is used as type `None`.",
    "message": " neighbor_machine_weights is declared to have type `Dict[int, float]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 701,
    "warning_line": "                                                neighbor_machine_weights: Dict[int, float] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n# TODO(hanbinhu) #81 Add dst_weight for hierarchical neighbor allreduce.\ndef hierarchical_neighbor_allreduce(tensor: torch.Tensor,\n                                    self_weight: float = None,\n                                    neighbor_machine_weights: Dict[int, float] = None,\n                                    send_neighbor_machines: List[int] = None,\n                                    enable_topo_check: bool = False,\n                                    name: Optional[str] = None) -> torch.Tensor:\n    \"\"\"\n    A function that performs weighted averaging of the input tensor over the negihbor machines and\n    itself in the Bluefog processes. It is similar to neighbor_allreduce. But each machine runs\n",
        "source_code_len": 713,
        "target_code": "\n@deprecated_function_arg(arg_name=\"neighbor_machine_weights\", fix=\"Use src_machine_weights instead\")\n@deprecated_function_arg(arg_name=\"send_neighbor_machines\", fix=\"Use dst_machine_weights instead\")\ndef hierarchical_neighbor_allreduce(tensor: torch.Tensor, *,\n                                    self_weight: Optional[float] = None,\n                                    src_machine_weights: Optional[Dict[int, float]] = None,\n                                    dst_machine_weights: Optional[Union[Dict[int, float],\n                                                                        List[int]]] = None,\n                                    enable_topo_check: Optional[bool] = False,\n                                    name: Optional[str] = None) -> torch.Tensor:\n    \"\"\"\n    A function that performs weighted averaging of the input tensor over the neighbor machines and\n    itself in the Bluefog processes. It is similar to neighbor_allreduce. But each machine runs\n",
        "target_code_len": 972,
        "diff_format": "@@ -646,11 +646,13 @@\n \n-# TODO(hanbinhu) #81 Add dst_weight for hierarchical neighbor allreduce.\n-def hierarchical_neighbor_allreduce(tensor: torch.Tensor,\n-                                    self_weight: float = None,\n-                                    neighbor_machine_weights: Dict[int, float] = None,\n-                                    send_neighbor_machines: List[int] = None,\n-                                    enable_topo_check: bool = False,\n+@deprecated_function_arg(arg_name=\"neighbor_machine_weights\", fix=\"Use src_machine_weights instead\")\n+@deprecated_function_arg(arg_name=\"send_neighbor_machines\", fix=\"Use dst_machine_weights instead\")\n+def hierarchical_neighbor_allreduce(tensor: torch.Tensor, *,\n+                                    self_weight: Optional[float] = None,\n+                                    src_machine_weights: Optional[Dict[int, float]] = None,\n+                                    dst_machine_weights: Optional[Union[Dict[int, float],\n+                                                                        List[int]]] = None,\n+                                    enable_topo_check: Optional[bool] = False,\n                                     name: Optional[str] = None) -> torch.Tensor:\n     \"\"\"\n-    A function that performs weighted averaging of the input tensor over the negihbor machines and\n+    A function that performs weighted averaging of the input tensor over the neighbor machines and\n     itself in the Bluefog processes. It is similar to neighbor_allreduce. But each machine runs\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n# TODO(hanbinhu) #81 Add dst_weight for hierarchical neighbor allreduce.\n<DED>def hierarchical_neighbor_allreduce(tensor: torch.Tensor,\n                                    self_weight: float = None,\n                                    neighbor_machine_weights: Dict[int, float] = None,\n                                    send_neighbor_machines: List[int] = None,\n                                    enable_topo_check: bool = False,\n                                    name: Optional[str] = None) -> torch.Tensor:\n    <IND>",
        "target_code_with_indent": "\n<DED>@deprecated_function_arg(arg_name=\"neighbor_machine_weights\", fix=\"Use src_machine_weights instead\")\n@deprecated_function_arg(arg_name=\"send_neighbor_machines\", fix=\"Use dst_machine_weights instead\")\ndef hierarchical_neighbor_allreduce(tensor: torch.Tensor, *,\n                                    self_weight: Optional[float] = None,\n                                    src_machine_weights: Optional[Dict[int, float]] = None,\n                                    dst_machine_weights: Optional[Union[Dict[int, float],\n                                                                        List[int]]] = None,\n                                    enable_topo_check: Optional[bool] = False,\n                                    name: Optional[str] = None) -> torch.Tensor:\n    <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    \"\"\"\n    # TODO(hhb) Implement the logics for topo check for hierarchical_neighbor_allreduce.\n\n    # TODO(ybc) add check for self_weight and neighbor_machine_weights.\n    if (self_weight is None and neighbor_machine_weights is not None) or \\\n       (self_weight is not None and send_neighbor_machines is None):\n        raise ValueError(\"Arguments self_weight and neighbor_machine_weights have to \"\n                         \"be presented at the same time\")\n    handle = hierarchical_neighbor_allreduce_nonblocking(\n        tensor, self_weight, neighbor_machine_weights, send_neighbor_machines,\n        enable_topo_check, name)\n    return synchronize(handle)\n\n\ndef hierarchical_neighbor_allreduce_nonblocking(tensor: torch.Tensor,\n                                                self_weight: float = None,\n                                                neighbor_machine_weights: Dict[int, float] = None,\n                                                send_neighbor_machines: List[int] = None,\n                                                enable_topo_check: bool = False,\n                                                name: Optional[str] = None) -> int:\n    \"\"\"\n",
        "source_code_len": 1169,
        "target_code": "    \"\"\"\n    # TODO(ybc) add check for self_weight and neighbor_machine_weights.\n    if (self_weight is None and src_machine_weights is not None) or \\\n       (self_weight is not None and src_machine_weights is None):\n        raise ValueError(\"Arguments self_weight and src_machine_weights have to be presented at \"\n                         \"the same time\")\n    handle = hierarchical_neighbor_allreduce_nonblocking(tensor,\n                                                         self_weight=self_weight,\n                                                         src_machine_weights=src_machine_weights,\n                                                         dst_machine_weights=dst_machine_weights,\n                                                         enable_topo_check=enable_topo_check,\n                                                         name=name)\n    return synchronize(handle)\n\n@deprecated_function_arg(arg_name=\"neighbor_machine_weights\", fix=\"Use src_machine_weights instead\")\n@deprecated_function_arg(arg_name=\"send_neighbor_machines\", fix=\"Use dst_machine_weights instead\")\ndef hierarchical_neighbor_allreduce_nonblocking(\n    tensor: torch.Tensor, *,\n    self_weight: Optional[float] = None,\n    src_machine_weights: Optional[Dict[int, float]] = None,\n    dst_machine_weights: Optional[Union[List[int],\n                                        Dict[int, float]]] = None,\n    enable_topo_check: Optional[bool] = False,\n    name: Optional[str] = None) -> int:\n    \"\"\"\n",
        "target_code_len": 1485,
        "diff_format": "@@ -685,21 +697,25 @@\n     \"\"\"\n-    # TODO(hhb) Implement the logics for topo check for hierarchical_neighbor_allreduce.\n-\n     # TODO(ybc) add check for self_weight and neighbor_machine_weights.\n-    if (self_weight is None and neighbor_machine_weights is not None) or \\\n-       (self_weight is not None and send_neighbor_machines is None):\n-        raise ValueError(\"Arguments self_weight and neighbor_machine_weights have to \"\n-                         \"be presented at the same time\")\n-    handle = hierarchical_neighbor_allreduce_nonblocking(\n-        tensor, self_weight, neighbor_machine_weights, send_neighbor_machines,\n-        enable_topo_check, name)\n+    if (self_weight is None and src_machine_weights is not None) or \\\n+       (self_weight is not None and src_machine_weights is None):\n+        raise ValueError(\"Arguments self_weight and src_machine_weights have to be presented at \"\n+                         \"the same time\")\n+    handle = hierarchical_neighbor_allreduce_nonblocking(tensor,\n+                                                         self_weight=self_weight,\n+                                                         src_machine_weights=src_machine_weights,\n+                                                         dst_machine_weights=dst_machine_weights,\n+                                                         enable_topo_check=enable_topo_check,\n+                                                         name=name)\n     return synchronize(handle)\n \n-\n-def hierarchical_neighbor_allreduce_nonblocking(tensor: torch.Tensor,\n-                                                self_weight: float = None,\n-                                                neighbor_machine_weights: Dict[int, float] = None,\n-                                                send_neighbor_machines: List[int] = None,\n-                                                enable_topo_check: bool = False,\n-                                                name: Optional[str] = None) -> int:\n+@deprecated_function_arg(arg_name=\"neighbor_machine_weights\", fix=\"Use src_machine_weights instead\")\n+@deprecated_function_arg(arg_name=\"send_neighbor_machines\", fix=\"Use dst_machine_weights instead\")\n+def hierarchical_neighbor_allreduce_nonblocking(\n+    tensor: torch.Tensor, *,\n+    self_weight: Optional[float] = None,\n+    src_machine_weights: Optional[Dict[int, float]] = None,\n+    dst_machine_weights: Optional[Union[List[int],\n+                                        Dict[int, float]]] = None,\n+    enable_topo_check: Optional[bool] = False,\n+    name: Optional[str] = None) -> int:\n     \"\"\"\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n    # TODO(hhb) Implement the logics for topo check for hierarchical_neighbor_allreduce.\n\n    # TODO(ybc) add check for self_weight and neighbor_machine_weights.\n    if (self_weight is None and neighbor_machine_weights is not None) or       (self_weight is not None and send_neighbor_machines is None):\n        <IND>raise ValueError(\"Arguments self_weight and neighbor_machine_weights have to \"\n                         \"be presented at the same time\")\n    <DED>handle = hierarchical_neighbor_allreduce_nonblocking(\n        tensor, self_weight, neighbor_machine_weights, send_neighbor_machines,\n        enable_topo_check, name)\n    return synchronize(handle)\n\n\n<DED>def hierarchical_neighbor_allreduce_nonblocking(tensor: torch.Tensor,\n                                                self_weight: float = None,\n                                                neighbor_machine_weights: Dict[int, float] = None,\n                                                send_neighbor_machines: List[int] = None,\n                                                enable_topo_check: bool = False,\n                                                name: Optional[str] = None) -> int:\n    <IND>",
        "target_code_with_indent": "\n    # TODO(ybc) add check for self_weight and neighbor_machine_weights.\n    if (self_weight is None and src_machine_weights is not None) or       (self_weight is not None and src_machine_weights is None):\n        <IND>raise ValueError(\"Arguments self_weight and src_machine_weights have to be presented at \"\n                         \"the same time\")\n    <DED>handle = hierarchical_neighbor_allreduce_nonblocking(tensor,\n                                                         self_weight=self_weight,\n                                                         src_machine_weights=src_machine_weights,\n                                                         dst_machine_weights=dst_machine_weights,\n                                                         enable_topo_check=enable_topo_check,\n                                                         name=name)\n    return synchronize(handle)\n\n<DED>@deprecated_function_arg(arg_name=\"neighbor_machine_weights\", fix=\"Use src_machine_weights instead\")\n@deprecated_function_arg(arg_name=\"send_neighbor_machines\", fix=\"Use dst_machine_weights instead\")\ndef hierarchical_neighbor_allreduce_nonblocking(\n    tensor: torch.Tensor, *,\n    self_weight: Optional[float] = None,\n    src_machine_weights: Optional[Dict[int, float]] = None,\n    dst_machine_weights: Optional[Union[List[int],\n                                        Dict[int, float]]] = None,\n    enable_topo_check: Optional[bool] = False,\n    name: Optional[str] = None) -> int:\n    <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    \"\"\"\n    if (self_weight is None and neighbor_machine_weights is not None) or \\\n       (self_weight is not None and neighbor_machine_weights is None):\n        raise ValueError(\"Arguments self_weight and neighbor_weights have to be presented at \"\n                         \"the same time\")\n\n    if send_neighbor_machines is None:\n        first_dim = tensor.shape[0] * len(in_neighbor_machine_ranks())\n    else:\n        first_dim = tensor.shape[0] * len(neighbor_machine_weights)\n    new_shape = torch.Size([first_dim] + list(tensor.shape[1:]))\n",
        "source_code_len": 545,
        "target_code": "    \"\"\"\n    # TODO(hanbinhu) #82 Symmetrical argument for self_weight, src_weights, dst_weights\n    if (self_weight is None and src_machine_weights is not None) or \\\n       (self_weight is not None and src_machine_weights is None):\n        raise ValueError(\"Arguments self_weight and src_machine_weights have to be presented at \"\n                         \"the same time\")\n\n    if dst_machine_weights is None:\n        first_dim = tensor.shape[0] * len(in_neighbor_machine_ranks())\n    else:\n        first_dim = tensor.shape[0] * len(dst_machine_weights)\n    new_shape = torch.Size([first_dim] + list(tensor.shape[1:]))\n",
        "target_code_len": 618,
        "diff_format": "@@ -737,11 +763,12 @@\n     \"\"\"\n-    if (self_weight is None and neighbor_machine_weights is not None) or \\\n-       (self_weight is not None and neighbor_machine_weights is None):\n-        raise ValueError(\"Arguments self_weight and neighbor_weights have to be presented at \"\n+    # TODO(hanbinhu) #82 Symmetrical argument for self_weight, src_weights, dst_weights\n+    if (self_weight is None and src_machine_weights is not None) or \\\n+       (self_weight is not None and src_machine_weights is None):\n+        raise ValueError(\"Arguments self_weight and src_machine_weights have to be presented at \"\n                          \"the same time\")\n \n-    if send_neighbor_machines is None:\n+    if dst_machine_weights is None:\n         first_dim = tensor.shape[0] * len(in_neighbor_machine_ranks())\n     else:\n-        first_dim = tensor.shape[0] * len(neighbor_machine_weights)\n+        first_dim = tensor.shape[0] * len(dst_machine_weights)\n     new_shape = torch.Size([first_dim] + list(tensor.shape[1:]))\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n    if (self_weight is None and neighbor_machine_weights is not None) or       (self_weight is not None and neighbor_machine_weights is None):\n        <IND>raise ValueError(\"Arguments self_weight and neighbor_weights have to be presented at \"\n                         \"the same time\")\n\n    <DED>if send_neighbor_machines is None:\n        <IND>first_dim = tensor.shape[0] * len(in_neighbor_machine_ranks())\n    <DED>else:\n        <IND>first_dim = tensor.shape[0] * len(neighbor_machine_weights)\n    <DED>new_shape = torch.Size([first_dim] + list(tensor.shape[1:]))\n",
        "target_code_with_indent": "\n    # TODO(hanbinhu) #82 Symmetrical argument for self_weight, src_weights, dst_weights\n    if (self_weight is None and src_machine_weights is not None) or       (self_weight is not None and src_machine_weights is None):\n        <IND>raise ValueError(\"Arguments self_weight and src_machine_weights have to be presented at \"\n                         \"the same time\")\n\n    <DED>if dst_machine_weights is None:\n        <IND>first_dim = tensor.shape[0] * len(in_neighbor_machine_ranks())\n    <DED>else:\n        <IND>first_dim = tensor.shape[0] * len(dst_machine_weights)\n    <DED>new_shape = torch.Size([first_dim] + list(tensor.shape[1:]))\n"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    return _hierarchical_neighbor_allreduce_nonblocking(\n        tensor, output, self_weight, neighbor_machine_weights,\n        send_neighbor_machines, enable_topo_check, name=name)\n\n",
        "source_code_len": 183,
        "target_code": "    return _hierarchical_neighbor_allreduce_nonblocking(\n        tensor, output, self_weight, src_machine_weights,\n        dst_machine_weights, enable_topo_check, name=name)\n\n",
        "target_code_len": 175,
        "diff_format": "@@ -750,4 +777,4 @@\n     return _hierarchical_neighbor_allreduce_nonblocking(\n-        tensor, output, self_weight, neighbor_machine_weights,\n-        send_neighbor_machines, enable_topo_check, name=name)\n+        tensor, output, self_weight, src_machine_weights,\n+        dst_machine_weights, enable_topo_check, name=name)\n \n",
        "source_code_with_indent": "    return _hierarchical_neighbor_allreduce_nonblocking(\n        tensor, output, self_weight, neighbor_machine_weights,\n        send_neighbor_machines, enable_topo_check, name=name)\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    return _hierarchical_neighbor_allreduce_nonblocking(\n        tensor, output, self_weight, src_machine_weights,\n        dst_machine_weights, enable_topo_check, name=name)\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "Bluefog-Lib/bluefog",
    "commit": "d9c450f2bdac11ff719b7e5f5434ee5ff56499b4",
    "filename": "bluefog/torch/mpi_ops.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Bluefog-Lib-bluefog/bluefog/torch/mpi_ops.py",
    "file_hunks_size": 19,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "bluefog/torch/mpi_ops.py:702:48 Incompatible variable type [9]: send_neighbor_machines is declared to have type `List[int]` but is used as type `None`.",
    "message": " send_neighbor_machines is declared to have type `List[int]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 702,
    "warning_line": "                                                send_neighbor_machines: List[int] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n# TODO(hanbinhu) #81 Add dst_weight for hierarchical neighbor allreduce.\ndef hierarchical_neighbor_allreduce(tensor: torch.Tensor,\n                                    self_weight: float = None,\n                                    neighbor_machine_weights: Dict[int, float] = None,\n                                    send_neighbor_machines: List[int] = None,\n                                    enable_topo_check: bool = False,\n                                    name: Optional[str] = None) -> torch.Tensor:\n    \"\"\"\n    A function that performs weighted averaging of the input tensor over the negihbor machines and\n    itself in the Bluefog processes. It is similar to neighbor_allreduce. But each machine runs\n",
        "source_code_len": 713,
        "target_code": "\n@deprecated_function_arg(arg_name=\"neighbor_machine_weights\", fix=\"Use src_machine_weights instead\")\n@deprecated_function_arg(arg_name=\"send_neighbor_machines\", fix=\"Use dst_machine_weights instead\")\ndef hierarchical_neighbor_allreduce(tensor: torch.Tensor, *,\n                                    self_weight: Optional[float] = None,\n                                    src_machine_weights: Optional[Dict[int, float]] = None,\n                                    dst_machine_weights: Optional[Union[Dict[int, float],\n                                                                        List[int]]] = None,\n                                    enable_topo_check: Optional[bool] = False,\n                                    name: Optional[str] = None) -> torch.Tensor:\n    \"\"\"\n    A function that performs weighted averaging of the input tensor over the neighbor machines and\n    itself in the Bluefog processes. It is similar to neighbor_allreduce. But each machine runs\n",
        "target_code_len": 972,
        "diff_format": "@@ -646,11 +646,13 @@\n \n-# TODO(hanbinhu) #81 Add dst_weight for hierarchical neighbor allreduce.\n-def hierarchical_neighbor_allreduce(tensor: torch.Tensor,\n-                                    self_weight: float = None,\n-                                    neighbor_machine_weights: Dict[int, float] = None,\n-                                    send_neighbor_machines: List[int] = None,\n-                                    enable_topo_check: bool = False,\n+@deprecated_function_arg(arg_name=\"neighbor_machine_weights\", fix=\"Use src_machine_weights instead\")\n+@deprecated_function_arg(arg_name=\"send_neighbor_machines\", fix=\"Use dst_machine_weights instead\")\n+def hierarchical_neighbor_allreduce(tensor: torch.Tensor, *,\n+                                    self_weight: Optional[float] = None,\n+                                    src_machine_weights: Optional[Dict[int, float]] = None,\n+                                    dst_machine_weights: Optional[Union[Dict[int, float],\n+                                                                        List[int]]] = None,\n+                                    enable_topo_check: Optional[bool] = False,\n                                     name: Optional[str] = None) -> torch.Tensor:\n     \"\"\"\n-    A function that performs weighted averaging of the input tensor over the negihbor machines and\n+    A function that performs weighted averaging of the input tensor over the neighbor machines and\n     itself in the Bluefog processes. It is similar to neighbor_allreduce. But each machine runs\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n# TODO(hanbinhu) #81 Add dst_weight for hierarchical neighbor allreduce.\n<DED>def hierarchical_neighbor_allreduce(tensor: torch.Tensor,\n                                    self_weight: float = None,\n                                    neighbor_machine_weights: Dict[int, float] = None,\n                                    send_neighbor_machines: List[int] = None,\n                                    enable_topo_check: bool = False,\n                                    name: Optional[str] = None) -> torch.Tensor:\n    <IND>",
        "target_code_with_indent": "\n<DED>@deprecated_function_arg(arg_name=\"neighbor_machine_weights\", fix=\"Use src_machine_weights instead\")\n@deprecated_function_arg(arg_name=\"send_neighbor_machines\", fix=\"Use dst_machine_weights instead\")\ndef hierarchical_neighbor_allreduce(tensor: torch.Tensor, *,\n                                    self_weight: Optional[float] = None,\n                                    src_machine_weights: Optional[Dict[int, float]] = None,\n                                    dst_machine_weights: Optional[Union[Dict[int, float],\n                                                                        List[int]]] = None,\n                                    enable_topo_check: Optional[bool] = False,\n                                    name: Optional[str] = None) -> torch.Tensor:\n    <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    \"\"\"\n    # TODO(hhb) Implement the logics for topo check for hierarchical_neighbor_allreduce.\n\n    # TODO(ybc) add check for self_weight and neighbor_machine_weights.\n    if (self_weight is None and neighbor_machine_weights is not None) or \\\n       (self_weight is not None and send_neighbor_machines is None):\n        raise ValueError(\"Arguments self_weight and neighbor_machine_weights have to \"\n                         \"be presented at the same time\")\n    handle = hierarchical_neighbor_allreduce_nonblocking(\n        tensor, self_weight, neighbor_machine_weights, send_neighbor_machines,\n        enable_topo_check, name)\n    return synchronize(handle)\n\n\ndef hierarchical_neighbor_allreduce_nonblocking(tensor: torch.Tensor,\n                                                self_weight: float = None,\n                                                neighbor_machine_weights: Dict[int, float] = None,\n                                                send_neighbor_machines: List[int] = None,\n                                                enable_topo_check: bool = False,\n                                                name: Optional[str] = None) -> int:\n    \"\"\"\n",
        "source_code_len": 1169,
        "target_code": "    \"\"\"\n    # TODO(ybc) add check for self_weight and neighbor_machine_weights.\n    if (self_weight is None and src_machine_weights is not None) or \\\n       (self_weight is not None and src_machine_weights is None):\n        raise ValueError(\"Arguments self_weight and src_machine_weights have to be presented at \"\n                         \"the same time\")\n    handle = hierarchical_neighbor_allreduce_nonblocking(tensor,\n                                                         self_weight=self_weight,\n                                                         src_machine_weights=src_machine_weights,\n                                                         dst_machine_weights=dst_machine_weights,\n                                                         enable_topo_check=enable_topo_check,\n                                                         name=name)\n    return synchronize(handle)\n\n@deprecated_function_arg(arg_name=\"neighbor_machine_weights\", fix=\"Use src_machine_weights instead\")\n@deprecated_function_arg(arg_name=\"send_neighbor_machines\", fix=\"Use dst_machine_weights instead\")\ndef hierarchical_neighbor_allreduce_nonblocking(\n    tensor: torch.Tensor, *,\n    self_weight: Optional[float] = None,\n    src_machine_weights: Optional[Dict[int, float]] = None,\n    dst_machine_weights: Optional[Union[List[int],\n                                        Dict[int, float]]] = None,\n    enable_topo_check: Optional[bool] = False,\n    name: Optional[str] = None) -> int:\n    \"\"\"\n",
        "target_code_len": 1485,
        "diff_format": "@@ -685,21 +697,25 @@\n     \"\"\"\n-    # TODO(hhb) Implement the logics for topo check for hierarchical_neighbor_allreduce.\n-\n     # TODO(ybc) add check for self_weight and neighbor_machine_weights.\n-    if (self_weight is None and neighbor_machine_weights is not None) or \\\n-       (self_weight is not None and send_neighbor_machines is None):\n-        raise ValueError(\"Arguments self_weight and neighbor_machine_weights have to \"\n-                         \"be presented at the same time\")\n-    handle = hierarchical_neighbor_allreduce_nonblocking(\n-        tensor, self_weight, neighbor_machine_weights, send_neighbor_machines,\n-        enable_topo_check, name)\n+    if (self_weight is None and src_machine_weights is not None) or \\\n+       (self_weight is not None and src_machine_weights is None):\n+        raise ValueError(\"Arguments self_weight and src_machine_weights have to be presented at \"\n+                         \"the same time\")\n+    handle = hierarchical_neighbor_allreduce_nonblocking(tensor,\n+                                                         self_weight=self_weight,\n+                                                         src_machine_weights=src_machine_weights,\n+                                                         dst_machine_weights=dst_machine_weights,\n+                                                         enable_topo_check=enable_topo_check,\n+                                                         name=name)\n     return synchronize(handle)\n \n-\n-def hierarchical_neighbor_allreduce_nonblocking(tensor: torch.Tensor,\n-                                                self_weight: float = None,\n-                                                neighbor_machine_weights: Dict[int, float] = None,\n-                                                send_neighbor_machines: List[int] = None,\n-                                                enable_topo_check: bool = False,\n-                                                name: Optional[str] = None) -> int:\n+@deprecated_function_arg(arg_name=\"neighbor_machine_weights\", fix=\"Use src_machine_weights instead\")\n+@deprecated_function_arg(arg_name=\"send_neighbor_machines\", fix=\"Use dst_machine_weights instead\")\n+def hierarchical_neighbor_allreduce_nonblocking(\n+    tensor: torch.Tensor, *,\n+    self_weight: Optional[float] = None,\n+    src_machine_weights: Optional[Dict[int, float]] = None,\n+    dst_machine_weights: Optional[Union[List[int],\n+                                        Dict[int, float]]] = None,\n+    enable_topo_check: Optional[bool] = False,\n+    name: Optional[str] = None) -> int:\n     \"\"\"\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n    # TODO(hhb) Implement the logics for topo check for hierarchical_neighbor_allreduce.\n\n    # TODO(ybc) add check for self_weight and neighbor_machine_weights.\n    if (self_weight is None and neighbor_machine_weights is not None) or       (self_weight is not None and send_neighbor_machines is None):\n        <IND>raise ValueError(\"Arguments self_weight and neighbor_machine_weights have to \"\n                         \"be presented at the same time\")\n    <DED>handle = hierarchical_neighbor_allreduce_nonblocking(\n        tensor, self_weight, neighbor_machine_weights, send_neighbor_machines,\n        enable_topo_check, name)\n    return synchronize(handle)\n\n\n<DED>def hierarchical_neighbor_allreduce_nonblocking(tensor: torch.Tensor,\n                                                self_weight: float = None,\n                                                neighbor_machine_weights: Dict[int, float] = None,\n                                                send_neighbor_machines: List[int] = None,\n                                                enable_topo_check: bool = False,\n                                                name: Optional[str] = None) -> int:\n    <IND>",
        "target_code_with_indent": "\n    # TODO(ybc) add check for self_weight and neighbor_machine_weights.\n    if (self_weight is None and src_machine_weights is not None) or       (self_weight is not None and src_machine_weights is None):\n        <IND>raise ValueError(\"Arguments self_weight and src_machine_weights have to be presented at \"\n                         \"the same time\")\n    <DED>handle = hierarchical_neighbor_allreduce_nonblocking(tensor,\n                                                         self_weight=self_weight,\n                                                         src_machine_weights=src_machine_weights,\n                                                         dst_machine_weights=dst_machine_weights,\n                                                         enable_topo_check=enable_topo_check,\n                                                         name=name)\n    return synchronize(handle)\n\n<DED>@deprecated_function_arg(arg_name=\"neighbor_machine_weights\", fix=\"Use src_machine_weights instead\")\n@deprecated_function_arg(arg_name=\"send_neighbor_machines\", fix=\"Use dst_machine_weights instead\")\ndef hierarchical_neighbor_allreduce_nonblocking(\n    tensor: torch.Tensor, *,\n    self_weight: Optional[float] = None,\n    src_machine_weights: Optional[Dict[int, float]] = None,\n    dst_machine_weights: Optional[Union[List[int],\n                                        Dict[int, float]]] = None,\n    enable_topo_check: Optional[bool] = False,\n    name: Optional[str] = None) -> int:\n    <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    \"\"\"\n    if (self_weight is None and neighbor_machine_weights is not None) or \\\n       (self_weight is not None and neighbor_machine_weights is None):\n        raise ValueError(\"Arguments self_weight and neighbor_weights have to be presented at \"\n                         \"the same time\")\n\n    if send_neighbor_machines is None:\n        first_dim = tensor.shape[0] * len(in_neighbor_machine_ranks())\n    else:\n        first_dim = tensor.shape[0] * len(neighbor_machine_weights)\n    new_shape = torch.Size([first_dim] + list(tensor.shape[1:]))\n",
        "source_code_len": 545,
        "target_code": "    \"\"\"\n    # TODO(hanbinhu) #82 Symmetrical argument for self_weight, src_weights, dst_weights\n    if (self_weight is None and src_machine_weights is not None) or \\\n       (self_weight is not None and src_machine_weights is None):\n        raise ValueError(\"Arguments self_weight and src_machine_weights have to be presented at \"\n                         \"the same time\")\n\n    if dst_machine_weights is None:\n        first_dim = tensor.shape[0] * len(in_neighbor_machine_ranks())\n    else:\n        first_dim = tensor.shape[0] * len(dst_machine_weights)\n    new_shape = torch.Size([first_dim] + list(tensor.shape[1:]))\n",
        "target_code_len": 618,
        "diff_format": "@@ -737,11 +763,12 @@\n     \"\"\"\n-    if (self_weight is None and neighbor_machine_weights is not None) or \\\n-       (self_weight is not None and neighbor_machine_weights is None):\n-        raise ValueError(\"Arguments self_weight and neighbor_weights have to be presented at \"\n+    # TODO(hanbinhu) #82 Symmetrical argument for self_weight, src_weights, dst_weights\n+    if (self_weight is None and src_machine_weights is not None) or \\\n+       (self_weight is not None and src_machine_weights is None):\n+        raise ValueError(\"Arguments self_weight and src_machine_weights have to be presented at \"\n                          \"the same time\")\n \n-    if send_neighbor_machines is None:\n+    if dst_machine_weights is None:\n         first_dim = tensor.shape[0] * len(in_neighbor_machine_ranks())\n     else:\n-        first_dim = tensor.shape[0] * len(neighbor_machine_weights)\n+        first_dim = tensor.shape[0] * len(dst_machine_weights)\n     new_shape = torch.Size([first_dim] + list(tensor.shape[1:]))\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n    if (self_weight is None and neighbor_machine_weights is not None) or       (self_weight is not None and neighbor_machine_weights is None):\n        <IND>raise ValueError(\"Arguments self_weight and neighbor_weights have to be presented at \"\n                         \"the same time\")\n\n    <DED>if send_neighbor_machines is None:\n        <IND>first_dim = tensor.shape[0] * len(in_neighbor_machine_ranks())\n    <DED>else:\n        <IND>first_dim = tensor.shape[0] * len(neighbor_machine_weights)\n    <DED>new_shape = torch.Size([first_dim] + list(tensor.shape[1:]))\n",
        "target_code_with_indent": "\n    # TODO(hanbinhu) #82 Symmetrical argument for self_weight, src_weights, dst_weights\n    if (self_weight is None and src_machine_weights is not None) or       (self_weight is not None and src_machine_weights is None):\n        <IND>raise ValueError(\"Arguments self_weight and src_machine_weights have to be presented at \"\n                         \"the same time\")\n\n    <DED>if dst_machine_weights is None:\n        <IND>first_dim = tensor.shape[0] * len(in_neighbor_machine_ranks())\n    <DED>else:\n        <IND>first_dim = tensor.shape[0] * len(dst_machine_weights)\n    <DED>new_shape = torch.Size([first_dim] + list(tensor.shape[1:]))\n"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    return _hierarchical_neighbor_allreduce_nonblocking(\n        tensor, output, self_weight, neighbor_machine_weights,\n        send_neighbor_machines, enable_topo_check, name=name)\n\n",
        "source_code_len": 183,
        "target_code": "    return _hierarchical_neighbor_allreduce_nonblocking(\n        tensor, output, self_weight, src_machine_weights,\n        dst_machine_weights, enable_topo_check, name=name)\n\n",
        "target_code_len": 175,
        "diff_format": "@@ -750,4 +777,4 @@\n     return _hierarchical_neighbor_allreduce_nonblocking(\n-        tensor, output, self_weight, neighbor_machine_weights,\n-        send_neighbor_machines, enable_topo_check, name=name)\n+        tensor, output, self_weight, src_machine_weights,\n+        dst_machine_weights, enable_topo_check, name=name)\n \n",
        "source_code_with_indent": "    return _hierarchical_neighbor_allreduce_nonblocking(\n        tensor, output, self_weight, neighbor_machine_weights,\n        send_neighbor_machines, enable_topo_check, name=name)\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    return _hierarchical_neighbor_allreduce_nonblocking(\n        tensor, output, self_weight, src_machine_weights,\n        dst_machine_weights, enable_topo_check, name=name)\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "Bluefog-Lib/bluefog",
    "commit": "d9c450f2bdac11ff719b7e5f5434ee5ff56499b4",
    "filename": "bluefog/torch/mpi_ops.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Bluefog-Lib-bluefog/bluefog/torch/mpi_ops.py",
    "file_hunks_size": 19,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "bluefog/torch/mpi_ops.py:838:56 Incompatible variable type [9]: self_weight is declared to have type `float` but is used as type `None`.",
    "message": " self_weight is declared to have type `float` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 838,
    "warning_line": "def pair_gossip(tensor: torch.Tensor, target_rank: int, self_weight: float = None,"
  },
  {
    "project": "Bluefog-Lib/bluefog",
    "commit": "d9c450f2bdac11ff719b7e5f5434ee5ff56499b4",
    "filename": "bluefog/torch/mpi_ops.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Bluefog-Lib-bluefog/bluefog/torch/mpi_ops.py",
    "file_hunks_size": 19,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "bluefog/torch/mpi_ops.py:865:68 Incompatible variable type [9]: self_weight is declared to have type `float` but is used as type `None`.",
    "message": " self_weight is declared to have type `float` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 865,
    "warning_line": "def pair_gossip_nonblocking(tensor: torch.Tensor, target_rank: int, self_weight: float = None,"
  }
]