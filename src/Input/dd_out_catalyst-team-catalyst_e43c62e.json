[
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/logging.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/callbacks/logging.py:27:14 Incompatible variable type [9]: always_show is declared to have type `List[str]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/logging.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/logging.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/callbacks/logging.py:27:45 Incompatible variable type [9]: never_show is declared to have type `List[str]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/logging.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/logging.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/callbacks/logging.py:153:40 Incompatible parameter type [6]: Expected `str` for 2nd positional only parameter to call `ConsoleLogger._setup_logger` but got `pathlib.Path`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/logging.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/logging.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/callbacks/logging.py:175:8 Incompatible variable type [9]: metric_names is declared to have type `List[str]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/logging.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/logging.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/callbacks/logging.py:280:14 Incompatible variable type [9]: metric_names is declared to have type `List[str]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/logging.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/meter.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/callbacks/meter.py:28:8 Incompatible variable type [9]: class_names is declared to have type `List[str]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/meter.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/metric.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/metric.py",
    "file_hunks_size": 7,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/metric.py:51:46 Incompatible parameter type [6]: Expected `Union[None, List[str], str]` for 1st positional only parameter to call `get_dictkey_auto_fn` but got `Union[Dict[str, str], List[str], str]`.",
    "message": " Expected `Union[None, List[str], str]` for 1st positional only parameter to call `get_dictkey_auto_fn` but got `Union[Dict[str, str], List[str], str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 51,
    "warning_line": "        self._get_input = get_dictkey_auto_fn(self.input_key)"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/metric.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/metric.py",
    "file_hunks_size": 7,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/metric.py:52:47 Incompatible parameter type [6]: Expected `Union[None, List[str], str]` for 1st positional only parameter to call `get_dictkey_auto_fn` but got `Union[Dict[str, str], List[str], str]`.",
    "message": " Expected `Union[None, List[str], str]` for 1st positional only parameter to call `get_dictkey_auto_fn` but got `Union[Dict[str, str], List[str], str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 52,
    "warning_line": "        self._get_output = get_dictkey_auto_fn(self.output_key)"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/metric.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/metric.py",
    "file_hunks_size": 7,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/metric.py:310:8 Incompatible variable type [9]: metrics is declared to have type `Union[Dict[str, float], List[str], str]` but is used as type `None`.",
    "message": " metrics is declared to have type `Union[Dict[str, float], List[str], str]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 310,
    "warning_line": "        metrics: Union[str, List[str], Dict[str, float]] = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/metrics/cmc_score.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/metrics/cmc_score.py",
    "file_hunks_size": 2,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/metrics/cmc_score.py:37:8 Incompatible variable type [9]: num_classes is declared to have type `int` but is used as type `None`.",
    "message": " num_classes is declared to have type `int` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 37,
    "warning_line": "        num_classes: int = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/metrics/dice.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/callbacks/metrics/dice.py:31:8 Incompatible variable type [9]: class_args is declared to have type `List[str]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/metrics/dice.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/metrics/f1_score.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/callbacks/metrics/f1_score.py:21:8 Incompatible variable type [9]: class_args is declared to have type `List[str]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/metrics/f1_score.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/metrics/iou.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/callbacks/metrics/iou.py:21:8 Incompatible variable type [9]: class_args is declared to have type `List[str]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/metrics/iou.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/metrics/perplexity.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/callbacks/metrics/perplexity.py:26:8 Incompatible variable type [9]: ignore_index is declared to have type `int` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/metrics/perplexity.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/metrics/ppv_tpr_f1.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/callbacks/metrics/ppv_tpr_f1.py:20:8 Incompatible variable type [9]: class_names is declared to have type `List[str]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/metrics/ppv_tpr_f1.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/metrics/precision.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/callbacks/metrics/precision.py:21:8 Incompatible variable type [9]: class_args is declared to have type `List[str]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/metrics/precision.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/metrics/precision.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/callbacks/metrics/precision.py:68:8 Incompatible variable type [9]: class_args is declared to have type `List[str]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/metrics/precision.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/metrics/recall.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/callbacks/metrics/recall.py:21:8 Incompatible variable type [9]: class_args is declared to have type `List[str]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/metrics/recall.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/optimizer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/optimizer.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/optimizer.py:46:8 Incompatible variable type [9]: metric_key is declared to have type `str` but is used as type `None`.",
    "message": " metric_key is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 46,
    "warning_line": "        metric_key: str = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/optimizer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/optimizer.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/optimizer.py:49:8 Incompatible variable type [9]: grad_clip_params is declared to have type `Dict[typing.Any, typing.Any]` but is used as type `None`.",
    "message": " grad_clip_params is declared to have type `Dict[typing.Any, typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 49,
    "warning_line": "        grad_clip_params: Dict = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/optimizer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/optimizer.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/optimizer.py:51:8 Incompatible variable type [9]: loss_key is declared to have type `str` but is used as type `None`.",
    "message": " loss_key is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 51,
    "warning_line": "        loss_key: str = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/optimizer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/optimizer.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/optimizer.py:54:8 Incompatible variable type [9]: use_amp is declared to have type `bool` but is used as type `None`.",
    "message": " use_amp is declared to have type `bool` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 54,
    "warning_line": "        use_amp: bool = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/optimizer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/optimizer.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/optimizer.py:55:8 Incompatible variable type [9]: use_apex is declared to have type `bool` but is used as type `None`.",
    "message": " use_apex is declared to have type `bool` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 55,
    "warning_line": "        use_apex: bool = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/optimizer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/optimizer.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/optimizer.py:115:8 Incompatible attribute type [8]: Attribute `_optimizer_wds` declared in class `OptimizerCallback` has type `List[typing.Any]` but is used as type `None`.",
    "message": " Attribute `_optimizer_wds` declared in class `OptimizerCallback` has type `List[typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible attribute type [8]",
    "warning_line_no": 115,
    "warning_line": "        self._optimizer_wds: List = None"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/optimizer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/optimizer.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/optimizer.py:116:8 Incompatible attribute type [8]: Attribute `_optimizer_step_fn` declared in class `OptimizerCallback` has type `typing.Callable[..., typing.Any]` but is used as type `None`.",
    "message": " Attribute `_optimizer_step_fn` declared in class `OptimizerCallback` has type `typing.Callable[..., typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible attribute type [8]",
    "warning_line_no": 116,
    "warning_line": "        self._optimizer_step_fn: Callable = None"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/optimizer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/optimizer.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/optimizer.py:141:39 Incompatible variable type [9]: grad_clip_fn is declared to have type `typing.Callable[..., typing.Any]` but is used as type `None`.",
    "message": " grad_clip_fn is declared to have type `typing.Callable[..., typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 141,
    "warning_line": "        self, *, optimizer: Optimizer, grad_clip_fn: Callable = None"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/pruning.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/pruning.py",
    "file_hunks_size": 11,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/pruning.py:137:16 Incompatible parameter type [6]: Expected `List[str]` for 3rd parameter `keys_to_prune` to call `prune_model` but got `Optional[List[str]]`.",
    "message": " Expected `List[str]` for 3rd parameter `keys_to_prune` to call `prune_model` but got `Optional[List[str]]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 137,
    "warning_line": "                keys_to_prune=self.keys_to_prune,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/pruning.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/pruning.py",
    "file_hunks_size": 11,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/pruning.py:138:16 Incompatible parameter type [6]: Expected `Union[float, int]` for 4th parameter `amount` to call `prune_model` but got `Union[None, float, int]`.",
    "message": " Expected `Union[float, int]` for 4th parameter `amount` to call `prune_model` but got `Union[None, float, int]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 138,
    "warning_line": "                amount=self.amount,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        pruning_fn: Union[Callable, str],\n        keys_to_prune: Optional[List[str]] = None,\n        amount: Optional[Union[int, float]] = 0.5,\n        prune_on_epoch_end: Optional[bool] = False,\n",
        "source_code_len": 196,
        "target_code": "        pruning_fn: Union[Callable, str],\n        amount: Union[int, float],\n        keys_to_prune: Optional[List[str]] = None,\n        prune_on_epoch_end: Optional[bool] = False,\n",
        "target_code_len": 180,
        "diff_format": "@@ -36,4 +42,4 @@\n         pruning_fn: Union[Callable, str],\n+        amount: Union[int, float],\n         keys_to_prune: Optional[List[str]] = None,\n-        amount: Optional[Union[int, float]] = 0.5,\n         prune_on_epoch_end: Optional[bool] = False,\n",
        "source_code_with_indent": "        pruning_fn: Union[Callable, str],\n        keys_to_prune: Optional[List[str]] = None,\n        amount: Optional[Union[int, float]] = 0.5,\n        prune_on_epoch_end: Optional[bool] = False,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        pruning_fn: Union[Callable, str],\n        amount: Union[int, float],\n        keys_to_prune: Optional[List[str]] = None,\n        prune_on_epoch_end: Optional[bool] = False,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/pruning.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/pruning.py",
    "file_hunks_size": 11,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/pruning.py:157:16 Incompatible parameter type [6]: Expected `List[str]` for 3rd parameter `keys_to_prune` to call `prune_model` but got `Optional[List[str]]`.",
    "message": " Expected `List[str]` for 3rd parameter `keys_to_prune` to call `prune_model` but got `Optional[List[str]]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 157,
    "warning_line": "                keys_to_prune=self.keys_to_prune,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/pruning.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/pruning.py",
    "file_hunks_size": 11,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/pruning.py:158:16 Incompatible parameter type [6]: Expected `Union[float, int]` for 4th parameter `amount` to call `prune_model` but got `Union[None, float, int]`.",
    "message": " Expected `Union[float, int]` for 4th parameter `amount` to call `prune_model` but got `Union[None, float, int]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 158,
    "warning_line": "                amount=self.amount,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        pruning_fn: Union[Callable, str],\n        keys_to_prune: Optional[List[str]] = None,\n        amount: Optional[Union[int, float]] = 0.5,\n        prune_on_epoch_end: Optional[bool] = False,\n",
        "source_code_len": 196,
        "target_code": "        pruning_fn: Union[Callable, str],\n        amount: Union[int, float],\n        keys_to_prune: Optional[List[str]] = None,\n        prune_on_epoch_end: Optional[bool] = False,\n",
        "target_code_len": 180,
        "diff_format": "@@ -36,4 +42,4 @@\n         pruning_fn: Union[Callable, str],\n+        amount: Union[int, float],\n         keys_to_prune: Optional[List[str]] = None,\n-        amount: Optional[Union[int, float]] = 0.5,\n         prune_on_epoch_end: Optional[bool] = False,\n",
        "source_code_with_indent": "        pruning_fn: Union[Callable, str],\n        keys_to_prune: Optional[List[str]] = None,\n        amount: Optional[Union[int, float]] = 0.5,\n        prune_on_epoch_end: Optional[bool] = False,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        pruning_fn: Union[Callable, str],\n        amount: Union[int, float],\n        keys_to_prune: Optional[List[str]] = None,\n        prune_on_epoch_end: Optional[bool] = False,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/quantization.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/quantization.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/quantization.py:29:8 Incompatible variable type [9]: out_dir is declared to have type `Union[Path, str]` but is used as type `None`.",
    "message": " out_dir is declared to have type `Union[Path, str]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 29,
    "warning_line": "        out_dir: Union[str, Path] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "from typing import Dict, Optional, Set, TYPE_CHECKING, Union\nfrom pathlib import Path\n\nimport torch\nfrom torch import quantization\n\nfrom catalyst.core.callback import Callback, CallbackOrder\nfrom catalyst.utils.quantization import save_quantized_model\n\nif TYPE_CHECKING:\n    from catalyst.core.runner import IRunner\n\n\nclass DynamicQuantizationCallback(Callback):\n    \"\"\"Dynamic Quantization Callback\n\n    This callback applying dynamic quantization to the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        metric: str = \"loss\",\n        minimize: bool = True,\n        min_delta: float = 1e-6,\n        mode: str = \"best\",\n        do_once: bool = True,\n        qconfig_spec: Optional[Union[Set, Dict]] = None,\n        dtype: Optional[torch.dtype] = torch.qint8,\n        out_dir: Union[str, Path] = None,\n        out_model: Union[str, Path] = None,\n        backend: str = None,\n    ):\n        \"\"\"Init method for callback\n\n        Args:\n            metric: Metric key we should trace model based on\n            minimize: Whether do we minimize metric or not\n            min_delta: Minimum value of change for metric to be\n                considered as improved\n            mode: One of `best` or `last`\n            do_once: Whether do we trace once per stage or every epoch\n            qconfig_spec: torch.quantization.quantize_dynamic\n                parameter, you can define layers to be quantize\n            dtype: type of the model parameters, default int8\n            out_dir (Union[str, Path]): Directory to save model to\n            out_model (Union[str, Path]): Path to save model to\n                (overrides `out_dir` argument)\n            backend: defines backend for quantization\n        \"\"\"\n        super().__init__(order=CallbackOrder.external)\n\n        if mode not in [\"best\", \"last\"]:\n            raise ValueError(\n                f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n            )\n\n        self.metric = metric\n        self.mode = mode\n        self.do_once = do_once\n        self.best_score = None\n        self.is_better = None\n        self.first_time = True\n        if minimize:\n            self.is_better = lambda score, best: score <= (best - min_delta)\n        else:\n            self.is_better = lambda score, best: score >= (best + min_delta)\n\n        self.opt_level = None\n\n        if out_model is not None:\n            out_model = Path(out_model)\n        self.out_model = out_model\n\n        if out_dir is not None:\n            out_dir = Path(out_dir)\n        self.out_dir = out_dir\n        self.qconfig_spec = qconfig_spec\n        self.dtype = dtype\n\n        if backend is not None:\n            torch.backends.quantized.engine = backend\n\n    def on_epoch_end(self, runner: \"IRunner\"):\n        \"\"\"\n        Performing model quantization on epoch end if condition metric is\n        improved\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if not self.do_once:\n            if self.mode == \"best\":\n                score = runner.valid_metrics[self.metric]\n\n                if self.best_score is None:\n                    self.best_score = score\n\n                if self.is_better(score, self.best_score) or self.first_time:\n                    self.best_score = score\n                    quantized_model = quantization.quantize_dynamic(\n                        runner.model.cpu(),\n                        qconfig_spec=self.qconfig_spec,\n                        dtype=self.dtype,\n                    )\n                    save_quantized_model(\n                        model=quantized_model,\n                        logdir=runner.logdir,\n                        checkpoint_name=self.mode,\n                        out_model=self.out_model,\n                        out_dir=self.out_dir,\n                    )\n                    self.first_time = False\n            else:\n                quantized_model = quantization.quantize_dynamic(\n                    runner.model.cpu(),\n                    qconfig_spec=self.qconfig_spec,\n                    dtype=self.dtype,\n                )\n                save_quantized_model(\n                    model=quantized_model,\n                    logdir=runner.logdir,\n                    checkpoint_name=self.mode,\n                    out_model=self.out_model,\n                    out_dir=self.out_dir,\n                )\n\n    def on_stage_end(self, runner: \"IRunner\") -> None:\n        \"\"\"\n        On stage end action.\n\n        Args:\n            runner: runner of your experiment\n        \"\"\"\n        if self.do_once:\n            quantized_model = quantization.quantize_dynamic(\n                runner.model.cpu(),\n                qconfig_spec=self.qconfig_spec,\n                dtype=self.dtype,\n            )\n            save_quantized_model(\n                model=quantized_model,\n                logdir=runner.logdir,\n                checkpoint_name=self.mode,\n                out_model=self.out_model,\n                out_dir=self.out_dir,\n            )\n\n\n__all__ = [\"DynamicQuantizationCallback\"]\n",
        "source_code_len": 5000,
        "target_code": "# # @TODO: make the same API for tracing/onnx/pruning/quantization\n# from typing import Dict, Optional, Set, TYPE_CHECKING, Union\n# from pathlib import Path\n#\n# import torch\n# from torch import quantization\n#\n# from catalyst.core.callback import Callback, CallbackOrder\n# from catalyst.utils.quantization import save_quantized_model\n#\n# if TYPE_CHECKING:\n#     from catalyst.core.runner import IRunner\n#\n#\n# class DynamicQuantizationCallback(Callback):\n#     \"\"\"Dynamic Quantization Callback\n#\n#     This callback applying dynamic quantization to the model.\n#     \"\"\"\n#\n#     def __init__(\n#         self,\n#         metric: str = \"loss\",\n#         minimize: bool = True,\n#         min_delta: float = 1e-6,\n#         mode: str = \"best\",\n#         do_once: bool = True,\n#         qconfig_spec: Optional[Union[Set, Dict]] = None,\n#         dtype: Optional[torch.dtype] = torch.qint8,\n#         out_dir: Union[str, Path] = None,\n#         out_model: Union[str, Path] = None,\n#         backend: str = None,\n#     ):\n#         \"\"\"Init method for callback\n#\n#         Args:\n#             metric: Metric key we should trace model based on\n#             minimize: Whether do we minimize metric or not\n#             min_delta: Minimum value of change for metric to be\n#                 considered as improved\n#             mode: One of `best` or `last`\n#             do_once: Whether do we trace once per stage or every epoch\n#             qconfig_spec: torch.quantization.quantize_dynamic\n#                 parameter, you can define layers to be quantize\n#             dtype: type of the model parameters, default int8\n#             out_dir (Union[str, Path]): Directory to save model to\n#             out_model (Union[str, Path]): Path to save model to\n#                 (overrides `out_dir` argument)\n#             backend: defines backend for quantization\n#         \"\"\"\n#         super().__init__(order=CallbackOrder.external)\n#\n#         if mode not in [\"best\", \"last\"]:\n#             raise ValueError(\n#                 f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n#             )\n#\n#         self.metric = metric\n#         self.mode = mode\n#         self.do_once = do_once\n#         self.best_score = None\n#         self.is_better = None\n#         self.first_time = True\n#         if minimize:\n#             self.is_better = lambda score, best: score <= (best - min_delta)\n#         else:\n#             self.is_better = lambda score, best: score >= (best + min_delta)\n#\n#         self.opt_level = None\n#\n#         if out_model is not None:\n#             out_model = Path(out_model)\n#         self.out_model = out_model\n#\n#         if out_dir is not None:\n#             out_dir = Path(out_dir)\n#         self.out_dir = out_dir\n#         self.qconfig_spec = qconfig_spec\n#         self.dtype = dtype\n#\n#         if backend is not None:\n#             torch.backends.quantized.engine = backend\n#\n#     def on_epoch_end(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model quantization on epoch end if condition metric is\n#         improved\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if not self.do_once:\n#             if self.mode == \"best\":\n#                 score = runner.valid_metrics[self.metric]\n#\n#                 if self.best_score is None:\n#                     self.best_score = score\n#\n#                 if self.is_better(score, self.best_score) or self.first_time:\n#                     self.best_score = score\n#                     quantized_model = quantization.quantize_dynamic(\n#                         runner.model.cpu(),\n#                         qconfig_spec=self.qconfig_spec,\n#                         dtype=self.dtype,\n#                     )\n#                     save_quantized_model(\n#                         model=quantized_model,\n#                         logdir=runner.logdir,\n#                         checkpoint_name=self.mode,\n#                         out_model=self.out_model,\n#                         out_dir=self.out_dir,\n#                     )\n#                     self.first_time = False\n#             else:\n#                 quantized_model = quantization.quantize_dynamic(\n#                     runner.model.cpu(),\n#                     qconfig_spec=self.qconfig_spec,\n#                     dtype=self.dtype,\n#                 )\n#                 save_quantized_model(\n#                     model=quantized_model,\n#                     logdir=runner.logdir,\n#                     checkpoint_name=self.mode,\n#                     out_model=self.out_model,\n#                     out_dir=self.out_dir,\n#                 )\n#\n#     def on_stage_end(self, runner: \"IRunner\") -> None:\n#         \"\"\"\n#         On stage end action.\n#\n#         Args:\n#             runner: runner of your experiment\n#         \"\"\"\n#         if self.do_once:\n#             quantized_model = quantization.quantize_dynamic(\n#                 runner.model.cpu(),\n#                 qconfig_spec=self.qconfig_spec,\n#                 dtype=self.dtype,\n#             )\n#             save_quantized_model(\n#                 model=quantized_model,\n#                 logdir=runner.logdir,\n#                 checkpoint_name=self.mode,\n#                 out_model=self.out_model,\n#                 out_dir=self.out_dir,\n#             )\n#\n#\n# __all__ = [\"DynamicQuantizationCallback\"]\n",
        "target_code_len": 5343,
        "diff_format": "@@ -1,149 +1,150 @@\n-from typing import Dict, Optional, Set, TYPE_CHECKING, Union\n-from pathlib import Path\n-\n-import torch\n-from torch import quantization\n-\n-from catalyst.core.callback import Callback, CallbackOrder\n-from catalyst.utils.quantization import save_quantized_model\n-\n-if TYPE_CHECKING:\n-    from catalyst.core.runner import IRunner\n-\n-\n-class DynamicQuantizationCallback(Callback):\n-    \"\"\"Dynamic Quantization Callback\n-\n-    This callback applying dynamic quantization to the model.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        metric: str = \"loss\",\n-        minimize: bool = True,\n-        min_delta: float = 1e-6,\n-        mode: str = \"best\",\n-        do_once: bool = True,\n-        qconfig_spec: Optional[Union[Set, Dict]] = None,\n-        dtype: Optional[torch.dtype] = torch.qint8,\n-        out_dir: Union[str, Path] = None,\n-        out_model: Union[str, Path] = None,\n-        backend: str = None,\n-    ):\n-        \"\"\"Init method for callback\n-\n-        Args:\n-            metric: Metric key we should trace model based on\n-            minimize: Whether do we minimize metric or not\n-            min_delta: Minimum value of change for metric to be\n-                considered as improved\n-            mode: One of `best` or `last`\n-            do_once: Whether do we trace once per stage or every epoch\n-            qconfig_spec: torch.quantization.quantize_dynamic\n-                parameter, you can define layers to be quantize\n-            dtype: type of the model parameters, default int8\n-            out_dir (Union[str, Path]): Directory to save model to\n-            out_model (Union[str, Path]): Path to save model to\n-                (overrides `out_dir` argument)\n-            backend: defines backend for quantization\n-        \"\"\"\n-        super().__init__(order=CallbackOrder.external)\n-\n-        if mode not in [\"best\", \"last\"]:\n-            raise ValueError(\n-                f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n-            )\n-\n-        self.metric = metric\n-        self.mode = mode\n-        self.do_once = do_once\n-        self.best_score = None\n-        self.is_better = None\n-        self.first_time = True\n-        if minimize:\n-            self.is_better = lambda score, best: score <= (best - min_delta)\n-        else:\n-            self.is_better = lambda score, best: score >= (best + min_delta)\n-\n-        self.opt_level = None\n-\n-        if out_model is not None:\n-            out_model = Path(out_model)\n-        self.out_model = out_model\n-\n-        if out_dir is not None:\n-            out_dir = Path(out_dir)\n-        self.out_dir = out_dir\n-        self.qconfig_spec = qconfig_spec\n-        self.dtype = dtype\n-\n-        if backend is not None:\n-            torch.backends.quantized.engine = backend\n-\n-    def on_epoch_end(self, runner: \"IRunner\"):\n-        \"\"\"\n-        Performing model quantization on epoch end if condition metric is\n-        improved\n-\n-        Args:\n-            runner: current runner\n-        \"\"\"\n-        if not self.do_once:\n-            if self.mode == \"best\":\n-                score = runner.valid_metrics[self.metric]\n-\n-                if self.best_score is None:\n-                    self.best_score = score\n-\n-                if self.is_better(score, self.best_score) or self.first_time:\n-                    self.best_score = score\n-                    quantized_model = quantization.quantize_dynamic(\n-                        runner.model.cpu(),\n-                        qconfig_spec=self.qconfig_spec,\n-                        dtype=self.dtype,\n-                    )\n-                    save_quantized_model(\n-                        model=quantized_model,\n-                        logdir=runner.logdir,\n-                        checkpoint_name=self.mode,\n-                        out_model=self.out_model,\n-                        out_dir=self.out_dir,\n-                    )\n-                    self.first_time = False\n-            else:\n-                quantized_model = quantization.quantize_dynamic(\n-                    runner.model.cpu(),\n-                    qconfig_spec=self.qconfig_spec,\n-                    dtype=self.dtype,\n-                )\n-                save_quantized_model(\n-                    model=quantized_model,\n-                    logdir=runner.logdir,\n-                    checkpoint_name=self.mode,\n-                    out_model=self.out_model,\n-                    out_dir=self.out_dir,\n-                )\n-\n-    def on_stage_end(self, runner: \"IRunner\") -> None:\n-        \"\"\"\n-        On stage end action.\n-\n-        Args:\n-            runner: runner of your experiment\n-        \"\"\"\n-        if self.do_once:\n-            quantized_model = quantization.quantize_dynamic(\n-                runner.model.cpu(),\n-                qconfig_spec=self.qconfig_spec,\n-                dtype=self.dtype,\n-            )\n-            save_quantized_model(\n-                model=quantized_model,\n-                logdir=runner.logdir,\n-                checkpoint_name=self.mode,\n-                out_model=self.out_model,\n-                out_dir=self.out_dir,\n-            )\n-\n-\n-__all__ = [\"DynamicQuantizationCallback\"]\n+# # @TODO: make the same API for tracing/onnx/pruning/quantization\n+# from typing import Dict, Optional, Set, TYPE_CHECKING, Union\n+# from pathlib import Path\n+#\n+# import torch\n+# from torch import quantization\n+#\n+# from catalyst.core.callback import Callback, CallbackOrder\n+# from catalyst.utils.quantization import save_quantized_model\n+#\n+# if TYPE_CHECKING:\n+#     from catalyst.core.runner import IRunner\n+#\n+#\n+# class DynamicQuantizationCallback(Callback):\n+#     \"\"\"Dynamic Quantization Callback\n+#\n+#     This callback applying dynamic quantization to the model.\n+#     \"\"\"\n+#\n+#     def __init__(\n+#         self,\n+#         metric: str = \"loss\",\n+#         minimize: bool = True,\n+#         min_delta: float = 1e-6,\n+#         mode: str = \"best\",\n+#         do_once: bool = True,\n+#         qconfig_spec: Optional[Union[Set, Dict]] = None,\n+#         dtype: Optional[torch.dtype] = torch.qint8,\n+#         out_dir: Union[str, Path] = None,\n+#         out_model: Union[str, Path] = None,\n+#         backend: str = None,\n+#     ):\n+#         \"\"\"Init method for callback\n+#\n+#         Args:\n+#             metric: Metric key we should trace model based on\n+#             minimize: Whether do we minimize metric or not\n+#             min_delta: Minimum value of change for metric to be\n+#                 considered as improved\n+#             mode: One of `best` or `last`\n+#             do_once: Whether do we trace once per stage or every epoch\n+#             qconfig_spec: torch.quantization.quantize_dynamic\n+#                 parameter, you can define layers to be quantize\n+#             dtype: type of the model parameters, default int8\n+#             out_dir (Union[str, Path]): Directory to save model to\n+#             out_model (Union[str, Path]): Path to save model to\n+#                 (overrides `out_dir` argument)\n+#             backend: defines backend for quantization\n+#         \"\"\"\n+#         super().__init__(order=CallbackOrder.external)\n+#\n+#         if mode not in [\"best\", \"last\"]:\n+#             raise ValueError(\n+#                 f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n+#             )\n+#\n+#         self.metric = metric\n+#         self.mode = mode\n+#         self.do_once = do_once\n+#         self.best_score = None\n+#         self.is_better = None\n+#         self.first_time = True\n+#         if minimize:\n+#             self.is_better = lambda score, best: score <= (best - min_delta)\n+#         else:\n+#             self.is_better = lambda score, best: score >= (best + min_delta)\n+#\n+#         self.opt_level = None\n+#\n+#         if out_model is not None:\n+#             out_model = Path(out_model)\n+#         self.out_model = out_model\n+#\n+#         if out_dir is not None:\n+#             out_dir = Path(out_dir)\n+#         self.out_dir = out_dir\n+#         self.qconfig_spec = qconfig_spec\n+#         self.dtype = dtype\n+#\n+#         if backend is not None:\n+#             torch.backends.quantized.engine = backend\n+#\n+#     def on_epoch_end(self, runner: \"IRunner\"):\n+#         \"\"\"\n+#         Performing model quantization on epoch end if condition metric is\n+#         improved\n+#\n+#         Args:\n+#             runner: current runner\n+#         \"\"\"\n+#         if not self.do_once:\n+#             if self.mode == \"best\":\n+#                 score = runner.valid_metrics[self.metric]\n+#\n+#                 if self.best_score is None:\n+#                     self.best_score = score\n+#\n+#                 if self.is_better(score, self.best_score) or self.first_time:\n+#                     self.best_score = score\n+#                     quantized_model = quantization.quantize_dynamic(\n+#                         runner.model.cpu(),\n+#                         qconfig_spec=self.qconfig_spec,\n+#                         dtype=self.dtype,\n+#                     )\n+#                     save_quantized_model(\n+#                         model=quantized_model,\n+#                         logdir=runner.logdir,\n+#                         checkpoint_name=self.mode,\n+#                         out_model=self.out_model,\n+#                         out_dir=self.out_dir,\n+#                     )\n+#                     self.first_time = False\n+#             else:\n+#                 quantized_model = quantization.quantize_dynamic(\n+#                     runner.model.cpu(),\n+#                     qconfig_spec=self.qconfig_spec,\n+#                     dtype=self.dtype,\n+#                 )\n+#                 save_quantized_model(\n+#                     model=quantized_model,\n+#                     logdir=runner.logdir,\n+#                     checkpoint_name=self.mode,\n+#                     out_model=self.out_model,\n+#                     out_dir=self.out_dir,\n+#                 )\n+#\n+#     def on_stage_end(self, runner: \"IRunner\") -> None:\n+#         \"\"\"\n+#         On stage end action.\n+#\n+#         Args:\n+#             runner: runner of your experiment\n+#         \"\"\"\n+#         if self.do_once:\n+#             quantized_model = quantization.quantize_dynamic(\n+#                 runner.model.cpu(),\n+#                 qconfig_spec=self.qconfig_spec,\n+#                 dtype=self.dtype,\n+#             )\n+#             save_quantized_model(\n+#                 model=quantized_model,\n+#                 logdir=runner.logdir,\n+#                 checkpoint_name=self.mode,\n+#                 out_model=self.out_model,\n+#                 out_dir=self.out_dir,\n+#             )\n+#\n+#\n+# __all__ = [\"DynamicQuantizationCallback\"]\n",
        "source_code_with_indent": "from typing import Dict, Optional, Set, TYPE_CHECKING, Union\nfrom pathlib import Path\n\nimport torch\nfrom torch import quantization\n\nfrom catalyst.core.callback import Callback, CallbackOrder\nfrom catalyst.utils.quantization import save_quantized_model\n\nif TYPE_CHECKING:\n    <IND>from catalyst.core.runner import IRunner\n\n\n<DED>class DynamicQuantizationCallback(Callback):\n    <IND>\"\"\"Dynamic Quantization Callback\n\n    This callback applying dynamic quantization to the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        metric: str = \"loss\",\n        minimize: bool = True,\n        min_delta: float = 1e-6,\n        mode: str = \"best\",\n        do_once: bool = True,\n        qconfig_spec: Optional[Union[Set, Dict]] = None,\n        dtype: Optional[torch.dtype] = torch.qint8,\n        out_dir: Union[str, Path] = None,\n        out_model: Union[str, Path] = None,\n        backend: str = None,\n    ):\n        <IND>\"\"\"Init method for callback\n\n        Args:\n            metric: Metric key we should trace model based on\n            minimize: Whether do we minimize metric or not\n            min_delta: Minimum value of change for metric to be\n                considered as improved\n            mode: One of `best` or `last`\n            do_once: Whether do we trace once per stage or every epoch\n            qconfig_spec: torch.quantization.quantize_dynamic\n                parameter, you can define layers to be quantize\n            dtype: type of the model parameters, default int8\n            out_dir (Union[str, Path]): Directory to save model to\n            out_model (Union[str, Path]): Path to save model to\n                (overrides `out_dir` argument)\n            backend: defines backend for quantization\n        \"\"\"\n        super().__init__(order=CallbackOrder.external)\n\n        if mode not in [\"best\", \"last\"]:\n            <IND>raise ValueError(\n                f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n            )\n\n        <DED>self.metric = metric\n        self.mode = mode\n        self.do_once = do_once\n        self.best_score = None\n        self.is_better = None\n        self.first_time = True\n        if minimize:\n            <IND>self.is_better = lambda score, best: score <= (best - min_delta)\n        <DED>else:\n            <IND>self.is_better = lambda score, best: score >= (best + min_delta)\n\n        <DED>self.opt_level = None\n\n        if out_model is not None:\n            <IND>out_model = Path(out_model)\n        <DED>self.out_model = out_model\n\n        if out_dir is not None:\n            <IND>out_dir = Path(out_dir)\n        <DED>self.out_dir = out_dir\n        self.qconfig_spec = qconfig_spec\n        self.dtype = dtype\n\n        if backend is not None:\n            <IND>torch.backends.quantized.engine = backend\n\n    <DED><DED>def on_epoch_end(self, runner: \"IRunner\"):\n        <IND>\"\"\"\n        Performing model quantization on epoch end if condition metric is\n        improved\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if not self.do_once:\n            <IND>if self.mode == \"best\":\n                <IND>score = runner.valid_metrics[self.metric]\n\n                if self.best_score is None:\n                    <IND>self.best_score = score\n\n                <DED>if self.is_better(score, self.best_score) or self.first_time:\n                    <IND>self.best_score = score\n                    quantized_model = quantization.quantize_dynamic(\n                        runner.model.cpu(),\n                        qconfig_spec=self.qconfig_spec,\n                        dtype=self.dtype,\n                    )\n                    save_quantized_model(\n                        model=quantized_model,\n                        logdir=runner.logdir,\n                        checkpoint_name=self.mode,\n                        out_model=self.out_model,\n                        out_dir=self.out_dir,\n                    )\n                    self.first_time = False\n            <DED><DED>else:\n                <IND>quantized_model = quantization.quantize_dynamic(\n                    runner.model.cpu(),\n                    qconfig_spec=self.qconfig_spec,\n                    dtype=self.dtype,\n                )\n                save_quantized_model(\n                    model=quantized_model,\n                    logdir=runner.logdir,\n                    checkpoint_name=self.mode,\n                    out_model=self.out_model,\n                    out_dir=self.out_dir,\n                )\n\n    <DED><DED><DED>def on_stage_end(self, runner: \"IRunner\") -> None:\n        <IND>\"\"\"\n        On stage end action.\n\n        Args:\n            runner: runner of your experiment\n        \"\"\"\n        if self.do_once:\n            <IND>quantized_model = quantization.quantize_dynamic(\n                runner.model.cpu(),\n                qconfig_spec=self.qconfig_spec,\n                dtype=self.dtype,\n            )\n            save_quantized_model(\n                model=quantized_model,\n                logdir=runner.logdir,\n                checkpoint_name=self.mode,\n                out_model=self.out_model,\n                out_dir=self.out_dir,\n            )\n\n\n<DED><DED><DED>__all__ = [\"DynamicQuantizationCallback\"]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "# # @TODO: make the same API for tracing/onnx/pruning/quantization\n# from typing import Dict, Optional, Set, TYPE_CHECKING, Union\n# from pathlib import Path\n#\n# import torch\n# from torch import quantization\n#\n# from catalyst.core.callback import Callback, CallbackOrder\n# from catalyst.utils.quantization import save_quantized_model\n#\n# if TYPE_CHECKING:\n#     from catalyst.core.runner import IRunner\n#\n#\n# class DynamicQuantizationCallback(Callback):\n#     \"\"\"Dynamic Quantization Callback\n#\n#     This callback applying dynamic quantization to the model.\n#     \"\"\"\n#\n#     def __init__(\n#         self,\n#         metric: str = \"loss\",\n#         minimize: bool = True,\n#         min_delta: float = 1e-6,\n#         mode: str = \"best\",\n#         do_once: bool = True,\n#         qconfig_spec: Optional[Union[Set, Dict]] = None,\n#         dtype: Optional[torch.dtype] = torch.qint8,\n#         out_dir: Union[str, Path] = None,\n#         out_model: Union[str, Path] = None,\n#         backend: str = None,\n#     ):\n#         \"\"\"Init method for callback\n#\n#         Args:\n#             metric: Metric key we should trace model based on\n#             minimize: Whether do we minimize metric or not\n#             min_delta: Minimum value of change for metric to be\n#                 considered as improved\n#             mode: One of `best` or `last`\n#             do_once: Whether do we trace once per stage or every epoch\n#             qconfig_spec: torch.quantization.quantize_dynamic\n#                 parameter, you can define layers to be quantize\n#             dtype: type of the model parameters, default int8\n#             out_dir (Union[str, Path]): Directory to save model to\n#             out_model (Union[str, Path]): Path to save model to\n#                 (overrides `out_dir` argument)\n#             backend: defines backend for quantization\n#         \"\"\"\n#         super().__init__(order=CallbackOrder.external)\n#\n#         if mode not in [\"best\", \"last\"]:\n#             raise ValueError(\n#                 f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n#             )\n#\n#         self.metric = metric\n#         self.mode = mode\n#         self.do_once = do_once\n#         self.best_score = None\n#         self.is_better = None\n#         self.first_time = True\n#         if minimize:\n#             self.is_better = lambda score, best: score <= (best - min_delta)\n#         else:\n#             self.is_better = lambda score, best: score >= (best + min_delta)\n#\n#         self.opt_level = None\n#\n#         if out_model is not None:\n#             out_model = Path(out_model)\n#         self.out_model = out_model\n#\n#         if out_dir is not None:\n#             out_dir = Path(out_dir)\n#         self.out_dir = out_dir\n#         self.qconfig_spec = qconfig_spec\n#         self.dtype = dtype\n#\n#         if backend is not None:\n#             torch.backends.quantized.engine = backend\n#\n#     def on_epoch_end(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model quantization on epoch end if condition metric is\n#         improved\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if not self.do_once:\n#             if self.mode == \"best\":\n#                 score = runner.valid_metrics[self.metric]\n#\n#                 if self.best_score is None:\n#                     self.best_score = score\n#\n#                 if self.is_better(score, self.best_score) or self.first_time:\n#                     self.best_score = score\n#                     quantized_model = quantization.quantize_dynamic(\n#                         runner.model.cpu(),\n#                         qconfig_spec=self.qconfig_spec,\n#                         dtype=self.dtype,\n#                     )\n#                     save_quantized_model(\n#                         model=quantized_model,\n#                         logdir=runner.logdir,\n#                         checkpoint_name=self.mode,\n#                         out_model=self.out_model,\n#                         out_dir=self.out_dir,\n#                     )\n#                     self.first_time = False\n#             else:\n#                 quantized_model = quantization.quantize_dynamic(\n#                     runner.model.cpu(),\n#                     qconfig_spec=self.qconfig_spec,\n#                     dtype=self.dtype,\n#                 )\n#                 save_quantized_model(\n#                     model=quantized_model,\n#                     logdir=runner.logdir,\n#                     checkpoint_name=self.mode,\n#                     out_model=self.out_model,\n#                     out_dir=self.out_dir,\n#                 )\n#\n#     def on_stage_end(self, runner: \"IRunner\") -> None:\n#         \"\"\"\n#         On stage end action.\n#\n#         Args:\n#             runner: runner of your experiment\n#         \"\"\"\n#         if self.do_once:\n#             quantized_model = quantization.quantize_dynamic(\n#                 runner.model.cpu(),\n#                 qconfig_spec=self.qconfig_spec,\n#                 dtype=self.dtype,\n#             )\n#             save_quantized_model(\n#                 model=quantized_model,\n#                 logdir=runner.logdir,\n#                 checkpoint_name=self.mode,\n#                 out_model=self.out_model,\n#                 out_dir=self.out_dir,\n#             )\n#\n#\n# __all__ = [\"DynamicQuantizationCallback\"]\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/quantization.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/quantization.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/quantization.py:30:8 Incompatible variable type [9]: out_model is declared to have type `Union[Path, str]` but is used as type `None`.",
    "message": " out_model is declared to have type `Union[Path, str]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 30,
    "warning_line": "        out_model: Union[str, Path] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "from typing import Dict, Optional, Set, TYPE_CHECKING, Union\nfrom pathlib import Path\n\nimport torch\nfrom torch import quantization\n\nfrom catalyst.core.callback import Callback, CallbackOrder\nfrom catalyst.utils.quantization import save_quantized_model\n\nif TYPE_CHECKING:\n    from catalyst.core.runner import IRunner\n\n\nclass DynamicQuantizationCallback(Callback):\n    \"\"\"Dynamic Quantization Callback\n\n    This callback applying dynamic quantization to the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        metric: str = \"loss\",\n        minimize: bool = True,\n        min_delta: float = 1e-6,\n        mode: str = \"best\",\n        do_once: bool = True,\n        qconfig_spec: Optional[Union[Set, Dict]] = None,\n        dtype: Optional[torch.dtype] = torch.qint8,\n        out_dir: Union[str, Path] = None,\n        out_model: Union[str, Path] = None,\n        backend: str = None,\n    ):\n        \"\"\"Init method for callback\n\n        Args:\n            metric: Metric key we should trace model based on\n            minimize: Whether do we minimize metric or not\n            min_delta: Minimum value of change for metric to be\n                considered as improved\n            mode: One of `best` or `last`\n            do_once: Whether do we trace once per stage or every epoch\n            qconfig_spec: torch.quantization.quantize_dynamic\n                parameter, you can define layers to be quantize\n            dtype: type of the model parameters, default int8\n            out_dir (Union[str, Path]): Directory to save model to\n            out_model (Union[str, Path]): Path to save model to\n                (overrides `out_dir` argument)\n            backend: defines backend for quantization\n        \"\"\"\n        super().__init__(order=CallbackOrder.external)\n\n        if mode not in [\"best\", \"last\"]:\n            raise ValueError(\n                f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n            )\n\n        self.metric = metric\n        self.mode = mode\n        self.do_once = do_once\n        self.best_score = None\n        self.is_better = None\n        self.first_time = True\n        if minimize:\n            self.is_better = lambda score, best: score <= (best - min_delta)\n        else:\n            self.is_better = lambda score, best: score >= (best + min_delta)\n\n        self.opt_level = None\n\n        if out_model is not None:\n            out_model = Path(out_model)\n        self.out_model = out_model\n\n        if out_dir is not None:\n            out_dir = Path(out_dir)\n        self.out_dir = out_dir\n        self.qconfig_spec = qconfig_spec\n        self.dtype = dtype\n\n        if backend is not None:\n            torch.backends.quantized.engine = backend\n\n    def on_epoch_end(self, runner: \"IRunner\"):\n        \"\"\"\n        Performing model quantization on epoch end if condition metric is\n        improved\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if not self.do_once:\n            if self.mode == \"best\":\n                score = runner.valid_metrics[self.metric]\n\n                if self.best_score is None:\n                    self.best_score = score\n\n                if self.is_better(score, self.best_score) or self.first_time:\n                    self.best_score = score\n                    quantized_model = quantization.quantize_dynamic(\n                        runner.model.cpu(),\n                        qconfig_spec=self.qconfig_spec,\n                        dtype=self.dtype,\n                    )\n                    save_quantized_model(\n                        model=quantized_model,\n                        logdir=runner.logdir,\n                        checkpoint_name=self.mode,\n                        out_model=self.out_model,\n                        out_dir=self.out_dir,\n                    )\n                    self.first_time = False\n            else:\n                quantized_model = quantization.quantize_dynamic(\n                    runner.model.cpu(),\n                    qconfig_spec=self.qconfig_spec,\n                    dtype=self.dtype,\n                )\n                save_quantized_model(\n                    model=quantized_model,\n                    logdir=runner.logdir,\n                    checkpoint_name=self.mode,\n                    out_model=self.out_model,\n                    out_dir=self.out_dir,\n                )\n\n    def on_stage_end(self, runner: \"IRunner\") -> None:\n        \"\"\"\n        On stage end action.\n\n        Args:\n            runner: runner of your experiment\n        \"\"\"\n        if self.do_once:\n            quantized_model = quantization.quantize_dynamic(\n                runner.model.cpu(),\n                qconfig_spec=self.qconfig_spec,\n                dtype=self.dtype,\n            )\n            save_quantized_model(\n                model=quantized_model,\n                logdir=runner.logdir,\n                checkpoint_name=self.mode,\n                out_model=self.out_model,\n                out_dir=self.out_dir,\n            )\n\n\n__all__ = [\"DynamicQuantizationCallback\"]\n",
        "source_code_len": 5000,
        "target_code": "# # @TODO: make the same API for tracing/onnx/pruning/quantization\n# from typing import Dict, Optional, Set, TYPE_CHECKING, Union\n# from pathlib import Path\n#\n# import torch\n# from torch import quantization\n#\n# from catalyst.core.callback import Callback, CallbackOrder\n# from catalyst.utils.quantization import save_quantized_model\n#\n# if TYPE_CHECKING:\n#     from catalyst.core.runner import IRunner\n#\n#\n# class DynamicQuantizationCallback(Callback):\n#     \"\"\"Dynamic Quantization Callback\n#\n#     This callback applying dynamic quantization to the model.\n#     \"\"\"\n#\n#     def __init__(\n#         self,\n#         metric: str = \"loss\",\n#         minimize: bool = True,\n#         min_delta: float = 1e-6,\n#         mode: str = \"best\",\n#         do_once: bool = True,\n#         qconfig_spec: Optional[Union[Set, Dict]] = None,\n#         dtype: Optional[torch.dtype] = torch.qint8,\n#         out_dir: Union[str, Path] = None,\n#         out_model: Union[str, Path] = None,\n#         backend: str = None,\n#     ):\n#         \"\"\"Init method for callback\n#\n#         Args:\n#             metric: Metric key we should trace model based on\n#             minimize: Whether do we minimize metric or not\n#             min_delta: Minimum value of change for metric to be\n#                 considered as improved\n#             mode: One of `best` or `last`\n#             do_once: Whether do we trace once per stage or every epoch\n#             qconfig_spec: torch.quantization.quantize_dynamic\n#                 parameter, you can define layers to be quantize\n#             dtype: type of the model parameters, default int8\n#             out_dir (Union[str, Path]): Directory to save model to\n#             out_model (Union[str, Path]): Path to save model to\n#                 (overrides `out_dir` argument)\n#             backend: defines backend for quantization\n#         \"\"\"\n#         super().__init__(order=CallbackOrder.external)\n#\n#         if mode not in [\"best\", \"last\"]:\n#             raise ValueError(\n#                 f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n#             )\n#\n#         self.metric = metric\n#         self.mode = mode\n#         self.do_once = do_once\n#         self.best_score = None\n#         self.is_better = None\n#         self.first_time = True\n#         if minimize:\n#             self.is_better = lambda score, best: score <= (best - min_delta)\n#         else:\n#             self.is_better = lambda score, best: score >= (best + min_delta)\n#\n#         self.opt_level = None\n#\n#         if out_model is not None:\n#             out_model = Path(out_model)\n#         self.out_model = out_model\n#\n#         if out_dir is not None:\n#             out_dir = Path(out_dir)\n#         self.out_dir = out_dir\n#         self.qconfig_spec = qconfig_spec\n#         self.dtype = dtype\n#\n#         if backend is not None:\n#             torch.backends.quantized.engine = backend\n#\n#     def on_epoch_end(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model quantization on epoch end if condition metric is\n#         improved\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if not self.do_once:\n#             if self.mode == \"best\":\n#                 score = runner.valid_metrics[self.metric]\n#\n#                 if self.best_score is None:\n#                     self.best_score = score\n#\n#                 if self.is_better(score, self.best_score) or self.first_time:\n#                     self.best_score = score\n#                     quantized_model = quantization.quantize_dynamic(\n#                         runner.model.cpu(),\n#                         qconfig_spec=self.qconfig_spec,\n#                         dtype=self.dtype,\n#                     )\n#                     save_quantized_model(\n#                         model=quantized_model,\n#                         logdir=runner.logdir,\n#                         checkpoint_name=self.mode,\n#                         out_model=self.out_model,\n#                         out_dir=self.out_dir,\n#                     )\n#                     self.first_time = False\n#             else:\n#                 quantized_model = quantization.quantize_dynamic(\n#                     runner.model.cpu(),\n#                     qconfig_spec=self.qconfig_spec,\n#                     dtype=self.dtype,\n#                 )\n#                 save_quantized_model(\n#                     model=quantized_model,\n#                     logdir=runner.logdir,\n#                     checkpoint_name=self.mode,\n#                     out_model=self.out_model,\n#                     out_dir=self.out_dir,\n#                 )\n#\n#     def on_stage_end(self, runner: \"IRunner\") -> None:\n#         \"\"\"\n#         On stage end action.\n#\n#         Args:\n#             runner: runner of your experiment\n#         \"\"\"\n#         if self.do_once:\n#             quantized_model = quantization.quantize_dynamic(\n#                 runner.model.cpu(),\n#                 qconfig_spec=self.qconfig_spec,\n#                 dtype=self.dtype,\n#             )\n#             save_quantized_model(\n#                 model=quantized_model,\n#                 logdir=runner.logdir,\n#                 checkpoint_name=self.mode,\n#                 out_model=self.out_model,\n#                 out_dir=self.out_dir,\n#             )\n#\n#\n# __all__ = [\"DynamicQuantizationCallback\"]\n",
        "target_code_len": 5343,
        "diff_format": "@@ -1,149 +1,150 @@\n-from typing import Dict, Optional, Set, TYPE_CHECKING, Union\n-from pathlib import Path\n-\n-import torch\n-from torch import quantization\n-\n-from catalyst.core.callback import Callback, CallbackOrder\n-from catalyst.utils.quantization import save_quantized_model\n-\n-if TYPE_CHECKING:\n-    from catalyst.core.runner import IRunner\n-\n-\n-class DynamicQuantizationCallback(Callback):\n-    \"\"\"Dynamic Quantization Callback\n-\n-    This callback applying dynamic quantization to the model.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        metric: str = \"loss\",\n-        minimize: bool = True,\n-        min_delta: float = 1e-6,\n-        mode: str = \"best\",\n-        do_once: bool = True,\n-        qconfig_spec: Optional[Union[Set, Dict]] = None,\n-        dtype: Optional[torch.dtype] = torch.qint8,\n-        out_dir: Union[str, Path] = None,\n-        out_model: Union[str, Path] = None,\n-        backend: str = None,\n-    ):\n-        \"\"\"Init method for callback\n-\n-        Args:\n-            metric: Metric key we should trace model based on\n-            minimize: Whether do we minimize metric or not\n-            min_delta: Minimum value of change for metric to be\n-                considered as improved\n-            mode: One of `best` or `last`\n-            do_once: Whether do we trace once per stage or every epoch\n-            qconfig_spec: torch.quantization.quantize_dynamic\n-                parameter, you can define layers to be quantize\n-            dtype: type of the model parameters, default int8\n-            out_dir (Union[str, Path]): Directory to save model to\n-            out_model (Union[str, Path]): Path to save model to\n-                (overrides `out_dir` argument)\n-            backend: defines backend for quantization\n-        \"\"\"\n-        super().__init__(order=CallbackOrder.external)\n-\n-        if mode not in [\"best\", \"last\"]:\n-            raise ValueError(\n-                f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n-            )\n-\n-        self.metric = metric\n-        self.mode = mode\n-        self.do_once = do_once\n-        self.best_score = None\n-        self.is_better = None\n-        self.first_time = True\n-        if minimize:\n-            self.is_better = lambda score, best: score <= (best - min_delta)\n-        else:\n-            self.is_better = lambda score, best: score >= (best + min_delta)\n-\n-        self.opt_level = None\n-\n-        if out_model is not None:\n-            out_model = Path(out_model)\n-        self.out_model = out_model\n-\n-        if out_dir is not None:\n-            out_dir = Path(out_dir)\n-        self.out_dir = out_dir\n-        self.qconfig_spec = qconfig_spec\n-        self.dtype = dtype\n-\n-        if backend is not None:\n-            torch.backends.quantized.engine = backend\n-\n-    def on_epoch_end(self, runner: \"IRunner\"):\n-        \"\"\"\n-        Performing model quantization on epoch end if condition metric is\n-        improved\n-\n-        Args:\n-            runner: current runner\n-        \"\"\"\n-        if not self.do_once:\n-            if self.mode == \"best\":\n-                score = runner.valid_metrics[self.metric]\n-\n-                if self.best_score is None:\n-                    self.best_score = score\n-\n-                if self.is_better(score, self.best_score) or self.first_time:\n-                    self.best_score = score\n-                    quantized_model = quantization.quantize_dynamic(\n-                        runner.model.cpu(),\n-                        qconfig_spec=self.qconfig_spec,\n-                        dtype=self.dtype,\n-                    )\n-                    save_quantized_model(\n-                        model=quantized_model,\n-                        logdir=runner.logdir,\n-                        checkpoint_name=self.mode,\n-                        out_model=self.out_model,\n-                        out_dir=self.out_dir,\n-                    )\n-                    self.first_time = False\n-            else:\n-                quantized_model = quantization.quantize_dynamic(\n-                    runner.model.cpu(),\n-                    qconfig_spec=self.qconfig_spec,\n-                    dtype=self.dtype,\n-                )\n-                save_quantized_model(\n-                    model=quantized_model,\n-                    logdir=runner.logdir,\n-                    checkpoint_name=self.mode,\n-                    out_model=self.out_model,\n-                    out_dir=self.out_dir,\n-                )\n-\n-    def on_stage_end(self, runner: \"IRunner\") -> None:\n-        \"\"\"\n-        On stage end action.\n-\n-        Args:\n-            runner: runner of your experiment\n-        \"\"\"\n-        if self.do_once:\n-            quantized_model = quantization.quantize_dynamic(\n-                runner.model.cpu(),\n-                qconfig_spec=self.qconfig_spec,\n-                dtype=self.dtype,\n-            )\n-            save_quantized_model(\n-                model=quantized_model,\n-                logdir=runner.logdir,\n-                checkpoint_name=self.mode,\n-                out_model=self.out_model,\n-                out_dir=self.out_dir,\n-            )\n-\n-\n-__all__ = [\"DynamicQuantizationCallback\"]\n+# # @TODO: make the same API for tracing/onnx/pruning/quantization\n+# from typing import Dict, Optional, Set, TYPE_CHECKING, Union\n+# from pathlib import Path\n+#\n+# import torch\n+# from torch import quantization\n+#\n+# from catalyst.core.callback import Callback, CallbackOrder\n+# from catalyst.utils.quantization import save_quantized_model\n+#\n+# if TYPE_CHECKING:\n+#     from catalyst.core.runner import IRunner\n+#\n+#\n+# class DynamicQuantizationCallback(Callback):\n+#     \"\"\"Dynamic Quantization Callback\n+#\n+#     This callback applying dynamic quantization to the model.\n+#     \"\"\"\n+#\n+#     def __init__(\n+#         self,\n+#         metric: str = \"loss\",\n+#         minimize: bool = True,\n+#         min_delta: float = 1e-6,\n+#         mode: str = \"best\",\n+#         do_once: bool = True,\n+#         qconfig_spec: Optional[Union[Set, Dict]] = None,\n+#         dtype: Optional[torch.dtype] = torch.qint8,\n+#         out_dir: Union[str, Path] = None,\n+#         out_model: Union[str, Path] = None,\n+#         backend: str = None,\n+#     ):\n+#         \"\"\"Init method for callback\n+#\n+#         Args:\n+#             metric: Metric key we should trace model based on\n+#             minimize: Whether do we minimize metric or not\n+#             min_delta: Minimum value of change for metric to be\n+#                 considered as improved\n+#             mode: One of `best` or `last`\n+#             do_once: Whether do we trace once per stage or every epoch\n+#             qconfig_spec: torch.quantization.quantize_dynamic\n+#                 parameter, you can define layers to be quantize\n+#             dtype: type of the model parameters, default int8\n+#             out_dir (Union[str, Path]): Directory to save model to\n+#             out_model (Union[str, Path]): Path to save model to\n+#                 (overrides `out_dir` argument)\n+#             backend: defines backend for quantization\n+#         \"\"\"\n+#         super().__init__(order=CallbackOrder.external)\n+#\n+#         if mode not in [\"best\", \"last\"]:\n+#             raise ValueError(\n+#                 f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n+#             )\n+#\n+#         self.metric = metric\n+#         self.mode = mode\n+#         self.do_once = do_once\n+#         self.best_score = None\n+#         self.is_better = None\n+#         self.first_time = True\n+#         if minimize:\n+#             self.is_better = lambda score, best: score <= (best - min_delta)\n+#         else:\n+#             self.is_better = lambda score, best: score >= (best + min_delta)\n+#\n+#         self.opt_level = None\n+#\n+#         if out_model is not None:\n+#             out_model = Path(out_model)\n+#         self.out_model = out_model\n+#\n+#         if out_dir is not None:\n+#             out_dir = Path(out_dir)\n+#         self.out_dir = out_dir\n+#         self.qconfig_spec = qconfig_spec\n+#         self.dtype = dtype\n+#\n+#         if backend is not None:\n+#             torch.backends.quantized.engine = backend\n+#\n+#     def on_epoch_end(self, runner: \"IRunner\"):\n+#         \"\"\"\n+#         Performing model quantization on epoch end if condition metric is\n+#         improved\n+#\n+#         Args:\n+#             runner: current runner\n+#         \"\"\"\n+#         if not self.do_once:\n+#             if self.mode == \"best\":\n+#                 score = runner.valid_metrics[self.metric]\n+#\n+#                 if self.best_score is None:\n+#                     self.best_score = score\n+#\n+#                 if self.is_better(score, self.best_score) or self.first_time:\n+#                     self.best_score = score\n+#                     quantized_model = quantization.quantize_dynamic(\n+#                         runner.model.cpu(),\n+#                         qconfig_spec=self.qconfig_spec,\n+#                         dtype=self.dtype,\n+#                     )\n+#                     save_quantized_model(\n+#                         model=quantized_model,\n+#                         logdir=runner.logdir,\n+#                         checkpoint_name=self.mode,\n+#                         out_model=self.out_model,\n+#                         out_dir=self.out_dir,\n+#                     )\n+#                     self.first_time = False\n+#             else:\n+#                 quantized_model = quantization.quantize_dynamic(\n+#                     runner.model.cpu(),\n+#                     qconfig_spec=self.qconfig_spec,\n+#                     dtype=self.dtype,\n+#                 )\n+#                 save_quantized_model(\n+#                     model=quantized_model,\n+#                     logdir=runner.logdir,\n+#                     checkpoint_name=self.mode,\n+#                     out_model=self.out_model,\n+#                     out_dir=self.out_dir,\n+#                 )\n+#\n+#     def on_stage_end(self, runner: \"IRunner\") -> None:\n+#         \"\"\"\n+#         On stage end action.\n+#\n+#         Args:\n+#             runner: runner of your experiment\n+#         \"\"\"\n+#         if self.do_once:\n+#             quantized_model = quantization.quantize_dynamic(\n+#                 runner.model.cpu(),\n+#                 qconfig_spec=self.qconfig_spec,\n+#                 dtype=self.dtype,\n+#             )\n+#             save_quantized_model(\n+#                 model=quantized_model,\n+#                 logdir=runner.logdir,\n+#                 checkpoint_name=self.mode,\n+#                 out_model=self.out_model,\n+#                 out_dir=self.out_dir,\n+#             )\n+#\n+#\n+# __all__ = [\"DynamicQuantizationCallback\"]\n",
        "source_code_with_indent": "from typing import Dict, Optional, Set, TYPE_CHECKING, Union\nfrom pathlib import Path\n\nimport torch\nfrom torch import quantization\n\nfrom catalyst.core.callback import Callback, CallbackOrder\nfrom catalyst.utils.quantization import save_quantized_model\n\nif TYPE_CHECKING:\n    <IND>from catalyst.core.runner import IRunner\n\n\n<DED>class DynamicQuantizationCallback(Callback):\n    <IND>\"\"\"Dynamic Quantization Callback\n\n    This callback applying dynamic quantization to the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        metric: str = \"loss\",\n        minimize: bool = True,\n        min_delta: float = 1e-6,\n        mode: str = \"best\",\n        do_once: bool = True,\n        qconfig_spec: Optional[Union[Set, Dict]] = None,\n        dtype: Optional[torch.dtype] = torch.qint8,\n        out_dir: Union[str, Path] = None,\n        out_model: Union[str, Path] = None,\n        backend: str = None,\n    ):\n        <IND>\"\"\"Init method for callback\n\n        Args:\n            metric: Metric key we should trace model based on\n            minimize: Whether do we minimize metric or not\n            min_delta: Minimum value of change for metric to be\n                considered as improved\n            mode: One of `best` or `last`\n            do_once: Whether do we trace once per stage or every epoch\n            qconfig_spec: torch.quantization.quantize_dynamic\n                parameter, you can define layers to be quantize\n            dtype: type of the model parameters, default int8\n            out_dir (Union[str, Path]): Directory to save model to\n            out_model (Union[str, Path]): Path to save model to\n                (overrides `out_dir` argument)\n            backend: defines backend for quantization\n        \"\"\"\n        super().__init__(order=CallbackOrder.external)\n\n        if mode not in [\"best\", \"last\"]:\n            <IND>raise ValueError(\n                f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n            )\n\n        <DED>self.metric = metric\n        self.mode = mode\n        self.do_once = do_once\n        self.best_score = None\n        self.is_better = None\n        self.first_time = True\n        if minimize:\n            <IND>self.is_better = lambda score, best: score <= (best - min_delta)\n        <DED>else:\n            <IND>self.is_better = lambda score, best: score >= (best + min_delta)\n\n        <DED>self.opt_level = None\n\n        if out_model is not None:\n            <IND>out_model = Path(out_model)\n        <DED>self.out_model = out_model\n\n        if out_dir is not None:\n            <IND>out_dir = Path(out_dir)\n        <DED>self.out_dir = out_dir\n        self.qconfig_spec = qconfig_spec\n        self.dtype = dtype\n\n        if backend is not None:\n            <IND>torch.backends.quantized.engine = backend\n\n    <DED><DED>def on_epoch_end(self, runner: \"IRunner\"):\n        <IND>\"\"\"\n        Performing model quantization on epoch end if condition metric is\n        improved\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if not self.do_once:\n            <IND>if self.mode == \"best\":\n                <IND>score = runner.valid_metrics[self.metric]\n\n                if self.best_score is None:\n                    <IND>self.best_score = score\n\n                <DED>if self.is_better(score, self.best_score) or self.first_time:\n                    <IND>self.best_score = score\n                    quantized_model = quantization.quantize_dynamic(\n                        runner.model.cpu(),\n                        qconfig_spec=self.qconfig_spec,\n                        dtype=self.dtype,\n                    )\n                    save_quantized_model(\n                        model=quantized_model,\n                        logdir=runner.logdir,\n                        checkpoint_name=self.mode,\n                        out_model=self.out_model,\n                        out_dir=self.out_dir,\n                    )\n                    self.first_time = False\n            <DED><DED>else:\n                <IND>quantized_model = quantization.quantize_dynamic(\n                    runner.model.cpu(),\n                    qconfig_spec=self.qconfig_spec,\n                    dtype=self.dtype,\n                )\n                save_quantized_model(\n                    model=quantized_model,\n                    logdir=runner.logdir,\n                    checkpoint_name=self.mode,\n                    out_model=self.out_model,\n                    out_dir=self.out_dir,\n                )\n\n    <DED><DED><DED>def on_stage_end(self, runner: \"IRunner\") -> None:\n        <IND>\"\"\"\n        On stage end action.\n\n        Args:\n            runner: runner of your experiment\n        \"\"\"\n        if self.do_once:\n            <IND>quantized_model = quantization.quantize_dynamic(\n                runner.model.cpu(),\n                qconfig_spec=self.qconfig_spec,\n                dtype=self.dtype,\n            )\n            save_quantized_model(\n                model=quantized_model,\n                logdir=runner.logdir,\n                checkpoint_name=self.mode,\n                out_model=self.out_model,\n                out_dir=self.out_dir,\n            )\n\n\n<DED><DED><DED>__all__ = [\"DynamicQuantizationCallback\"]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "# # @TODO: make the same API for tracing/onnx/pruning/quantization\n# from typing import Dict, Optional, Set, TYPE_CHECKING, Union\n# from pathlib import Path\n#\n# import torch\n# from torch import quantization\n#\n# from catalyst.core.callback import Callback, CallbackOrder\n# from catalyst.utils.quantization import save_quantized_model\n#\n# if TYPE_CHECKING:\n#     from catalyst.core.runner import IRunner\n#\n#\n# class DynamicQuantizationCallback(Callback):\n#     \"\"\"Dynamic Quantization Callback\n#\n#     This callback applying dynamic quantization to the model.\n#     \"\"\"\n#\n#     def __init__(\n#         self,\n#         metric: str = \"loss\",\n#         minimize: bool = True,\n#         min_delta: float = 1e-6,\n#         mode: str = \"best\",\n#         do_once: bool = True,\n#         qconfig_spec: Optional[Union[Set, Dict]] = None,\n#         dtype: Optional[torch.dtype] = torch.qint8,\n#         out_dir: Union[str, Path] = None,\n#         out_model: Union[str, Path] = None,\n#         backend: str = None,\n#     ):\n#         \"\"\"Init method for callback\n#\n#         Args:\n#             metric: Metric key we should trace model based on\n#             minimize: Whether do we minimize metric or not\n#             min_delta: Minimum value of change for metric to be\n#                 considered as improved\n#             mode: One of `best` or `last`\n#             do_once: Whether do we trace once per stage or every epoch\n#             qconfig_spec: torch.quantization.quantize_dynamic\n#                 parameter, you can define layers to be quantize\n#             dtype: type of the model parameters, default int8\n#             out_dir (Union[str, Path]): Directory to save model to\n#             out_model (Union[str, Path]): Path to save model to\n#                 (overrides `out_dir` argument)\n#             backend: defines backend for quantization\n#         \"\"\"\n#         super().__init__(order=CallbackOrder.external)\n#\n#         if mode not in [\"best\", \"last\"]:\n#             raise ValueError(\n#                 f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n#             )\n#\n#         self.metric = metric\n#         self.mode = mode\n#         self.do_once = do_once\n#         self.best_score = None\n#         self.is_better = None\n#         self.first_time = True\n#         if minimize:\n#             self.is_better = lambda score, best: score <= (best - min_delta)\n#         else:\n#             self.is_better = lambda score, best: score >= (best + min_delta)\n#\n#         self.opt_level = None\n#\n#         if out_model is not None:\n#             out_model = Path(out_model)\n#         self.out_model = out_model\n#\n#         if out_dir is not None:\n#             out_dir = Path(out_dir)\n#         self.out_dir = out_dir\n#         self.qconfig_spec = qconfig_spec\n#         self.dtype = dtype\n#\n#         if backend is not None:\n#             torch.backends.quantized.engine = backend\n#\n#     def on_epoch_end(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model quantization on epoch end if condition metric is\n#         improved\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if not self.do_once:\n#             if self.mode == \"best\":\n#                 score = runner.valid_metrics[self.metric]\n#\n#                 if self.best_score is None:\n#                     self.best_score = score\n#\n#                 if self.is_better(score, self.best_score) or self.first_time:\n#                     self.best_score = score\n#                     quantized_model = quantization.quantize_dynamic(\n#                         runner.model.cpu(),\n#                         qconfig_spec=self.qconfig_spec,\n#                         dtype=self.dtype,\n#                     )\n#                     save_quantized_model(\n#                         model=quantized_model,\n#                         logdir=runner.logdir,\n#                         checkpoint_name=self.mode,\n#                         out_model=self.out_model,\n#                         out_dir=self.out_dir,\n#                     )\n#                     self.first_time = False\n#             else:\n#                 quantized_model = quantization.quantize_dynamic(\n#                     runner.model.cpu(),\n#                     qconfig_spec=self.qconfig_spec,\n#                     dtype=self.dtype,\n#                 )\n#                 save_quantized_model(\n#                     model=quantized_model,\n#                     logdir=runner.logdir,\n#                     checkpoint_name=self.mode,\n#                     out_model=self.out_model,\n#                     out_dir=self.out_dir,\n#                 )\n#\n#     def on_stage_end(self, runner: \"IRunner\") -> None:\n#         \"\"\"\n#         On stage end action.\n#\n#         Args:\n#             runner: runner of your experiment\n#         \"\"\"\n#         if self.do_once:\n#             quantized_model = quantization.quantize_dynamic(\n#                 runner.model.cpu(),\n#                 qconfig_spec=self.qconfig_spec,\n#                 dtype=self.dtype,\n#             )\n#             save_quantized_model(\n#                 model=quantized_model,\n#                 logdir=runner.logdir,\n#                 checkpoint_name=self.mode,\n#                 out_model=self.out_model,\n#                 out_dir=self.out_dir,\n#             )\n#\n#\n# __all__ = [\"DynamicQuantizationCallback\"]\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/quantization.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/quantization.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/quantization.py:31:8 Incompatible variable type [9]: backend is declared to have type `str` but is used as type `None`.",
    "message": " backend is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 31,
    "warning_line": "        backend: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "from typing import Dict, Optional, Set, TYPE_CHECKING, Union\nfrom pathlib import Path\n\nimport torch\nfrom torch import quantization\n\nfrom catalyst.core.callback import Callback, CallbackOrder\nfrom catalyst.utils.quantization import save_quantized_model\n\nif TYPE_CHECKING:\n    from catalyst.core.runner import IRunner\n\n\nclass DynamicQuantizationCallback(Callback):\n    \"\"\"Dynamic Quantization Callback\n\n    This callback applying dynamic quantization to the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        metric: str = \"loss\",\n        minimize: bool = True,\n        min_delta: float = 1e-6,\n        mode: str = \"best\",\n        do_once: bool = True,\n        qconfig_spec: Optional[Union[Set, Dict]] = None,\n        dtype: Optional[torch.dtype] = torch.qint8,\n        out_dir: Union[str, Path] = None,\n        out_model: Union[str, Path] = None,\n        backend: str = None,\n    ):\n        \"\"\"Init method for callback\n\n        Args:\n            metric: Metric key we should trace model based on\n            minimize: Whether do we minimize metric or not\n            min_delta: Minimum value of change for metric to be\n                considered as improved\n            mode: One of `best` or `last`\n            do_once: Whether do we trace once per stage or every epoch\n            qconfig_spec: torch.quantization.quantize_dynamic\n                parameter, you can define layers to be quantize\n            dtype: type of the model parameters, default int8\n            out_dir (Union[str, Path]): Directory to save model to\n            out_model (Union[str, Path]): Path to save model to\n                (overrides `out_dir` argument)\n            backend: defines backend for quantization\n        \"\"\"\n        super().__init__(order=CallbackOrder.external)\n\n        if mode not in [\"best\", \"last\"]:\n            raise ValueError(\n                f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n            )\n\n        self.metric = metric\n        self.mode = mode\n        self.do_once = do_once\n        self.best_score = None\n        self.is_better = None\n        self.first_time = True\n        if minimize:\n            self.is_better = lambda score, best: score <= (best - min_delta)\n        else:\n            self.is_better = lambda score, best: score >= (best + min_delta)\n\n        self.opt_level = None\n\n        if out_model is not None:\n            out_model = Path(out_model)\n        self.out_model = out_model\n\n        if out_dir is not None:\n            out_dir = Path(out_dir)\n        self.out_dir = out_dir\n        self.qconfig_spec = qconfig_spec\n        self.dtype = dtype\n\n        if backend is not None:\n            torch.backends.quantized.engine = backend\n\n    def on_epoch_end(self, runner: \"IRunner\"):\n        \"\"\"\n        Performing model quantization on epoch end if condition metric is\n        improved\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if not self.do_once:\n            if self.mode == \"best\":\n                score = runner.valid_metrics[self.metric]\n\n                if self.best_score is None:\n                    self.best_score = score\n\n                if self.is_better(score, self.best_score) or self.first_time:\n                    self.best_score = score\n                    quantized_model = quantization.quantize_dynamic(\n                        runner.model.cpu(),\n                        qconfig_spec=self.qconfig_spec,\n                        dtype=self.dtype,\n                    )\n                    save_quantized_model(\n                        model=quantized_model,\n                        logdir=runner.logdir,\n                        checkpoint_name=self.mode,\n                        out_model=self.out_model,\n                        out_dir=self.out_dir,\n                    )\n                    self.first_time = False\n            else:\n                quantized_model = quantization.quantize_dynamic(\n                    runner.model.cpu(),\n                    qconfig_spec=self.qconfig_spec,\n                    dtype=self.dtype,\n                )\n                save_quantized_model(\n                    model=quantized_model,\n                    logdir=runner.logdir,\n                    checkpoint_name=self.mode,\n                    out_model=self.out_model,\n                    out_dir=self.out_dir,\n                )\n\n    def on_stage_end(self, runner: \"IRunner\") -> None:\n        \"\"\"\n        On stage end action.\n\n        Args:\n            runner: runner of your experiment\n        \"\"\"\n        if self.do_once:\n            quantized_model = quantization.quantize_dynamic(\n                runner.model.cpu(),\n                qconfig_spec=self.qconfig_spec,\n                dtype=self.dtype,\n            )\n            save_quantized_model(\n                model=quantized_model,\n                logdir=runner.logdir,\n                checkpoint_name=self.mode,\n                out_model=self.out_model,\n                out_dir=self.out_dir,\n            )\n\n\n__all__ = [\"DynamicQuantizationCallback\"]\n",
        "source_code_len": 5000,
        "target_code": "# # @TODO: make the same API for tracing/onnx/pruning/quantization\n# from typing import Dict, Optional, Set, TYPE_CHECKING, Union\n# from pathlib import Path\n#\n# import torch\n# from torch import quantization\n#\n# from catalyst.core.callback import Callback, CallbackOrder\n# from catalyst.utils.quantization import save_quantized_model\n#\n# if TYPE_CHECKING:\n#     from catalyst.core.runner import IRunner\n#\n#\n# class DynamicQuantizationCallback(Callback):\n#     \"\"\"Dynamic Quantization Callback\n#\n#     This callback applying dynamic quantization to the model.\n#     \"\"\"\n#\n#     def __init__(\n#         self,\n#         metric: str = \"loss\",\n#         minimize: bool = True,\n#         min_delta: float = 1e-6,\n#         mode: str = \"best\",\n#         do_once: bool = True,\n#         qconfig_spec: Optional[Union[Set, Dict]] = None,\n#         dtype: Optional[torch.dtype] = torch.qint8,\n#         out_dir: Union[str, Path] = None,\n#         out_model: Union[str, Path] = None,\n#         backend: str = None,\n#     ):\n#         \"\"\"Init method for callback\n#\n#         Args:\n#             metric: Metric key we should trace model based on\n#             minimize: Whether do we minimize metric or not\n#             min_delta: Minimum value of change for metric to be\n#                 considered as improved\n#             mode: One of `best` or `last`\n#             do_once: Whether do we trace once per stage or every epoch\n#             qconfig_spec: torch.quantization.quantize_dynamic\n#                 parameter, you can define layers to be quantize\n#             dtype: type of the model parameters, default int8\n#             out_dir (Union[str, Path]): Directory to save model to\n#             out_model (Union[str, Path]): Path to save model to\n#                 (overrides `out_dir` argument)\n#             backend: defines backend for quantization\n#         \"\"\"\n#         super().__init__(order=CallbackOrder.external)\n#\n#         if mode not in [\"best\", \"last\"]:\n#             raise ValueError(\n#                 f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n#             )\n#\n#         self.metric = metric\n#         self.mode = mode\n#         self.do_once = do_once\n#         self.best_score = None\n#         self.is_better = None\n#         self.first_time = True\n#         if minimize:\n#             self.is_better = lambda score, best: score <= (best - min_delta)\n#         else:\n#             self.is_better = lambda score, best: score >= (best + min_delta)\n#\n#         self.opt_level = None\n#\n#         if out_model is not None:\n#             out_model = Path(out_model)\n#         self.out_model = out_model\n#\n#         if out_dir is not None:\n#             out_dir = Path(out_dir)\n#         self.out_dir = out_dir\n#         self.qconfig_spec = qconfig_spec\n#         self.dtype = dtype\n#\n#         if backend is not None:\n#             torch.backends.quantized.engine = backend\n#\n#     def on_epoch_end(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model quantization on epoch end if condition metric is\n#         improved\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if not self.do_once:\n#             if self.mode == \"best\":\n#                 score = runner.valid_metrics[self.metric]\n#\n#                 if self.best_score is None:\n#                     self.best_score = score\n#\n#                 if self.is_better(score, self.best_score) or self.first_time:\n#                     self.best_score = score\n#                     quantized_model = quantization.quantize_dynamic(\n#                         runner.model.cpu(),\n#                         qconfig_spec=self.qconfig_spec,\n#                         dtype=self.dtype,\n#                     )\n#                     save_quantized_model(\n#                         model=quantized_model,\n#                         logdir=runner.logdir,\n#                         checkpoint_name=self.mode,\n#                         out_model=self.out_model,\n#                         out_dir=self.out_dir,\n#                     )\n#                     self.first_time = False\n#             else:\n#                 quantized_model = quantization.quantize_dynamic(\n#                     runner.model.cpu(),\n#                     qconfig_spec=self.qconfig_spec,\n#                     dtype=self.dtype,\n#                 )\n#                 save_quantized_model(\n#                     model=quantized_model,\n#                     logdir=runner.logdir,\n#                     checkpoint_name=self.mode,\n#                     out_model=self.out_model,\n#                     out_dir=self.out_dir,\n#                 )\n#\n#     def on_stage_end(self, runner: \"IRunner\") -> None:\n#         \"\"\"\n#         On stage end action.\n#\n#         Args:\n#             runner: runner of your experiment\n#         \"\"\"\n#         if self.do_once:\n#             quantized_model = quantization.quantize_dynamic(\n#                 runner.model.cpu(),\n#                 qconfig_spec=self.qconfig_spec,\n#                 dtype=self.dtype,\n#             )\n#             save_quantized_model(\n#                 model=quantized_model,\n#                 logdir=runner.logdir,\n#                 checkpoint_name=self.mode,\n#                 out_model=self.out_model,\n#                 out_dir=self.out_dir,\n#             )\n#\n#\n# __all__ = [\"DynamicQuantizationCallback\"]\n",
        "target_code_len": 5343,
        "diff_format": "@@ -1,149 +1,150 @@\n-from typing import Dict, Optional, Set, TYPE_CHECKING, Union\n-from pathlib import Path\n-\n-import torch\n-from torch import quantization\n-\n-from catalyst.core.callback import Callback, CallbackOrder\n-from catalyst.utils.quantization import save_quantized_model\n-\n-if TYPE_CHECKING:\n-    from catalyst.core.runner import IRunner\n-\n-\n-class DynamicQuantizationCallback(Callback):\n-    \"\"\"Dynamic Quantization Callback\n-\n-    This callback applying dynamic quantization to the model.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        metric: str = \"loss\",\n-        minimize: bool = True,\n-        min_delta: float = 1e-6,\n-        mode: str = \"best\",\n-        do_once: bool = True,\n-        qconfig_spec: Optional[Union[Set, Dict]] = None,\n-        dtype: Optional[torch.dtype] = torch.qint8,\n-        out_dir: Union[str, Path] = None,\n-        out_model: Union[str, Path] = None,\n-        backend: str = None,\n-    ):\n-        \"\"\"Init method for callback\n-\n-        Args:\n-            metric: Metric key we should trace model based on\n-            minimize: Whether do we minimize metric or not\n-            min_delta: Minimum value of change for metric to be\n-                considered as improved\n-            mode: One of `best` or `last`\n-            do_once: Whether do we trace once per stage or every epoch\n-            qconfig_spec: torch.quantization.quantize_dynamic\n-                parameter, you can define layers to be quantize\n-            dtype: type of the model parameters, default int8\n-            out_dir (Union[str, Path]): Directory to save model to\n-            out_model (Union[str, Path]): Path to save model to\n-                (overrides `out_dir` argument)\n-            backend: defines backend for quantization\n-        \"\"\"\n-        super().__init__(order=CallbackOrder.external)\n-\n-        if mode not in [\"best\", \"last\"]:\n-            raise ValueError(\n-                f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n-            )\n-\n-        self.metric = metric\n-        self.mode = mode\n-        self.do_once = do_once\n-        self.best_score = None\n-        self.is_better = None\n-        self.first_time = True\n-        if minimize:\n-            self.is_better = lambda score, best: score <= (best - min_delta)\n-        else:\n-            self.is_better = lambda score, best: score >= (best + min_delta)\n-\n-        self.opt_level = None\n-\n-        if out_model is not None:\n-            out_model = Path(out_model)\n-        self.out_model = out_model\n-\n-        if out_dir is not None:\n-            out_dir = Path(out_dir)\n-        self.out_dir = out_dir\n-        self.qconfig_spec = qconfig_spec\n-        self.dtype = dtype\n-\n-        if backend is not None:\n-            torch.backends.quantized.engine = backend\n-\n-    def on_epoch_end(self, runner: \"IRunner\"):\n-        \"\"\"\n-        Performing model quantization on epoch end if condition metric is\n-        improved\n-\n-        Args:\n-            runner: current runner\n-        \"\"\"\n-        if not self.do_once:\n-            if self.mode == \"best\":\n-                score = runner.valid_metrics[self.metric]\n-\n-                if self.best_score is None:\n-                    self.best_score = score\n-\n-                if self.is_better(score, self.best_score) or self.first_time:\n-                    self.best_score = score\n-                    quantized_model = quantization.quantize_dynamic(\n-                        runner.model.cpu(),\n-                        qconfig_spec=self.qconfig_spec,\n-                        dtype=self.dtype,\n-                    )\n-                    save_quantized_model(\n-                        model=quantized_model,\n-                        logdir=runner.logdir,\n-                        checkpoint_name=self.mode,\n-                        out_model=self.out_model,\n-                        out_dir=self.out_dir,\n-                    )\n-                    self.first_time = False\n-            else:\n-                quantized_model = quantization.quantize_dynamic(\n-                    runner.model.cpu(),\n-                    qconfig_spec=self.qconfig_spec,\n-                    dtype=self.dtype,\n-                )\n-                save_quantized_model(\n-                    model=quantized_model,\n-                    logdir=runner.logdir,\n-                    checkpoint_name=self.mode,\n-                    out_model=self.out_model,\n-                    out_dir=self.out_dir,\n-                )\n-\n-    def on_stage_end(self, runner: \"IRunner\") -> None:\n-        \"\"\"\n-        On stage end action.\n-\n-        Args:\n-            runner: runner of your experiment\n-        \"\"\"\n-        if self.do_once:\n-            quantized_model = quantization.quantize_dynamic(\n-                runner.model.cpu(),\n-                qconfig_spec=self.qconfig_spec,\n-                dtype=self.dtype,\n-            )\n-            save_quantized_model(\n-                model=quantized_model,\n-                logdir=runner.logdir,\n-                checkpoint_name=self.mode,\n-                out_model=self.out_model,\n-                out_dir=self.out_dir,\n-            )\n-\n-\n-__all__ = [\"DynamicQuantizationCallback\"]\n+# # @TODO: make the same API for tracing/onnx/pruning/quantization\n+# from typing import Dict, Optional, Set, TYPE_CHECKING, Union\n+# from pathlib import Path\n+#\n+# import torch\n+# from torch import quantization\n+#\n+# from catalyst.core.callback import Callback, CallbackOrder\n+# from catalyst.utils.quantization import save_quantized_model\n+#\n+# if TYPE_CHECKING:\n+#     from catalyst.core.runner import IRunner\n+#\n+#\n+# class DynamicQuantizationCallback(Callback):\n+#     \"\"\"Dynamic Quantization Callback\n+#\n+#     This callback applying dynamic quantization to the model.\n+#     \"\"\"\n+#\n+#     def __init__(\n+#         self,\n+#         metric: str = \"loss\",\n+#         minimize: bool = True,\n+#         min_delta: float = 1e-6,\n+#         mode: str = \"best\",\n+#         do_once: bool = True,\n+#         qconfig_spec: Optional[Union[Set, Dict]] = None,\n+#         dtype: Optional[torch.dtype] = torch.qint8,\n+#         out_dir: Union[str, Path] = None,\n+#         out_model: Union[str, Path] = None,\n+#         backend: str = None,\n+#     ):\n+#         \"\"\"Init method for callback\n+#\n+#         Args:\n+#             metric: Metric key we should trace model based on\n+#             minimize: Whether do we minimize metric or not\n+#             min_delta: Minimum value of change for metric to be\n+#                 considered as improved\n+#             mode: One of `best` or `last`\n+#             do_once: Whether do we trace once per stage or every epoch\n+#             qconfig_spec: torch.quantization.quantize_dynamic\n+#                 parameter, you can define layers to be quantize\n+#             dtype: type of the model parameters, default int8\n+#             out_dir (Union[str, Path]): Directory to save model to\n+#             out_model (Union[str, Path]): Path to save model to\n+#                 (overrides `out_dir` argument)\n+#             backend: defines backend for quantization\n+#         \"\"\"\n+#         super().__init__(order=CallbackOrder.external)\n+#\n+#         if mode not in [\"best\", \"last\"]:\n+#             raise ValueError(\n+#                 f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n+#             )\n+#\n+#         self.metric = metric\n+#         self.mode = mode\n+#         self.do_once = do_once\n+#         self.best_score = None\n+#         self.is_better = None\n+#         self.first_time = True\n+#         if minimize:\n+#             self.is_better = lambda score, best: score <= (best - min_delta)\n+#         else:\n+#             self.is_better = lambda score, best: score >= (best + min_delta)\n+#\n+#         self.opt_level = None\n+#\n+#         if out_model is not None:\n+#             out_model = Path(out_model)\n+#         self.out_model = out_model\n+#\n+#         if out_dir is not None:\n+#             out_dir = Path(out_dir)\n+#         self.out_dir = out_dir\n+#         self.qconfig_spec = qconfig_spec\n+#         self.dtype = dtype\n+#\n+#         if backend is not None:\n+#             torch.backends.quantized.engine = backend\n+#\n+#     def on_epoch_end(self, runner: \"IRunner\"):\n+#         \"\"\"\n+#         Performing model quantization on epoch end if condition metric is\n+#         improved\n+#\n+#         Args:\n+#             runner: current runner\n+#         \"\"\"\n+#         if not self.do_once:\n+#             if self.mode == \"best\":\n+#                 score = runner.valid_metrics[self.metric]\n+#\n+#                 if self.best_score is None:\n+#                     self.best_score = score\n+#\n+#                 if self.is_better(score, self.best_score) or self.first_time:\n+#                     self.best_score = score\n+#                     quantized_model = quantization.quantize_dynamic(\n+#                         runner.model.cpu(),\n+#                         qconfig_spec=self.qconfig_spec,\n+#                         dtype=self.dtype,\n+#                     )\n+#                     save_quantized_model(\n+#                         model=quantized_model,\n+#                         logdir=runner.logdir,\n+#                         checkpoint_name=self.mode,\n+#                         out_model=self.out_model,\n+#                         out_dir=self.out_dir,\n+#                     )\n+#                     self.first_time = False\n+#             else:\n+#                 quantized_model = quantization.quantize_dynamic(\n+#                     runner.model.cpu(),\n+#                     qconfig_spec=self.qconfig_spec,\n+#                     dtype=self.dtype,\n+#                 )\n+#                 save_quantized_model(\n+#                     model=quantized_model,\n+#                     logdir=runner.logdir,\n+#                     checkpoint_name=self.mode,\n+#                     out_model=self.out_model,\n+#                     out_dir=self.out_dir,\n+#                 )\n+#\n+#     def on_stage_end(self, runner: \"IRunner\") -> None:\n+#         \"\"\"\n+#         On stage end action.\n+#\n+#         Args:\n+#             runner: runner of your experiment\n+#         \"\"\"\n+#         if self.do_once:\n+#             quantized_model = quantization.quantize_dynamic(\n+#                 runner.model.cpu(),\n+#                 qconfig_spec=self.qconfig_spec,\n+#                 dtype=self.dtype,\n+#             )\n+#             save_quantized_model(\n+#                 model=quantized_model,\n+#                 logdir=runner.logdir,\n+#                 checkpoint_name=self.mode,\n+#                 out_model=self.out_model,\n+#                 out_dir=self.out_dir,\n+#             )\n+#\n+#\n+# __all__ = [\"DynamicQuantizationCallback\"]\n",
        "source_code_with_indent": "from typing import Dict, Optional, Set, TYPE_CHECKING, Union\nfrom pathlib import Path\n\nimport torch\nfrom torch import quantization\n\nfrom catalyst.core.callback import Callback, CallbackOrder\nfrom catalyst.utils.quantization import save_quantized_model\n\nif TYPE_CHECKING:\n    <IND>from catalyst.core.runner import IRunner\n\n\n<DED>class DynamicQuantizationCallback(Callback):\n    <IND>\"\"\"Dynamic Quantization Callback\n\n    This callback applying dynamic quantization to the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        metric: str = \"loss\",\n        minimize: bool = True,\n        min_delta: float = 1e-6,\n        mode: str = \"best\",\n        do_once: bool = True,\n        qconfig_spec: Optional[Union[Set, Dict]] = None,\n        dtype: Optional[torch.dtype] = torch.qint8,\n        out_dir: Union[str, Path] = None,\n        out_model: Union[str, Path] = None,\n        backend: str = None,\n    ):\n        <IND>\"\"\"Init method for callback\n\n        Args:\n            metric: Metric key we should trace model based on\n            minimize: Whether do we minimize metric or not\n            min_delta: Minimum value of change for metric to be\n                considered as improved\n            mode: One of `best` or `last`\n            do_once: Whether do we trace once per stage or every epoch\n            qconfig_spec: torch.quantization.quantize_dynamic\n                parameter, you can define layers to be quantize\n            dtype: type of the model parameters, default int8\n            out_dir (Union[str, Path]): Directory to save model to\n            out_model (Union[str, Path]): Path to save model to\n                (overrides `out_dir` argument)\n            backend: defines backend for quantization\n        \"\"\"\n        super().__init__(order=CallbackOrder.external)\n\n        if mode not in [\"best\", \"last\"]:\n            <IND>raise ValueError(\n                f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n            )\n\n        <DED>self.metric = metric\n        self.mode = mode\n        self.do_once = do_once\n        self.best_score = None\n        self.is_better = None\n        self.first_time = True\n        if minimize:\n            <IND>self.is_better = lambda score, best: score <= (best - min_delta)\n        <DED>else:\n            <IND>self.is_better = lambda score, best: score >= (best + min_delta)\n\n        <DED>self.opt_level = None\n\n        if out_model is not None:\n            <IND>out_model = Path(out_model)\n        <DED>self.out_model = out_model\n\n        if out_dir is not None:\n            <IND>out_dir = Path(out_dir)\n        <DED>self.out_dir = out_dir\n        self.qconfig_spec = qconfig_spec\n        self.dtype = dtype\n\n        if backend is not None:\n            <IND>torch.backends.quantized.engine = backend\n\n    <DED><DED>def on_epoch_end(self, runner: \"IRunner\"):\n        <IND>\"\"\"\n        Performing model quantization on epoch end if condition metric is\n        improved\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if not self.do_once:\n            <IND>if self.mode == \"best\":\n                <IND>score = runner.valid_metrics[self.metric]\n\n                if self.best_score is None:\n                    <IND>self.best_score = score\n\n                <DED>if self.is_better(score, self.best_score) or self.first_time:\n                    <IND>self.best_score = score\n                    quantized_model = quantization.quantize_dynamic(\n                        runner.model.cpu(),\n                        qconfig_spec=self.qconfig_spec,\n                        dtype=self.dtype,\n                    )\n                    save_quantized_model(\n                        model=quantized_model,\n                        logdir=runner.logdir,\n                        checkpoint_name=self.mode,\n                        out_model=self.out_model,\n                        out_dir=self.out_dir,\n                    )\n                    self.first_time = False\n            <DED><DED>else:\n                <IND>quantized_model = quantization.quantize_dynamic(\n                    runner.model.cpu(),\n                    qconfig_spec=self.qconfig_spec,\n                    dtype=self.dtype,\n                )\n                save_quantized_model(\n                    model=quantized_model,\n                    logdir=runner.logdir,\n                    checkpoint_name=self.mode,\n                    out_model=self.out_model,\n                    out_dir=self.out_dir,\n                )\n\n    <DED><DED><DED>def on_stage_end(self, runner: \"IRunner\") -> None:\n        <IND>\"\"\"\n        On stage end action.\n\n        Args:\n            runner: runner of your experiment\n        \"\"\"\n        if self.do_once:\n            <IND>quantized_model = quantization.quantize_dynamic(\n                runner.model.cpu(),\n                qconfig_spec=self.qconfig_spec,\n                dtype=self.dtype,\n            )\n            save_quantized_model(\n                model=quantized_model,\n                logdir=runner.logdir,\n                checkpoint_name=self.mode,\n                out_model=self.out_model,\n                out_dir=self.out_dir,\n            )\n\n\n<DED><DED><DED>__all__ = [\"DynamicQuantizationCallback\"]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "# # @TODO: make the same API for tracing/onnx/pruning/quantization\n# from typing import Dict, Optional, Set, TYPE_CHECKING, Union\n# from pathlib import Path\n#\n# import torch\n# from torch import quantization\n#\n# from catalyst.core.callback import Callback, CallbackOrder\n# from catalyst.utils.quantization import save_quantized_model\n#\n# if TYPE_CHECKING:\n#     from catalyst.core.runner import IRunner\n#\n#\n# class DynamicQuantizationCallback(Callback):\n#     \"\"\"Dynamic Quantization Callback\n#\n#     This callback applying dynamic quantization to the model.\n#     \"\"\"\n#\n#     def __init__(\n#         self,\n#         metric: str = \"loss\",\n#         minimize: bool = True,\n#         min_delta: float = 1e-6,\n#         mode: str = \"best\",\n#         do_once: bool = True,\n#         qconfig_spec: Optional[Union[Set, Dict]] = None,\n#         dtype: Optional[torch.dtype] = torch.qint8,\n#         out_dir: Union[str, Path] = None,\n#         out_model: Union[str, Path] = None,\n#         backend: str = None,\n#     ):\n#         \"\"\"Init method for callback\n#\n#         Args:\n#             metric: Metric key we should trace model based on\n#             minimize: Whether do we minimize metric or not\n#             min_delta: Minimum value of change for metric to be\n#                 considered as improved\n#             mode: One of `best` or `last`\n#             do_once: Whether do we trace once per stage or every epoch\n#             qconfig_spec: torch.quantization.quantize_dynamic\n#                 parameter, you can define layers to be quantize\n#             dtype: type of the model parameters, default int8\n#             out_dir (Union[str, Path]): Directory to save model to\n#             out_model (Union[str, Path]): Path to save model to\n#                 (overrides `out_dir` argument)\n#             backend: defines backend for quantization\n#         \"\"\"\n#         super().__init__(order=CallbackOrder.external)\n#\n#         if mode not in [\"best\", \"last\"]:\n#             raise ValueError(\n#                 f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n#             )\n#\n#         self.metric = metric\n#         self.mode = mode\n#         self.do_once = do_once\n#         self.best_score = None\n#         self.is_better = None\n#         self.first_time = True\n#         if minimize:\n#             self.is_better = lambda score, best: score <= (best - min_delta)\n#         else:\n#             self.is_better = lambda score, best: score >= (best + min_delta)\n#\n#         self.opt_level = None\n#\n#         if out_model is not None:\n#             out_model = Path(out_model)\n#         self.out_model = out_model\n#\n#         if out_dir is not None:\n#             out_dir = Path(out_dir)\n#         self.out_dir = out_dir\n#         self.qconfig_spec = qconfig_spec\n#         self.dtype = dtype\n#\n#         if backend is not None:\n#             torch.backends.quantized.engine = backend\n#\n#     def on_epoch_end(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model quantization on epoch end if condition metric is\n#         improved\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if not self.do_once:\n#             if self.mode == \"best\":\n#                 score = runner.valid_metrics[self.metric]\n#\n#                 if self.best_score is None:\n#                     self.best_score = score\n#\n#                 if self.is_better(score, self.best_score) or self.first_time:\n#                     self.best_score = score\n#                     quantized_model = quantization.quantize_dynamic(\n#                         runner.model.cpu(),\n#                         qconfig_spec=self.qconfig_spec,\n#                         dtype=self.dtype,\n#                     )\n#                     save_quantized_model(\n#                         model=quantized_model,\n#                         logdir=runner.logdir,\n#                         checkpoint_name=self.mode,\n#                         out_model=self.out_model,\n#                         out_dir=self.out_dir,\n#                     )\n#                     self.first_time = False\n#             else:\n#                 quantized_model = quantization.quantize_dynamic(\n#                     runner.model.cpu(),\n#                     qconfig_spec=self.qconfig_spec,\n#                     dtype=self.dtype,\n#                 )\n#                 save_quantized_model(\n#                     model=quantized_model,\n#                     logdir=runner.logdir,\n#                     checkpoint_name=self.mode,\n#                     out_model=self.out_model,\n#                     out_dir=self.out_dir,\n#                 )\n#\n#     def on_stage_end(self, runner: \"IRunner\") -> None:\n#         \"\"\"\n#         On stage end action.\n#\n#         Args:\n#             runner: runner of your experiment\n#         \"\"\"\n#         if self.do_once:\n#             quantized_model = quantization.quantize_dynamic(\n#                 runner.model.cpu(),\n#                 qconfig_spec=self.qconfig_spec,\n#                 dtype=self.dtype,\n#             )\n#             save_quantized_model(\n#                 model=quantized_model,\n#                 logdir=runner.logdir,\n#                 checkpoint_name=self.mode,\n#                 out_model=self.out_model,\n#                 out_dir=self.out_dir,\n#             )\n#\n#\n# __all__ = [\"DynamicQuantizationCallback\"]\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/scheduler.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/scheduler.py",
    "file_hunks_size": 30,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/scheduler.py:96:8 Incompatible variable type [9]: reduced_metric is declared to have type `str` but is used as type `None`.",
    "message": " reduced_metric is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 96,
    "warning_line": "        reduced_metric: str = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/scheduler.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/scheduler.py",
    "file_hunks_size": 30,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/scheduler.py:428:60 Unsupported operand [58]: `/` is not supported for operand types `int` and `Optional[int]`.",
    "message": " `/` is not supported for operand types `int` and `Optional[int]`.",
    "rule_id": "Unsupported operand [58]",
    "warning_line_no": 428,
    "warning_line": "            self.lr_step = (self.final_lr - self.init_lr) / self.num_steps"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/tests/test_tracer_callback.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/callbacks/tests/test_tracer_callback.py:49:16 Incompatible parameter type [6]: Expected `Tuple[int]` for 1st parameter `size` to call `_TracedNet.conv2d_size_out` but got `Tuple[typing.Any, typing.Any]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/tests/test_tracer_callback.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/tests/test_tracer_callback.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/callbacks/tests/test_tracer_callback.py:50:16 Incompatible parameter type [6]: Expected `Tuple[int]` for 2nd parameter `kernel_size` to call `_TracedNet.conv2d_size_out` but got `Tuple[typing.Any, typing.Any]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/tests/test_tracer_callback.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/tests/test_tracer_callback.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/callbacks/tests/test_tracer_callback.py:51:16 Incompatible parameter type [6]: Expected `Tuple[int]` for 3rd parameter `stride` to call `_TracedNet.conv2d_size_out` but got `Tuple[typing.Any, typing.Any]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/tests/test_tracer_callback.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/tracing.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/tracing.py:26:8 Incompatible variable type [9]: opt_level is declared to have type `str` but is used as type `None`.",
    "message": " opt_level is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 26,
    "warning_line": "        opt_level: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "from typing import TYPE_CHECKING, Union\nfrom pathlib import Path\nimport warnings\n\nfrom catalyst.core.callback import Callback, CallbackNode, CallbackOrder\nfrom catalyst.utils.tracing import save_traced_model, trace_model_from_runner\n\nif TYPE_CHECKING:\n    from catalyst.core.runner import IRunner\n\n\nclass TracingCallback(Callback):\n    \"\"\"\n    Traces model during training if `metric` provided is improved.\n    \"\"\"\n\n    def __init__(\n        self,\n        metric: str = \"loss\",\n        minimize: bool = True,\n        min_delta: float = 1e-6,\n        mode: str = \"best\",\n        do_once: bool = True,\n        method_name: str = \"forward\",\n        requires_grad: bool = False,\n        opt_level: str = None,\n        trace_mode: str = \"eval\",\n        out_dir: Union[str, Path] = None,\n        out_model: Union[str, Path] = None,\n    ):\n        \"\"\"\n        Args:\n            metric: Metric key we should trace model based on\n            minimize: Whether do we minimize metric or not\n            min_delta: Minimum value of change for metric to be\n                considered as improved\n            mode: One of `best` or `last`\n            do_once: Whether do we trace once per stage or every epoch\n            method_name: Model's method name that will be\n                used as entrypoint during tracing\n            requires_grad: Flag to use grads\n            opt_level: AMP FP16 init level\n            trace_mode: Mode for model to trace\n                (``train`` or ``eval``)\n            out_dir (Union[str, Path]): Directory to save model to\n            out_model (Union[str, Path]): Path to save model to\n                (overrides `out_dir` argument)\n        \"\"\"\n        super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n\n        if trace_mode not in [\"train\", \"eval\"]:\n            raise ValueError(\n                f\"Unknown `trace_mode` '{trace_mode}'. \"\n                f\"Must be 'eval' or 'train'\"\n            )\n\n        if mode not in [\"best\", \"last\"]:\n            raise ValueError(\n                f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n            )\n\n        if opt_level is not None:\n            warnings.warn(\n                \"TracingCallback: \"\n                \"`opt_level` is not supported yet, \"\n                \"model will be traced as is\",\n                stacklevel=2,\n            )\n\n        self.metric = metric\n        self.mode = mode\n        self.do_once = do_once\n        self.best_score = None\n        self.is_better = None\n        if minimize:\n            self.is_better = lambda score, best: score <= (best - min_delta)\n        else:\n            self.is_better = lambda score, best: score >= (best + min_delta)\n\n        self.requires_grad = requires_grad\n        self.method_name = method_name\n        self.trace_mode = trace_mode\n        self.opt_level = None\n\n        if out_model is not None:\n            out_model = Path(out_model)\n        self.out_model = out_model\n\n        if out_dir is not None:\n            out_dir = Path(out_dir)\n        self.out_dir = out_dir\n\n    def _trace(self, runner: \"IRunner\"):\n        \"\"\"\n        Performing model tracing on epoch end if condition metric is improved.\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if self.opt_level is not None:\n            device = \"cuda\"\n        else:\n            device = \"cpu\"\n\n        # the only case we need to restore model from previous checkpoint\n        # is when we need to trace best model only once in the end of stage\n        checkpoint_name_to_restore = None\n        if self.do_once and self.mode == \"best\":\n            checkpoint_name_to_restore = \"best\"\n\n        traced_model = trace_model_from_runner(\n            runner=runner,\n            checkpoint_name=checkpoint_name_to_restore,\n            method_name=self.method_name,\n            mode=self.trace_mode,\n            requires_grad=self.requires_grad,\n            opt_level=self.opt_level,\n            device=device,\n        )\n\n        save_traced_model(\n            model=traced_model,\n            logdir=runner.logdir,\n            checkpoint_name=self.mode,\n            method_name=self.method_name,\n            mode=self.trace_mode,\n            requires_grad=self.requires_grad,\n            opt_level=self.opt_level,\n            out_model=self.out_model,\n            out_dir=self.out_dir,\n        )\n\n    def on_epoch_end(self, runner: \"IRunner\"):\n        \"\"\"\n        Performing model tracing on epoch end if condition metric is improved\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if not self.do_once:\n            if self.mode == \"best\":\n                score = runner.valid_metrics[self.metric]\n\n                if self.best_score is None:\n                    self.best_score = score\n\n                # TODO: EarlyStoppingCallback and TracingCallback\n                #  will never work very first epoch\n                if self.is_better(score, self.best_score):\n                    self.best_score = score\n                    self._trace(runner)\n            else:\n                self._trace(runner)\n\n    def on_stage_end(self, runner: \"IRunner\"):\n        \"\"\"\n        Performing model tracing on stage end if `do_once` is True.\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if self.do_once:\n            self._trace(runner)\n\n\n# backward compatibility\nTracerCallback = TracingCallback\n\n__all__ = [\"TracingCallback\", \"TracerCallback\"]\n",
        "source_code_len": 5420,
        "target_code": "# @TODO: make the same API for tracing/onnx/pruning/quantization\n# from typing import TYPE_CHECKING, Union\n# from pathlib import Path\n# import warnings\n#\n# from catalyst.core.callback import Callback, CallbackNode, CallbackOrder\n# from catalyst.utils.tracing import save_traced_model, trace_model_from_runner\n#\n# if TYPE_CHECKING:\n#     from catalyst.core.runner import IRunner\n#\n#\n# class TracingCallback(Callback):\n#     \"\"\"\n#     Traces model during training if `metric` provided is improved.\n#     \"\"\"\n#\n#     def __init__(\n#         self,\n#         metric: str = \"loss\",\n#         minimize: bool = True,\n#         min_delta: float = 1e-6,\n#         mode: str = \"best\",\n#         do_once: bool = True,\n#         method_name: str = \"forward\",\n#         requires_grad: bool = False,\n#         opt_level: str = None,\n#         trace_mode: str = \"eval\",\n#         out_dir: Union[str, Path] = None,\n#         out_model: Union[str, Path] = None,\n#     ):\n#         \"\"\"\n#         Args:\n#             metric: Metric key we should trace model based on\n#             minimize: Whether do we minimize metric or not\n#             min_delta: Minimum value of change for metric to be\n#                 considered as improved\n#             mode: One of `best` or `last`\n#             do_once: Whether do we trace once per stage or every epoch\n#             method_name: Model's method name that will be\n#                 used as entrypoint during tracing\n#             requires_grad: Flag to use grads\n#             opt_level: AMP FP16 init level\n#             trace_mode: Mode for model to trace\n#                 (``train`` or ``eval``)\n#             out_dir (Union[str, Path]): Directory to save model to\n#             out_model (Union[str, Path]): Path to save model to\n#                 (overrides `out_dir` argument)\n#         \"\"\"\n#         super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n#\n#         if trace_mode not in [\"train\", \"eval\"]:\n#             raise ValueError(\n#                 f\"Unknown `trace_mode` '{trace_mode}'. \"\n#                 f\"Must be 'eval' or 'train'\"\n#             )\n#\n#         if mode not in [\"best\", \"last\"]:\n#             raise ValueError(\n#                 f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n#             )\n#\n#         if opt_level is not None:\n#             warnings.warn(\n#                 \"TracingCallback: \"\n#                 \"`opt_level` is not supported yet, \"\n#                 \"model will be traced as is\",\n#                 stacklevel=2,\n#             )\n#\n#         self.metric = metric\n#         self.mode = mode\n#         self.do_once = do_once\n#         self.best_score = None\n#         self.is_better = None\n#         if minimize:\n#             self.is_better = lambda score, best: score <= (best - min_delta)\n#         else:\n#             self.is_better = lambda score, best: score >= (best + min_delta)\n#\n#         self.requires_grad = requires_grad\n#         self.method_name = method_name\n#         self.trace_mode = trace_mode\n#         self.opt_level = None\n#\n#         if out_model is not None:\n#             out_model = Path(out_model)\n#         self.out_model = out_model\n#\n#         if out_dir is not None:\n#             out_dir = Path(out_dir)\n#         self.out_dir = out_dir\n#\n#     def _trace(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model tracing on epoch end if condition metric is improved.\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if self.opt_level is not None:\n#             device = \"cuda\"\n#         else:\n#             device = \"cpu\"\n#\n#         # the only case we need to restore model from previous checkpoint\n#         # is when we need to trace best model only once in the end of stage\n#         checkpoint_name_to_restore = None\n#         if self.do_once and self.mode == \"best\":\n#             checkpoint_name_to_restore = \"best\"\n#\n#         traced_model = trace_model_from_runner(\n#             runner=runner,\n#             checkpoint_name=checkpoint_name_to_restore,\n#             method_name=self.method_name,\n#             mode=self.trace_mode,\n#             requires_grad=self.requires_grad,\n#             opt_level=self.opt_level,\n#             device=device,\n#         )\n#\n#         save_traced_model(\n#             model=traced_model,\n#             logdir=runner.logdir,\n#             checkpoint_name=self.mode,\n#             method_name=self.method_name,\n#             mode=self.trace_mode,\n#             requires_grad=self.requires_grad,\n#             opt_level=self.opt_level,\n#             out_model=self.out_model,\n#             out_dir=self.out_dir,\n#         )\n#\n#     def on_epoch_end(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model tracing on epoch end if condition metric is improved\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if not self.do_once:\n#             if self.mode == \"best\":\n#                 score = runner.valid_metrics[self.metric]\n#\n#                 if self.best_score is None:\n#                     self.best_score = score\n#\n#                 # TODO: EarlyStoppingCallback and TracingCallback\n#                 #  will never work very first epoch\n#                 if self.is_better(score, self.best_score):\n#                     self.best_score = score\n#                     self._trace(runner)\n#             else:\n#                 self._trace(runner)\n#\n#     def on_stage_end(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model tracing on stage end if `do_once` is True.\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if self.do_once:\n#             self._trace(runner)\n#\n#\n# __all__ = [\"TracingCallback\"]\n",
        "target_code_len": 5715,
        "diff_format": "@@ -1,169 +1,167 @@\n-from typing import TYPE_CHECKING, Union\n-from pathlib import Path\n-import warnings\n-\n-from catalyst.core.callback import Callback, CallbackNode, CallbackOrder\n-from catalyst.utils.tracing import save_traced_model, trace_model_from_runner\n-\n-if TYPE_CHECKING:\n-    from catalyst.core.runner import IRunner\n-\n-\n-class TracingCallback(Callback):\n-    \"\"\"\n-    Traces model during training if `metric` provided is improved.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        metric: str = \"loss\",\n-        minimize: bool = True,\n-        min_delta: float = 1e-6,\n-        mode: str = \"best\",\n-        do_once: bool = True,\n-        method_name: str = \"forward\",\n-        requires_grad: bool = False,\n-        opt_level: str = None,\n-        trace_mode: str = \"eval\",\n-        out_dir: Union[str, Path] = None,\n-        out_model: Union[str, Path] = None,\n-    ):\n-        \"\"\"\n-        Args:\n-            metric: Metric key we should trace model based on\n-            minimize: Whether do we minimize metric or not\n-            min_delta: Minimum value of change for metric to be\n-                considered as improved\n-            mode: One of `best` or `last`\n-            do_once: Whether do we trace once per stage or every epoch\n-            method_name: Model's method name that will be\n-                used as entrypoint during tracing\n-            requires_grad: Flag to use grads\n-            opt_level: AMP FP16 init level\n-            trace_mode: Mode for model to trace\n-                (``train`` or ``eval``)\n-            out_dir (Union[str, Path]): Directory to save model to\n-            out_model (Union[str, Path]): Path to save model to\n-                (overrides `out_dir` argument)\n-        \"\"\"\n-        super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n-\n-        if trace_mode not in [\"train\", \"eval\"]:\n-            raise ValueError(\n-                f\"Unknown `trace_mode` '{trace_mode}'. \"\n-                f\"Must be 'eval' or 'train'\"\n-            )\n-\n-        if mode not in [\"best\", \"last\"]:\n-            raise ValueError(\n-                f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n-            )\n-\n-        if opt_level is not None:\n-            warnings.warn(\n-                \"TracingCallback: \"\n-                \"`opt_level` is not supported yet, \"\n-                \"model will be traced as is\",\n-                stacklevel=2,\n-            )\n-\n-        self.metric = metric\n-        self.mode = mode\n-        self.do_once = do_once\n-        self.best_score = None\n-        self.is_better = None\n-        if minimize:\n-            self.is_better = lambda score, best: score <= (best - min_delta)\n-        else:\n-            self.is_better = lambda score, best: score >= (best + min_delta)\n-\n-        self.requires_grad = requires_grad\n-        self.method_name = method_name\n-        self.trace_mode = trace_mode\n-        self.opt_level = None\n-\n-        if out_model is not None:\n-            out_model = Path(out_model)\n-        self.out_model = out_model\n-\n-        if out_dir is not None:\n-            out_dir = Path(out_dir)\n-        self.out_dir = out_dir\n-\n-    def _trace(self, runner: \"IRunner\"):\n-        \"\"\"\n-        Performing model tracing on epoch end if condition metric is improved.\n-\n-        Args:\n-            runner: current runner\n-        \"\"\"\n-        if self.opt_level is not None:\n-            device = \"cuda\"\n-        else:\n-            device = \"cpu\"\n-\n-        # the only case we need to restore model from previous checkpoint\n-        # is when we need to trace best model only once in the end of stage\n-        checkpoint_name_to_restore = None\n-        if self.do_once and self.mode == \"best\":\n-            checkpoint_name_to_restore = \"best\"\n-\n-        traced_model = trace_model_from_runner(\n-            runner=runner,\n-            checkpoint_name=checkpoint_name_to_restore,\n-            method_name=self.method_name,\n-            mode=self.trace_mode,\n-            requires_grad=self.requires_grad,\n-            opt_level=self.opt_level,\n-            device=device,\n-        )\n-\n-        save_traced_model(\n-            model=traced_model,\n-            logdir=runner.logdir,\n-            checkpoint_name=self.mode,\n-            method_name=self.method_name,\n-            mode=self.trace_mode,\n-            requires_grad=self.requires_grad,\n-            opt_level=self.opt_level,\n-            out_model=self.out_model,\n-            out_dir=self.out_dir,\n-        )\n-\n-    def on_epoch_end(self, runner: \"IRunner\"):\n-        \"\"\"\n-        Performing model tracing on epoch end if condition metric is improved\n-\n-        Args:\n-            runner: current runner\n-        \"\"\"\n-        if not self.do_once:\n-            if self.mode == \"best\":\n-                score = runner.valid_metrics[self.metric]\n-\n-                if self.best_score is None:\n-                    self.best_score = score\n-\n-                # TODO: EarlyStoppingCallback and TracingCallback\n-                #  will never work very first epoch\n-                if self.is_better(score, self.best_score):\n-                    self.best_score = score\n-                    self._trace(runner)\n-            else:\n-                self._trace(runner)\n-\n-    def on_stage_end(self, runner: \"IRunner\"):\n-        \"\"\"\n-        Performing model tracing on stage end if `do_once` is True.\n-\n-        Args:\n-            runner: current runner\n-        \"\"\"\n-        if self.do_once:\n-            self._trace(runner)\n-\n-\n-# backward compatibility\n-TracerCallback = TracingCallback\n-\n-__all__ = [\"TracingCallback\", \"TracerCallback\"]\n+# @TODO: make the same API for tracing/onnx/pruning/quantization\n+# from typing import TYPE_CHECKING, Union\n+# from pathlib import Path\n+# import warnings\n+#\n+# from catalyst.core.callback import Callback, CallbackNode, CallbackOrder\n+# from catalyst.utils.tracing import save_traced_model, trace_model_from_runner\n+#\n+# if TYPE_CHECKING:\n+#     from catalyst.core.runner import IRunner\n+#\n+#\n+# class TracingCallback(Callback):\n+#     \"\"\"\n+#     Traces model during training if `metric` provided is improved.\n+#     \"\"\"\n+#\n+#     def __init__(\n+#         self,\n+#         metric: str = \"loss\",\n+#         minimize: bool = True,\n+#         min_delta: float = 1e-6,\n+#         mode: str = \"best\",\n+#         do_once: bool = True,\n+#         method_name: str = \"forward\",\n+#         requires_grad: bool = False,\n+#         opt_level: str = None,\n+#         trace_mode: str = \"eval\",\n+#         out_dir: Union[str, Path] = None,\n+#         out_model: Union[str, Path] = None,\n+#     ):\n+#         \"\"\"\n+#         Args:\n+#             metric: Metric key we should trace model based on\n+#             minimize: Whether do we minimize metric or not\n+#             min_delta: Minimum value of change for metric to be\n+#                 considered as improved\n+#             mode: One of `best` or `last`\n+#             do_once: Whether do we trace once per stage or every epoch\n+#             method_name: Model's method name that will be\n+#                 used as entrypoint during tracing\n+#             requires_grad: Flag to use grads\n+#             opt_level: AMP FP16 init level\n+#             trace_mode: Mode for model to trace\n+#                 (``train`` or ``eval``)\n+#             out_dir (Union[str, Path]): Directory to save model to\n+#             out_model (Union[str, Path]): Path to save model to\n+#                 (overrides `out_dir` argument)\n+#         \"\"\"\n+#         super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n+#\n+#         if trace_mode not in [\"train\", \"eval\"]:\n+#             raise ValueError(\n+#                 f\"Unknown `trace_mode` '{trace_mode}'. \"\n+#                 f\"Must be 'eval' or 'train'\"\n+#             )\n+#\n+#         if mode not in [\"best\", \"last\"]:\n+#             raise ValueError(\n+#                 f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n+#             )\n+#\n+#         if opt_level is not None:\n+#             warnings.warn(\n+#                 \"TracingCallback: \"\n+#                 \"`opt_level` is not supported yet, \"\n+#                 \"model will be traced as is\",\n+#                 stacklevel=2,\n+#             )\n+#\n+#         self.metric = metric\n+#         self.mode = mode\n+#         self.do_once = do_once\n+#         self.best_score = None\n+#         self.is_better = None\n+#         if minimize:\n+#             self.is_better = lambda score, best: score <= (best - min_delta)\n+#         else:\n+#             self.is_better = lambda score, best: score >= (best + min_delta)\n+#\n+#         self.requires_grad = requires_grad\n+#         self.method_name = method_name\n+#         self.trace_mode = trace_mode\n+#         self.opt_level = None\n+#\n+#         if out_model is not None:\n+#             out_model = Path(out_model)\n+#         self.out_model = out_model\n+#\n+#         if out_dir is not None:\n+#             out_dir = Path(out_dir)\n+#         self.out_dir = out_dir\n+#\n+#     def _trace(self, runner: \"IRunner\"):\n+#         \"\"\"\n+#         Performing model tracing on epoch end if condition metric is improved.\n+#\n+#         Args:\n+#             runner: current runner\n+#         \"\"\"\n+#         if self.opt_level is not None:\n+#             device = \"cuda\"\n+#         else:\n+#             device = \"cpu\"\n+#\n+#         # the only case we need to restore model from previous checkpoint\n+#         # is when we need to trace best model only once in the end of stage\n+#         checkpoint_name_to_restore = None\n+#         if self.do_once and self.mode == \"best\":\n+#             checkpoint_name_to_restore = \"best\"\n+#\n+#         traced_model = trace_model_from_runner(\n+#             runner=runner,\n+#             checkpoint_name=checkpoint_name_to_restore,\n+#             method_name=self.method_name,\n+#             mode=self.trace_mode,\n+#             requires_grad=self.requires_grad,\n+#             opt_level=self.opt_level,\n+#             device=device,\n+#         )\n+#\n+#         save_traced_model(\n+#             model=traced_model,\n+#             logdir=runner.logdir,\n+#             checkpoint_name=self.mode,\n+#             method_name=self.method_name,\n+#             mode=self.trace_mode,\n+#             requires_grad=self.requires_grad,\n+#             opt_level=self.opt_level,\n+#             out_model=self.out_model,\n+#             out_dir=self.out_dir,\n+#         )\n+#\n+#     def on_epoch_end(self, runner: \"IRunner\"):\n+#         \"\"\"\n+#         Performing model tracing on epoch end if condition metric is improved\n+#\n+#         Args:\n+#             runner: current runner\n+#         \"\"\"\n+#         if not self.do_once:\n+#             if self.mode == \"best\":\n+#                 score = runner.valid_metrics[self.metric]\n+#\n+#                 if self.best_score is None:\n+#                     self.best_score = score\n+#\n+#                 # TODO: EarlyStoppingCallback and TracingCallback\n+#                 #  will never work very first epoch\n+#                 if self.is_better(score, self.best_score):\n+#                     self.best_score = score\n+#                     self._trace(runner)\n+#             else:\n+#                 self._trace(runner)\n+#\n+#     def on_stage_end(self, runner: \"IRunner\"):\n+#         \"\"\"\n+#         Performing model tracing on stage end if `do_once` is True.\n+#\n+#         Args:\n+#             runner: current runner\n+#         \"\"\"\n+#         if self.do_once:\n+#             self._trace(runner)\n+#\n+#\n+# __all__ = [\"TracingCallback\"]\n",
        "source_code_with_indent": "from typing import TYPE_CHECKING, Union\nfrom pathlib import Path\nimport warnings\n\nfrom catalyst.core.callback import Callback, CallbackNode, CallbackOrder\nfrom catalyst.utils.tracing import save_traced_model, trace_model_from_runner\n\nif TYPE_CHECKING:\n    <IND>from catalyst.core.runner import IRunner\n\n\n<DED>class TracingCallback(Callback):\n    <IND>\"\"\"\n    Traces model during training if `metric` provided is improved.\n    \"\"\"\n\n    def __init__(\n        self,\n        metric: str = \"loss\",\n        minimize: bool = True,\n        min_delta: float = 1e-6,\n        mode: str = \"best\",\n        do_once: bool = True,\n        method_name: str = \"forward\",\n        requires_grad: bool = False,\n        opt_level: str = None,\n        trace_mode: str = \"eval\",\n        out_dir: Union[str, Path] = None,\n        out_model: Union[str, Path] = None,\n    ):\n        <IND>\"\"\"\n        Args:\n            metric: Metric key we should trace model based on\n            minimize: Whether do we minimize metric or not\n            min_delta: Minimum value of change for metric to be\n                considered as improved\n            mode: One of `best` or `last`\n            do_once: Whether do we trace once per stage or every epoch\n            method_name: Model's method name that will be\n                used as entrypoint during tracing\n            requires_grad: Flag to use grads\n            opt_level: AMP FP16 init level\n            trace_mode: Mode for model to trace\n                (``train`` or ``eval``)\n            out_dir (Union[str, Path]): Directory to save model to\n            out_model (Union[str, Path]): Path to save model to\n                (overrides `out_dir` argument)\n        \"\"\"\n        super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n\n        if trace_mode not in [\"train\", \"eval\"]:\n            <IND>raise ValueError(\n                f\"Unknown `trace_mode` '{trace_mode}'. \"\n                f\"Must be 'eval' or 'train'\"\n            )\n\n        <DED>if mode not in [\"best\", \"last\"]:\n            <IND>raise ValueError(\n                f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n            )\n\n        <DED>if opt_level is not None:\n            <IND>warnings.warn(\n                \"TracingCallback: \"\n                \"`opt_level` is not supported yet, \"\n                \"model will be traced as is\",\n                stacklevel=2,\n            )\n\n        <DED>self.metric = metric\n        self.mode = mode\n        self.do_once = do_once\n        self.best_score = None\n        self.is_better = None\n        if minimize:\n            <IND>self.is_better = lambda score, best: score <= (best - min_delta)\n        <DED>else:\n            <IND>self.is_better = lambda score, best: score >= (best + min_delta)\n\n        <DED>self.requires_grad = requires_grad\n        self.method_name = method_name\n        self.trace_mode = trace_mode\n        self.opt_level = None\n\n        if out_model is not None:\n            <IND>out_model = Path(out_model)\n        <DED>self.out_model = out_model\n\n        if out_dir is not None:\n            <IND>out_dir = Path(out_dir)\n        <DED>self.out_dir = out_dir\n\n    <DED>def _trace(self, runner: \"IRunner\"):\n        <IND>\"\"\"\n        Performing model tracing on epoch end if condition metric is improved.\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if self.opt_level is not None:\n            <IND>device = \"cuda\"\n        <DED>else:\n            <IND>device = \"cpu\"\n\n        # the only case we need to restore model from previous checkpoint\n        # is when we need to trace best model only once in the end of stage\n        <DED>checkpoint_name_to_restore = None\n        if self.do_once and self.mode == \"best\":\n            <IND>checkpoint_name_to_restore = \"best\"\n\n        <DED>traced_model = trace_model_from_runner(\n            runner=runner,\n            checkpoint_name=checkpoint_name_to_restore,\n            method_name=self.method_name,\n            mode=self.trace_mode,\n            requires_grad=self.requires_grad,\n            opt_level=self.opt_level,\n            device=device,\n        )\n\n        save_traced_model(\n            model=traced_model,\n            logdir=runner.logdir,\n            checkpoint_name=self.mode,\n            method_name=self.method_name,\n            mode=self.trace_mode,\n            requires_grad=self.requires_grad,\n            opt_level=self.opt_level,\n            out_model=self.out_model,\n            out_dir=self.out_dir,\n        )\n\n    <DED>def on_epoch_end(self, runner: \"IRunner\"):\n        <IND>\"\"\"\n        Performing model tracing on epoch end if condition metric is improved\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if not self.do_once:\n            <IND>if self.mode == \"best\":\n                <IND>score = runner.valid_metrics[self.metric]\n\n                if self.best_score is None:\n                    <IND>self.best_score = score\n\n                # TODO: EarlyStoppingCallback and TracingCallback\n                #  will never work very first epoch\n                <DED>if self.is_better(score, self.best_score):\n                    <IND>self.best_score = score\n                    self._trace(runner)\n            <DED><DED>else:\n                <IND>self._trace(runner)\n\n    <DED><DED><DED>def on_stage_end(self, runner: \"IRunner\"):\n        <IND>\"\"\"\n        Performing model tracing on stage end if `do_once` is True.\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if self.do_once:\n            <IND>self._trace(runner)\n\n\n# backward compatibility\n<DED><DED><DED>TracerCallback = TracingCallback\n\n__all__ = [\"TracingCallback\", \"TracerCallback\"]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "# @TODO: make the same API for tracing/onnx/pruning/quantization\n# from typing import TYPE_CHECKING, Union\n# from pathlib import Path\n# import warnings\n#\n# from catalyst.core.callback import Callback, CallbackNode, CallbackOrder\n# from catalyst.utils.tracing import save_traced_model, trace_model_from_runner\n#\n# if TYPE_CHECKING:\n#     from catalyst.core.runner import IRunner\n#\n#\n# class TracingCallback(Callback):\n#     \"\"\"\n#     Traces model during training if `metric` provided is improved.\n#     \"\"\"\n#\n#     def __init__(\n#         self,\n#         metric: str = \"loss\",\n#         minimize: bool = True,\n#         min_delta: float = 1e-6,\n#         mode: str = \"best\",\n#         do_once: bool = True,\n#         method_name: str = \"forward\",\n#         requires_grad: bool = False,\n#         opt_level: str = None,\n#         trace_mode: str = \"eval\",\n#         out_dir: Union[str, Path] = None,\n#         out_model: Union[str, Path] = None,\n#     ):\n#         \"\"\"\n#         Args:\n#             metric: Metric key we should trace model based on\n#             minimize: Whether do we minimize metric or not\n#             min_delta: Minimum value of change for metric to be\n#                 considered as improved\n#             mode: One of `best` or `last`\n#             do_once: Whether do we trace once per stage or every epoch\n#             method_name: Model's method name that will be\n#                 used as entrypoint during tracing\n#             requires_grad: Flag to use grads\n#             opt_level: AMP FP16 init level\n#             trace_mode: Mode for model to trace\n#                 (``train`` or ``eval``)\n#             out_dir (Union[str, Path]): Directory to save model to\n#             out_model (Union[str, Path]): Path to save model to\n#                 (overrides `out_dir` argument)\n#         \"\"\"\n#         super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n#\n#         if trace_mode not in [\"train\", \"eval\"]:\n#             raise ValueError(\n#                 f\"Unknown `trace_mode` '{trace_mode}'. \"\n#                 f\"Must be 'eval' or 'train'\"\n#             )\n#\n#         if mode not in [\"best\", \"last\"]:\n#             raise ValueError(\n#                 f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n#             )\n#\n#         if opt_level is not None:\n#             warnings.warn(\n#                 \"TracingCallback: \"\n#                 \"`opt_level` is not supported yet, \"\n#                 \"model will be traced as is\",\n#                 stacklevel=2,\n#             )\n#\n#         self.metric = metric\n#         self.mode = mode\n#         self.do_once = do_once\n#         self.best_score = None\n#         self.is_better = None\n#         if minimize:\n#             self.is_better = lambda score, best: score <= (best - min_delta)\n#         else:\n#             self.is_better = lambda score, best: score >= (best + min_delta)\n#\n#         self.requires_grad = requires_grad\n#         self.method_name = method_name\n#         self.trace_mode = trace_mode\n#         self.opt_level = None\n#\n#         if out_model is not None:\n#             out_model = Path(out_model)\n#         self.out_model = out_model\n#\n#         if out_dir is not None:\n#             out_dir = Path(out_dir)\n#         self.out_dir = out_dir\n#\n#     def _trace(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model tracing on epoch end if condition metric is improved.\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if self.opt_level is not None:\n#             device = \"cuda\"\n#         else:\n#             device = \"cpu\"\n#\n#         # the only case we need to restore model from previous checkpoint\n#         # is when we need to trace best model only once in the end of stage\n#         checkpoint_name_to_restore = None\n#         if self.do_once and self.mode == \"best\":\n#             checkpoint_name_to_restore = \"best\"\n#\n#         traced_model = trace_model_from_runner(\n#             runner=runner,\n#             checkpoint_name=checkpoint_name_to_restore,\n#             method_name=self.method_name,\n#             mode=self.trace_mode,\n#             requires_grad=self.requires_grad,\n#             opt_level=self.opt_level,\n#             device=device,\n#         )\n#\n#         save_traced_model(\n#             model=traced_model,\n#             logdir=runner.logdir,\n#             checkpoint_name=self.mode,\n#             method_name=self.method_name,\n#             mode=self.trace_mode,\n#             requires_grad=self.requires_grad,\n#             opt_level=self.opt_level,\n#             out_model=self.out_model,\n#             out_dir=self.out_dir,\n#         )\n#\n#     def on_epoch_end(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model tracing on epoch end if condition metric is improved\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if not self.do_once:\n#             if self.mode == \"best\":\n#                 score = runner.valid_metrics[self.metric]\n#\n#                 if self.best_score is None:\n#                     self.best_score = score\n#\n#                 # TODO: EarlyStoppingCallback and TracingCallback\n#                 #  will never work very first epoch\n#                 if self.is_better(score, self.best_score):\n#                     self.best_score = score\n#                     self._trace(runner)\n#             else:\n#                 self._trace(runner)\n#\n#     def on_stage_end(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model tracing on stage end if `do_once` is True.\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if self.do_once:\n#             self._trace(runner)\n#\n#\n# __all__ = [\"TracingCallback\"]\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/tracing.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/tracing.py:28:8 Incompatible variable type [9]: out_dir is declared to have type `Union[Path, str]` but is used as type `None`.",
    "message": " out_dir is declared to have type `Union[Path, str]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 28,
    "warning_line": "        out_dir: Union[str, Path] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "from typing import TYPE_CHECKING, Union\nfrom pathlib import Path\nimport warnings\n\nfrom catalyst.core.callback import Callback, CallbackNode, CallbackOrder\nfrom catalyst.utils.tracing import save_traced_model, trace_model_from_runner\n\nif TYPE_CHECKING:\n    from catalyst.core.runner import IRunner\n\n\nclass TracingCallback(Callback):\n    \"\"\"\n    Traces model during training if `metric` provided is improved.\n    \"\"\"\n\n    def __init__(\n        self,\n        metric: str = \"loss\",\n        minimize: bool = True,\n        min_delta: float = 1e-6,\n        mode: str = \"best\",\n        do_once: bool = True,\n        method_name: str = \"forward\",\n        requires_grad: bool = False,\n        opt_level: str = None,\n        trace_mode: str = \"eval\",\n        out_dir: Union[str, Path] = None,\n        out_model: Union[str, Path] = None,\n    ):\n        \"\"\"\n        Args:\n            metric: Metric key we should trace model based on\n            minimize: Whether do we minimize metric or not\n            min_delta: Minimum value of change for metric to be\n                considered as improved\n            mode: One of `best` or `last`\n            do_once: Whether do we trace once per stage or every epoch\n            method_name: Model's method name that will be\n                used as entrypoint during tracing\n            requires_grad: Flag to use grads\n            opt_level: AMP FP16 init level\n            trace_mode: Mode for model to trace\n                (``train`` or ``eval``)\n            out_dir (Union[str, Path]): Directory to save model to\n            out_model (Union[str, Path]): Path to save model to\n                (overrides `out_dir` argument)\n        \"\"\"\n        super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n\n        if trace_mode not in [\"train\", \"eval\"]:\n            raise ValueError(\n                f\"Unknown `trace_mode` '{trace_mode}'. \"\n                f\"Must be 'eval' or 'train'\"\n            )\n\n        if mode not in [\"best\", \"last\"]:\n            raise ValueError(\n                f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n            )\n\n        if opt_level is not None:\n            warnings.warn(\n                \"TracingCallback: \"\n                \"`opt_level` is not supported yet, \"\n                \"model will be traced as is\",\n                stacklevel=2,\n            )\n\n        self.metric = metric\n        self.mode = mode\n        self.do_once = do_once\n        self.best_score = None\n        self.is_better = None\n        if minimize:\n            self.is_better = lambda score, best: score <= (best - min_delta)\n        else:\n            self.is_better = lambda score, best: score >= (best + min_delta)\n\n        self.requires_grad = requires_grad\n        self.method_name = method_name\n        self.trace_mode = trace_mode\n        self.opt_level = None\n\n        if out_model is not None:\n            out_model = Path(out_model)\n        self.out_model = out_model\n\n        if out_dir is not None:\n            out_dir = Path(out_dir)\n        self.out_dir = out_dir\n\n    def _trace(self, runner: \"IRunner\"):\n        \"\"\"\n        Performing model tracing on epoch end if condition metric is improved.\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if self.opt_level is not None:\n            device = \"cuda\"\n        else:\n            device = \"cpu\"\n\n        # the only case we need to restore model from previous checkpoint\n        # is when we need to trace best model only once in the end of stage\n        checkpoint_name_to_restore = None\n        if self.do_once and self.mode == \"best\":\n            checkpoint_name_to_restore = \"best\"\n\n        traced_model = trace_model_from_runner(\n            runner=runner,\n            checkpoint_name=checkpoint_name_to_restore,\n            method_name=self.method_name,\n            mode=self.trace_mode,\n            requires_grad=self.requires_grad,\n            opt_level=self.opt_level,\n            device=device,\n        )\n\n        save_traced_model(\n            model=traced_model,\n            logdir=runner.logdir,\n            checkpoint_name=self.mode,\n            method_name=self.method_name,\n            mode=self.trace_mode,\n            requires_grad=self.requires_grad,\n            opt_level=self.opt_level,\n            out_model=self.out_model,\n            out_dir=self.out_dir,\n        )\n\n    def on_epoch_end(self, runner: \"IRunner\"):\n        \"\"\"\n        Performing model tracing on epoch end if condition metric is improved\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if not self.do_once:\n            if self.mode == \"best\":\n                score = runner.valid_metrics[self.metric]\n\n                if self.best_score is None:\n                    self.best_score = score\n\n                # TODO: EarlyStoppingCallback and TracingCallback\n                #  will never work very first epoch\n                if self.is_better(score, self.best_score):\n                    self.best_score = score\n                    self._trace(runner)\n            else:\n                self._trace(runner)\n\n    def on_stage_end(self, runner: \"IRunner\"):\n        \"\"\"\n        Performing model tracing on stage end if `do_once` is True.\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if self.do_once:\n            self._trace(runner)\n\n\n# backward compatibility\nTracerCallback = TracingCallback\n\n__all__ = [\"TracingCallback\", \"TracerCallback\"]\n",
        "source_code_len": 5420,
        "target_code": "# @TODO: make the same API for tracing/onnx/pruning/quantization\n# from typing import TYPE_CHECKING, Union\n# from pathlib import Path\n# import warnings\n#\n# from catalyst.core.callback import Callback, CallbackNode, CallbackOrder\n# from catalyst.utils.tracing import save_traced_model, trace_model_from_runner\n#\n# if TYPE_CHECKING:\n#     from catalyst.core.runner import IRunner\n#\n#\n# class TracingCallback(Callback):\n#     \"\"\"\n#     Traces model during training if `metric` provided is improved.\n#     \"\"\"\n#\n#     def __init__(\n#         self,\n#         metric: str = \"loss\",\n#         minimize: bool = True,\n#         min_delta: float = 1e-6,\n#         mode: str = \"best\",\n#         do_once: bool = True,\n#         method_name: str = \"forward\",\n#         requires_grad: bool = False,\n#         opt_level: str = None,\n#         trace_mode: str = \"eval\",\n#         out_dir: Union[str, Path] = None,\n#         out_model: Union[str, Path] = None,\n#     ):\n#         \"\"\"\n#         Args:\n#             metric: Metric key we should trace model based on\n#             minimize: Whether do we minimize metric or not\n#             min_delta: Minimum value of change for metric to be\n#                 considered as improved\n#             mode: One of `best` or `last`\n#             do_once: Whether do we trace once per stage or every epoch\n#             method_name: Model's method name that will be\n#                 used as entrypoint during tracing\n#             requires_grad: Flag to use grads\n#             opt_level: AMP FP16 init level\n#             trace_mode: Mode for model to trace\n#                 (``train`` or ``eval``)\n#             out_dir (Union[str, Path]): Directory to save model to\n#             out_model (Union[str, Path]): Path to save model to\n#                 (overrides `out_dir` argument)\n#         \"\"\"\n#         super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n#\n#         if trace_mode not in [\"train\", \"eval\"]:\n#             raise ValueError(\n#                 f\"Unknown `trace_mode` '{trace_mode}'. \"\n#                 f\"Must be 'eval' or 'train'\"\n#             )\n#\n#         if mode not in [\"best\", \"last\"]:\n#             raise ValueError(\n#                 f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n#             )\n#\n#         if opt_level is not None:\n#             warnings.warn(\n#                 \"TracingCallback: \"\n#                 \"`opt_level` is not supported yet, \"\n#                 \"model will be traced as is\",\n#                 stacklevel=2,\n#             )\n#\n#         self.metric = metric\n#         self.mode = mode\n#         self.do_once = do_once\n#         self.best_score = None\n#         self.is_better = None\n#         if minimize:\n#             self.is_better = lambda score, best: score <= (best - min_delta)\n#         else:\n#             self.is_better = lambda score, best: score >= (best + min_delta)\n#\n#         self.requires_grad = requires_grad\n#         self.method_name = method_name\n#         self.trace_mode = trace_mode\n#         self.opt_level = None\n#\n#         if out_model is not None:\n#             out_model = Path(out_model)\n#         self.out_model = out_model\n#\n#         if out_dir is not None:\n#             out_dir = Path(out_dir)\n#         self.out_dir = out_dir\n#\n#     def _trace(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model tracing on epoch end if condition metric is improved.\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if self.opt_level is not None:\n#             device = \"cuda\"\n#         else:\n#             device = \"cpu\"\n#\n#         # the only case we need to restore model from previous checkpoint\n#         # is when we need to trace best model only once in the end of stage\n#         checkpoint_name_to_restore = None\n#         if self.do_once and self.mode == \"best\":\n#             checkpoint_name_to_restore = \"best\"\n#\n#         traced_model = trace_model_from_runner(\n#             runner=runner,\n#             checkpoint_name=checkpoint_name_to_restore,\n#             method_name=self.method_name,\n#             mode=self.trace_mode,\n#             requires_grad=self.requires_grad,\n#             opt_level=self.opt_level,\n#             device=device,\n#         )\n#\n#         save_traced_model(\n#             model=traced_model,\n#             logdir=runner.logdir,\n#             checkpoint_name=self.mode,\n#             method_name=self.method_name,\n#             mode=self.trace_mode,\n#             requires_grad=self.requires_grad,\n#             opt_level=self.opt_level,\n#             out_model=self.out_model,\n#             out_dir=self.out_dir,\n#         )\n#\n#     def on_epoch_end(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model tracing on epoch end if condition metric is improved\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if not self.do_once:\n#             if self.mode == \"best\":\n#                 score = runner.valid_metrics[self.metric]\n#\n#                 if self.best_score is None:\n#                     self.best_score = score\n#\n#                 # TODO: EarlyStoppingCallback and TracingCallback\n#                 #  will never work very first epoch\n#                 if self.is_better(score, self.best_score):\n#                     self.best_score = score\n#                     self._trace(runner)\n#             else:\n#                 self._trace(runner)\n#\n#     def on_stage_end(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model tracing on stage end if `do_once` is True.\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if self.do_once:\n#             self._trace(runner)\n#\n#\n# __all__ = [\"TracingCallback\"]\n",
        "target_code_len": 5715,
        "diff_format": "@@ -1,169 +1,167 @@\n-from typing import TYPE_CHECKING, Union\n-from pathlib import Path\n-import warnings\n-\n-from catalyst.core.callback import Callback, CallbackNode, CallbackOrder\n-from catalyst.utils.tracing import save_traced_model, trace_model_from_runner\n-\n-if TYPE_CHECKING:\n-    from catalyst.core.runner import IRunner\n-\n-\n-class TracingCallback(Callback):\n-    \"\"\"\n-    Traces model during training if `metric` provided is improved.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        metric: str = \"loss\",\n-        minimize: bool = True,\n-        min_delta: float = 1e-6,\n-        mode: str = \"best\",\n-        do_once: bool = True,\n-        method_name: str = \"forward\",\n-        requires_grad: bool = False,\n-        opt_level: str = None,\n-        trace_mode: str = \"eval\",\n-        out_dir: Union[str, Path] = None,\n-        out_model: Union[str, Path] = None,\n-    ):\n-        \"\"\"\n-        Args:\n-            metric: Metric key we should trace model based on\n-            minimize: Whether do we minimize metric or not\n-            min_delta: Minimum value of change for metric to be\n-                considered as improved\n-            mode: One of `best` or `last`\n-            do_once: Whether do we trace once per stage or every epoch\n-            method_name: Model's method name that will be\n-                used as entrypoint during tracing\n-            requires_grad: Flag to use grads\n-            opt_level: AMP FP16 init level\n-            trace_mode: Mode for model to trace\n-                (``train`` or ``eval``)\n-            out_dir (Union[str, Path]): Directory to save model to\n-            out_model (Union[str, Path]): Path to save model to\n-                (overrides `out_dir` argument)\n-        \"\"\"\n-        super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n-\n-        if trace_mode not in [\"train\", \"eval\"]:\n-            raise ValueError(\n-                f\"Unknown `trace_mode` '{trace_mode}'. \"\n-                f\"Must be 'eval' or 'train'\"\n-            )\n-\n-        if mode not in [\"best\", \"last\"]:\n-            raise ValueError(\n-                f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n-            )\n-\n-        if opt_level is not None:\n-            warnings.warn(\n-                \"TracingCallback: \"\n-                \"`opt_level` is not supported yet, \"\n-                \"model will be traced as is\",\n-                stacklevel=2,\n-            )\n-\n-        self.metric = metric\n-        self.mode = mode\n-        self.do_once = do_once\n-        self.best_score = None\n-        self.is_better = None\n-        if minimize:\n-            self.is_better = lambda score, best: score <= (best - min_delta)\n-        else:\n-            self.is_better = lambda score, best: score >= (best + min_delta)\n-\n-        self.requires_grad = requires_grad\n-        self.method_name = method_name\n-        self.trace_mode = trace_mode\n-        self.opt_level = None\n-\n-        if out_model is not None:\n-            out_model = Path(out_model)\n-        self.out_model = out_model\n-\n-        if out_dir is not None:\n-            out_dir = Path(out_dir)\n-        self.out_dir = out_dir\n-\n-    def _trace(self, runner: \"IRunner\"):\n-        \"\"\"\n-        Performing model tracing on epoch end if condition metric is improved.\n-\n-        Args:\n-            runner: current runner\n-        \"\"\"\n-        if self.opt_level is not None:\n-            device = \"cuda\"\n-        else:\n-            device = \"cpu\"\n-\n-        # the only case we need to restore model from previous checkpoint\n-        # is when we need to trace best model only once in the end of stage\n-        checkpoint_name_to_restore = None\n-        if self.do_once and self.mode == \"best\":\n-            checkpoint_name_to_restore = \"best\"\n-\n-        traced_model = trace_model_from_runner(\n-            runner=runner,\n-            checkpoint_name=checkpoint_name_to_restore,\n-            method_name=self.method_name,\n-            mode=self.trace_mode,\n-            requires_grad=self.requires_grad,\n-            opt_level=self.opt_level,\n-            device=device,\n-        )\n-\n-        save_traced_model(\n-            model=traced_model,\n-            logdir=runner.logdir,\n-            checkpoint_name=self.mode,\n-            method_name=self.method_name,\n-            mode=self.trace_mode,\n-            requires_grad=self.requires_grad,\n-            opt_level=self.opt_level,\n-            out_model=self.out_model,\n-            out_dir=self.out_dir,\n-        )\n-\n-    def on_epoch_end(self, runner: \"IRunner\"):\n-        \"\"\"\n-        Performing model tracing on epoch end if condition metric is improved\n-\n-        Args:\n-            runner: current runner\n-        \"\"\"\n-        if not self.do_once:\n-            if self.mode == \"best\":\n-                score = runner.valid_metrics[self.metric]\n-\n-                if self.best_score is None:\n-                    self.best_score = score\n-\n-                # TODO: EarlyStoppingCallback and TracingCallback\n-                #  will never work very first epoch\n-                if self.is_better(score, self.best_score):\n-                    self.best_score = score\n-                    self._trace(runner)\n-            else:\n-                self._trace(runner)\n-\n-    def on_stage_end(self, runner: \"IRunner\"):\n-        \"\"\"\n-        Performing model tracing on stage end if `do_once` is True.\n-\n-        Args:\n-            runner: current runner\n-        \"\"\"\n-        if self.do_once:\n-            self._trace(runner)\n-\n-\n-# backward compatibility\n-TracerCallback = TracingCallback\n-\n-__all__ = [\"TracingCallback\", \"TracerCallback\"]\n+# @TODO: make the same API for tracing/onnx/pruning/quantization\n+# from typing import TYPE_CHECKING, Union\n+# from pathlib import Path\n+# import warnings\n+#\n+# from catalyst.core.callback import Callback, CallbackNode, CallbackOrder\n+# from catalyst.utils.tracing import save_traced_model, trace_model_from_runner\n+#\n+# if TYPE_CHECKING:\n+#     from catalyst.core.runner import IRunner\n+#\n+#\n+# class TracingCallback(Callback):\n+#     \"\"\"\n+#     Traces model during training if `metric` provided is improved.\n+#     \"\"\"\n+#\n+#     def __init__(\n+#         self,\n+#         metric: str = \"loss\",\n+#         minimize: bool = True,\n+#         min_delta: float = 1e-6,\n+#         mode: str = \"best\",\n+#         do_once: bool = True,\n+#         method_name: str = \"forward\",\n+#         requires_grad: bool = False,\n+#         opt_level: str = None,\n+#         trace_mode: str = \"eval\",\n+#         out_dir: Union[str, Path] = None,\n+#         out_model: Union[str, Path] = None,\n+#     ):\n+#         \"\"\"\n+#         Args:\n+#             metric: Metric key we should trace model based on\n+#             minimize: Whether do we minimize metric or not\n+#             min_delta: Minimum value of change for metric to be\n+#                 considered as improved\n+#             mode: One of `best` or `last`\n+#             do_once: Whether do we trace once per stage or every epoch\n+#             method_name: Model's method name that will be\n+#                 used as entrypoint during tracing\n+#             requires_grad: Flag to use grads\n+#             opt_level: AMP FP16 init level\n+#             trace_mode: Mode for model to trace\n+#                 (``train`` or ``eval``)\n+#             out_dir (Union[str, Path]): Directory to save model to\n+#             out_model (Union[str, Path]): Path to save model to\n+#                 (overrides `out_dir` argument)\n+#         \"\"\"\n+#         super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n+#\n+#         if trace_mode not in [\"train\", \"eval\"]:\n+#             raise ValueError(\n+#                 f\"Unknown `trace_mode` '{trace_mode}'. \"\n+#                 f\"Must be 'eval' or 'train'\"\n+#             )\n+#\n+#         if mode not in [\"best\", \"last\"]:\n+#             raise ValueError(\n+#                 f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n+#             )\n+#\n+#         if opt_level is not None:\n+#             warnings.warn(\n+#                 \"TracingCallback: \"\n+#                 \"`opt_level` is not supported yet, \"\n+#                 \"model will be traced as is\",\n+#                 stacklevel=2,\n+#             )\n+#\n+#         self.metric = metric\n+#         self.mode = mode\n+#         self.do_once = do_once\n+#         self.best_score = None\n+#         self.is_better = None\n+#         if minimize:\n+#             self.is_better = lambda score, best: score <= (best - min_delta)\n+#         else:\n+#             self.is_better = lambda score, best: score >= (best + min_delta)\n+#\n+#         self.requires_grad = requires_grad\n+#         self.method_name = method_name\n+#         self.trace_mode = trace_mode\n+#         self.opt_level = None\n+#\n+#         if out_model is not None:\n+#             out_model = Path(out_model)\n+#         self.out_model = out_model\n+#\n+#         if out_dir is not None:\n+#             out_dir = Path(out_dir)\n+#         self.out_dir = out_dir\n+#\n+#     def _trace(self, runner: \"IRunner\"):\n+#         \"\"\"\n+#         Performing model tracing on epoch end if condition metric is improved.\n+#\n+#         Args:\n+#             runner: current runner\n+#         \"\"\"\n+#         if self.opt_level is not None:\n+#             device = \"cuda\"\n+#         else:\n+#             device = \"cpu\"\n+#\n+#         # the only case we need to restore model from previous checkpoint\n+#         # is when we need to trace best model only once in the end of stage\n+#         checkpoint_name_to_restore = None\n+#         if self.do_once and self.mode == \"best\":\n+#             checkpoint_name_to_restore = \"best\"\n+#\n+#         traced_model = trace_model_from_runner(\n+#             runner=runner,\n+#             checkpoint_name=checkpoint_name_to_restore,\n+#             method_name=self.method_name,\n+#             mode=self.trace_mode,\n+#             requires_grad=self.requires_grad,\n+#             opt_level=self.opt_level,\n+#             device=device,\n+#         )\n+#\n+#         save_traced_model(\n+#             model=traced_model,\n+#             logdir=runner.logdir,\n+#             checkpoint_name=self.mode,\n+#             method_name=self.method_name,\n+#             mode=self.trace_mode,\n+#             requires_grad=self.requires_grad,\n+#             opt_level=self.opt_level,\n+#             out_model=self.out_model,\n+#             out_dir=self.out_dir,\n+#         )\n+#\n+#     def on_epoch_end(self, runner: \"IRunner\"):\n+#         \"\"\"\n+#         Performing model tracing on epoch end if condition metric is improved\n+#\n+#         Args:\n+#             runner: current runner\n+#         \"\"\"\n+#         if not self.do_once:\n+#             if self.mode == \"best\":\n+#                 score = runner.valid_metrics[self.metric]\n+#\n+#                 if self.best_score is None:\n+#                     self.best_score = score\n+#\n+#                 # TODO: EarlyStoppingCallback and TracingCallback\n+#                 #  will never work very first epoch\n+#                 if self.is_better(score, self.best_score):\n+#                     self.best_score = score\n+#                     self._trace(runner)\n+#             else:\n+#                 self._trace(runner)\n+#\n+#     def on_stage_end(self, runner: \"IRunner\"):\n+#         \"\"\"\n+#         Performing model tracing on stage end if `do_once` is True.\n+#\n+#         Args:\n+#             runner: current runner\n+#         \"\"\"\n+#         if self.do_once:\n+#             self._trace(runner)\n+#\n+#\n+# __all__ = [\"TracingCallback\"]\n",
        "source_code_with_indent": "from typing import TYPE_CHECKING, Union\nfrom pathlib import Path\nimport warnings\n\nfrom catalyst.core.callback import Callback, CallbackNode, CallbackOrder\nfrom catalyst.utils.tracing import save_traced_model, trace_model_from_runner\n\nif TYPE_CHECKING:\n    <IND>from catalyst.core.runner import IRunner\n\n\n<DED>class TracingCallback(Callback):\n    <IND>\"\"\"\n    Traces model during training if `metric` provided is improved.\n    \"\"\"\n\n    def __init__(\n        self,\n        metric: str = \"loss\",\n        minimize: bool = True,\n        min_delta: float = 1e-6,\n        mode: str = \"best\",\n        do_once: bool = True,\n        method_name: str = \"forward\",\n        requires_grad: bool = False,\n        opt_level: str = None,\n        trace_mode: str = \"eval\",\n        out_dir: Union[str, Path] = None,\n        out_model: Union[str, Path] = None,\n    ):\n        <IND>\"\"\"\n        Args:\n            metric: Metric key we should trace model based on\n            minimize: Whether do we minimize metric or not\n            min_delta: Minimum value of change for metric to be\n                considered as improved\n            mode: One of `best` or `last`\n            do_once: Whether do we trace once per stage or every epoch\n            method_name: Model's method name that will be\n                used as entrypoint during tracing\n            requires_grad: Flag to use grads\n            opt_level: AMP FP16 init level\n            trace_mode: Mode for model to trace\n                (``train`` or ``eval``)\n            out_dir (Union[str, Path]): Directory to save model to\n            out_model (Union[str, Path]): Path to save model to\n                (overrides `out_dir` argument)\n        \"\"\"\n        super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n\n        if trace_mode not in [\"train\", \"eval\"]:\n            <IND>raise ValueError(\n                f\"Unknown `trace_mode` '{trace_mode}'. \"\n                f\"Must be 'eval' or 'train'\"\n            )\n\n        <DED>if mode not in [\"best\", \"last\"]:\n            <IND>raise ValueError(\n                f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n            )\n\n        <DED>if opt_level is not None:\n            <IND>warnings.warn(\n                \"TracingCallback: \"\n                \"`opt_level` is not supported yet, \"\n                \"model will be traced as is\",\n                stacklevel=2,\n            )\n\n        <DED>self.metric = metric\n        self.mode = mode\n        self.do_once = do_once\n        self.best_score = None\n        self.is_better = None\n        if minimize:\n            <IND>self.is_better = lambda score, best: score <= (best - min_delta)\n        <DED>else:\n            <IND>self.is_better = lambda score, best: score >= (best + min_delta)\n\n        <DED>self.requires_grad = requires_grad\n        self.method_name = method_name\n        self.trace_mode = trace_mode\n        self.opt_level = None\n\n        if out_model is not None:\n            <IND>out_model = Path(out_model)\n        <DED>self.out_model = out_model\n\n        if out_dir is not None:\n            <IND>out_dir = Path(out_dir)\n        <DED>self.out_dir = out_dir\n\n    <DED>def _trace(self, runner: \"IRunner\"):\n        <IND>\"\"\"\n        Performing model tracing on epoch end if condition metric is improved.\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if self.opt_level is not None:\n            <IND>device = \"cuda\"\n        <DED>else:\n            <IND>device = \"cpu\"\n\n        # the only case we need to restore model from previous checkpoint\n        # is when we need to trace best model only once in the end of stage\n        <DED>checkpoint_name_to_restore = None\n        if self.do_once and self.mode == \"best\":\n            <IND>checkpoint_name_to_restore = \"best\"\n\n        <DED>traced_model = trace_model_from_runner(\n            runner=runner,\n            checkpoint_name=checkpoint_name_to_restore,\n            method_name=self.method_name,\n            mode=self.trace_mode,\n            requires_grad=self.requires_grad,\n            opt_level=self.opt_level,\n            device=device,\n        )\n\n        save_traced_model(\n            model=traced_model,\n            logdir=runner.logdir,\n            checkpoint_name=self.mode,\n            method_name=self.method_name,\n            mode=self.trace_mode,\n            requires_grad=self.requires_grad,\n            opt_level=self.opt_level,\n            out_model=self.out_model,\n            out_dir=self.out_dir,\n        )\n\n    <DED>def on_epoch_end(self, runner: \"IRunner\"):\n        <IND>\"\"\"\n        Performing model tracing on epoch end if condition metric is improved\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if not self.do_once:\n            <IND>if self.mode == \"best\":\n                <IND>score = runner.valid_metrics[self.metric]\n\n                if self.best_score is None:\n                    <IND>self.best_score = score\n\n                # TODO: EarlyStoppingCallback and TracingCallback\n                #  will never work very first epoch\n                <DED>if self.is_better(score, self.best_score):\n                    <IND>self.best_score = score\n                    self._trace(runner)\n            <DED><DED>else:\n                <IND>self._trace(runner)\n\n    <DED><DED><DED>def on_stage_end(self, runner: \"IRunner\"):\n        <IND>\"\"\"\n        Performing model tracing on stage end if `do_once` is True.\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if self.do_once:\n            <IND>self._trace(runner)\n\n\n# backward compatibility\n<DED><DED><DED>TracerCallback = TracingCallback\n\n__all__ = [\"TracingCallback\", \"TracerCallback\"]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "# @TODO: make the same API for tracing/onnx/pruning/quantization\n# from typing import TYPE_CHECKING, Union\n# from pathlib import Path\n# import warnings\n#\n# from catalyst.core.callback import Callback, CallbackNode, CallbackOrder\n# from catalyst.utils.tracing import save_traced_model, trace_model_from_runner\n#\n# if TYPE_CHECKING:\n#     from catalyst.core.runner import IRunner\n#\n#\n# class TracingCallback(Callback):\n#     \"\"\"\n#     Traces model during training if `metric` provided is improved.\n#     \"\"\"\n#\n#     def __init__(\n#         self,\n#         metric: str = \"loss\",\n#         minimize: bool = True,\n#         min_delta: float = 1e-6,\n#         mode: str = \"best\",\n#         do_once: bool = True,\n#         method_name: str = \"forward\",\n#         requires_grad: bool = False,\n#         opt_level: str = None,\n#         trace_mode: str = \"eval\",\n#         out_dir: Union[str, Path] = None,\n#         out_model: Union[str, Path] = None,\n#     ):\n#         \"\"\"\n#         Args:\n#             metric: Metric key we should trace model based on\n#             minimize: Whether do we minimize metric or not\n#             min_delta: Minimum value of change for metric to be\n#                 considered as improved\n#             mode: One of `best` or `last`\n#             do_once: Whether do we trace once per stage or every epoch\n#             method_name: Model's method name that will be\n#                 used as entrypoint during tracing\n#             requires_grad: Flag to use grads\n#             opt_level: AMP FP16 init level\n#             trace_mode: Mode for model to trace\n#                 (``train`` or ``eval``)\n#             out_dir (Union[str, Path]): Directory to save model to\n#             out_model (Union[str, Path]): Path to save model to\n#                 (overrides `out_dir` argument)\n#         \"\"\"\n#         super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n#\n#         if trace_mode not in [\"train\", \"eval\"]:\n#             raise ValueError(\n#                 f\"Unknown `trace_mode` '{trace_mode}'. \"\n#                 f\"Must be 'eval' or 'train'\"\n#             )\n#\n#         if mode not in [\"best\", \"last\"]:\n#             raise ValueError(\n#                 f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n#             )\n#\n#         if opt_level is not None:\n#             warnings.warn(\n#                 \"TracingCallback: \"\n#                 \"`opt_level` is not supported yet, \"\n#                 \"model will be traced as is\",\n#                 stacklevel=2,\n#             )\n#\n#         self.metric = metric\n#         self.mode = mode\n#         self.do_once = do_once\n#         self.best_score = None\n#         self.is_better = None\n#         if minimize:\n#             self.is_better = lambda score, best: score <= (best - min_delta)\n#         else:\n#             self.is_better = lambda score, best: score >= (best + min_delta)\n#\n#         self.requires_grad = requires_grad\n#         self.method_name = method_name\n#         self.trace_mode = trace_mode\n#         self.opt_level = None\n#\n#         if out_model is not None:\n#             out_model = Path(out_model)\n#         self.out_model = out_model\n#\n#         if out_dir is not None:\n#             out_dir = Path(out_dir)\n#         self.out_dir = out_dir\n#\n#     def _trace(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model tracing on epoch end if condition metric is improved.\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if self.opt_level is not None:\n#             device = \"cuda\"\n#         else:\n#             device = \"cpu\"\n#\n#         # the only case we need to restore model from previous checkpoint\n#         # is when we need to trace best model only once in the end of stage\n#         checkpoint_name_to_restore = None\n#         if self.do_once and self.mode == \"best\":\n#             checkpoint_name_to_restore = \"best\"\n#\n#         traced_model = trace_model_from_runner(\n#             runner=runner,\n#             checkpoint_name=checkpoint_name_to_restore,\n#             method_name=self.method_name,\n#             mode=self.trace_mode,\n#             requires_grad=self.requires_grad,\n#             opt_level=self.opt_level,\n#             device=device,\n#         )\n#\n#         save_traced_model(\n#             model=traced_model,\n#             logdir=runner.logdir,\n#             checkpoint_name=self.mode,\n#             method_name=self.method_name,\n#             mode=self.trace_mode,\n#             requires_grad=self.requires_grad,\n#             opt_level=self.opt_level,\n#             out_model=self.out_model,\n#             out_dir=self.out_dir,\n#         )\n#\n#     def on_epoch_end(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model tracing on epoch end if condition metric is improved\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if not self.do_once:\n#             if self.mode == \"best\":\n#                 score = runner.valid_metrics[self.metric]\n#\n#                 if self.best_score is None:\n#                     self.best_score = score\n#\n#                 # TODO: EarlyStoppingCallback and TracingCallback\n#                 #  will never work very first epoch\n#                 if self.is_better(score, self.best_score):\n#                     self.best_score = score\n#                     self._trace(runner)\n#             else:\n#                 self._trace(runner)\n#\n#     def on_stage_end(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model tracing on stage end if `do_once` is True.\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if self.do_once:\n#             self._trace(runner)\n#\n#\n# __all__ = [\"TracingCallback\"]\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/tracing.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/tracing.py:29:8 Incompatible variable type [9]: out_model is declared to have type `Union[Path, str]` but is used as type `None`.",
    "message": " out_model is declared to have type `Union[Path, str]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 29,
    "warning_line": "        out_model: Union[str, Path] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "from typing import TYPE_CHECKING, Union\nfrom pathlib import Path\nimport warnings\n\nfrom catalyst.core.callback import Callback, CallbackNode, CallbackOrder\nfrom catalyst.utils.tracing import save_traced_model, trace_model_from_runner\n\nif TYPE_CHECKING:\n    from catalyst.core.runner import IRunner\n\n\nclass TracingCallback(Callback):\n    \"\"\"\n    Traces model during training if `metric` provided is improved.\n    \"\"\"\n\n    def __init__(\n        self,\n        metric: str = \"loss\",\n        minimize: bool = True,\n        min_delta: float = 1e-6,\n        mode: str = \"best\",\n        do_once: bool = True,\n        method_name: str = \"forward\",\n        requires_grad: bool = False,\n        opt_level: str = None,\n        trace_mode: str = \"eval\",\n        out_dir: Union[str, Path] = None,\n        out_model: Union[str, Path] = None,\n    ):\n        \"\"\"\n        Args:\n            metric: Metric key we should trace model based on\n            minimize: Whether do we minimize metric or not\n            min_delta: Minimum value of change for metric to be\n                considered as improved\n            mode: One of `best` or `last`\n            do_once: Whether do we trace once per stage or every epoch\n            method_name: Model's method name that will be\n                used as entrypoint during tracing\n            requires_grad: Flag to use grads\n            opt_level: AMP FP16 init level\n            trace_mode: Mode for model to trace\n                (``train`` or ``eval``)\n            out_dir (Union[str, Path]): Directory to save model to\n            out_model (Union[str, Path]): Path to save model to\n                (overrides `out_dir` argument)\n        \"\"\"\n        super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n\n        if trace_mode not in [\"train\", \"eval\"]:\n            raise ValueError(\n                f\"Unknown `trace_mode` '{trace_mode}'. \"\n                f\"Must be 'eval' or 'train'\"\n            )\n\n        if mode not in [\"best\", \"last\"]:\n            raise ValueError(\n                f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n            )\n\n        if opt_level is not None:\n            warnings.warn(\n                \"TracingCallback: \"\n                \"`opt_level` is not supported yet, \"\n                \"model will be traced as is\",\n                stacklevel=2,\n            )\n\n        self.metric = metric\n        self.mode = mode\n        self.do_once = do_once\n        self.best_score = None\n        self.is_better = None\n        if minimize:\n            self.is_better = lambda score, best: score <= (best - min_delta)\n        else:\n            self.is_better = lambda score, best: score >= (best + min_delta)\n\n        self.requires_grad = requires_grad\n        self.method_name = method_name\n        self.trace_mode = trace_mode\n        self.opt_level = None\n\n        if out_model is not None:\n            out_model = Path(out_model)\n        self.out_model = out_model\n\n        if out_dir is not None:\n            out_dir = Path(out_dir)\n        self.out_dir = out_dir\n\n    def _trace(self, runner: \"IRunner\"):\n        \"\"\"\n        Performing model tracing on epoch end if condition metric is improved.\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if self.opt_level is not None:\n            device = \"cuda\"\n        else:\n            device = \"cpu\"\n\n        # the only case we need to restore model from previous checkpoint\n        # is when we need to trace best model only once in the end of stage\n        checkpoint_name_to_restore = None\n        if self.do_once and self.mode == \"best\":\n            checkpoint_name_to_restore = \"best\"\n\n        traced_model = trace_model_from_runner(\n            runner=runner,\n            checkpoint_name=checkpoint_name_to_restore,\n            method_name=self.method_name,\n            mode=self.trace_mode,\n            requires_grad=self.requires_grad,\n            opt_level=self.opt_level,\n            device=device,\n        )\n\n        save_traced_model(\n            model=traced_model,\n            logdir=runner.logdir,\n            checkpoint_name=self.mode,\n            method_name=self.method_name,\n            mode=self.trace_mode,\n            requires_grad=self.requires_grad,\n            opt_level=self.opt_level,\n            out_model=self.out_model,\n            out_dir=self.out_dir,\n        )\n\n    def on_epoch_end(self, runner: \"IRunner\"):\n        \"\"\"\n        Performing model tracing on epoch end if condition metric is improved\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if not self.do_once:\n            if self.mode == \"best\":\n                score = runner.valid_metrics[self.metric]\n\n                if self.best_score is None:\n                    self.best_score = score\n\n                # TODO: EarlyStoppingCallback and TracingCallback\n                #  will never work very first epoch\n                if self.is_better(score, self.best_score):\n                    self.best_score = score\n                    self._trace(runner)\n            else:\n                self._trace(runner)\n\n    def on_stage_end(self, runner: \"IRunner\"):\n        \"\"\"\n        Performing model tracing on stage end if `do_once` is True.\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if self.do_once:\n            self._trace(runner)\n\n\n# backward compatibility\nTracerCallback = TracingCallback\n\n__all__ = [\"TracingCallback\", \"TracerCallback\"]\n",
        "source_code_len": 5420,
        "target_code": "# @TODO: make the same API for tracing/onnx/pruning/quantization\n# from typing import TYPE_CHECKING, Union\n# from pathlib import Path\n# import warnings\n#\n# from catalyst.core.callback import Callback, CallbackNode, CallbackOrder\n# from catalyst.utils.tracing import save_traced_model, trace_model_from_runner\n#\n# if TYPE_CHECKING:\n#     from catalyst.core.runner import IRunner\n#\n#\n# class TracingCallback(Callback):\n#     \"\"\"\n#     Traces model during training if `metric` provided is improved.\n#     \"\"\"\n#\n#     def __init__(\n#         self,\n#         metric: str = \"loss\",\n#         minimize: bool = True,\n#         min_delta: float = 1e-6,\n#         mode: str = \"best\",\n#         do_once: bool = True,\n#         method_name: str = \"forward\",\n#         requires_grad: bool = False,\n#         opt_level: str = None,\n#         trace_mode: str = \"eval\",\n#         out_dir: Union[str, Path] = None,\n#         out_model: Union[str, Path] = None,\n#     ):\n#         \"\"\"\n#         Args:\n#             metric: Metric key we should trace model based on\n#             minimize: Whether do we minimize metric or not\n#             min_delta: Minimum value of change for metric to be\n#                 considered as improved\n#             mode: One of `best` or `last`\n#             do_once: Whether do we trace once per stage or every epoch\n#             method_name: Model's method name that will be\n#                 used as entrypoint during tracing\n#             requires_grad: Flag to use grads\n#             opt_level: AMP FP16 init level\n#             trace_mode: Mode for model to trace\n#                 (``train`` or ``eval``)\n#             out_dir (Union[str, Path]): Directory to save model to\n#             out_model (Union[str, Path]): Path to save model to\n#                 (overrides `out_dir` argument)\n#         \"\"\"\n#         super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n#\n#         if trace_mode not in [\"train\", \"eval\"]:\n#             raise ValueError(\n#                 f\"Unknown `trace_mode` '{trace_mode}'. \"\n#                 f\"Must be 'eval' or 'train'\"\n#             )\n#\n#         if mode not in [\"best\", \"last\"]:\n#             raise ValueError(\n#                 f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n#             )\n#\n#         if opt_level is not None:\n#             warnings.warn(\n#                 \"TracingCallback: \"\n#                 \"`opt_level` is not supported yet, \"\n#                 \"model will be traced as is\",\n#                 stacklevel=2,\n#             )\n#\n#         self.metric = metric\n#         self.mode = mode\n#         self.do_once = do_once\n#         self.best_score = None\n#         self.is_better = None\n#         if minimize:\n#             self.is_better = lambda score, best: score <= (best - min_delta)\n#         else:\n#             self.is_better = lambda score, best: score >= (best + min_delta)\n#\n#         self.requires_grad = requires_grad\n#         self.method_name = method_name\n#         self.trace_mode = trace_mode\n#         self.opt_level = None\n#\n#         if out_model is not None:\n#             out_model = Path(out_model)\n#         self.out_model = out_model\n#\n#         if out_dir is not None:\n#             out_dir = Path(out_dir)\n#         self.out_dir = out_dir\n#\n#     def _trace(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model tracing on epoch end if condition metric is improved.\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if self.opt_level is not None:\n#             device = \"cuda\"\n#         else:\n#             device = \"cpu\"\n#\n#         # the only case we need to restore model from previous checkpoint\n#         # is when we need to trace best model only once in the end of stage\n#         checkpoint_name_to_restore = None\n#         if self.do_once and self.mode == \"best\":\n#             checkpoint_name_to_restore = \"best\"\n#\n#         traced_model = trace_model_from_runner(\n#             runner=runner,\n#             checkpoint_name=checkpoint_name_to_restore,\n#             method_name=self.method_name,\n#             mode=self.trace_mode,\n#             requires_grad=self.requires_grad,\n#             opt_level=self.opt_level,\n#             device=device,\n#         )\n#\n#         save_traced_model(\n#             model=traced_model,\n#             logdir=runner.logdir,\n#             checkpoint_name=self.mode,\n#             method_name=self.method_name,\n#             mode=self.trace_mode,\n#             requires_grad=self.requires_grad,\n#             opt_level=self.opt_level,\n#             out_model=self.out_model,\n#             out_dir=self.out_dir,\n#         )\n#\n#     def on_epoch_end(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model tracing on epoch end if condition metric is improved\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if not self.do_once:\n#             if self.mode == \"best\":\n#                 score = runner.valid_metrics[self.metric]\n#\n#                 if self.best_score is None:\n#                     self.best_score = score\n#\n#                 # TODO: EarlyStoppingCallback and TracingCallback\n#                 #  will never work very first epoch\n#                 if self.is_better(score, self.best_score):\n#                     self.best_score = score\n#                     self._trace(runner)\n#             else:\n#                 self._trace(runner)\n#\n#     def on_stage_end(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model tracing on stage end if `do_once` is True.\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if self.do_once:\n#             self._trace(runner)\n#\n#\n# __all__ = [\"TracingCallback\"]\n",
        "target_code_len": 5715,
        "diff_format": "@@ -1,169 +1,167 @@\n-from typing import TYPE_CHECKING, Union\n-from pathlib import Path\n-import warnings\n-\n-from catalyst.core.callback import Callback, CallbackNode, CallbackOrder\n-from catalyst.utils.tracing import save_traced_model, trace_model_from_runner\n-\n-if TYPE_CHECKING:\n-    from catalyst.core.runner import IRunner\n-\n-\n-class TracingCallback(Callback):\n-    \"\"\"\n-    Traces model during training if `metric` provided is improved.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        metric: str = \"loss\",\n-        minimize: bool = True,\n-        min_delta: float = 1e-6,\n-        mode: str = \"best\",\n-        do_once: bool = True,\n-        method_name: str = \"forward\",\n-        requires_grad: bool = False,\n-        opt_level: str = None,\n-        trace_mode: str = \"eval\",\n-        out_dir: Union[str, Path] = None,\n-        out_model: Union[str, Path] = None,\n-    ):\n-        \"\"\"\n-        Args:\n-            metric: Metric key we should trace model based on\n-            minimize: Whether do we minimize metric or not\n-            min_delta: Minimum value of change for metric to be\n-                considered as improved\n-            mode: One of `best` or `last`\n-            do_once: Whether do we trace once per stage or every epoch\n-            method_name: Model's method name that will be\n-                used as entrypoint during tracing\n-            requires_grad: Flag to use grads\n-            opt_level: AMP FP16 init level\n-            trace_mode: Mode for model to trace\n-                (``train`` or ``eval``)\n-            out_dir (Union[str, Path]): Directory to save model to\n-            out_model (Union[str, Path]): Path to save model to\n-                (overrides `out_dir` argument)\n-        \"\"\"\n-        super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n-\n-        if trace_mode not in [\"train\", \"eval\"]:\n-            raise ValueError(\n-                f\"Unknown `trace_mode` '{trace_mode}'. \"\n-                f\"Must be 'eval' or 'train'\"\n-            )\n-\n-        if mode not in [\"best\", \"last\"]:\n-            raise ValueError(\n-                f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n-            )\n-\n-        if opt_level is not None:\n-            warnings.warn(\n-                \"TracingCallback: \"\n-                \"`opt_level` is not supported yet, \"\n-                \"model will be traced as is\",\n-                stacklevel=2,\n-            )\n-\n-        self.metric = metric\n-        self.mode = mode\n-        self.do_once = do_once\n-        self.best_score = None\n-        self.is_better = None\n-        if minimize:\n-            self.is_better = lambda score, best: score <= (best - min_delta)\n-        else:\n-            self.is_better = lambda score, best: score >= (best + min_delta)\n-\n-        self.requires_grad = requires_grad\n-        self.method_name = method_name\n-        self.trace_mode = trace_mode\n-        self.opt_level = None\n-\n-        if out_model is not None:\n-            out_model = Path(out_model)\n-        self.out_model = out_model\n-\n-        if out_dir is not None:\n-            out_dir = Path(out_dir)\n-        self.out_dir = out_dir\n-\n-    def _trace(self, runner: \"IRunner\"):\n-        \"\"\"\n-        Performing model tracing on epoch end if condition metric is improved.\n-\n-        Args:\n-            runner: current runner\n-        \"\"\"\n-        if self.opt_level is not None:\n-            device = \"cuda\"\n-        else:\n-            device = \"cpu\"\n-\n-        # the only case we need to restore model from previous checkpoint\n-        # is when we need to trace best model only once in the end of stage\n-        checkpoint_name_to_restore = None\n-        if self.do_once and self.mode == \"best\":\n-            checkpoint_name_to_restore = \"best\"\n-\n-        traced_model = trace_model_from_runner(\n-            runner=runner,\n-            checkpoint_name=checkpoint_name_to_restore,\n-            method_name=self.method_name,\n-            mode=self.trace_mode,\n-            requires_grad=self.requires_grad,\n-            opt_level=self.opt_level,\n-            device=device,\n-        )\n-\n-        save_traced_model(\n-            model=traced_model,\n-            logdir=runner.logdir,\n-            checkpoint_name=self.mode,\n-            method_name=self.method_name,\n-            mode=self.trace_mode,\n-            requires_grad=self.requires_grad,\n-            opt_level=self.opt_level,\n-            out_model=self.out_model,\n-            out_dir=self.out_dir,\n-        )\n-\n-    def on_epoch_end(self, runner: \"IRunner\"):\n-        \"\"\"\n-        Performing model tracing on epoch end if condition metric is improved\n-\n-        Args:\n-            runner: current runner\n-        \"\"\"\n-        if not self.do_once:\n-            if self.mode == \"best\":\n-                score = runner.valid_metrics[self.metric]\n-\n-                if self.best_score is None:\n-                    self.best_score = score\n-\n-                # TODO: EarlyStoppingCallback and TracingCallback\n-                #  will never work very first epoch\n-                if self.is_better(score, self.best_score):\n-                    self.best_score = score\n-                    self._trace(runner)\n-            else:\n-                self._trace(runner)\n-\n-    def on_stage_end(self, runner: \"IRunner\"):\n-        \"\"\"\n-        Performing model tracing on stage end if `do_once` is True.\n-\n-        Args:\n-            runner: current runner\n-        \"\"\"\n-        if self.do_once:\n-            self._trace(runner)\n-\n-\n-# backward compatibility\n-TracerCallback = TracingCallback\n-\n-__all__ = [\"TracingCallback\", \"TracerCallback\"]\n+# @TODO: make the same API for tracing/onnx/pruning/quantization\n+# from typing import TYPE_CHECKING, Union\n+# from pathlib import Path\n+# import warnings\n+#\n+# from catalyst.core.callback import Callback, CallbackNode, CallbackOrder\n+# from catalyst.utils.tracing import save_traced_model, trace_model_from_runner\n+#\n+# if TYPE_CHECKING:\n+#     from catalyst.core.runner import IRunner\n+#\n+#\n+# class TracingCallback(Callback):\n+#     \"\"\"\n+#     Traces model during training if `metric` provided is improved.\n+#     \"\"\"\n+#\n+#     def __init__(\n+#         self,\n+#         metric: str = \"loss\",\n+#         minimize: bool = True,\n+#         min_delta: float = 1e-6,\n+#         mode: str = \"best\",\n+#         do_once: bool = True,\n+#         method_name: str = \"forward\",\n+#         requires_grad: bool = False,\n+#         opt_level: str = None,\n+#         trace_mode: str = \"eval\",\n+#         out_dir: Union[str, Path] = None,\n+#         out_model: Union[str, Path] = None,\n+#     ):\n+#         \"\"\"\n+#         Args:\n+#             metric: Metric key we should trace model based on\n+#             minimize: Whether do we minimize metric or not\n+#             min_delta: Minimum value of change for metric to be\n+#                 considered as improved\n+#             mode: One of `best` or `last`\n+#             do_once: Whether do we trace once per stage or every epoch\n+#             method_name: Model's method name that will be\n+#                 used as entrypoint during tracing\n+#             requires_grad: Flag to use grads\n+#             opt_level: AMP FP16 init level\n+#             trace_mode: Mode for model to trace\n+#                 (``train`` or ``eval``)\n+#             out_dir (Union[str, Path]): Directory to save model to\n+#             out_model (Union[str, Path]): Path to save model to\n+#                 (overrides `out_dir` argument)\n+#         \"\"\"\n+#         super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n+#\n+#         if trace_mode not in [\"train\", \"eval\"]:\n+#             raise ValueError(\n+#                 f\"Unknown `trace_mode` '{trace_mode}'. \"\n+#                 f\"Must be 'eval' or 'train'\"\n+#             )\n+#\n+#         if mode not in [\"best\", \"last\"]:\n+#             raise ValueError(\n+#                 f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n+#             )\n+#\n+#         if opt_level is not None:\n+#             warnings.warn(\n+#                 \"TracingCallback: \"\n+#                 \"`opt_level` is not supported yet, \"\n+#                 \"model will be traced as is\",\n+#                 stacklevel=2,\n+#             )\n+#\n+#         self.metric = metric\n+#         self.mode = mode\n+#         self.do_once = do_once\n+#         self.best_score = None\n+#         self.is_better = None\n+#         if minimize:\n+#             self.is_better = lambda score, best: score <= (best - min_delta)\n+#         else:\n+#             self.is_better = lambda score, best: score >= (best + min_delta)\n+#\n+#         self.requires_grad = requires_grad\n+#         self.method_name = method_name\n+#         self.trace_mode = trace_mode\n+#         self.opt_level = None\n+#\n+#         if out_model is not None:\n+#             out_model = Path(out_model)\n+#         self.out_model = out_model\n+#\n+#         if out_dir is not None:\n+#             out_dir = Path(out_dir)\n+#         self.out_dir = out_dir\n+#\n+#     def _trace(self, runner: \"IRunner\"):\n+#         \"\"\"\n+#         Performing model tracing on epoch end if condition metric is improved.\n+#\n+#         Args:\n+#             runner: current runner\n+#         \"\"\"\n+#         if self.opt_level is not None:\n+#             device = \"cuda\"\n+#         else:\n+#             device = \"cpu\"\n+#\n+#         # the only case we need to restore model from previous checkpoint\n+#         # is when we need to trace best model only once in the end of stage\n+#         checkpoint_name_to_restore = None\n+#         if self.do_once and self.mode == \"best\":\n+#             checkpoint_name_to_restore = \"best\"\n+#\n+#         traced_model = trace_model_from_runner(\n+#             runner=runner,\n+#             checkpoint_name=checkpoint_name_to_restore,\n+#             method_name=self.method_name,\n+#             mode=self.trace_mode,\n+#             requires_grad=self.requires_grad,\n+#             opt_level=self.opt_level,\n+#             device=device,\n+#         )\n+#\n+#         save_traced_model(\n+#             model=traced_model,\n+#             logdir=runner.logdir,\n+#             checkpoint_name=self.mode,\n+#             method_name=self.method_name,\n+#             mode=self.trace_mode,\n+#             requires_grad=self.requires_grad,\n+#             opt_level=self.opt_level,\n+#             out_model=self.out_model,\n+#             out_dir=self.out_dir,\n+#         )\n+#\n+#     def on_epoch_end(self, runner: \"IRunner\"):\n+#         \"\"\"\n+#         Performing model tracing on epoch end if condition metric is improved\n+#\n+#         Args:\n+#             runner: current runner\n+#         \"\"\"\n+#         if not self.do_once:\n+#             if self.mode == \"best\":\n+#                 score = runner.valid_metrics[self.metric]\n+#\n+#                 if self.best_score is None:\n+#                     self.best_score = score\n+#\n+#                 # TODO: EarlyStoppingCallback and TracingCallback\n+#                 #  will never work very first epoch\n+#                 if self.is_better(score, self.best_score):\n+#                     self.best_score = score\n+#                     self._trace(runner)\n+#             else:\n+#                 self._trace(runner)\n+#\n+#     def on_stage_end(self, runner: \"IRunner\"):\n+#         \"\"\"\n+#         Performing model tracing on stage end if `do_once` is True.\n+#\n+#         Args:\n+#             runner: current runner\n+#         \"\"\"\n+#         if self.do_once:\n+#             self._trace(runner)\n+#\n+#\n+# __all__ = [\"TracingCallback\"]\n",
        "source_code_with_indent": "from typing import TYPE_CHECKING, Union\nfrom pathlib import Path\nimport warnings\n\nfrom catalyst.core.callback import Callback, CallbackNode, CallbackOrder\nfrom catalyst.utils.tracing import save_traced_model, trace_model_from_runner\n\nif TYPE_CHECKING:\n    <IND>from catalyst.core.runner import IRunner\n\n\n<DED>class TracingCallback(Callback):\n    <IND>\"\"\"\n    Traces model during training if `metric` provided is improved.\n    \"\"\"\n\n    def __init__(\n        self,\n        metric: str = \"loss\",\n        minimize: bool = True,\n        min_delta: float = 1e-6,\n        mode: str = \"best\",\n        do_once: bool = True,\n        method_name: str = \"forward\",\n        requires_grad: bool = False,\n        opt_level: str = None,\n        trace_mode: str = \"eval\",\n        out_dir: Union[str, Path] = None,\n        out_model: Union[str, Path] = None,\n    ):\n        <IND>\"\"\"\n        Args:\n            metric: Metric key we should trace model based on\n            minimize: Whether do we minimize metric or not\n            min_delta: Minimum value of change for metric to be\n                considered as improved\n            mode: One of `best` or `last`\n            do_once: Whether do we trace once per stage or every epoch\n            method_name: Model's method name that will be\n                used as entrypoint during tracing\n            requires_grad: Flag to use grads\n            opt_level: AMP FP16 init level\n            trace_mode: Mode for model to trace\n                (``train`` or ``eval``)\n            out_dir (Union[str, Path]): Directory to save model to\n            out_model (Union[str, Path]): Path to save model to\n                (overrides `out_dir` argument)\n        \"\"\"\n        super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n\n        if trace_mode not in [\"train\", \"eval\"]:\n            <IND>raise ValueError(\n                f\"Unknown `trace_mode` '{trace_mode}'. \"\n                f\"Must be 'eval' or 'train'\"\n            )\n\n        <DED>if mode not in [\"best\", \"last\"]:\n            <IND>raise ValueError(\n                f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n            )\n\n        <DED>if opt_level is not None:\n            <IND>warnings.warn(\n                \"TracingCallback: \"\n                \"`opt_level` is not supported yet, \"\n                \"model will be traced as is\",\n                stacklevel=2,\n            )\n\n        <DED>self.metric = metric\n        self.mode = mode\n        self.do_once = do_once\n        self.best_score = None\n        self.is_better = None\n        if minimize:\n            <IND>self.is_better = lambda score, best: score <= (best - min_delta)\n        <DED>else:\n            <IND>self.is_better = lambda score, best: score >= (best + min_delta)\n\n        <DED>self.requires_grad = requires_grad\n        self.method_name = method_name\n        self.trace_mode = trace_mode\n        self.opt_level = None\n\n        if out_model is not None:\n            <IND>out_model = Path(out_model)\n        <DED>self.out_model = out_model\n\n        if out_dir is not None:\n            <IND>out_dir = Path(out_dir)\n        <DED>self.out_dir = out_dir\n\n    <DED>def _trace(self, runner: \"IRunner\"):\n        <IND>\"\"\"\n        Performing model tracing on epoch end if condition metric is improved.\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if self.opt_level is not None:\n            <IND>device = \"cuda\"\n        <DED>else:\n            <IND>device = \"cpu\"\n\n        # the only case we need to restore model from previous checkpoint\n        # is when we need to trace best model only once in the end of stage\n        <DED>checkpoint_name_to_restore = None\n        if self.do_once and self.mode == \"best\":\n            <IND>checkpoint_name_to_restore = \"best\"\n\n        <DED>traced_model = trace_model_from_runner(\n            runner=runner,\n            checkpoint_name=checkpoint_name_to_restore,\n            method_name=self.method_name,\n            mode=self.trace_mode,\n            requires_grad=self.requires_grad,\n            opt_level=self.opt_level,\n            device=device,\n        )\n\n        save_traced_model(\n            model=traced_model,\n            logdir=runner.logdir,\n            checkpoint_name=self.mode,\n            method_name=self.method_name,\n            mode=self.trace_mode,\n            requires_grad=self.requires_grad,\n            opt_level=self.opt_level,\n            out_model=self.out_model,\n            out_dir=self.out_dir,\n        )\n\n    <DED>def on_epoch_end(self, runner: \"IRunner\"):\n        <IND>\"\"\"\n        Performing model tracing on epoch end if condition metric is improved\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if not self.do_once:\n            <IND>if self.mode == \"best\":\n                <IND>score = runner.valid_metrics[self.metric]\n\n                if self.best_score is None:\n                    <IND>self.best_score = score\n\n                # TODO: EarlyStoppingCallback and TracingCallback\n                #  will never work very first epoch\n                <DED>if self.is_better(score, self.best_score):\n                    <IND>self.best_score = score\n                    self._trace(runner)\n            <DED><DED>else:\n                <IND>self._trace(runner)\n\n    <DED><DED><DED>def on_stage_end(self, runner: \"IRunner\"):\n        <IND>\"\"\"\n        Performing model tracing on stage end if `do_once` is True.\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if self.do_once:\n            <IND>self._trace(runner)\n\n\n# backward compatibility\n<DED><DED><DED>TracerCallback = TracingCallback\n\n__all__ = [\"TracingCallback\", \"TracerCallback\"]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "# @TODO: make the same API for tracing/onnx/pruning/quantization\n# from typing import TYPE_CHECKING, Union\n# from pathlib import Path\n# import warnings\n#\n# from catalyst.core.callback import Callback, CallbackNode, CallbackOrder\n# from catalyst.utils.tracing import save_traced_model, trace_model_from_runner\n#\n# if TYPE_CHECKING:\n#     from catalyst.core.runner import IRunner\n#\n#\n# class TracingCallback(Callback):\n#     \"\"\"\n#     Traces model during training if `metric` provided is improved.\n#     \"\"\"\n#\n#     def __init__(\n#         self,\n#         metric: str = \"loss\",\n#         minimize: bool = True,\n#         min_delta: float = 1e-6,\n#         mode: str = \"best\",\n#         do_once: bool = True,\n#         method_name: str = \"forward\",\n#         requires_grad: bool = False,\n#         opt_level: str = None,\n#         trace_mode: str = \"eval\",\n#         out_dir: Union[str, Path] = None,\n#         out_model: Union[str, Path] = None,\n#     ):\n#         \"\"\"\n#         Args:\n#             metric: Metric key we should trace model based on\n#             minimize: Whether do we minimize metric or not\n#             min_delta: Minimum value of change for metric to be\n#                 considered as improved\n#             mode: One of `best` or `last`\n#             do_once: Whether do we trace once per stage or every epoch\n#             method_name: Model's method name that will be\n#                 used as entrypoint during tracing\n#             requires_grad: Flag to use grads\n#             opt_level: AMP FP16 init level\n#             trace_mode: Mode for model to trace\n#                 (``train`` or ``eval``)\n#             out_dir (Union[str, Path]): Directory to save model to\n#             out_model (Union[str, Path]): Path to save model to\n#                 (overrides `out_dir` argument)\n#         \"\"\"\n#         super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n#\n#         if trace_mode not in [\"train\", \"eval\"]:\n#             raise ValueError(\n#                 f\"Unknown `trace_mode` '{trace_mode}'. \"\n#                 f\"Must be 'eval' or 'train'\"\n#             )\n#\n#         if mode not in [\"best\", \"last\"]:\n#             raise ValueError(\n#                 f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n#             )\n#\n#         if opt_level is not None:\n#             warnings.warn(\n#                 \"TracingCallback: \"\n#                 \"`opt_level` is not supported yet, \"\n#                 \"model will be traced as is\",\n#                 stacklevel=2,\n#             )\n#\n#         self.metric = metric\n#         self.mode = mode\n#         self.do_once = do_once\n#         self.best_score = None\n#         self.is_better = None\n#         if minimize:\n#             self.is_better = lambda score, best: score <= (best - min_delta)\n#         else:\n#             self.is_better = lambda score, best: score >= (best + min_delta)\n#\n#         self.requires_grad = requires_grad\n#         self.method_name = method_name\n#         self.trace_mode = trace_mode\n#         self.opt_level = None\n#\n#         if out_model is not None:\n#             out_model = Path(out_model)\n#         self.out_model = out_model\n#\n#         if out_dir is not None:\n#             out_dir = Path(out_dir)\n#         self.out_dir = out_dir\n#\n#     def _trace(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model tracing on epoch end if condition metric is improved.\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if self.opt_level is not None:\n#             device = \"cuda\"\n#         else:\n#             device = \"cpu\"\n#\n#         # the only case we need to restore model from previous checkpoint\n#         # is when we need to trace best model only once in the end of stage\n#         checkpoint_name_to_restore = None\n#         if self.do_once and self.mode == \"best\":\n#             checkpoint_name_to_restore = \"best\"\n#\n#         traced_model = trace_model_from_runner(\n#             runner=runner,\n#             checkpoint_name=checkpoint_name_to_restore,\n#             method_name=self.method_name,\n#             mode=self.trace_mode,\n#             requires_grad=self.requires_grad,\n#             opt_level=self.opt_level,\n#             device=device,\n#         )\n#\n#         save_traced_model(\n#             model=traced_model,\n#             logdir=runner.logdir,\n#             checkpoint_name=self.mode,\n#             method_name=self.method_name,\n#             mode=self.trace_mode,\n#             requires_grad=self.requires_grad,\n#             opt_level=self.opt_level,\n#             out_model=self.out_model,\n#             out_dir=self.out_dir,\n#         )\n#\n#     def on_epoch_end(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model tracing on epoch end if condition metric is improved\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if not self.do_once:\n#             if self.mode == \"best\":\n#                 score = runner.valid_metrics[self.metric]\n#\n#                 if self.best_score is None:\n#                     self.best_score = score\n#\n#                 # TODO: EarlyStoppingCallback and TracingCallback\n#                 #  will never work very first epoch\n#                 if self.is_better(score, self.best_score):\n#                     self.best_score = score\n#                     self._trace(runner)\n#             else:\n#                 self._trace(runner)\n#\n#     def on_stage_end(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model tracing on stage end if `do_once` is True.\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if self.do_once:\n#             self._trace(runner)\n#\n#\n# __all__ = [\"TracingCallback\"]\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/callbacks/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/callbacks/tracing.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/callbacks/tracing.py:113:12 Incompatible parameter type [6]: Expected `str` for 2nd parameter `checkpoint_name` to call `trace_model_from_runner` but got `typing.Optional[str]`.",
    "message": " Expected `str` for 2nd parameter `checkpoint_name` to call `trace_model_from_runner` but got `typing.Optional[str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 113,
    "warning_line": "            checkpoint_name=checkpoint_name_to_restore,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "from typing import TYPE_CHECKING, Union\nfrom pathlib import Path\nimport warnings\n\nfrom catalyst.core.callback import Callback, CallbackNode, CallbackOrder\nfrom catalyst.utils.tracing import save_traced_model, trace_model_from_runner\n\nif TYPE_CHECKING:\n    from catalyst.core.runner import IRunner\n\n\nclass TracingCallback(Callback):\n    \"\"\"\n    Traces model during training if `metric` provided is improved.\n    \"\"\"\n\n    def __init__(\n        self,\n        metric: str = \"loss\",\n        minimize: bool = True,\n        min_delta: float = 1e-6,\n        mode: str = \"best\",\n        do_once: bool = True,\n        method_name: str = \"forward\",\n        requires_grad: bool = False,\n        opt_level: str = None,\n        trace_mode: str = \"eval\",\n        out_dir: Union[str, Path] = None,\n        out_model: Union[str, Path] = None,\n    ):\n        \"\"\"\n        Args:\n            metric: Metric key we should trace model based on\n            minimize: Whether do we minimize metric or not\n            min_delta: Minimum value of change for metric to be\n                considered as improved\n            mode: One of `best` or `last`\n            do_once: Whether do we trace once per stage or every epoch\n            method_name: Model's method name that will be\n                used as entrypoint during tracing\n            requires_grad: Flag to use grads\n            opt_level: AMP FP16 init level\n            trace_mode: Mode for model to trace\n                (``train`` or ``eval``)\n            out_dir (Union[str, Path]): Directory to save model to\n            out_model (Union[str, Path]): Path to save model to\n                (overrides `out_dir` argument)\n        \"\"\"\n        super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n\n        if trace_mode not in [\"train\", \"eval\"]:\n            raise ValueError(\n                f\"Unknown `trace_mode` '{trace_mode}'. \"\n                f\"Must be 'eval' or 'train'\"\n            )\n\n        if mode not in [\"best\", \"last\"]:\n            raise ValueError(\n                f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n            )\n\n        if opt_level is not None:\n            warnings.warn(\n                \"TracingCallback: \"\n                \"`opt_level` is not supported yet, \"\n                \"model will be traced as is\",\n                stacklevel=2,\n            )\n\n        self.metric = metric\n        self.mode = mode\n        self.do_once = do_once\n        self.best_score = None\n        self.is_better = None\n        if minimize:\n            self.is_better = lambda score, best: score <= (best - min_delta)\n        else:\n            self.is_better = lambda score, best: score >= (best + min_delta)\n\n        self.requires_grad = requires_grad\n        self.method_name = method_name\n        self.trace_mode = trace_mode\n        self.opt_level = None\n\n        if out_model is not None:\n            out_model = Path(out_model)\n        self.out_model = out_model\n\n        if out_dir is not None:\n            out_dir = Path(out_dir)\n        self.out_dir = out_dir\n\n    def _trace(self, runner: \"IRunner\"):\n        \"\"\"\n        Performing model tracing on epoch end if condition metric is improved.\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if self.opt_level is not None:\n            device = \"cuda\"\n        else:\n            device = \"cpu\"\n\n        # the only case we need to restore model from previous checkpoint\n        # is when we need to trace best model only once in the end of stage\n        checkpoint_name_to_restore = None\n        if self.do_once and self.mode == \"best\":\n            checkpoint_name_to_restore = \"best\"\n\n        traced_model = trace_model_from_runner(\n            runner=runner,\n            checkpoint_name=checkpoint_name_to_restore,\n            method_name=self.method_name,\n            mode=self.trace_mode,\n            requires_grad=self.requires_grad,\n            opt_level=self.opt_level,\n            device=device,\n        )\n\n        save_traced_model(\n            model=traced_model,\n            logdir=runner.logdir,\n            checkpoint_name=self.mode,\n            method_name=self.method_name,\n            mode=self.trace_mode,\n            requires_grad=self.requires_grad,\n            opt_level=self.opt_level,\n            out_model=self.out_model,\n            out_dir=self.out_dir,\n        )\n\n    def on_epoch_end(self, runner: \"IRunner\"):\n        \"\"\"\n        Performing model tracing on epoch end if condition metric is improved\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if not self.do_once:\n            if self.mode == \"best\":\n                score = runner.valid_metrics[self.metric]\n\n                if self.best_score is None:\n                    self.best_score = score\n\n                # TODO: EarlyStoppingCallback and TracingCallback\n                #  will never work very first epoch\n                if self.is_better(score, self.best_score):\n                    self.best_score = score\n                    self._trace(runner)\n            else:\n                self._trace(runner)\n\n    def on_stage_end(self, runner: \"IRunner\"):\n        \"\"\"\n        Performing model tracing on stage end if `do_once` is True.\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if self.do_once:\n            self._trace(runner)\n\n\n# backward compatibility\nTracerCallback = TracingCallback\n\n__all__ = [\"TracingCallback\", \"TracerCallback\"]\n",
        "source_code_len": 5420,
        "target_code": "# @TODO: make the same API for tracing/onnx/pruning/quantization\n# from typing import TYPE_CHECKING, Union\n# from pathlib import Path\n# import warnings\n#\n# from catalyst.core.callback import Callback, CallbackNode, CallbackOrder\n# from catalyst.utils.tracing import save_traced_model, trace_model_from_runner\n#\n# if TYPE_CHECKING:\n#     from catalyst.core.runner import IRunner\n#\n#\n# class TracingCallback(Callback):\n#     \"\"\"\n#     Traces model during training if `metric` provided is improved.\n#     \"\"\"\n#\n#     def __init__(\n#         self,\n#         metric: str = \"loss\",\n#         minimize: bool = True,\n#         min_delta: float = 1e-6,\n#         mode: str = \"best\",\n#         do_once: bool = True,\n#         method_name: str = \"forward\",\n#         requires_grad: bool = False,\n#         opt_level: str = None,\n#         trace_mode: str = \"eval\",\n#         out_dir: Union[str, Path] = None,\n#         out_model: Union[str, Path] = None,\n#     ):\n#         \"\"\"\n#         Args:\n#             metric: Metric key we should trace model based on\n#             minimize: Whether do we minimize metric or not\n#             min_delta: Minimum value of change for metric to be\n#                 considered as improved\n#             mode: One of `best` or `last`\n#             do_once: Whether do we trace once per stage or every epoch\n#             method_name: Model's method name that will be\n#                 used as entrypoint during tracing\n#             requires_grad: Flag to use grads\n#             opt_level: AMP FP16 init level\n#             trace_mode: Mode for model to trace\n#                 (``train`` or ``eval``)\n#             out_dir (Union[str, Path]): Directory to save model to\n#             out_model (Union[str, Path]): Path to save model to\n#                 (overrides `out_dir` argument)\n#         \"\"\"\n#         super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n#\n#         if trace_mode not in [\"train\", \"eval\"]:\n#             raise ValueError(\n#                 f\"Unknown `trace_mode` '{trace_mode}'. \"\n#                 f\"Must be 'eval' or 'train'\"\n#             )\n#\n#         if mode not in [\"best\", \"last\"]:\n#             raise ValueError(\n#                 f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n#             )\n#\n#         if opt_level is not None:\n#             warnings.warn(\n#                 \"TracingCallback: \"\n#                 \"`opt_level` is not supported yet, \"\n#                 \"model will be traced as is\",\n#                 stacklevel=2,\n#             )\n#\n#         self.metric = metric\n#         self.mode = mode\n#         self.do_once = do_once\n#         self.best_score = None\n#         self.is_better = None\n#         if minimize:\n#             self.is_better = lambda score, best: score <= (best - min_delta)\n#         else:\n#             self.is_better = lambda score, best: score >= (best + min_delta)\n#\n#         self.requires_grad = requires_grad\n#         self.method_name = method_name\n#         self.trace_mode = trace_mode\n#         self.opt_level = None\n#\n#         if out_model is not None:\n#             out_model = Path(out_model)\n#         self.out_model = out_model\n#\n#         if out_dir is not None:\n#             out_dir = Path(out_dir)\n#         self.out_dir = out_dir\n#\n#     def _trace(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model tracing on epoch end if condition metric is improved.\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if self.opt_level is not None:\n#             device = \"cuda\"\n#         else:\n#             device = \"cpu\"\n#\n#         # the only case we need to restore model from previous checkpoint\n#         # is when we need to trace best model only once in the end of stage\n#         checkpoint_name_to_restore = None\n#         if self.do_once and self.mode == \"best\":\n#             checkpoint_name_to_restore = \"best\"\n#\n#         traced_model = trace_model_from_runner(\n#             runner=runner,\n#             checkpoint_name=checkpoint_name_to_restore,\n#             method_name=self.method_name,\n#             mode=self.trace_mode,\n#             requires_grad=self.requires_grad,\n#             opt_level=self.opt_level,\n#             device=device,\n#         )\n#\n#         save_traced_model(\n#             model=traced_model,\n#             logdir=runner.logdir,\n#             checkpoint_name=self.mode,\n#             method_name=self.method_name,\n#             mode=self.trace_mode,\n#             requires_grad=self.requires_grad,\n#             opt_level=self.opt_level,\n#             out_model=self.out_model,\n#             out_dir=self.out_dir,\n#         )\n#\n#     def on_epoch_end(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model tracing on epoch end if condition metric is improved\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if not self.do_once:\n#             if self.mode == \"best\":\n#                 score = runner.valid_metrics[self.metric]\n#\n#                 if self.best_score is None:\n#                     self.best_score = score\n#\n#                 # TODO: EarlyStoppingCallback and TracingCallback\n#                 #  will never work very first epoch\n#                 if self.is_better(score, self.best_score):\n#                     self.best_score = score\n#                     self._trace(runner)\n#             else:\n#                 self._trace(runner)\n#\n#     def on_stage_end(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model tracing on stage end if `do_once` is True.\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if self.do_once:\n#             self._trace(runner)\n#\n#\n# __all__ = [\"TracingCallback\"]\n",
        "target_code_len": 5715,
        "diff_format": "@@ -1,169 +1,167 @@\n-from typing import TYPE_CHECKING, Union\n-from pathlib import Path\n-import warnings\n-\n-from catalyst.core.callback import Callback, CallbackNode, CallbackOrder\n-from catalyst.utils.tracing import save_traced_model, trace_model_from_runner\n-\n-if TYPE_CHECKING:\n-    from catalyst.core.runner import IRunner\n-\n-\n-class TracingCallback(Callback):\n-    \"\"\"\n-    Traces model during training if `metric` provided is improved.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        metric: str = \"loss\",\n-        minimize: bool = True,\n-        min_delta: float = 1e-6,\n-        mode: str = \"best\",\n-        do_once: bool = True,\n-        method_name: str = \"forward\",\n-        requires_grad: bool = False,\n-        opt_level: str = None,\n-        trace_mode: str = \"eval\",\n-        out_dir: Union[str, Path] = None,\n-        out_model: Union[str, Path] = None,\n-    ):\n-        \"\"\"\n-        Args:\n-            metric: Metric key we should trace model based on\n-            minimize: Whether do we minimize metric or not\n-            min_delta: Minimum value of change for metric to be\n-                considered as improved\n-            mode: One of `best` or `last`\n-            do_once: Whether do we trace once per stage or every epoch\n-            method_name: Model's method name that will be\n-                used as entrypoint during tracing\n-            requires_grad: Flag to use grads\n-            opt_level: AMP FP16 init level\n-            trace_mode: Mode for model to trace\n-                (``train`` or ``eval``)\n-            out_dir (Union[str, Path]): Directory to save model to\n-            out_model (Union[str, Path]): Path to save model to\n-                (overrides `out_dir` argument)\n-        \"\"\"\n-        super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n-\n-        if trace_mode not in [\"train\", \"eval\"]:\n-            raise ValueError(\n-                f\"Unknown `trace_mode` '{trace_mode}'. \"\n-                f\"Must be 'eval' or 'train'\"\n-            )\n-\n-        if mode not in [\"best\", \"last\"]:\n-            raise ValueError(\n-                f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n-            )\n-\n-        if opt_level is not None:\n-            warnings.warn(\n-                \"TracingCallback: \"\n-                \"`opt_level` is not supported yet, \"\n-                \"model will be traced as is\",\n-                stacklevel=2,\n-            )\n-\n-        self.metric = metric\n-        self.mode = mode\n-        self.do_once = do_once\n-        self.best_score = None\n-        self.is_better = None\n-        if minimize:\n-            self.is_better = lambda score, best: score <= (best - min_delta)\n-        else:\n-            self.is_better = lambda score, best: score >= (best + min_delta)\n-\n-        self.requires_grad = requires_grad\n-        self.method_name = method_name\n-        self.trace_mode = trace_mode\n-        self.opt_level = None\n-\n-        if out_model is not None:\n-            out_model = Path(out_model)\n-        self.out_model = out_model\n-\n-        if out_dir is not None:\n-            out_dir = Path(out_dir)\n-        self.out_dir = out_dir\n-\n-    def _trace(self, runner: \"IRunner\"):\n-        \"\"\"\n-        Performing model tracing on epoch end if condition metric is improved.\n-\n-        Args:\n-            runner: current runner\n-        \"\"\"\n-        if self.opt_level is not None:\n-            device = \"cuda\"\n-        else:\n-            device = \"cpu\"\n-\n-        # the only case we need to restore model from previous checkpoint\n-        # is when we need to trace best model only once in the end of stage\n-        checkpoint_name_to_restore = None\n-        if self.do_once and self.mode == \"best\":\n-            checkpoint_name_to_restore = \"best\"\n-\n-        traced_model = trace_model_from_runner(\n-            runner=runner,\n-            checkpoint_name=checkpoint_name_to_restore,\n-            method_name=self.method_name,\n-            mode=self.trace_mode,\n-            requires_grad=self.requires_grad,\n-            opt_level=self.opt_level,\n-            device=device,\n-        )\n-\n-        save_traced_model(\n-            model=traced_model,\n-            logdir=runner.logdir,\n-            checkpoint_name=self.mode,\n-            method_name=self.method_name,\n-            mode=self.trace_mode,\n-            requires_grad=self.requires_grad,\n-            opt_level=self.opt_level,\n-            out_model=self.out_model,\n-            out_dir=self.out_dir,\n-        )\n-\n-    def on_epoch_end(self, runner: \"IRunner\"):\n-        \"\"\"\n-        Performing model tracing on epoch end if condition metric is improved\n-\n-        Args:\n-            runner: current runner\n-        \"\"\"\n-        if not self.do_once:\n-            if self.mode == \"best\":\n-                score = runner.valid_metrics[self.metric]\n-\n-                if self.best_score is None:\n-                    self.best_score = score\n-\n-                # TODO: EarlyStoppingCallback and TracingCallback\n-                #  will never work very first epoch\n-                if self.is_better(score, self.best_score):\n-                    self.best_score = score\n-                    self._trace(runner)\n-            else:\n-                self._trace(runner)\n-\n-    def on_stage_end(self, runner: \"IRunner\"):\n-        \"\"\"\n-        Performing model tracing on stage end if `do_once` is True.\n-\n-        Args:\n-            runner: current runner\n-        \"\"\"\n-        if self.do_once:\n-            self._trace(runner)\n-\n-\n-# backward compatibility\n-TracerCallback = TracingCallback\n-\n-__all__ = [\"TracingCallback\", \"TracerCallback\"]\n+# @TODO: make the same API for tracing/onnx/pruning/quantization\n+# from typing import TYPE_CHECKING, Union\n+# from pathlib import Path\n+# import warnings\n+#\n+# from catalyst.core.callback import Callback, CallbackNode, CallbackOrder\n+# from catalyst.utils.tracing import save_traced_model, trace_model_from_runner\n+#\n+# if TYPE_CHECKING:\n+#     from catalyst.core.runner import IRunner\n+#\n+#\n+# class TracingCallback(Callback):\n+#     \"\"\"\n+#     Traces model during training if `metric` provided is improved.\n+#     \"\"\"\n+#\n+#     def __init__(\n+#         self,\n+#         metric: str = \"loss\",\n+#         minimize: bool = True,\n+#         min_delta: float = 1e-6,\n+#         mode: str = \"best\",\n+#         do_once: bool = True,\n+#         method_name: str = \"forward\",\n+#         requires_grad: bool = False,\n+#         opt_level: str = None,\n+#         trace_mode: str = \"eval\",\n+#         out_dir: Union[str, Path] = None,\n+#         out_model: Union[str, Path] = None,\n+#     ):\n+#         \"\"\"\n+#         Args:\n+#             metric: Metric key we should trace model based on\n+#             minimize: Whether do we minimize metric or not\n+#             min_delta: Minimum value of change for metric to be\n+#                 considered as improved\n+#             mode: One of `best` or `last`\n+#             do_once: Whether do we trace once per stage or every epoch\n+#             method_name: Model's method name that will be\n+#                 used as entrypoint during tracing\n+#             requires_grad: Flag to use grads\n+#             opt_level: AMP FP16 init level\n+#             trace_mode: Mode for model to trace\n+#                 (``train`` or ``eval``)\n+#             out_dir (Union[str, Path]): Directory to save model to\n+#             out_model (Union[str, Path]): Path to save model to\n+#                 (overrides `out_dir` argument)\n+#         \"\"\"\n+#         super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n+#\n+#         if trace_mode not in [\"train\", \"eval\"]:\n+#             raise ValueError(\n+#                 f\"Unknown `trace_mode` '{trace_mode}'. \"\n+#                 f\"Must be 'eval' or 'train'\"\n+#             )\n+#\n+#         if mode not in [\"best\", \"last\"]:\n+#             raise ValueError(\n+#                 f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n+#             )\n+#\n+#         if opt_level is not None:\n+#             warnings.warn(\n+#                 \"TracingCallback: \"\n+#                 \"`opt_level` is not supported yet, \"\n+#                 \"model will be traced as is\",\n+#                 stacklevel=2,\n+#             )\n+#\n+#         self.metric = metric\n+#         self.mode = mode\n+#         self.do_once = do_once\n+#         self.best_score = None\n+#         self.is_better = None\n+#         if minimize:\n+#             self.is_better = lambda score, best: score <= (best - min_delta)\n+#         else:\n+#             self.is_better = lambda score, best: score >= (best + min_delta)\n+#\n+#         self.requires_grad = requires_grad\n+#         self.method_name = method_name\n+#         self.trace_mode = trace_mode\n+#         self.opt_level = None\n+#\n+#         if out_model is not None:\n+#             out_model = Path(out_model)\n+#         self.out_model = out_model\n+#\n+#         if out_dir is not None:\n+#             out_dir = Path(out_dir)\n+#         self.out_dir = out_dir\n+#\n+#     def _trace(self, runner: \"IRunner\"):\n+#         \"\"\"\n+#         Performing model tracing on epoch end if condition metric is improved.\n+#\n+#         Args:\n+#             runner: current runner\n+#         \"\"\"\n+#         if self.opt_level is not None:\n+#             device = \"cuda\"\n+#         else:\n+#             device = \"cpu\"\n+#\n+#         # the only case we need to restore model from previous checkpoint\n+#         # is when we need to trace best model only once in the end of stage\n+#         checkpoint_name_to_restore = None\n+#         if self.do_once and self.mode == \"best\":\n+#             checkpoint_name_to_restore = \"best\"\n+#\n+#         traced_model = trace_model_from_runner(\n+#             runner=runner,\n+#             checkpoint_name=checkpoint_name_to_restore,\n+#             method_name=self.method_name,\n+#             mode=self.trace_mode,\n+#             requires_grad=self.requires_grad,\n+#             opt_level=self.opt_level,\n+#             device=device,\n+#         )\n+#\n+#         save_traced_model(\n+#             model=traced_model,\n+#             logdir=runner.logdir,\n+#             checkpoint_name=self.mode,\n+#             method_name=self.method_name,\n+#             mode=self.trace_mode,\n+#             requires_grad=self.requires_grad,\n+#             opt_level=self.opt_level,\n+#             out_model=self.out_model,\n+#             out_dir=self.out_dir,\n+#         )\n+#\n+#     def on_epoch_end(self, runner: \"IRunner\"):\n+#         \"\"\"\n+#         Performing model tracing on epoch end if condition metric is improved\n+#\n+#         Args:\n+#             runner: current runner\n+#         \"\"\"\n+#         if not self.do_once:\n+#             if self.mode == \"best\":\n+#                 score = runner.valid_metrics[self.metric]\n+#\n+#                 if self.best_score is None:\n+#                     self.best_score = score\n+#\n+#                 # TODO: EarlyStoppingCallback and TracingCallback\n+#                 #  will never work very first epoch\n+#                 if self.is_better(score, self.best_score):\n+#                     self.best_score = score\n+#                     self._trace(runner)\n+#             else:\n+#                 self._trace(runner)\n+#\n+#     def on_stage_end(self, runner: \"IRunner\"):\n+#         \"\"\"\n+#         Performing model tracing on stage end if `do_once` is True.\n+#\n+#         Args:\n+#             runner: current runner\n+#         \"\"\"\n+#         if self.do_once:\n+#             self._trace(runner)\n+#\n+#\n+# __all__ = [\"TracingCallback\"]\n",
        "source_code_with_indent": "from typing import TYPE_CHECKING, Union\nfrom pathlib import Path\nimport warnings\n\nfrom catalyst.core.callback import Callback, CallbackNode, CallbackOrder\nfrom catalyst.utils.tracing import save_traced_model, trace_model_from_runner\n\nif TYPE_CHECKING:\n    <IND>from catalyst.core.runner import IRunner\n\n\n<DED>class TracingCallback(Callback):\n    <IND>\"\"\"\n    Traces model during training if `metric` provided is improved.\n    \"\"\"\n\n    def __init__(\n        self,\n        metric: str = \"loss\",\n        minimize: bool = True,\n        min_delta: float = 1e-6,\n        mode: str = \"best\",\n        do_once: bool = True,\n        method_name: str = \"forward\",\n        requires_grad: bool = False,\n        opt_level: str = None,\n        trace_mode: str = \"eval\",\n        out_dir: Union[str, Path] = None,\n        out_model: Union[str, Path] = None,\n    ):\n        <IND>\"\"\"\n        Args:\n            metric: Metric key we should trace model based on\n            minimize: Whether do we minimize metric or not\n            min_delta: Minimum value of change for metric to be\n                considered as improved\n            mode: One of `best` or `last`\n            do_once: Whether do we trace once per stage or every epoch\n            method_name: Model's method name that will be\n                used as entrypoint during tracing\n            requires_grad: Flag to use grads\n            opt_level: AMP FP16 init level\n            trace_mode: Mode for model to trace\n                (``train`` or ``eval``)\n            out_dir (Union[str, Path]): Directory to save model to\n            out_model (Union[str, Path]): Path to save model to\n                (overrides `out_dir` argument)\n        \"\"\"\n        super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n\n        if trace_mode not in [\"train\", \"eval\"]:\n            <IND>raise ValueError(\n                f\"Unknown `trace_mode` '{trace_mode}'. \"\n                f\"Must be 'eval' or 'train'\"\n            )\n\n        <DED>if mode not in [\"best\", \"last\"]:\n            <IND>raise ValueError(\n                f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n            )\n\n        <DED>if opt_level is not None:\n            <IND>warnings.warn(\n                \"TracingCallback: \"\n                \"`opt_level` is not supported yet, \"\n                \"model will be traced as is\",\n                stacklevel=2,\n            )\n\n        <DED>self.metric = metric\n        self.mode = mode\n        self.do_once = do_once\n        self.best_score = None\n        self.is_better = None\n        if minimize:\n            <IND>self.is_better = lambda score, best: score <= (best - min_delta)\n        <DED>else:\n            <IND>self.is_better = lambda score, best: score >= (best + min_delta)\n\n        <DED>self.requires_grad = requires_grad\n        self.method_name = method_name\n        self.trace_mode = trace_mode\n        self.opt_level = None\n\n        if out_model is not None:\n            <IND>out_model = Path(out_model)\n        <DED>self.out_model = out_model\n\n        if out_dir is not None:\n            <IND>out_dir = Path(out_dir)\n        <DED>self.out_dir = out_dir\n\n    <DED>def _trace(self, runner: \"IRunner\"):\n        <IND>\"\"\"\n        Performing model tracing on epoch end if condition metric is improved.\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if self.opt_level is not None:\n            <IND>device = \"cuda\"\n        <DED>else:\n            <IND>device = \"cpu\"\n\n        # the only case we need to restore model from previous checkpoint\n        # is when we need to trace best model only once in the end of stage\n        <DED>checkpoint_name_to_restore = None\n        if self.do_once and self.mode == \"best\":\n            <IND>checkpoint_name_to_restore = \"best\"\n\n        <DED>traced_model = trace_model_from_runner(\n            runner=runner,\n            checkpoint_name=checkpoint_name_to_restore,\n            method_name=self.method_name,\n            mode=self.trace_mode,\n            requires_grad=self.requires_grad,\n            opt_level=self.opt_level,\n            device=device,\n        )\n\n        save_traced_model(\n            model=traced_model,\n            logdir=runner.logdir,\n            checkpoint_name=self.mode,\n            method_name=self.method_name,\n            mode=self.trace_mode,\n            requires_grad=self.requires_grad,\n            opt_level=self.opt_level,\n            out_model=self.out_model,\n            out_dir=self.out_dir,\n        )\n\n    <DED>def on_epoch_end(self, runner: \"IRunner\"):\n        <IND>\"\"\"\n        Performing model tracing on epoch end if condition metric is improved\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if not self.do_once:\n            <IND>if self.mode == \"best\":\n                <IND>score = runner.valid_metrics[self.metric]\n\n                if self.best_score is None:\n                    <IND>self.best_score = score\n\n                # TODO: EarlyStoppingCallback and TracingCallback\n                #  will never work very first epoch\n                <DED>if self.is_better(score, self.best_score):\n                    <IND>self.best_score = score\n                    self._trace(runner)\n            <DED><DED>else:\n                <IND>self._trace(runner)\n\n    <DED><DED><DED>def on_stage_end(self, runner: \"IRunner\"):\n        <IND>\"\"\"\n        Performing model tracing on stage end if `do_once` is True.\n\n        Args:\n            runner: current runner\n        \"\"\"\n        if self.do_once:\n            <IND>self._trace(runner)\n\n\n# backward compatibility\n<DED><DED><DED>TracerCallback = TracingCallback\n\n__all__ = [\"TracingCallback\", \"TracerCallback\"]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "# @TODO: make the same API for tracing/onnx/pruning/quantization\n# from typing import TYPE_CHECKING, Union\n# from pathlib import Path\n# import warnings\n#\n# from catalyst.core.callback import Callback, CallbackNode, CallbackOrder\n# from catalyst.utils.tracing import save_traced_model, trace_model_from_runner\n#\n# if TYPE_CHECKING:\n#     from catalyst.core.runner import IRunner\n#\n#\n# class TracingCallback(Callback):\n#     \"\"\"\n#     Traces model during training if `metric` provided is improved.\n#     \"\"\"\n#\n#     def __init__(\n#         self,\n#         metric: str = \"loss\",\n#         minimize: bool = True,\n#         min_delta: float = 1e-6,\n#         mode: str = \"best\",\n#         do_once: bool = True,\n#         method_name: str = \"forward\",\n#         requires_grad: bool = False,\n#         opt_level: str = None,\n#         trace_mode: str = \"eval\",\n#         out_dir: Union[str, Path] = None,\n#         out_model: Union[str, Path] = None,\n#     ):\n#         \"\"\"\n#         Args:\n#             metric: Metric key we should trace model based on\n#             minimize: Whether do we minimize metric or not\n#             min_delta: Minimum value of change for metric to be\n#                 considered as improved\n#             mode: One of `best` or `last`\n#             do_once: Whether do we trace once per stage or every epoch\n#             method_name: Model's method name that will be\n#                 used as entrypoint during tracing\n#             requires_grad: Flag to use grads\n#             opt_level: AMP FP16 init level\n#             trace_mode: Mode for model to trace\n#                 (``train`` or ``eval``)\n#             out_dir (Union[str, Path]): Directory to save model to\n#             out_model (Union[str, Path]): Path to save model to\n#                 (overrides `out_dir` argument)\n#         \"\"\"\n#         super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n#\n#         if trace_mode not in [\"train\", \"eval\"]:\n#             raise ValueError(\n#                 f\"Unknown `trace_mode` '{trace_mode}'. \"\n#                 f\"Must be 'eval' or 'train'\"\n#             )\n#\n#         if mode not in [\"best\", \"last\"]:\n#             raise ValueError(\n#                 f\"Unknown `mode` '{mode}'. \" f\"Must be 'best' or 'last'\"\n#             )\n#\n#         if opt_level is not None:\n#             warnings.warn(\n#                 \"TracingCallback: \"\n#                 \"`opt_level` is not supported yet, \"\n#                 \"model will be traced as is\",\n#                 stacklevel=2,\n#             )\n#\n#         self.metric = metric\n#         self.mode = mode\n#         self.do_once = do_once\n#         self.best_score = None\n#         self.is_better = None\n#         if minimize:\n#             self.is_better = lambda score, best: score <= (best - min_delta)\n#         else:\n#             self.is_better = lambda score, best: score >= (best + min_delta)\n#\n#         self.requires_grad = requires_grad\n#         self.method_name = method_name\n#         self.trace_mode = trace_mode\n#         self.opt_level = None\n#\n#         if out_model is not None:\n#             out_model = Path(out_model)\n#         self.out_model = out_model\n#\n#         if out_dir is not None:\n#             out_dir = Path(out_dir)\n#         self.out_dir = out_dir\n#\n#     def _trace(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model tracing on epoch end if condition metric is improved.\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if self.opt_level is not None:\n#             device = \"cuda\"\n#         else:\n#             device = \"cpu\"\n#\n#         # the only case we need to restore model from previous checkpoint\n#         # is when we need to trace best model only once in the end of stage\n#         checkpoint_name_to_restore = None\n#         if self.do_once and self.mode == \"best\":\n#             checkpoint_name_to_restore = \"best\"\n#\n#         traced_model = trace_model_from_runner(\n#             runner=runner,\n#             checkpoint_name=checkpoint_name_to_restore,\n#             method_name=self.method_name,\n#             mode=self.trace_mode,\n#             requires_grad=self.requires_grad,\n#             opt_level=self.opt_level,\n#             device=device,\n#         )\n#\n#         save_traced_model(\n#             model=traced_model,\n#             logdir=runner.logdir,\n#             checkpoint_name=self.mode,\n#             method_name=self.method_name,\n#             mode=self.trace_mode,\n#             requires_grad=self.requires_grad,\n#             opt_level=self.opt_level,\n#             out_model=self.out_model,\n#             out_dir=self.out_dir,\n#         )\n#\n#     def on_epoch_end(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model tracing on epoch end if condition metric is improved\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if not self.do_once:\n#             if self.mode == \"best\":\n#                 score = runner.valid_metrics[self.metric]\n#\n#                 if self.best_score is None:\n#                     self.best_score = score\n#\n#                 # TODO: EarlyStoppingCallback and TracingCallback\n#                 #  will never work very first epoch\n#                 if self.is_better(score, self.best_score):\n#                     self.best_score = score\n#                     self._trace(runner)\n#             else:\n#                 self._trace(runner)\n#\n#     def on_stage_end(self, runner: \"IRunner\"):\n#         \"\"\"\n#         Performing model tracing on stage end if `do_once` is True.\n#\n#         Args:\n#             runner: current runner\n#         \"\"\"\n#         if self.do_once:\n#             self._trace(runner)\n#\n#\n# __all__ = [\"TracingCallback\"]\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/callbacks/alchemy_logger.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/callbacks/alchemy_logger.py:53:8 Incompatible variable type [9]: metric_names is declared to have type `List[str]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/callbacks/alchemy_logger.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/callbacks/confusion_matrix_logger.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/callbacks/confusion_matrix_logger.py:38:8 Incompatible variable type [9]: class_names is declared to have type `List[str]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/callbacks/confusion_matrix_logger.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/callbacks/confusion_matrix_logger.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/callbacks/confusion_matrix_logger.py:39:8 Incompatible variable type [9]: num_classes is declared to have type `int` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/callbacks/confusion_matrix_logger.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/callbacks/confusion_matrix_logger.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/callbacks/confusion_matrix_logger.py:40:8 Incompatible variable type [9]: plot_params is declared to have type `Dict[typing.Any, typing.Any]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/callbacks/confusion_matrix_logger.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/callbacks/confusion_matrix_logger.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/callbacks/confusion_matrix_logger.py:42:8 Incompatible variable type [9]: version is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/callbacks/confusion_matrix_logger.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/callbacks/cutmix_callback.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/callbacks/cutmix_callback.py:29:8 Incompatible variable type [9]: fields is declared to have type `List[str]` but is used as type `typing.Tuple[str]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/callbacks/cutmix_callback.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/callbacks/mixup_callback.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/callbacks/mixup_callback.py:30:8 Incompatible variable type [9]: fields is declared to have type `List[str]` but is used as type `typing.Tuple[str]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/callbacks/mixup_callback.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/callbacks/neptune_logger.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/callbacks/neptune_logger.py:71:8 Incompatible variable type [9]: metric_names is declared to have type `List[str]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/callbacks/neptune_logger.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/callbacks/telegram_logger.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/callbacks/telegram_logger.py:25:8 Incompatible variable type [9]: token is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/callbacks/telegram_logger.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/callbacks/telegram_logger.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/callbacks/telegram_logger.py:26:8 Incompatible variable type [9]: chat_id is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/callbacks/telegram_logger.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/callbacks/telegram_logger.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/callbacks/telegram_logger.py:27:8 Incompatible variable type [9]: metric_names is declared to have type `List[str]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/callbacks/telegram_logger.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/callbacks/tests/test_gradnorm_logger.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/callbacks/tests/test_gradnorm_logger.py:38:16 Incompatible parameter type [6]: Expected `Tuple[int]` for 1st parameter `size` to call `_SimpleNet.conv2d_size_out` but got `Tuple[typing.Any, typing.Any]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/callbacks/tests/test_gradnorm_logger.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/callbacks/tests/test_gradnorm_logger.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/callbacks/tests/test_gradnorm_logger.py:39:16 Incompatible parameter type [6]: Expected `Tuple[int]` for 2nd parameter `kernel_size` to call `_SimpleNet.conv2d_size_out` but got `Tuple[typing.Any, typing.Any]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/callbacks/tests/test_gradnorm_logger.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/callbacks/tests/test_gradnorm_logger.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/callbacks/tests/test_gradnorm_logger.py:40:16 Incompatible parameter type [6]: Expected `Tuple[int]` for 3rd parameter `stride` to call `_SimpleNet.conv2d_size_out` but got `Tuple[typing.Any, typing.Any]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/callbacks/tests/test_gradnorm_logger.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/callbacks/wandb_logger.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/callbacks/wandb_logger.py:67:8 Incompatible variable type [9]: metric_names is declared to have type `List[str]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/callbacks/wandb_logger.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/callbacks/wandb_logger.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/callbacks/wandb_logger.py:70:8 Incompatible variable type [9]: log is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/callbacks/wandb_logger.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/data/cv/mixins/blur.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/data/cv/mixins/blur.py:18:8 Incompatible variable type [9]: blur is declared to have type `List[str]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/data/cv/mixins/blur.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/data/cv/mixins/flare.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/data/cv/mixins/flare.py:14:8 Incompatible variable type [9]: sunflare_params is declared to have type `Dict[typing.Any, typing.Any]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/data/cv/mixins/flare.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/data/cv/mixins/rotate.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/data/cv/mixins/rotate.py:15:8 Incompatible variable type [9]: targets_key is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/data/cv/mixins/rotate.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/data/cv/mixins/rotate.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/data/cv/mixins/rotate.py:18:8 Incompatible variable type [9]: one_hot_classes is declared to have type `int` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/data/cv/mixins/rotate.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/data/reader.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/data/reader.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/contrib/data/reader.py:149:47 Incompatible variable type [9]: mixins is declared to have type `List[typing.Any]` but is used as type `None`.",
    "message": " mixins is declared to have type `List[typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 149,
    "warning_line": "    def __init__(self, readers: List[IReader], mixins: list = None):",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    def __init__(self, readers: List[IReader], mixins: list = None):\n        \"\"\"\n        Args:\n            readers: list of reader to compose\n            mixins: list of mixins to use\n        \"\"\"\n        self.readers = readers\n        self.mixins = mixins or []\n\n",
        "source_code_len": 264,
        "target_code": "\n    def __init__(self, transforms: List[IReader]):\n        \"\"\"\n        Args:\n            transforms: list of reader to compose\n            mixins: list of mixins to use\n        \"\"\"\n        self.transforms = transforms\n\n",
        "target_code_len": 220,
        "diff_format": "@@ -148,10 +143,9 @@\n \n-    def __init__(self, readers: List[IReader], mixins: list = None):\n+    def __init__(self, transforms: List[IReader]):\n         \"\"\"\n         Args:\n-            readers: list of reader to compose\n+            transforms: list of reader to compose\n             mixins: list of mixins to use\n         \"\"\"\n-        self.readers = readers\n-        self.mixins = mixins or []\n+        self.transforms = transforms\n \n",
        "source_code_with_indent": "\n    def __init__(self, readers: List[IReader], mixins: list = None):\n        <IND>\"\"\"\n        Args:\n            readers: list of reader to compose\n            mixins: list of mixins to use\n        \"\"\"\n        self.readers = readers\n        self.mixins = mixins or []\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    def __init__(self, transforms: List[IReader]):\n        <IND>\"\"\"\n        Args:\n            transforms: list of reader to compose\n            mixins: list of mixins to use\n        \"\"\"\n        self.transforms = transforms\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/datasets/cv/functional.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/datasets/cv/misc.py",
    "file_hunks_size": 2,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/contrib/datasets/cv/functional.py:46:4 Incompatible attribute type [8]: Attribute `resources` declared in class `ImageClassificationDataset` has type `Iterable[Tuple[str, str]]` but is used as type `None`.",
    "message": " Attribute `resources` declared in class `ImageClassificationDataset` has type `Iterable[Tuple[str, str]]` but is used as type `None`.",
    "rule_id": "Incompatible attribute type [8]",
    "warning_line_no": 46,
    "warning_line": "    resources: Iterable[Tuple[str, str]] = None"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/nn/criterion/dice.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/nn/criterion/dice.py",
    "file_hunks_size": 4,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/contrib/nn/criterion/dice.py:18:8 Incompatible variable type [9]: threshold is declared to have type `float` but is used as type `None`.",
    "message": " threshold is declared to have type `float` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 18,
    "warning_line": "        threshold: float = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/nn/criterion/dice.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/nn/criterion/dice.py",
    "file_hunks_size": 4,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/contrib/nn/criterion/dice.py:50:8 Incompatible variable type [9]: threshold is declared to have type `float` but is used as type `None`.",
    "message": " threshold is declared to have type `float` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 50,
    "warning_line": "        threshold: float = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/scripts/find_thresholds.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/scripts/find_thresholds.py:108:52 Incompatible variable type [9]: ignore_label is declared to have type `int` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/scripts/find_thresholds.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/scripts/find_thresholds.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/scripts/find_thresholds.py:190:4 Incompatible variable type [9]: ignore_label is declared to have type `int` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/scripts/find_thresholds.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/scripts/find_thresholds.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/scripts/find_thresholds.py:227:4 Incompatible variable type [9]: thresholds is declared to have type `Dict[int, float]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/scripts/find_thresholds.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/scripts/find_thresholds.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/scripts/find_thresholds.py:228:4 Incompatible variable type [9]: classes is declared to have type `List[int]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/scripts/find_thresholds.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/scripts/find_thresholds.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/scripts/find_thresholds.py:277:41 Incompatible variable type [9]: suffix is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/scripts/find_thresholds.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/scripts/find_thresholds.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/scripts/find_thresholds.py:278:4 Incompatible variable type [9]: outpath is declared to have type `Path` but is used as type `str`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/scripts/find_thresholds.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/scripts/find_thresholds.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/scripts/find_thresholds.py:280:8 Incompatible variable type [9]: outpath is declared to have type `Path` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/scripts/find_thresholds.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/tools/tensorboard.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/tools/tensorboard.py:76:38 Incompatible parameter type [6]: Expected `Union[array.array[typing.Any], bytearray, bytes, memoryview, mmap.mmap]` for 2nd positional only parameter to call `struct.unpack` but got `Optional[bytes]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/tools/tensorboard.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/tools/tensorboard.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/tools/tensorboard.py:86:4 Inconsistent override [15]: `catalyst.contrib.tools.tensorboard.EventsFileReader.__iter__` overrides method defined in `typing.Iterable` inconsistently. Returned type `unknown` is not a subtype of the overridden return `typing.Iterator[typing.Any]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/tools/tensorboard.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/tools/tensorboard.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/tools/tensorboard.py:178:16 Incompatible return type [7]: Expected `Optional[SummaryItem]` but got `typing.Generator[None, None, None]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/tools/tensorboard.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/tools/tensorboard.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/tools/tensorboard.py:187:24 Incompatible return type [7]: Expected `Optional[SummaryItem]` but got `typing.Generator[SummaryItem, None, None]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/tools/tensorboard.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/tools/tensorboard.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/tools/tensorboard.py:195:20 Incompatible return type [7]: Expected `Optional[SummaryItem]` but got `typing.Generator[None, None, None]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/tools/tensorboard.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/tools/tensorboard.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/tools/tensorboard.py:208:4 Inconsistent override [15]: `catalyst.contrib.tools.tensorboard.SummaryReader.__iter__` overrides method defined in `typing.Iterable` inconsistently. Returned type `SummaryItem` is not a subtype of the overridden return `typing.Iterator[typing.Any]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/tools/tensorboard.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/tools/tensorboard.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/tools/tensorboard.py:218:16 Incompatible return type [7]: Expected `SummaryItem` but got `typing.Generator[typing.Any, None, None]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/tools/tensorboard.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/utils/cv/image.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/utils/image.py",
    "file_hunks_size": 6,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/contrib/utils/cv/image.py:45:4 Incompatible variable type [9]: rootpath is declared to have type `Union[pathlib.Path, str]` but is used as type `None`.",
    "message": " rootpath is declared to have type `Union[pathlib.Path, str]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 45,
    "warning_line": "    rootpath: Union[str, pathlib.Path] = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/utils/cv/image.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/utils/image.py",
    "file_hunks_size": 6,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/contrib/utils/cv/image.py:123:4 Incompatible variable type [9]: Unable to unpack `None`, expected a tuple.",
    "message": " Unable to unpack `None`, expected a tuple.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 123,
    "warning_line": "    clip_range: Tuple[int, int] = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/utils/cv/image.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/utils/image.py",
    "file_hunks_size": 6,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/contrib/utils/cv/image.py:125:4 Incompatible variable type [9]: rootpath is declared to have type `Union[pathlib.Path, str]` but is used as type `None`.",
    "message": " rootpath is declared to have type `Union[pathlib.Path, str]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 125,
    "warning_line": "    rootpath: Union[str, pathlib.Path] = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/utils/nlp/text.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/utils/nlp/text.py:63:4 Incompatible variable type [9]: pooling_groups is declared to have type `List[str]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/utils/nlp/text.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/utils/nlp/text.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/utils/nlp/text.py:65:4 Incompatible variable type [9]: level is declared to have type `Union[int, str]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/utils/nlp/text.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/utils/pandas.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/utils/pandas.py",
    "file_hunks_size": 15,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/contrib/utils/pandas.py:61:4 Incompatible return type [7]: Expected `Dict[str, object]` but got `typing.DefaultDict[typing.Any, List[typing.Any]]`.",
    "message": " Expected `Dict[str, object]` but got `typing.DefaultDict[typing.Any, List[typing.Any]]`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 61,
    "warning_line": "    return dataset",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "# flake8: noqa\n# TODO: add docs and refactor for pure contrib\nfrom typing import Callable, Dict, List, Optional, Tuple, Union\nfrom collections import defaultdict\n",
        "source_code_len": 162,
        "target_code": "# flake8: noqa\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union\nfrom collections import defaultdict\n",
        "target_code_len": 120,
        "diff_format": "@@ -1,4 +1,3 @@\n # flake8: noqa\n-# TODO: add docs and refactor for pure contrib\n-from typing import Callable, Dict, List, Optional, Tuple, Union\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n from collections import defaultdict\n",
        "source_code_with_indent": "# flake8: noqa\n# TODO: add docs and refactor for pure contrib\nfrom typing import Callable, Dict, List, Optional, Tuple, Union\nfrom collections import defaultdict\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "# flake8: noqa\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union\nfrom collections import defaultdict\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nDictDataset = Dict[str, object]\n\n",
        "source_code_len": 34,
        "target_code": "\nDictDataset = Dict[str, List[Any]]\n\n",
        "target_code_len": 37,
        "diff_format": "@@ -15,3 +14,3 @@\n \n-DictDataset = Dict[str, object]\n+DictDataset = Dict[str, List[Any]]\n \n",
        "source_code_with_indent": "\nDictDataset = Dict[str, object]\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\nDictDataset = Dict[str, List[Any]]\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/utils/pandas.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/utils/pandas.py",
    "file_hunks_size": 15,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/contrib/utils/pandas.py:105:4 Incompatible return type [7]: Expected `Tuple[Dict[str, object], Dict[str, object]]` but got `Tuple[typing.DefaultDict[typing.Any, List[typing.Any]], typing.DefaultDict[typing.Any, List[typing.Any]]]`.",
    "message": " Expected `Tuple[Dict[str, object], Dict[str, object]]` but got `Tuple[typing.DefaultDict[typing.Any, List[typing.Any]], typing.DefaultDict[typing.Any, List[typing.Any]]]`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 105,
    "warning_line": "    return train_dataset, test_dataset",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "# flake8: noqa\n# TODO: add docs and refactor for pure contrib\nfrom typing import Callable, Dict, List, Optional, Tuple, Union\nfrom collections import defaultdict\n",
        "source_code_len": 162,
        "target_code": "# flake8: noqa\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union\nfrom collections import defaultdict\n",
        "target_code_len": 120,
        "diff_format": "@@ -1,4 +1,3 @@\n # flake8: noqa\n-# TODO: add docs and refactor for pure contrib\n-from typing import Callable, Dict, List, Optional, Tuple, Union\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n from collections import defaultdict\n",
        "source_code_with_indent": "# flake8: noqa\n# TODO: add docs and refactor for pure contrib\nfrom typing import Callable, Dict, List, Optional, Tuple, Union\nfrom collections import defaultdict\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "# flake8: noqa\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union\nfrom collections import defaultdict\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nDictDataset = Dict[str, object]\n\n",
        "source_code_len": 34,
        "target_code": "\nDictDataset = Dict[str, List[Any]]\n\n",
        "target_code_len": 37,
        "diff_format": "@@ -15,3 +14,3 @@\n \n-DictDataset = Dict[str, object]\n+DictDataset = Dict[str, List[Any]]\n \n",
        "source_code_with_indent": "\nDictDataset = Dict[str, object]\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\nDictDataset = Dict[str, List[Any]]\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/utils/pandas.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/utils/pandas.py",
    "file_hunks_size": 15,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/contrib/utils/pandas.py:473:4 Incompatible variable type [9]: in_csv_train is declared to have type `str` but is used as type `None`.",
    "message": " in_csv_train is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 473,
    "warning_line": "    in_csv_train: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    if class_column is not None:\n        result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    else:\n        result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_len": 682,
        "target_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    if class_column is not None:\n        df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    else:\n        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_len": 562,
        "diff_format": "@@ -415,22 +394,15 @@\n     if args_are_not_none(tag2class, tag_column, class_column):\n-        dataframe = map_dataframe(\n-            dataframe, tag_column, class_column, tag2class\n-        )\n+        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n \n     if class_column is not None:\n-        result_dataframe = split_dataframe_on_stratified_folds(\n-            dataframe,\n-            class_column=class_column,\n-            random_state=seed,\n-            n_folds=n_folds,\n+        df_all = split_dataframe_on_stratified_folds(\n+            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n         )\n     else:\n-        result_dataframe = split_dataframe_on_folds(\n-            dataframe, random_state=seed, n_folds=n_folds\n-        )\n-\n-    fold_series = result_dataframe[\"fold\"]\n+        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n+\n+    fold_series = df_all[\"fold\"]\n \n     train_folds = folds_to_list(train_folds)\n-    df_train = result_dataframe[fold_series.isin(train_folds)]\n+    df_train = df_all[fold_series.isin(train_folds)]\n \n",
        "source_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    <DED>if class_column is not None:\n        <IND>result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    <DED>fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    <DED>if class_column is not None:\n        <IND>df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    <DED>fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\ndef merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    \"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        for csv_path in paths.split(\",\"):\n            dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    return result\n\n\ndef read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        if fold_df is not None:\n            fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\ndef read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    \"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    if from_one_df:\n        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    else:\n        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    for data in [df_train, df_valid, df_infer]:\n        if data is not None and \"fold\" in data.columns:\n            del data[\"fold\"]\n\n    result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_len": 6590,
        "target_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_len": 6715,
        "diff_format": "@@ -438,196 +410,183 @@\n         mask = ~fold_series.isin(train_folds)\n-        valid_folds = result_dataframe[mask][\"fold\"]\n+        valid_folds = df_all[mask][\"fold\"]\n \n     valid_folds = folds_to_list(valid_folds)\n-    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n+    df_valid = df_all[fold_series.isin(valid_folds)]\n \n     infer_folds = folds_to_list(infer_folds or [])\n-    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n-\n-    return result_dataframe, df_train, df_valid, df_infer\n-\n-\n-def merge_multiple_fold_csv(\n-    fold_name: str, paths: Optional[str]\n-) -> pd.DataFrame:\n-    \"\"\"Reads csv into one DataFrame with column ``fold``.\n-\n-    Args:\n-        fold_name: current fold name\n-        paths: paths to csv separated by commas\n-\n-    Returns:\n-         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n-    \"\"\"\n-    result = pd.DataFrame()\n-    if paths is not None:\n-        for csv_path in paths.split(\",\"):\n-            dataframe = pd.read_csv(csv_path)\n-            dataframe[\"fold\"] = fold_name\n-            result = result.append(dataframe, ignore_index=True)\n-\n-    return result\n-\n-\n-def read_multiple_dataframes(\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n-    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n-\n-    Args:\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-        tag2class (Dict[str, int], optional): mapping from label names into int\n-        tag_column (str, optional): column with label names\n-        class_column (str, optional): column to use for split\n-\n-    Returns:\n-        tuple: tuple with 4 dataframes\n-            whole dataframe, train part, valid part and infer part\n-    \"\"\"\n-    assert any(\n-        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n-    )\n-\n-    result_df = None\n-    fold_dfs = {}\n-    for fold_df, fold_name in zip(\n-        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n-    ):\n-        if fold_df is not None:\n-            fold_df = merge_multiple_fold_csv(\n-                fold_name=fold_name, paths=fold_df\n-            )\n-            if args_are_not_none(tag2class, tag_column, class_column):\n-                fold_df = map_dataframe(\n-                    fold_df, tag_column, class_column, tag2class\n-                )\n-            fold_dfs[fold_name] = fold_df\n-\n-            result_df = (\n-                fold_df\n-                if result_df is None\n-                else result_df.append(fold_df, ignore_index=True)\n-            )\n-\n-    output = (\n-        result_df,\n-        fold_dfs.get(\"train\", None),\n-        fold_dfs.get(\"valid\", None),\n-        fold_dfs.get(\"infer\", None),\n-    )\n-\n-    return output\n-\n-\n-def read_csv_data(\n-    in_csv: str = None,\n-    train_folds: Optional[List[int]] = None,\n-    valid_folds: Optional[List[int]] = None,\n-    infer_folds: Optional[List[int]] = None,\n-    seed: int = 42,\n-    n_folds: int = 5,\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-    \"\"\"\n-    From giving path ``in_csv`` reads a dataframe\n-    and split it to train/valid/infer folds\n-    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n-    reads independent folds.\n-\n-    .. note::\n-       This function can be used with different combinations of params.\n-        First block is used to get dataset from one `csv`:\n-            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n-        Second includes paths to different csv for train/valid and infer parts:\n-            in_csv_train, in_csv_valid, in_csv_infer\n-        The other params (tag2class, tag_column, class_column) are optional\n-            for any previous block\n-\n-    Args:\n-        in_csv: paths to whole dataset\n-        train_folds: train folds\n-        valid_folds (List[int], optional): valid folds.\n-            If none takes all folds not included in ``train_folds``\n-        infer_folds (List[int], optional): infer folds.\n-            If none takes all folds not included in ``train_folds``\n-            and ``valid_folds``\n-        seed: seed for split\n-        n_folds: number of folds\n-\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-\n-        tag2class (Dict[str, int]): mapping from label names into ints\n-        tag_column: column with label names\n-        class_column: column to use for split\n-\n-    Returns:\n-        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-            tuple with 4 elements\n-            (whole dataframe,\n-            list with train data,\n-            list with valid data\n-            and list with infer data)\n-    \"\"\"\n-    from_one_df: bool = in_csv is not None\n-    from_multiple_df: bool = (\n-        in_csv_train is not None\n-        or in_csv_valid is not None\n-        or in_csv_infer is not None\n-    )\n-\n-    if from_one_df == from_multiple_df:\n-        raise ValueError(\n-            \"You should pass `in_csv` \"\n-            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n-        )\n-\n-    if from_one_df:\n-        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n-        dataframe, df_train, df_valid, df_infer = split_dataframe(\n-            dataframe,\n-            train_folds=train_folds,\n-            valid_folds=valid_folds,\n-            infer_folds=infer_folds,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-            seed=seed,\n-            n_folds=n_folds,\n-        )\n-    else:\n-        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n-            in_csv_train=in_csv_train,\n-            in_csv_valid=in_csv_valid,\n-            in_csv_infer=in_csv_infer,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-        )\n-\n-    for data in [df_train, df_valid, df_infer]:\n-        if data is not None and \"fold\" in data.columns:\n-            del data[\"fold\"]\n-\n-    result = (\n-        dataframe,\n-        dataframe_to_list(df_train) if df_train is not None else None,\n-        dataframe_to_list(df_valid) if df_valid is not None else None,\n-        dataframe_to_list(df_infer) if df_infer is not None else None,\n-    )\n-\n-    return result\n+    df_infer = df_all[fold_series.isin(infer_folds)]\n+\n+    return df_all, df_train, df_valid, df_infer\n+\n+\n+# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n+#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n+#\n+#     Args:\n+#         fold_name: current fold name\n+#         paths: paths to csv separated by commas\n+#\n+#     Returns:\n+#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n+#     \"\"\"\n+#     result = pd.DataFrame()\n+#     if paths is not None:\n+#         for csv_path in paths.split(\",\"):\n+#             dataframe = pd.read_csv(csv_path)\n+#             dataframe[\"fold\"] = fold_name\n+#             result = result.append(dataframe, ignore_index=True)\n+#\n+#     return result\n+\n+\n+# def read_multiple_dataframes(\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n+#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n+#\n+#     Args:\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#         tag2class (Dict[str, int], optional): mapping from label names into int\n+#         tag_column (str, optional): column with label names\n+#         class_column (str, optional): column to use for split\n+#\n+#     Returns:\n+#         tuple: tuple with 4 dataframes\n+#             whole dataframe, train part, valid part and infer part\n+#     \"\"\"\n+#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n+#\n+#     result_df = None\n+#     fold_dfs = {}\n+#     for fold_df, fold_name in zip(\n+#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n+#     ):\n+#         if fold_df is not None:\n+#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n+#             if args_are_not_none(tag2class, tag_column, class_column):\n+#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n+#             fold_dfs[fold_name] = fold_df\n+#\n+#             result_df = (\n+#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n+#             )\n+#\n+#     output = (\n+#         result_df,\n+#         fold_dfs.get(\"train\", None),\n+#         fold_dfs.get(\"valid\", None),\n+#         fold_dfs.get(\"infer\", None),\n+#     )\n+#\n+#     return output\n+\n+\n+# def read_csv_data(\n+#     in_csv: str = None,\n+#     train_folds: Optional[List[int]] = None,\n+#     valid_folds: Optional[List[int]] = None,\n+#     infer_folds: Optional[List[int]] = None,\n+#     seed: int = 42,\n+#     n_folds: int = 5,\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#     \"\"\"\n+#     From giving path ``in_csv`` reads a dataframe\n+#     and split it to train/valid/infer folds\n+#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n+#     reads independent folds.\n+#\n+#     .. note::\n+#        This function can be used with different combinations of params.\n+#         First block is used to get dataset from one `csv`:\n+#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n+#         Second includes paths to different csv for train/valid and infer parts:\n+#             in_csv_train, in_csv_valid, in_csv_infer\n+#         The other params (tag2class, tag_column, class_column) are optional\n+#             for any previous block\n+#\n+#     Args:\n+#         in_csv: paths to whole dataset\n+#         train_folds: train folds\n+#         valid_folds (List[int], optional): valid folds.\n+#             If none takes all folds not included in ``train_folds``\n+#         infer_folds (List[int], optional): infer folds.\n+#             If none takes all folds not included in ``train_folds``\n+#             and ``valid_folds``\n+#         seed: seed for split\n+#         n_folds: number of folds\n+#\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#\n+#         tag2class (Dict[str, int]): mapping from label names into ints\n+#         tag_column: column with label names\n+#         class_column: column to use for split\n+#\n+#     Returns:\n+#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#             tuple with 4 elements\n+#             (whole dataframe,\n+#             list with train data,\n+#             list with valid data\n+#             and list with infer data)\n+#     \"\"\"\n+#     from_one_df: bool = in_csv is not None\n+#     from_multiple_df: bool = (\n+#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n+#     )\n+#\n+#     if from_one_df == from_multiple_df:\n+#         raise ValueError(\n+#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n+#         )\n+#\n+#     if from_one_df:\n+#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n+#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n+#             dataframe,\n+#             train_folds=train_folds,\n+#             valid_folds=valid_folds,\n+#             infer_folds=infer_folds,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#             seed=seed,\n+#             n_folds=n_folds,\n+#         )\n+#     else:\n+#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n+#             in_csv_train=in_csv_train,\n+#             in_csv_valid=in_csv_valid,\n+#             in_csv_infer=in_csv_infer,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#         )\n+#\n+#     for data in [df_train, df_valid, df_infer]:\n+#         if data is not None and \"fold\" in data.columns:\n+#             del data[\"fold\"]\n+#\n+#     result = (\n+#         dataframe,\n+#         dataframe_to_list(df_train) if df_train is not None else None,\n+#         dataframe_to_list(df_valid) if df_valid is not None else None,\n+#         dataframe_to_list(df_infer) if df_infer is not None else None,\n+#     )\n+#\n+#     return result\n \n",
        "source_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\n<DED>def merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    <IND>\"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        <IND>for csv_path in paths.split(\",\"):\n            <IND>dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    <DED><DED>return result\n\n\n<DED>def read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    <IND>\"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        <IND>if fold_df is not None:\n            <IND>fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                <IND>fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            <DED>fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    <DED><DED>output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\n<DED>def read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    <IND>\"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        <IND>raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    <DED>if from_one_df:\n        <IND>dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    <DED>for data in [df_train, df_valid, df_infer]:\n        <IND>if data is not None and \"fold\" in data.columns:\n            <IND>del data[\"fold\"]\n\n    <DED><DED>result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/utils/pandas.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/utils/pandas.py",
    "file_hunks_size": 15,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/contrib/utils/pandas.py:474:4 Incompatible variable type [9]: in_csv_valid is declared to have type `str` but is used as type `None`.",
    "message": " in_csv_valid is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 474,
    "warning_line": "    in_csv_valid: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    if class_column is not None:\n        result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    else:\n        result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_len": 682,
        "target_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    if class_column is not None:\n        df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    else:\n        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_len": 562,
        "diff_format": "@@ -415,22 +394,15 @@\n     if args_are_not_none(tag2class, tag_column, class_column):\n-        dataframe = map_dataframe(\n-            dataframe, tag_column, class_column, tag2class\n-        )\n+        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n \n     if class_column is not None:\n-        result_dataframe = split_dataframe_on_stratified_folds(\n-            dataframe,\n-            class_column=class_column,\n-            random_state=seed,\n-            n_folds=n_folds,\n+        df_all = split_dataframe_on_stratified_folds(\n+            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n         )\n     else:\n-        result_dataframe = split_dataframe_on_folds(\n-            dataframe, random_state=seed, n_folds=n_folds\n-        )\n-\n-    fold_series = result_dataframe[\"fold\"]\n+        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n+\n+    fold_series = df_all[\"fold\"]\n \n     train_folds = folds_to_list(train_folds)\n-    df_train = result_dataframe[fold_series.isin(train_folds)]\n+    df_train = df_all[fold_series.isin(train_folds)]\n \n",
        "source_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    <DED>if class_column is not None:\n        <IND>result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    <DED>fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    <DED>if class_column is not None:\n        <IND>df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    <DED>fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\ndef merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    \"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        for csv_path in paths.split(\",\"):\n            dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    return result\n\n\ndef read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        if fold_df is not None:\n            fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\ndef read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    \"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    if from_one_df:\n        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    else:\n        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    for data in [df_train, df_valid, df_infer]:\n        if data is not None and \"fold\" in data.columns:\n            del data[\"fold\"]\n\n    result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_len": 6590,
        "target_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_len": 6715,
        "diff_format": "@@ -438,196 +410,183 @@\n         mask = ~fold_series.isin(train_folds)\n-        valid_folds = result_dataframe[mask][\"fold\"]\n+        valid_folds = df_all[mask][\"fold\"]\n \n     valid_folds = folds_to_list(valid_folds)\n-    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n+    df_valid = df_all[fold_series.isin(valid_folds)]\n \n     infer_folds = folds_to_list(infer_folds or [])\n-    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n-\n-    return result_dataframe, df_train, df_valid, df_infer\n-\n-\n-def merge_multiple_fold_csv(\n-    fold_name: str, paths: Optional[str]\n-) -> pd.DataFrame:\n-    \"\"\"Reads csv into one DataFrame with column ``fold``.\n-\n-    Args:\n-        fold_name: current fold name\n-        paths: paths to csv separated by commas\n-\n-    Returns:\n-         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n-    \"\"\"\n-    result = pd.DataFrame()\n-    if paths is not None:\n-        for csv_path in paths.split(\",\"):\n-            dataframe = pd.read_csv(csv_path)\n-            dataframe[\"fold\"] = fold_name\n-            result = result.append(dataframe, ignore_index=True)\n-\n-    return result\n-\n-\n-def read_multiple_dataframes(\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n-    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n-\n-    Args:\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-        tag2class (Dict[str, int], optional): mapping from label names into int\n-        tag_column (str, optional): column with label names\n-        class_column (str, optional): column to use for split\n-\n-    Returns:\n-        tuple: tuple with 4 dataframes\n-            whole dataframe, train part, valid part and infer part\n-    \"\"\"\n-    assert any(\n-        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n-    )\n-\n-    result_df = None\n-    fold_dfs = {}\n-    for fold_df, fold_name in zip(\n-        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n-    ):\n-        if fold_df is not None:\n-            fold_df = merge_multiple_fold_csv(\n-                fold_name=fold_name, paths=fold_df\n-            )\n-            if args_are_not_none(tag2class, tag_column, class_column):\n-                fold_df = map_dataframe(\n-                    fold_df, tag_column, class_column, tag2class\n-                )\n-            fold_dfs[fold_name] = fold_df\n-\n-            result_df = (\n-                fold_df\n-                if result_df is None\n-                else result_df.append(fold_df, ignore_index=True)\n-            )\n-\n-    output = (\n-        result_df,\n-        fold_dfs.get(\"train\", None),\n-        fold_dfs.get(\"valid\", None),\n-        fold_dfs.get(\"infer\", None),\n-    )\n-\n-    return output\n-\n-\n-def read_csv_data(\n-    in_csv: str = None,\n-    train_folds: Optional[List[int]] = None,\n-    valid_folds: Optional[List[int]] = None,\n-    infer_folds: Optional[List[int]] = None,\n-    seed: int = 42,\n-    n_folds: int = 5,\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-    \"\"\"\n-    From giving path ``in_csv`` reads a dataframe\n-    and split it to train/valid/infer folds\n-    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n-    reads independent folds.\n-\n-    .. note::\n-       This function can be used with different combinations of params.\n-        First block is used to get dataset from one `csv`:\n-            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n-        Second includes paths to different csv for train/valid and infer parts:\n-            in_csv_train, in_csv_valid, in_csv_infer\n-        The other params (tag2class, tag_column, class_column) are optional\n-            for any previous block\n-\n-    Args:\n-        in_csv: paths to whole dataset\n-        train_folds: train folds\n-        valid_folds (List[int], optional): valid folds.\n-            If none takes all folds not included in ``train_folds``\n-        infer_folds (List[int], optional): infer folds.\n-            If none takes all folds not included in ``train_folds``\n-            and ``valid_folds``\n-        seed: seed for split\n-        n_folds: number of folds\n-\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-\n-        tag2class (Dict[str, int]): mapping from label names into ints\n-        tag_column: column with label names\n-        class_column: column to use for split\n-\n-    Returns:\n-        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-            tuple with 4 elements\n-            (whole dataframe,\n-            list with train data,\n-            list with valid data\n-            and list with infer data)\n-    \"\"\"\n-    from_one_df: bool = in_csv is not None\n-    from_multiple_df: bool = (\n-        in_csv_train is not None\n-        or in_csv_valid is not None\n-        or in_csv_infer is not None\n-    )\n-\n-    if from_one_df == from_multiple_df:\n-        raise ValueError(\n-            \"You should pass `in_csv` \"\n-            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n-        )\n-\n-    if from_one_df:\n-        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n-        dataframe, df_train, df_valid, df_infer = split_dataframe(\n-            dataframe,\n-            train_folds=train_folds,\n-            valid_folds=valid_folds,\n-            infer_folds=infer_folds,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-            seed=seed,\n-            n_folds=n_folds,\n-        )\n-    else:\n-        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n-            in_csv_train=in_csv_train,\n-            in_csv_valid=in_csv_valid,\n-            in_csv_infer=in_csv_infer,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-        )\n-\n-    for data in [df_train, df_valid, df_infer]:\n-        if data is not None and \"fold\" in data.columns:\n-            del data[\"fold\"]\n-\n-    result = (\n-        dataframe,\n-        dataframe_to_list(df_train) if df_train is not None else None,\n-        dataframe_to_list(df_valid) if df_valid is not None else None,\n-        dataframe_to_list(df_infer) if df_infer is not None else None,\n-    )\n-\n-    return result\n+    df_infer = df_all[fold_series.isin(infer_folds)]\n+\n+    return df_all, df_train, df_valid, df_infer\n+\n+\n+# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n+#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n+#\n+#     Args:\n+#         fold_name: current fold name\n+#         paths: paths to csv separated by commas\n+#\n+#     Returns:\n+#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n+#     \"\"\"\n+#     result = pd.DataFrame()\n+#     if paths is not None:\n+#         for csv_path in paths.split(\",\"):\n+#             dataframe = pd.read_csv(csv_path)\n+#             dataframe[\"fold\"] = fold_name\n+#             result = result.append(dataframe, ignore_index=True)\n+#\n+#     return result\n+\n+\n+# def read_multiple_dataframes(\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n+#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n+#\n+#     Args:\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#         tag2class (Dict[str, int], optional): mapping from label names into int\n+#         tag_column (str, optional): column with label names\n+#         class_column (str, optional): column to use for split\n+#\n+#     Returns:\n+#         tuple: tuple with 4 dataframes\n+#             whole dataframe, train part, valid part and infer part\n+#     \"\"\"\n+#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n+#\n+#     result_df = None\n+#     fold_dfs = {}\n+#     for fold_df, fold_name in zip(\n+#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n+#     ):\n+#         if fold_df is not None:\n+#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n+#             if args_are_not_none(tag2class, tag_column, class_column):\n+#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n+#             fold_dfs[fold_name] = fold_df\n+#\n+#             result_df = (\n+#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n+#             )\n+#\n+#     output = (\n+#         result_df,\n+#         fold_dfs.get(\"train\", None),\n+#         fold_dfs.get(\"valid\", None),\n+#         fold_dfs.get(\"infer\", None),\n+#     )\n+#\n+#     return output\n+\n+\n+# def read_csv_data(\n+#     in_csv: str = None,\n+#     train_folds: Optional[List[int]] = None,\n+#     valid_folds: Optional[List[int]] = None,\n+#     infer_folds: Optional[List[int]] = None,\n+#     seed: int = 42,\n+#     n_folds: int = 5,\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#     \"\"\"\n+#     From giving path ``in_csv`` reads a dataframe\n+#     and split it to train/valid/infer folds\n+#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n+#     reads independent folds.\n+#\n+#     .. note::\n+#        This function can be used with different combinations of params.\n+#         First block is used to get dataset from one `csv`:\n+#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n+#         Second includes paths to different csv for train/valid and infer parts:\n+#             in_csv_train, in_csv_valid, in_csv_infer\n+#         The other params (tag2class, tag_column, class_column) are optional\n+#             for any previous block\n+#\n+#     Args:\n+#         in_csv: paths to whole dataset\n+#         train_folds: train folds\n+#         valid_folds (List[int], optional): valid folds.\n+#             If none takes all folds not included in ``train_folds``\n+#         infer_folds (List[int], optional): infer folds.\n+#             If none takes all folds not included in ``train_folds``\n+#             and ``valid_folds``\n+#         seed: seed for split\n+#         n_folds: number of folds\n+#\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#\n+#         tag2class (Dict[str, int]): mapping from label names into ints\n+#         tag_column: column with label names\n+#         class_column: column to use for split\n+#\n+#     Returns:\n+#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#             tuple with 4 elements\n+#             (whole dataframe,\n+#             list with train data,\n+#             list with valid data\n+#             and list with infer data)\n+#     \"\"\"\n+#     from_one_df: bool = in_csv is not None\n+#     from_multiple_df: bool = (\n+#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n+#     )\n+#\n+#     if from_one_df == from_multiple_df:\n+#         raise ValueError(\n+#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n+#         )\n+#\n+#     if from_one_df:\n+#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n+#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n+#             dataframe,\n+#             train_folds=train_folds,\n+#             valid_folds=valid_folds,\n+#             infer_folds=infer_folds,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#             seed=seed,\n+#             n_folds=n_folds,\n+#         )\n+#     else:\n+#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n+#             in_csv_train=in_csv_train,\n+#             in_csv_valid=in_csv_valid,\n+#             in_csv_infer=in_csv_infer,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#         )\n+#\n+#     for data in [df_train, df_valid, df_infer]:\n+#         if data is not None and \"fold\" in data.columns:\n+#             del data[\"fold\"]\n+#\n+#     result = (\n+#         dataframe,\n+#         dataframe_to_list(df_train) if df_train is not None else None,\n+#         dataframe_to_list(df_valid) if df_valid is not None else None,\n+#         dataframe_to_list(df_infer) if df_infer is not None else None,\n+#     )\n+#\n+#     return result\n \n",
        "source_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\n<DED>def merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    <IND>\"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        <IND>for csv_path in paths.split(\",\"):\n            <IND>dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    <DED><DED>return result\n\n\n<DED>def read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    <IND>\"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        <IND>if fold_df is not None:\n            <IND>fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                <IND>fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            <DED>fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    <DED><DED>output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\n<DED>def read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    <IND>\"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        <IND>raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    <DED>if from_one_df:\n        <IND>dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    <DED>for data in [df_train, df_valid, df_infer]:\n        <IND>if data is not None and \"fold\" in data.columns:\n            <IND>del data[\"fold\"]\n\n    <DED><DED>result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/utils/pandas.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/utils/pandas.py",
    "file_hunks_size": 15,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/contrib/utils/pandas.py:475:4 Incompatible variable type [9]: in_csv_infer is declared to have type `str` but is used as type `None`.",
    "message": " in_csv_infer is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 475,
    "warning_line": "    in_csv_infer: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    if class_column is not None:\n        result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    else:\n        result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_len": 682,
        "target_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    if class_column is not None:\n        df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    else:\n        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_len": 562,
        "diff_format": "@@ -415,22 +394,15 @@\n     if args_are_not_none(tag2class, tag_column, class_column):\n-        dataframe = map_dataframe(\n-            dataframe, tag_column, class_column, tag2class\n-        )\n+        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n \n     if class_column is not None:\n-        result_dataframe = split_dataframe_on_stratified_folds(\n-            dataframe,\n-            class_column=class_column,\n-            random_state=seed,\n-            n_folds=n_folds,\n+        df_all = split_dataframe_on_stratified_folds(\n+            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n         )\n     else:\n-        result_dataframe = split_dataframe_on_folds(\n-            dataframe, random_state=seed, n_folds=n_folds\n-        )\n-\n-    fold_series = result_dataframe[\"fold\"]\n+        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n+\n+    fold_series = df_all[\"fold\"]\n \n     train_folds = folds_to_list(train_folds)\n-    df_train = result_dataframe[fold_series.isin(train_folds)]\n+    df_train = df_all[fold_series.isin(train_folds)]\n \n",
        "source_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    <DED>if class_column is not None:\n        <IND>result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    <DED>fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    <DED>if class_column is not None:\n        <IND>df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    <DED>fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\ndef merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    \"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        for csv_path in paths.split(\",\"):\n            dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    return result\n\n\ndef read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        if fold_df is not None:\n            fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\ndef read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    \"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    if from_one_df:\n        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    else:\n        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    for data in [df_train, df_valid, df_infer]:\n        if data is not None and \"fold\" in data.columns:\n            del data[\"fold\"]\n\n    result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_len": 6590,
        "target_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_len": 6715,
        "diff_format": "@@ -438,196 +410,183 @@\n         mask = ~fold_series.isin(train_folds)\n-        valid_folds = result_dataframe[mask][\"fold\"]\n+        valid_folds = df_all[mask][\"fold\"]\n \n     valid_folds = folds_to_list(valid_folds)\n-    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n+    df_valid = df_all[fold_series.isin(valid_folds)]\n \n     infer_folds = folds_to_list(infer_folds or [])\n-    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n-\n-    return result_dataframe, df_train, df_valid, df_infer\n-\n-\n-def merge_multiple_fold_csv(\n-    fold_name: str, paths: Optional[str]\n-) -> pd.DataFrame:\n-    \"\"\"Reads csv into one DataFrame with column ``fold``.\n-\n-    Args:\n-        fold_name: current fold name\n-        paths: paths to csv separated by commas\n-\n-    Returns:\n-         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n-    \"\"\"\n-    result = pd.DataFrame()\n-    if paths is not None:\n-        for csv_path in paths.split(\",\"):\n-            dataframe = pd.read_csv(csv_path)\n-            dataframe[\"fold\"] = fold_name\n-            result = result.append(dataframe, ignore_index=True)\n-\n-    return result\n-\n-\n-def read_multiple_dataframes(\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n-    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n-\n-    Args:\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-        tag2class (Dict[str, int], optional): mapping from label names into int\n-        tag_column (str, optional): column with label names\n-        class_column (str, optional): column to use for split\n-\n-    Returns:\n-        tuple: tuple with 4 dataframes\n-            whole dataframe, train part, valid part and infer part\n-    \"\"\"\n-    assert any(\n-        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n-    )\n-\n-    result_df = None\n-    fold_dfs = {}\n-    for fold_df, fold_name in zip(\n-        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n-    ):\n-        if fold_df is not None:\n-            fold_df = merge_multiple_fold_csv(\n-                fold_name=fold_name, paths=fold_df\n-            )\n-            if args_are_not_none(tag2class, tag_column, class_column):\n-                fold_df = map_dataframe(\n-                    fold_df, tag_column, class_column, tag2class\n-                )\n-            fold_dfs[fold_name] = fold_df\n-\n-            result_df = (\n-                fold_df\n-                if result_df is None\n-                else result_df.append(fold_df, ignore_index=True)\n-            )\n-\n-    output = (\n-        result_df,\n-        fold_dfs.get(\"train\", None),\n-        fold_dfs.get(\"valid\", None),\n-        fold_dfs.get(\"infer\", None),\n-    )\n-\n-    return output\n-\n-\n-def read_csv_data(\n-    in_csv: str = None,\n-    train_folds: Optional[List[int]] = None,\n-    valid_folds: Optional[List[int]] = None,\n-    infer_folds: Optional[List[int]] = None,\n-    seed: int = 42,\n-    n_folds: int = 5,\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-    \"\"\"\n-    From giving path ``in_csv`` reads a dataframe\n-    and split it to train/valid/infer folds\n-    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n-    reads independent folds.\n-\n-    .. note::\n-       This function can be used with different combinations of params.\n-        First block is used to get dataset from one `csv`:\n-            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n-        Second includes paths to different csv for train/valid and infer parts:\n-            in_csv_train, in_csv_valid, in_csv_infer\n-        The other params (tag2class, tag_column, class_column) are optional\n-            for any previous block\n-\n-    Args:\n-        in_csv: paths to whole dataset\n-        train_folds: train folds\n-        valid_folds (List[int], optional): valid folds.\n-            If none takes all folds not included in ``train_folds``\n-        infer_folds (List[int], optional): infer folds.\n-            If none takes all folds not included in ``train_folds``\n-            and ``valid_folds``\n-        seed: seed for split\n-        n_folds: number of folds\n-\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-\n-        tag2class (Dict[str, int]): mapping from label names into ints\n-        tag_column: column with label names\n-        class_column: column to use for split\n-\n-    Returns:\n-        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-            tuple with 4 elements\n-            (whole dataframe,\n-            list with train data,\n-            list with valid data\n-            and list with infer data)\n-    \"\"\"\n-    from_one_df: bool = in_csv is not None\n-    from_multiple_df: bool = (\n-        in_csv_train is not None\n-        or in_csv_valid is not None\n-        or in_csv_infer is not None\n-    )\n-\n-    if from_one_df == from_multiple_df:\n-        raise ValueError(\n-            \"You should pass `in_csv` \"\n-            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n-        )\n-\n-    if from_one_df:\n-        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n-        dataframe, df_train, df_valid, df_infer = split_dataframe(\n-            dataframe,\n-            train_folds=train_folds,\n-            valid_folds=valid_folds,\n-            infer_folds=infer_folds,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-            seed=seed,\n-            n_folds=n_folds,\n-        )\n-    else:\n-        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n-            in_csv_train=in_csv_train,\n-            in_csv_valid=in_csv_valid,\n-            in_csv_infer=in_csv_infer,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-        )\n-\n-    for data in [df_train, df_valid, df_infer]:\n-        if data is not None and \"fold\" in data.columns:\n-            del data[\"fold\"]\n-\n-    result = (\n-        dataframe,\n-        dataframe_to_list(df_train) if df_train is not None else None,\n-        dataframe_to_list(df_valid) if df_valid is not None else None,\n-        dataframe_to_list(df_infer) if df_infer is not None else None,\n-    )\n-\n-    return result\n+    df_infer = df_all[fold_series.isin(infer_folds)]\n+\n+    return df_all, df_train, df_valid, df_infer\n+\n+\n+# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n+#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n+#\n+#     Args:\n+#         fold_name: current fold name\n+#         paths: paths to csv separated by commas\n+#\n+#     Returns:\n+#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n+#     \"\"\"\n+#     result = pd.DataFrame()\n+#     if paths is not None:\n+#         for csv_path in paths.split(\",\"):\n+#             dataframe = pd.read_csv(csv_path)\n+#             dataframe[\"fold\"] = fold_name\n+#             result = result.append(dataframe, ignore_index=True)\n+#\n+#     return result\n+\n+\n+# def read_multiple_dataframes(\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n+#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n+#\n+#     Args:\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#         tag2class (Dict[str, int], optional): mapping from label names into int\n+#         tag_column (str, optional): column with label names\n+#         class_column (str, optional): column to use for split\n+#\n+#     Returns:\n+#         tuple: tuple with 4 dataframes\n+#             whole dataframe, train part, valid part and infer part\n+#     \"\"\"\n+#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n+#\n+#     result_df = None\n+#     fold_dfs = {}\n+#     for fold_df, fold_name in zip(\n+#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n+#     ):\n+#         if fold_df is not None:\n+#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n+#             if args_are_not_none(tag2class, tag_column, class_column):\n+#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n+#             fold_dfs[fold_name] = fold_df\n+#\n+#             result_df = (\n+#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n+#             )\n+#\n+#     output = (\n+#         result_df,\n+#         fold_dfs.get(\"train\", None),\n+#         fold_dfs.get(\"valid\", None),\n+#         fold_dfs.get(\"infer\", None),\n+#     )\n+#\n+#     return output\n+\n+\n+# def read_csv_data(\n+#     in_csv: str = None,\n+#     train_folds: Optional[List[int]] = None,\n+#     valid_folds: Optional[List[int]] = None,\n+#     infer_folds: Optional[List[int]] = None,\n+#     seed: int = 42,\n+#     n_folds: int = 5,\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#     \"\"\"\n+#     From giving path ``in_csv`` reads a dataframe\n+#     and split it to train/valid/infer folds\n+#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n+#     reads independent folds.\n+#\n+#     .. note::\n+#        This function can be used with different combinations of params.\n+#         First block is used to get dataset from one `csv`:\n+#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n+#         Second includes paths to different csv for train/valid and infer parts:\n+#             in_csv_train, in_csv_valid, in_csv_infer\n+#         The other params (tag2class, tag_column, class_column) are optional\n+#             for any previous block\n+#\n+#     Args:\n+#         in_csv: paths to whole dataset\n+#         train_folds: train folds\n+#         valid_folds (List[int], optional): valid folds.\n+#             If none takes all folds not included in ``train_folds``\n+#         infer_folds (List[int], optional): infer folds.\n+#             If none takes all folds not included in ``train_folds``\n+#             and ``valid_folds``\n+#         seed: seed for split\n+#         n_folds: number of folds\n+#\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#\n+#         tag2class (Dict[str, int]): mapping from label names into ints\n+#         tag_column: column with label names\n+#         class_column: column to use for split\n+#\n+#     Returns:\n+#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#             tuple with 4 elements\n+#             (whole dataframe,\n+#             list with train data,\n+#             list with valid data\n+#             and list with infer data)\n+#     \"\"\"\n+#     from_one_df: bool = in_csv is not None\n+#     from_multiple_df: bool = (\n+#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n+#     )\n+#\n+#     if from_one_df == from_multiple_df:\n+#         raise ValueError(\n+#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n+#         )\n+#\n+#     if from_one_df:\n+#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n+#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n+#             dataframe,\n+#             train_folds=train_folds,\n+#             valid_folds=valid_folds,\n+#             infer_folds=infer_folds,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#             seed=seed,\n+#             n_folds=n_folds,\n+#         )\n+#     else:\n+#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n+#             in_csv_train=in_csv_train,\n+#             in_csv_valid=in_csv_valid,\n+#             in_csv_infer=in_csv_infer,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#         )\n+#\n+#     for data in [df_train, df_valid, df_infer]:\n+#         if data is not None and \"fold\" in data.columns:\n+#             del data[\"fold\"]\n+#\n+#     result = (\n+#         dataframe,\n+#         dataframe_to_list(df_train) if df_train is not None else None,\n+#         dataframe_to_list(df_valid) if df_valid is not None else None,\n+#         dataframe_to_list(df_infer) if df_infer is not None else None,\n+#     )\n+#\n+#     return result\n \n",
        "source_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\n<DED>def merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    <IND>\"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        <IND>for csv_path in paths.split(\",\"):\n            <IND>dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    <DED><DED>return result\n\n\n<DED>def read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    <IND>\"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        <IND>if fold_df is not None:\n            <IND>fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                <IND>fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            <DED>fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    <DED><DED>output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\n<DED>def read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    <IND>\"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        <IND>raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    <DED>if from_one_df:\n        <IND>dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    <DED>for data in [df_train, df_valid, df_infer]:\n        <IND>if data is not None and \"fold\" in data.columns:\n            <IND>del data[\"fold\"]\n\n    <DED><DED>result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/utils/pandas.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/utils/pandas.py",
    "file_hunks_size": 15,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/contrib/utils/pandas.py:477:4 Incompatible variable type [9]: class_column is declared to have type `str` but is used as type `None`.",
    "message": " class_column is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 477,
    "warning_line": "    class_column: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    if class_column is not None:\n        result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    else:\n        result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_len": 682,
        "target_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    if class_column is not None:\n        df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    else:\n        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_len": 562,
        "diff_format": "@@ -415,22 +394,15 @@\n     if args_are_not_none(tag2class, tag_column, class_column):\n-        dataframe = map_dataframe(\n-            dataframe, tag_column, class_column, tag2class\n-        )\n+        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n \n     if class_column is not None:\n-        result_dataframe = split_dataframe_on_stratified_folds(\n-            dataframe,\n-            class_column=class_column,\n-            random_state=seed,\n-            n_folds=n_folds,\n+        df_all = split_dataframe_on_stratified_folds(\n+            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n         )\n     else:\n-        result_dataframe = split_dataframe_on_folds(\n-            dataframe, random_state=seed, n_folds=n_folds\n-        )\n-\n-    fold_series = result_dataframe[\"fold\"]\n+        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n+\n+    fold_series = df_all[\"fold\"]\n \n     train_folds = folds_to_list(train_folds)\n-    df_train = result_dataframe[fold_series.isin(train_folds)]\n+    df_train = df_all[fold_series.isin(train_folds)]\n \n",
        "source_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    <DED>if class_column is not None:\n        <IND>result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    <DED>fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    <DED>if class_column is not None:\n        <IND>df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    <DED>fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\ndef merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    \"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        for csv_path in paths.split(\",\"):\n            dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    return result\n\n\ndef read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        if fold_df is not None:\n            fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\ndef read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    \"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    if from_one_df:\n        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    else:\n        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    for data in [df_train, df_valid, df_infer]:\n        if data is not None and \"fold\" in data.columns:\n            del data[\"fold\"]\n\n    result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_len": 6590,
        "target_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_len": 6715,
        "diff_format": "@@ -438,196 +410,183 @@\n         mask = ~fold_series.isin(train_folds)\n-        valid_folds = result_dataframe[mask][\"fold\"]\n+        valid_folds = df_all[mask][\"fold\"]\n \n     valid_folds = folds_to_list(valid_folds)\n-    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n+    df_valid = df_all[fold_series.isin(valid_folds)]\n \n     infer_folds = folds_to_list(infer_folds or [])\n-    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n-\n-    return result_dataframe, df_train, df_valid, df_infer\n-\n-\n-def merge_multiple_fold_csv(\n-    fold_name: str, paths: Optional[str]\n-) -> pd.DataFrame:\n-    \"\"\"Reads csv into one DataFrame with column ``fold``.\n-\n-    Args:\n-        fold_name: current fold name\n-        paths: paths to csv separated by commas\n-\n-    Returns:\n-         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n-    \"\"\"\n-    result = pd.DataFrame()\n-    if paths is not None:\n-        for csv_path in paths.split(\",\"):\n-            dataframe = pd.read_csv(csv_path)\n-            dataframe[\"fold\"] = fold_name\n-            result = result.append(dataframe, ignore_index=True)\n-\n-    return result\n-\n-\n-def read_multiple_dataframes(\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n-    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n-\n-    Args:\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-        tag2class (Dict[str, int], optional): mapping from label names into int\n-        tag_column (str, optional): column with label names\n-        class_column (str, optional): column to use for split\n-\n-    Returns:\n-        tuple: tuple with 4 dataframes\n-            whole dataframe, train part, valid part and infer part\n-    \"\"\"\n-    assert any(\n-        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n-    )\n-\n-    result_df = None\n-    fold_dfs = {}\n-    for fold_df, fold_name in zip(\n-        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n-    ):\n-        if fold_df is not None:\n-            fold_df = merge_multiple_fold_csv(\n-                fold_name=fold_name, paths=fold_df\n-            )\n-            if args_are_not_none(tag2class, tag_column, class_column):\n-                fold_df = map_dataframe(\n-                    fold_df, tag_column, class_column, tag2class\n-                )\n-            fold_dfs[fold_name] = fold_df\n-\n-            result_df = (\n-                fold_df\n-                if result_df is None\n-                else result_df.append(fold_df, ignore_index=True)\n-            )\n-\n-    output = (\n-        result_df,\n-        fold_dfs.get(\"train\", None),\n-        fold_dfs.get(\"valid\", None),\n-        fold_dfs.get(\"infer\", None),\n-    )\n-\n-    return output\n-\n-\n-def read_csv_data(\n-    in_csv: str = None,\n-    train_folds: Optional[List[int]] = None,\n-    valid_folds: Optional[List[int]] = None,\n-    infer_folds: Optional[List[int]] = None,\n-    seed: int = 42,\n-    n_folds: int = 5,\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-    \"\"\"\n-    From giving path ``in_csv`` reads a dataframe\n-    and split it to train/valid/infer folds\n-    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n-    reads independent folds.\n-\n-    .. note::\n-       This function can be used with different combinations of params.\n-        First block is used to get dataset from one `csv`:\n-            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n-        Second includes paths to different csv for train/valid and infer parts:\n-            in_csv_train, in_csv_valid, in_csv_infer\n-        The other params (tag2class, tag_column, class_column) are optional\n-            for any previous block\n-\n-    Args:\n-        in_csv: paths to whole dataset\n-        train_folds: train folds\n-        valid_folds (List[int], optional): valid folds.\n-            If none takes all folds not included in ``train_folds``\n-        infer_folds (List[int], optional): infer folds.\n-            If none takes all folds not included in ``train_folds``\n-            and ``valid_folds``\n-        seed: seed for split\n-        n_folds: number of folds\n-\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-\n-        tag2class (Dict[str, int]): mapping from label names into ints\n-        tag_column: column with label names\n-        class_column: column to use for split\n-\n-    Returns:\n-        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-            tuple with 4 elements\n-            (whole dataframe,\n-            list with train data,\n-            list with valid data\n-            and list with infer data)\n-    \"\"\"\n-    from_one_df: bool = in_csv is not None\n-    from_multiple_df: bool = (\n-        in_csv_train is not None\n-        or in_csv_valid is not None\n-        or in_csv_infer is not None\n-    )\n-\n-    if from_one_df == from_multiple_df:\n-        raise ValueError(\n-            \"You should pass `in_csv` \"\n-            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n-        )\n-\n-    if from_one_df:\n-        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n-        dataframe, df_train, df_valid, df_infer = split_dataframe(\n-            dataframe,\n-            train_folds=train_folds,\n-            valid_folds=valid_folds,\n-            infer_folds=infer_folds,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-            seed=seed,\n-            n_folds=n_folds,\n-        )\n-    else:\n-        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n-            in_csv_train=in_csv_train,\n-            in_csv_valid=in_csv_valid,\n-            in_csv_infer=in_csv_infer,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-        )\n-\n-    for data in [df_train, df_valid, df_infer]:\n-        if data is not None and \"fold\" in data.columns:\n-            del data[\"fold\"]\n-\n-    result = (\n-        dataframe,\n-        dataframe_to_list(df_train) if df_train is not None else None,\n-        dataframe_to_list(df_valid) if df_valid is not None else None,\n-        dataframe_to_list(df_infer) if df_infer is not None else None,\n-    )\n-\n-    return result\n+    df_infer = df_all[fold_series.isin(infer_folds)]\n+\n+    return df_all, df_train, df_valid, df_infer\n+\n+\n+# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n+#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n+#\n+#     Args:\n+#         fold_name: current fold name\n+#         paths: paths to csv separated by commas\n+#\n+#     Returns:\n+#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n+#     \"\"\"\n+#     result = pd.DataFrame()\n+#     if paths is not None:\n+#         for csv_path in paths.split(\",\"):\n+#             dataframe = pd.read_csv(csv_path)\n+#             dataframe[\"fold\"] = fold_name\n+#             result = result.append(dataframe, ignore_index=True)\n+#\n+#     return result\n+\n+\n+# def read_multiple_dataframes(\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n+#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n+#\n+#     Args:\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#         tag2class (Dict[str, int], optional): mapping from label names into int\n+#         tag_column (str, optional): column with label names\n+#         class_column (str, optional): column to use for split\n+#\n+#     Returns:\n+#         tuple: tuple with 4 dataframes\n+#             whole dataframe, train part, valid part and infer part\n+#     \"\"\"\n+#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n+#\n+#     result_df = None\n+#     fold_dfs = {}\n+#     for fold_df, fold_name in zip(\n+#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n+#     ):\n+#         if fold_df is not None:\n+#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n+#             if args_are_not_none(tag2class, tag_column, class_column):\n+#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n+#             fold_dfs[fold_name] = fold_df\n+#\n+#             result_df = (\n+#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n+#             )\n+#\n+#     output = (\n+#         result_df,\n+#         fold_dfs.get(\"train\", None),\n+#         fold_dfs.get(\"valid\", None),\n+#         fold_dfs.get(\"infer\", None),\n+#     )\n+#\n+#     return output\n+\n+\n+# def read_csv_data(\n+#     in_csv: str = None,\n+#     train_folds: Optional[List[int]] = None,\n+#     valid_folds: Optional[List[int]] = None,\n+#     infer_folds: Optional[List[int]] = None,\n+#     seed: int = 42,\n+#     n_folds: int = 5,\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#     \"\"\"\n+#     From giving path ``in_csv`` reads a dataframe\n+#     and split it to train/valid/infer folds\n+#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n+#     reads independent folds.\n+#\n+#     .. note::\n+#        This function can be used with different combinations of params.\n+#         First block is used to get dataset from one `csv`:\n+#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n+#         Second includes paths to different csv for train/valid and infer parts:\n+#             in_csv_train, in_csv_valid, in_csv_infer\n+#         The other params (tag2class, tag_column, class_column) are optional\n+#             for any previous block\n+#\n+#     Args:\n+#         in_csv: paths to whole dataset\n+#         train_folds: train folds\n+#         valid_folds (List[int], optional): valid folds.\n+#             If none takes all folds not included in ``train_folds``\n+#         infer_folds (List[int], optional): infer folds.\n+#             If none takes all folds not included in ``train_folds``\n+#             and ``valid_folds``\n+#         seed: seed for split\n+#         n_folds: number of folds\n+#\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#\n+#         tag2class (Dict[str, int]): mapping from label names into ints\n+#         tag_column: column with label names\n+#         class_column: column to use for split\n+#\n+#     Returns:\n+#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#             tuple with 4 elements\n+#             (whole dataframe,\n+#             list with train data,\n+#             list with valid data\n+#             and list with infer data)\n+#     \"\"\"\n+#     from_one_df: bool = in_csv is not None\n+#     from_multiple_df: bool = (\n+#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n+#     )\n+#\n+#     if from_one_df == from_multiple_df:\n+#         raise ValueError(\n+#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n+#         )\n+#\n+#     if from_one_df:\n+#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n+#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n+#             dataframe,\n+#             train_folds=train_folds,\n+#             valid_folds=valid_folds,\n+#             infer_folds=infer_folds,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#             seed=seed,\n+#             n_folds=n_folds,\n+#         )\n+#     else:\n+#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n+#             in_csv_train=in_csv_train,\n+#             in_csv_valid=in_csv_valid,\n+#             in_csv_infer=in_csv_infer,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#         )\n+#\n+#     for data in [df_train, df_valid, df_infer]:\n+#         if data is not None and \"fold\" in data.columns:\n+#             del data[\"fold\"]\n+#\n+#     result = (\n+#         dataframe,\n+#         dataframe_to_list(df_train) if df_train is not None else None,\n+#         dataframe_to_list(df_valid) if df_valid is not None else None,\n+#         dataframe_to_list(df_infer) if df_infer is not None else None,\n+#     )\n+#\n+#     return result\n \n",
        "source_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\n<DED>def merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    <IND>\"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        <IND>for csv_path in paths.split(\",\"):\n            <IND>dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    <DED><DED>return result\n\n\n<DED>def read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    <IND>\"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        <IND>if fold_df is not None:\n            <IND>fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                <IND>fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            <DED>fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    <DED><DED>output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\n<DED>def read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    <IND>\"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        <IND>raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    <DED>if from_one_df:\n        <IND>dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    <DED>for data in [df_train, df_valid, df_infer]:\n        <IND>if data is not None and \"fold\" in data.columns:\n            <IND>del data[\"fold\"]\n\n    <DED><DED>result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/utils/pandas.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/utils/pandas.py",
    "file_hunks_size": 15,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/contrib/utils/pandas.py:478:4 Incompatible variable type [9]: tag_column is declared to have type `str` but is used as type `None`.",
    "message": " tag_column is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 478,
    "warning_line": "    tag_column: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    if class_column is not None:\n        result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    else:\n        result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_len": 682,
        "target_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    if class_column is not None:\n        df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    else:\n        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_len": 562,
        "diff_format": "@@ -415,22 +394,15 @@\n     if args_are_not_none(tag2class, tag_column, class_column):\n-        dataframe = map_dataframe(\n-            dataframe, tag_column, class_column, tag2class\n-        )\n+        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n \n     if class_column is not None:\n-        result_dataframe = split_dataframe_on_stratified_folds(\n-            dataframe,\n-            class_column=class_column,\n-            random_state=seed,\n-            n_folds=n_folds,\n+        df_all = split_dataframe_on_stratified_folds(\n+            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n         )\n     else:\n-        result_dataframe = split_dataframe_on_folds(\n-            dataframe, random_state=seed, n_folds=n_folds\n-        )\n-\n-    fold_series = result_dataframe[\"fold\"]\n+        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n+\n+    fold_series = df_all[\"fold\"]\n \n     train_folds = folds_to_list(train_folds)\n-    df_train = result_dataframe[fold_series.isin(train_folds)]\n+    df_train = df_all[fold_series.isin(train_folds)]\n \n",
        "source_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    <DED>if class_column is not None:\n        <IND>result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    <DED>fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    <DED>if class_column is not None:\n        <IND>df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    <DED>fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\ndef merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    \"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        for csv_path in paths.split(\",\"):\n            dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    return result\n\n\ndef read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        if fold_df is not None:\n            fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\ndef read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    \"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    if from_one_df:\n        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    else:\n        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    for data in [df_train, df_valid, df_infer]:\n        if data is not None and \"fold\" in data.columns:\n            del data[\"fold\"]\n\n    result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_len": 6590,
        "target_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_len": 6715,
        "diff_format": "@@ -438,196 +410,183 @@\n         mask = ~fold_series.isin(train_folds)\n-        valid_folds = result_dataframe[mask][\"fold\"]\n+        valid_folds = df_all[mask][\"fold\"]\n \n     valid_folds = folds_to_list(valid_folds)\n-    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n+    df_valid = df_all[fold_series.isin(valid_folds)]\n \n     infer_folds = folds_to_list(infer_folds or [])\n-    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n-\n-    return result_dataframe, df_train, df_valid, df_infer\n-\n-\n-def merge_multiple_fold_csv(\n-    fold_name: str, paths: Optional[str]\n-) -> pd.DataFrame:\n-    \"\"\"Reads csv into one DataFrame with column ``fold``.\n-\n-    Args:\n-        fold_name: current fold name\n-        paths: paths to csv separated by commas\n-\n-    Returns:\n-         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n-    \"\"\"\n-    result = pd.DataFrame()\n-    if paths is not None:\n-        for csv_path in paths.split(\",\"):\n-            dataframe = pd.read_csv(csv_path)\n-            dataframe[\"fold\"] = fold_name\n-            result = result.append(dataframe, ignore_index=True)\n-\n-    return result\n-\n-\n-def read_multiple_dataframes(\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n-    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n-\n-    Args:\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-        tag2class (Dict[str, int], optional): mapping from label names into int\n-        tag_column (str, optional): column with label names\n-        class_column (str, optional): column to use for split\n-\n-    Returns:\n-        tuple: tuple with 4 dataframes\n-            whole dataframe, train part, valid part and infer part\n-    \"\"\"\n-    assert any(\n-        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n-    )\n-\n-    result_df = None\n-    fold_dfs = {}\n-    for fold_df, fold_name in zip(\n-        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n-    ):\n-        if fold_df is not None:\n-            fold_df = merge_multiple_fold_csv(\n-                fold_name=fold_name, paths=fold_df\n-            )\n-            if args_are_not_none(tag2class, tag_column, class_column):\n-                fold_df = map_dataframe(\n-                    fold_df, tag_column, class_column, tag2class\n-                )\n-            fold_dfs[fold_name] = fold_df\n-\n-            result_df = (\n-                fold_df\n-                if result_df is None\n-                else result_df.append(fold_df, ignore_index=True)\n-            )\n-\n-    output = (\n-        result_df,\n-        fold_dfs.get(\"train\", None),\n-        fold_dfs.get(\"valid\", None),\n-        fold_dfs.get(\"infer\", None),\n-    )\n-\n-    return output\n-\n-\n-def read_csv_data(\n-    in_csv: str = None,\n-    train_folds: Optional[List[int]] = None,\n-    valid_folds: Optional[List[int]] = None,\n-    infer_folds: Optional[List[int]] = None,\n-    seed: int = 42,\n-    n_folds: int = 5,\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-    \"\"\"\n-    From giving path ``in_csv`` reads a dataframe\n-    and split it to train/valid/infer folds\n-    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n-    reads independent folds.\n-\n-    .. note::\n-       This function can be used with different combinations of params.\n-        First block is used to get dataset from one `csv`:\n-            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n-        Second includes paths to different csv for train/valid and infer parts:\n-            in_csv_train, in_csv_valid, in_csv_infer\n-        The other params (tag2class, tag_column, class_column) are optional\n-            for any previous block\n-\n-    Args:\n-        in_csv: paths to whole dataset\n-        train_folds: train folds\n-        valid_folds (List[int], optional): valid folds.\n-            If none takes all folds not included in ``train_folds``\n-        infer_folds (List[int], optional): infer folds.\n-            If none takes all folds not included in ``train_folds``\n-            and ``valid_folds``\n-        seed: seed for split\n-        n_folds: number of folds\n-\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-\n-        tag2class (Dict[str, int]): mapping from label names into ints\n-        tag_column: column with label names\n-        class_column: column to use for split\n-\n-    Returns:\n-        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-            tuple with 4 elements\n-            (whole dataframe,\n-            list with train data,\n-            list with valid data\n-            and list with infer data)\n-    \"\"\"\n-    from_one_df: bool = in_csv is not None\n-    from_multiple_df: bool = (\n-        in_csv_train is not None\n-        or in_csv_valid is not None\n-        or in_csv_infer is not None\n-    )\n-\n-    if from_one_df == from_multiple_df:\n-        raise ValueError(\n-            \"You should pass `in_csv` \"\n-            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n-        )\n-\n-    if from_one_df:\n-        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n-        dataframe, df_train, df_valid, df_infer = split_dataframe(\n-            dataframe,\n-            train_folds=train_folds,\n-            valid_folds=valid_folds,\n-            infer_folds=infer_folds,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-            seed=seed,\n-            n_folds=n_folds,\n-        )\n-    else:\n-        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n-            in_csv_train=in_csv_train,\n-            in_csv_valid=in_csv_valid,\n-            in_csv_infer=in_csv_infer,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-        )\n-\n-    for data in [df_train, df_valid, df_infer]:\n-        if data is not None and \"fold\" in data.columns:\n-            del data[\"fold\"]\n-\n-    result = (\n-        dataframe,\n-        dataframe_to_list(df_train) if df_train is not None else None,\n-        dataframe_to_list(df_valid) if df_valid is not None else None,\n-        dataframe_to_list(df_infer) if df_infer is not None else None,\n-    )\n-\n-    return result\n+    df_infer = df_all[fold_series.isin(infer_folds)]\n+\n+    return df_all, df_train, df_valid, df_infer\n+\n+\n+# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n+#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n+#\n+#     Args:\n+#         fold_name: current fold name\n+#         paths: paths to csv separated by commas\n+#\n+#     Returns:\n+#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n+#     \"\"\"\n+#     result = pd.DataFrame()\n+#     if paths is not None:\n+#         for csv_path in paths.split(\",\"):\n+#             dataframe = pd.read_csv(csv_path)\n+#             dataframe[\"fold\"] = fold_name\n+#             result = result.append(dataframe, ignore_index=True)\n+#\n+#     return result\n+\n+\n+# def read_multiple_dataframes(\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n+#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n+#\n+#     Args:\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#         tag2class (Dict[str, int], optional): mapping from label names into int\n+#         tag_column (str, optional): column with label names\n+#         class_column (str, optional): column to use for split\n+#\n+#     Returns:\n+#         tuple: tuple with 4 dataframes\n+#             whole dataframe, train part, valid part and infer part\n+#     \"\"\"\n+#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n+#\n+#     result_df = None\n+#     fold_dfs = {}\n+#     for fold_df, fold_name in zip(\n+#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n+#     ):\n+#         if fold_df is not None:\n+#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n+#             if args_are_not_none(tag2class, tag_column, class_column):\n+#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n+#             fold_dfs[fold_name] = fold_df\n+#\n+#             result_df = (\n+#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n+#             )\n+#\n+#     output = (\n+#         result_df,\n+#         fold_dfs.get(\"train\", None),\n+#         fold_dfs.get(\"valid\", None),\n+#         fold_dfs.get(\"infer\", None),\n+#     )\n+#\n+#     return output\n+\n+\n+# def read_csv_data(\n+#     in_csv: str = None,\n+#     train_folds: Optional[List[int]] = None,\n+#     valid_folds: Optional[List[int]] = None,\n+#     infer_folds: Optional[List[int]] = None,\n+#     seed: int = 42,\n+#     n_folds: int = 5,\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#     \"\"\"\n+#     From giving path ``in_csv`` reads a dataframe\n+#     and split it to train/valid/infer folds\n+#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n+#     reads independent folds.\n+#\n+#     .. note::\n+#        This function can be used with different combinations of params.\n+#         First block is used to get dataset from one `csv`:\n+#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n+#         Second includes paths to different csv for train/valid and infer parts:\n+#             in_csv_train, in_csv_valid, in_csv_infer\n+#         The other params (tag2class, tag_column, class_column) are optional\n+#             for any previous block\n+#\n+#     Args:\n+#         in_csv: paths to whole dataset\n+#         train_folds: train folds\n+#         valid_folds (List[int], optional): valid folds.\n+#             If none takes all folds not included in ``train_folds``\n+#         infer_folds (List[int], optional): infer folds.\n+#             If none takes all folds not included in ``train_folds``\n+#             and ``valid_folds``\n+#         seed: seed for split\n+#         n_folds: number of folds\n+#\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#\n+#         tag2class (Dict[str, int]): mapping from label names into ints\n+#         tag_column: column with label names\n+#         class_column: column to use for split\n+#\n+#     Returns:\n+#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#             tuple with 4 elements\n+#             (whole dataframe,\n+#             list with train data,\n+#             list with valid data\n+#             and list with infer data)\n+#     \"\"\"\n+#     from_one_df: bool = in_csv is not None\n+#     from_multiple_df: bool = (\n+#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n+#     )\n+#\n+#     if from_one_df == from_multiple_df:\n+#         raise ValueError(\n+#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n+#         )\n+#\n+#     if from_one_df:\n+#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n+#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n+#             dataframe,\n+#             train_folds=train_folds,\n+#             valid_folds=valid_folds,\n+#             infer_folds=infer_folds,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#             seed=seed,\n+#             n_folds=n_folds,\n+#         )\n+#     else:\n+#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n+#             in_csv_train=in_csv_train,\n+#             in_csv_valid=in_csv_valid,\n+#             in_csv_infer=in_csv_infer,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#         )\n+#\n+#     for data in [df_train, df_valid, df_infer]:\n+#         if data is not None and \"fold\" in data.columns:\n+#             del data[\"fold\"]\n+#\n+#     result = (\n+#         dataframe,\n+#         dataframe_to_list(df_train) if df_train is not None else None,\n+#         dataframe_to_list(df_valid) if df_valid is not None else None,\n+#         dataframe_to_list(df_infer) if df_infer is not None else None,\n+#     )\n+#\n+#     return result\n \n",
        "source_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\n<DED>def merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    <IND>\"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        <IND>for csv_path in paths.split(\",\"):\n            <IND>dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    <DED><DED>return result\n\n\n<DED>def read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    <IND>\"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        <IND>if fold_df is not None:\n            <IND>fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                <IND>fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            <DED>fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    <DED><DED>output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\n<DED>def read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    <IND>\"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        <IND>raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    <DED>if from_one_df:\n        <IND>dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    <DED>for data in [df_train, df_valid, df_infer]:\n        <IND>if data is not None and \"fold\" in data.columns:\n            <IND>del data[\"fold\"]\n\n    <DED><DED>result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/utils/pandas.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/utils/pandas.py",
    "file_hunks_size": 15,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/contrib/utils/pandas.py:509:55 Incompatible parameter type [6]: Expected `Dict[str, int]` for 4th positional only parameter to call `map_dataframe` but got `Optional[Dict[str, int]]`.",
    "message": " Expected `Dict[str, int]` for 4th positional only parameter to call `map_dataframe` but got `Optional[Dict[str, int]]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 509,
    "warning_line": "                    fold_df, tag_column, class_column, tag2class",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    if class_column is not None:\n        result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    else:\n        result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_len": 682,
        "target_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    if class_column is not None:\n        df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    else:\n        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_len": 562,
        "diff_format": "@@ -415,22 +394,15 @@\n     if args_are_not_none(tag2class, tag_column, class_column):\n-        dataframe = map_dataframe(\n-            dataframe, tag_column, class_column, tag2class\n-        )\n+        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n \n     if class_column is not None:\n-        result_dataframe = split_dataframe_on_stratified_folds(\n-            dataframe,\n-            class_column=class_column,\n-            random_state=seed,\n-            n_folds=n_folds,\n+        df_all = split_dataframe_on_stratified_folds(\n+            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n         )\n     else:\n-        result_dataframe = split_dataframe_on_folds(\n-            dataframe, random_state=seed, n_folds=n_folds\n-        )\n-\n-    fold_series = result_dataframe[\"fold\"]\n+        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n+\n+    fold_series = df_all[\"fold\"]\n \n     train_folds = folds_to_list(train_folds)\n-    df_train = result_dataframe[fold_series.isin(train_folds)]\n+    df_train = df_all[fold_series.isin(train_folds)]\n \n",
        "source_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    <DED>if class_column is not None:\n        <IND>result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    <DED>fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    <DED>if class_column is not None:\n        <IND>df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    <DED>fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\ndef merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    \"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        for csv_path in paths.split(\",\"):\n            dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    return result\n\n\ndef read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        if fold_df is not None:\n            fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\ndef read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    \"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    if from_one_df:\n        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    else:\n        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    for data in [df_train, df_valid, df_infer]:\n        if data is not None and \"fold\" in data.columns:\n            del data[\"fold\"]\n\n    result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_len": 6590,
        "target_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_len": 6715,
        "diff_format": "@@ -438,196 +410,183 @@\n         mask = ~fold_series.isin(train_folds)\n-        valid_folds = result_dataframe[mask][\"fold\"]\n+        valid_folds = df_all[mask][\"fold\"]\n \n     valid_folds = folds_to_list(valid_folds)\n-    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n+    df_valid = df_all[fold_series.isin(valid_folds)]\n \n     infer_folds = folds_to_list(infer_folds or [])\n-    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n-\n-    return result_dataframe, df_train, df_valid, df_infer\n-\n-\n-def merge_multiple_fold_csv(\n-    fold_name: str, paths: Optional[str]\n-) -> pd.DataFrame:\n-    \"\"\"Reads csv into one DataFrame with column ``fold``.\n-\n-    Args:\n-        fold_name: current fold name\n-        paths: paths to csv separated by commas\n-\n-    Returns:\n-         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n-    \"\"\"\n-    result = pd.DataFrame()\n-    if paths is not None:\n-        for csv_path in paths.split(\",\"):\n-            dataframe = pd.read_csv(csv_path)\n-            dataframe[\"fold\"] = fold_name\n-            result = result.append(dataframe, ignore_index=True)\n-\n-    return result\n-\n-\n-def read_multiple_dataframes(\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n-    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n-\n-    Args:\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-        tag2class (Dict[str, int], optional): mapping from label names into int\n-        tag_column (str, optional): column with label names\n-        class_column (str, optional): column to use for split\n-\n-    Returns:\n-        tuple: tuple with 4 dataframes\n-            whole dataframe, train part, valid part and infer part\n-    \"\"\"\n-    assert any(\n-        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n-    )\n-\n-    result_df = None\n-    fold_dfs = {}\n-    for fold_df, fold_name in zip(\n-        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n-    ):\n-        if fold_df is not None:\n-            fold_df = merge_multiple_fold_csv(\n-                fold_name=fold_name, paths=fold_df\n-            )\n-            if args_are_not_none(tag2class, tag_column, class_column):\n-                fold_df = map_dataframe(\n-                    fold_df, tag_column, class_column, tag2class\n-                )\n-            fold_dfs[fold_name] = fold_df\n-\n-            result_df = (\n-                fold_df\n-                if result_df is None\n-                else result_df.append(fold_df, ignore_index=True)\n-            )\n-\n-    output = (\n-        result_df,\n-        fold_dfs.get(\"train\", None),\n-        fold_dfs.get(\"valid\", None),\n-        fold_dfs.get(\"infer\", None),\n-    )\n-\n-    return output\n-\n-\n-def read_csv_data(\n-    in_csv: str = None,\n-    train_folds: Optional[List[int]] = None,\n-    valid_folds: Optional[List[int]] = None,\n-    infer_folds: Optional[List[int]] = None,\n-    seed: int = 42,\n-    n_folds: int = 5,\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-    \"\"\"\n-    From giving path ``in_csv`` reads a dataframe\n-    and split it to train/valid/infer folds\n-    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n-    reads independent folds.\n-\n-    .. note::\n-       This function can be used with different combinations of params.\n-        First block is used to get dataset from one `csv`:\n-            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n-        Second includes paths to different csv for train/valid and infer parts:\n-            in_csv_train, in_csv_valid, in_csv_infer\n-        The other params (tag2class, tag_column, class_column) are optional\n-            for any previous block\n-\n-    Args:\n-        in_csv: paths to whole dataset\n-        train_folds: train folds\n-        valid_folds (List[int], optional): valid folds.\n-            If none takes all folds not included in ``train_folds``\n-        infer_folds (List[int], optional): infer folds.\n-            If none takes all folds not included in ``train_folds``\n-            and ``valid_folds``\n-        seed: seed for split\n-        n_folds: number of folds\n-\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-\n-        tag2class (Dict[str, int]): mapping from label names into ints\n-        tag_column: column with label names\n-        class_column: column to use for split\n-\n-    Returns:\n-        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-            tuple with 4 elements\n-            (whole dataframe,\n-            list with train data,\n-            list with valid data\n-            and list with infer data)\n-    \"\"\"\n-    from_one_df: bool = in_csv is not None\n-    from_multiple_df: bool = (\n-        in_csv_train is not None\n-        or in_csv_valid is not None\n-        or in_csv_infer is not None\n-    )\n-\n-    if from_one_df == from_multiple_df:\n-        raise ValueError(\n-            \"You should pass `in_csv` \"\n-            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n-        )\n-\n-    if from_one_df:\n-        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n-        dataframe, df_train, df_valid, df_infer = split_dataframe(\n-            dataframe,\n-            train_folds=train_folds,\n-            valid_folds=valid_folds,\n-            infer_folds=infer_folds,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-            seed=seed,\n-            n_folds=n_folds,\n-        )\n-    else:\n-        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n-            in_csv_train=in_csv_train,\n-            in_csv_valid=in_csv_valid,\n-            in_csv_infer=in_csv_infer,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-        )\n-\n-    for data in [df_train, df_valid, df_infer]:\n-        if data is not None and \"fold\" in data.columns:\n-            del data[\"fold\"]\n-\n-    result = (\n-        dataframe,\n-        dataframe_to_list(df_train) if df_train is not None else None,\n-        dataframe_to_list(df_valid) if df_valid is not None else None,\n-        dataframe_to_list(df_infer) if df_infer is not None else None,\n-    )\n-\n-    return result\n+    df_infer = df_all[fold_series.isin(infer_folds)]\n+\n+    return df_all, df_train, df_valid, df_infer\n+\n+\n+# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n+#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n+#\n+#     Args:\n+#         fold_name: current fold name\n+#         paths: paths to csv separated by commas\n+#\n+#     Returns:\n+#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n+#     \"\"\"\n+#     result = pd.DataFrame()\n+#     if paths is not None:\n+#         for csv_path in paths.split(\",\"):\n+#             dataframe = pd.read_csv(csv_path)\n+#             dataframe[\"fold\"] = fold_name\n+#             result = result.append(dataframe, ignore_index=True)\n+#\n+#     return result\n+\n+\n+# def read_multiple_dataframes(\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n+#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n+#\n+#     Args:\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#         tag2class (Dict[str, int], optional): mapping from label names into int\n+#         tag_column (str, optional): column with label names\n+#         class_column (str, optional): column to use for split\n+#\n+#     Returns:\n+#         tuple: tuple with 4 dataframes\n+#             whole dataframe, train part, valid part and infer part\n+#     \"\"\"\n+#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n+#\n+#     result_df = None\n+#     fold_dfs = {}\n+#     for fold_df, fold_name in zip(\n+#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n+#     ):\n+#         if fold_df is not None:\n+#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n+#             if args_are_not_none(tag2class, tag_column, class_column):\n+#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n+#             fold_dfs[fold_name] = fold_df\n+#\n+#             result_df = (\n+#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n+#             )\n+#\n+#     output = (\n+#         result_df,\n+#         fold_dfs.get(\"train\", None),\n+#         fold_dfs.get(\"valid\", None),\n+#         fold_dfs.get(\"infer\", None),\n+#     )\n+#\n+#     return output\n+\n+\n+# def read_csv_data(\n+#     in_csv: str = None,\n+#     train_folds: Optional[List[int]] = None,\n+#     valid_folds: Optional[List[int]] = None,\n+#     infer_folds: Optional[List[int]] = None,\n+#     seed: int = 42,\n+#     n_folds: int = 5,\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#     \"\"\"\n+#     From giving path ``in_csv`` reads a dataframe\n+#     and split it to train/valid/infer folds\n+#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n+#     reads independent folds.\n+#\n+#     .. note::\n+#        This function can be used with different combinations of params.\n+#         First block is used to get dataset from one `csv`:\n+#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n+#         Second includes paths to different csv for train/valid and infer parts:\n+#             in_csv_train, in_csv_valid, in_csv_infer\n+#         The other params (tag2class, tag_column, class_column) are optional\n+#             for any previous block\n+#\n+#     Args:\n+#         in_csv: paths to whole dataset\n+#         train_folds: train folds\n+#         valid_folds (List[int], optional): valid folds.\n+#             If none takes all folds not included in ``train_folds``\n+#         infer_folds (List[int], optional): infer folds.\n+#             If none takes all folds not included in ``train_folds``\n+#             and ``valid_folds``\n+#         seed: seed for split\n+#         n_folds: number of folds\n+#\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#\n+#         tag2class (Dict[str, int]): mapping from label names into ints\n+#         tag_column: column with label names\n+#         class_column: column to use for split\n+#\n+#     Returns:\n+#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#             tuple with 4 elements\n+#             (whole dataframe,\n+#             list with train data,\n+#             list with valid data\n+#             and list with infer data)\n+#     \"\"\"\n+#     from_one_df: bool = in_csv is not None\n+#     from_multiple_df: bool = (\n+#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n+#     )\n+#\n+#     if from_one_df == from_multiple_df:\n+#         raise ValueError(\n+#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n+#         )\n+#\n+#     if from_one_df:\n+#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n+#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n+#             dataframe,\n+#             train_folds=train_folds,\n+#             valid_folds=valid_folds,\n+#             infer_folds=infer_folds,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#             seed=seed,\n+#             n_folds=n_folds,\n+#         )\n+#     else:\n+#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n+#             in_csv_train=in_csv_train,\n+#             in_csv_valid=in_csv_valid,\n+#             in_csv_infer=in_csv_infer,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#         )\n+#\n+#     for data in [df_train, df_valid, df_infer]:\n+#         if data is not None and \"fold\" in data.columns:\n+#             del data[\"fold\"]\n+#\n+#     result = (\n+#         dataframe,\n+#         dataframe_to_list(df_train) if df_train is not None else None,\n+#         dataframe_to_list(df_valid) if df_valid is not None else None,\n+#         dataframe_to_list(df_infer) if df_infer is not None else None,\n+#     )\n+#\n+#     return result\n \n",
        "source_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\n<DED>def merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    <IND>\"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        <IND>for csv_path in paths.split(\",\"):\n            <IND>dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    <DED><DED>return result\n\n\n<DED>def read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    <IND>\"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        <IND>if fold_df is not None:\n            <IND>fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                <IND>fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            <DED>fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    <DED><DED>output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\n<DED>def read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    <IND>\"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        <IND>raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    <DED>if from_one_df:\n        <IND>dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    <DED>for data in [df_train, df_valid, df_infer]:\n        <IND>if data is not None and \"fold\" in data.columns:\n            <IND>del data[\"fold\"]\n\n    <DED><DED>result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/utils/pandas.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/utils/pandas.py",
    "file_hunks_size": 15,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/contrib/utils/pandas.py:530:4 Incompatible variable type [9]: in_csv is declared to have type `str` but is used as type `None`.",
    "message": " in_csv is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 530,
    "warning_line": "    in_csv: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    if class_column is not None:\n        result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    else:\n        result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_len": 682,
        "target_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    if class_column is not None:\n        df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    else:\n        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_len": 562,
        "diff_format": "@@ -415,22 +394,15 @@\n     if args_are_not_none(tag2class, tag_column, class_column):\n-        dataframe = map_dataframe(\n-            dataframe, tag_column, class_column, tag2class\n-        )\n+        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n \n     if class_column is not None:\n-        result_dataframe = split_dataframe_on_stratified_folds(\n-            dataframe,\n-            class_column=class_column,\n-            random_state=seed,\n-            n_folds=n_folds,\n+        df_all = split_dataframe_on_stratified_folds(\n+            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n         )\n     else:\n-        result_dataframe = split_dataframe_on_folds(\n-            dataframe, random_state=seed, n_folds=n_folds\n-        )\n-\n-    fold_series = result_dataframe[\"fold\"]\n+        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n+\n+    fold_series = df_all[\"fold\"]\n \n     train_folds = folds_to_list(train_folds)\n-    df_train = result_dataframe[fold_series.isin(train_folds)]\n+    df_train = df_all[fold_series.isin(train_folds)]\n \n",
        "source_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    <DED>if class_column is not None:\n        <IND>result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    <DED>fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    <DED>if class_column is not None:\n        <IND>df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    <DED>fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\ndef merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    \"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        for csv_path in paths.split(\",\"):\n            dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    return result\n\n\ndef read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        if fold_df is not None:\n            fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\ndef read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    \"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    if from_one_df:\n        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    else:\n        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    for data in [df_train, df_valid, df_infer]:\n        if data is not None and \"fold\" in data.columns:\n            del data[\"fold\"]\n\n    result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_len": 6590,
        "target_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_len": 6715,
        "diff_format": "@@ -438,196 +410,183 @@\n         mask = ~fold_series.isin(train_folds)\n-        valid_folds = result_dataframe[mask][\"fold\"]\n+        valid_folds = df_all[mask][\"fold\"]\n \n     valid_folds = folds_to_list(valid_folds)\n-    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n+    df_valid = df_all[fold_series.isin(valid_folds)]\n \n     infer_folds = folds_to_list(infer_folds or [])\n-    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n-\n-    return result_dataframe, df_train, df_valid, df_infer\n-\n-\n-def merge_multiple_fold_csv(\n-    fold_name: str, paths: Optional[str]\n-) -> pd.DataFrame:\n-    \"\"\"Reads csv into one DataFrame with column ``fold``.\n-\n-    Args:\n-        fold_name: current fold name\n-        paths: paths to csv separated by commas\n-\n-    Returns:\n-         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n-    \"\"\"\n-    result = pd.DataFrame()\n-    if paths is not None:\n-        for csv_path in paths.split(\",\"):\n-            dataframe = pd.read_csv(csv_path)\n-            dataframe[\"fold\"] = fold_name\n-            result = result.append(dataframe, ignore_index=True)\n-\n-    return result\n-\n-\n-def read_multiple_dataframes(\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n-    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n-\n-    Args:\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-        tag2class (Dict[str, int], optional): mapping from label names into int\n-        tag_column (str, optional): column with label names\n-        class_column (str, optional): column to use for split\n-\n-    Returns:\n-        tuple: tuple with 4 dataframes\n-            whole dataframe, train part, valid part and infer part\n-    \"\"\"\n-    assert any(\n-        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n-    )\n-\n-    result_df = None\n-    fold_dfs = {}\n-    for fold_df, fold_name in zip(\n-        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n-    ):\n-        if fold_df is not None:\n-            fold_df = merge_multiple_fold_csv(\n-                fold_name=fold_name, paths=fold_df\n-            )\n-            if args_are_not_none(tag2class, tag_column, class_column):\n-                fold_df = map_dataframe(\n-                    fold_df, tag_column, class_column, tag2class\n-                )\n-            fold_dfs[fold_name] = fold_df\n-\n-            result_df = (\n-                fold_df\n-                if result_df is None\n-                else result_df.append(fold_df, ignore_index=True)\n-            )\n-\n-    output = (\n-        result_df,\n-        fold_dfs.get(\"train\", None),\n-        fold_dfs.get(\"valid\", None),\n-        fold_dfs.get(\"infer\", None),\n-    )\n-\n-    return output\n-\n-\n-def read_csv_data(\n-    in_csv: str = None,\n-    train_folds: Optional[List[int]] = None,\n-    valid_folds: Optional[List[int]] = None,\n-    infer_folds: Optional[List[int]] = None,\n-    seed: int = 42,\n-    n_folds: int = 5,\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-    \"\"\"\n-    From giving path ``in_csv`` reads a dataframe\n-    and split it to train/valid/infer folds\n-    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n-    reads independent folds.\n-\n-    .. note::\n-       This function can be used with different combinations of params.\n-        First block is used to get dataset from one `csv`:\n-            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n-        Second includes paths to different csv for train/valid and infer parts:\n-            in_csv_train, in_csv_valid, in_csv_infer\n-        The other params (tag2class, tag_column, class_column) are optional\n-            for any previous block\n-\n-    Args:\n-        in_csv: paths to whole dataset\n-        train_folds: train folds\n-        valid_folds (List[int], optional): valid folds.\n-            If none takes all folds not included in ``train_folds``\n-        infer_folds (List[int], optional): infer folds.\n-            If none takes all folds not included in ``train_folds``\n-            and ``valid_folds``\n-        seed: seed for split\n-        n_folds: number of folds\n-\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-\n-        tag2class (Dict[str, int]): mapping from label names into ints\n-        tag_column: column with label names\n-        class_column: column to use for split\n-\n-    Returns:\n-        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-            tuple with 4 elements\n-            (whole dataframe,\n-            list with train data,\n-            list with valid data\n-            and list with infer data)\n-    \"\"\"\n-    from_one_df: bool = in_csv is not None\n-    from_multiple_df: bool = (\n-        in_csv_train is not None\n-        or in_csv_valid is not None\n-        or in_csv_infer is not None\n-    )\n-\n-    if from_one_df == from_multiple_df:\n-        raise ValueError(\n-            \"You should pass `in_csv` \"\n-            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n-        )\n-\n-    if from_one_df:\n-        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n-        dataframe, df_train, df_valid, df_infer = split_dataframe(\n-            dataframe,\n-            train_folds=train_folds,\n-            valid_folds=valid_folds,\n-            infer_folds=infer_folds,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-            seed=seed,\n-            n_folds=n_folds,\n-        )\n-    else:\n-        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n-            in_csv_train=in_csv_train,\n-            in_csv_valid=in_csv_valid,\n-            in_csv_infer=in_csv_infer,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-        )\n-\n-    for data in [df_train, df_valid, df_infer]:\n-        if data is not None and \"fold\" in data.columns:\n-            del data[\"fold\"]\n-\n-    result = (\n-        dataframe,\n-        dataframe_to_list(df_train) if df_train is not None else None,\n-        dataframe_to_list(df_valid) if df_valid is not None else None,\n-        dataframe_to_list(df_infer) if df_infer is not None else None,\n-    )\n-\n-    return result\n+    df_infer = df_all[fold_series.isin(infer_folds)]\n+\n+    return df_all, df_train, df_valid, df_infer\n+\n+\n+# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n+#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n+#\n+#     Args:\n+#         fold_name: current fold name\n+#         paths: paths to csv separated by commas\n+#\n+#     Returns:\n+#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n+#     \"\"\"\n+#     result = pd.DataFrame()\n+#     if paths is not None:\n+#         for csv_path in paths.split(\",\"):\n+#             dataframe = pd.read_csv(csv_path)\n+#             dataframe[\"fold\"] = fold_name\n+#             result = result.append(dataframe, ignore_index=True)\n+#\n+#     return result\n+\n+\n+# def read_multiple_dataframes(\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n+#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n+#\n+#     Args:\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#         tag2class (Dict[str, int], optional): mapping from label names into int\n+#         tag_column (str, optional): column with label names\n+#         class_column (str, optional): column to use for split\n+#\n+#     Returns:\n+#         tuple: tuple with 4 dataframes\n+#             whole dataframe, train part, valid part and infer part\n+#     \"\"\"\n+#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n+#\n+#     result_df = None\n+#     fold_dfs = {}\n+#     for fold_df, fold_name in zip(\n+#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n+#     ):\n+#         if fold_df is not None:\n+#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n+#             if args_are_not_none(tag2class, tag_column, class_column):\n+#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n+#             fold_dfs[fold_name] = fold_df\n+#\n+#             result_df = (\n+#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n+#             )\n+#\n+#     output = (\n+#         result_df,\n+#         fold_dfs.get(\"train\", None),\n+#         fold_dfs.get(\"valid\", None),\n+#         fold_dfs.get(\"infer\", None),\n+#     )\n+#\n+#     return output\n+\n+\n+# def read_csv_data(\n+#     in_csv: str = None,\n+#     train_folds: Optional[List[int]] = None,\n+#     valid_folds: Optional[List[int]] = None,\n+#     infer_folds: Optional[List[int]] = None,\n+#     seed: int = 42,\n+#     n_folds: int = 5,\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#     \"\"\"\n+#     From giving path ``in_csv`` reads a dataframe\n+#     and split it to train/valid/infer folds\n+#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n+#     reads independent folds.\n+#\n+#     .. note::\n+#        This function can be used with different combinations of params.\n+#         First block is used to get dataset from one `csv`:\n+#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n+#         Second includes paths to different csv for train/valid and infer parts:\n+#             in_csv_train, in_csv_valid, in_csv_infer\n+#         The other params (tag2class, tag_column, class_column) are optional\n+#             for any previous block\n+#\n+#     Args:\n+#         in_csv: paths to whole dataset\n+#         train_folds: train folds\n+#         valid_folds (List[int], optional): valid folds.\n+#             If none takes all folds not included in ``train_folds``\n+#         infer_folds (List[int], optional): infer folds.\n+#             If none takes all folds not included in ``train_folds``\n+#             and ``valid_folds``\n+#         seed: seed for split\n+#         n_folds: number of folds\n+#\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#\n+#         tag2class (Dict[str, int]): mapping from label names into ints\n+#         tag_column: column with label names\n+#         class_column: column to use for split\n+#\n+#     Returns:\n+#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#             tuple with 4 elements\n+#             (whole dataframe,\n+#             list with train data,\n+#             list with valid data\n+#             and list with infer data)\n+#     \"\"\"\n+#     from_one_df: bool = in_csv is not None\n+#     from_multiple_df: bool = (\n+#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n+#     )\n+#\n+#     if from_one_df == from_multiple_df:\n+#         raise ValueError(\n+#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n+#         )\n+#\n+#     if from_one_df:\n+#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n+#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n+#             dataframe,\n+#             train_folds=train_folds,\n+#             valid_folds=valid_folds,\n+#             infer_folds=infer_folds,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#             seed=seed,\n+#             n_folds=n_folds,\n+#         )\n+#     else:\n+#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n+#             in_csv_train=in_csv_train,\n+#             in_csv_valid=in_csv_valid,\n+#             in_csv_infer=in_csv_infer,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#         )\n+#\n+#     for data in [df_train, df_valid, df_infer]:\n+#         if data is not None and \"fold\" in data.columns:\n+#             del data[\"fold\"]\n+#\n+#     result = (\n+#         dataframe,\n+#         dataframe_to_list(df_train) if df_train is not None else None,\n+#         dataframe_to_list(df_valid) if df_valid is not None else None,\n+#         dataframe_to_list(df_infer) if df_infer is not None else None,\n+#     )\n+#\n+#     return result\n \n",
        "source_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\n<DED>def merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    <IND>\"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        <IND>for csv_path in paths.split(\",\"):\n            <IND>dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    <DED><DED>return result\n\n\n<DED>def read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    <IND>\"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        <IND>if fold_df is not None:\n            <IND>fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                <IND>fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            <DED>fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    <DED><DED>output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\n<DED>def read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    <IND>\"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        <IND>raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    <DED>if from_one_df:\n        <IND>dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    <DED>for data in [df_train, df_valid, df_infer]:\n        <IND>if data is not None and \"fold\" in data.columns:\n            <IND>del data[\"fold\"]\n\n    <DED><DED>result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/utils/pandas.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/utils/pandas.py",
    "file_hunks_size": 15,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/contrib/utils/pandas.py:536:4 Incompatible variable type [9]: in_csv_train is declared to have type `str` but is used as type `None`.",
    "message": " in_csv_train is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 536,
    "warning_line": "    in_csv_train: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    if class_column is not None:\n        result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    else:\n        result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_len": 682,
        "target_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    if class_column is not None:\n        df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    else:\n        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_len": 562,
        "diff_format": "@@ -415,22 +394,15 @@\n     if args_are_not_none(tag2class, tag_column, class_column):\n-        dataframe = map_dataframe(\n-            dataframe, tag_column, class_column, tag2class\n-        )\n+        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n \n     if class_column is not None:\n-        result_dataframe = split_dataframe_on_stratified_folds(\n-            dataframe,\n-            class_column=class_column,\n-            random_state=seed,\n-            n_folds=n_folds,\n+        df_all = split_dataframe_on_stratified_folds(\n+            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n         )\n     else:\n-        result_dataframe = split_dataframe_on_folds(\n-            dataframe, random_state=seed, n_folds=n_folds\n-        )\n-\n-    fold_series = result_dataframe[\"fold\"]\n+        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n+\n+    fold_series = df_all[\"fold\"]\n \n     train_folds = folds_to_list(train_folds)\n-    df_train = result_dataframe[fold_series.isin(train_folds)]\n+    df_train = df_all[fold_series.isin(train_folds)]\n \n",
        "source_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    <DED>if class_column is not None:\n        <IND>result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    <DED>fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    <DED>if class_column is not None:\n        <IND>df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    <DED>fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\ndef merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    \"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        for csv_path in paths.split(\",\"):\n            dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    return result\n\n\ndef read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        if fold_df is not None:\n            fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\ndef read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    \"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    if from_one_df:\n        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    else:\n        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    for data in [df_train, df_valid, df_infer]:\n        if data is not None and \"fold\" in data.columns:\n            del data[\"fold\"]\n\n    result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_len": 6590,
        "target_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_len": 6715,
        "diff_format": "@@ -438,196 +410,183 @@\n         mask = ~fold_series.isin(train_folds)\n-        valid_folds = result_dataframe[mask][\"fold\"]\n+        valid_folds = df_all[mask][\"fold\"]\n \n     valid_folds = folds_to_list(valid_folds)\n-    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n+    df_valid = df_all[fold_series.isin(valid_folds)]\n \n     infer_folds = folds_to_list(infer_folds or [])\n-    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n-\n-    return result_dataframe, df_train, df_valid, df_infer\n-\n-\n-def merge_multiple_fold_csv(\n-    fold_name: str, paths: Optional[str]\n-) -> pd.DataFrame:\n-    \"\"\"Reads csv into one DataFrame with column ``fold``.\n-\n-    Args:\n-        fold_name: current fold name\n-        paths: paths to csv separated by commas\n-\n-    Returns:\n-         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n-    \"\"\"\n-    result = pd.DataFrame()\n-    if paths is not None:\n-        for csv_path in paths.split(\",\"):\n-            dataframe = pd.read_csv(csv_path)\n-            dataframe[\"fold\"] = fold_name\n-            result = result.append(dataframe, ignore_index=True)\n-\n-    return result\n-\n-\n-def read_multiple_dataframes(\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n-    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n-\n-    Args:\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-        tag2class (Dict[str, int], optional): mapping from label names into int\n-        tag_column (str, optional): column with label names\n-        class_column (str, optional): column to use for split\n-\n-    Returns:\n-        tuple: tuple with 4 dataframes\n-            whole dataframe, train part, valid part and infer part\n-    \"\"\"\n-    assert any(\n-        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n-    )\n-\n-    result_df = None\n-    fold_dfs = {}\n-    for fold_df, fold_name in zip(\n-        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n-    ):\n-        if fold_df is not None:\n-            fold_df = merge_multiple_fold_csv(\n-                fold_name=fold_name, paths=fold_df\n-            )\n-            if args_are_not_none(tag2class, tag_column, class_column):\n-                fold_df = map_dataframe(\n-                    fold_df, tag_column, class_column, tag2class\n-                )\n-            fold_dfs[fold_name] = fold_df\n-\n-            result_df = (\n-                fold_df\n-                if result_df is None\n-                else result_df.append(fold_df, ignore_index=True)\n-            )\n-\n-    output = (\n-        result_df,\n-        fold_dfs.get(\"train\", None),\n-        fold_dfs.get(\"valid\", None),\n-        fold_dfs.get(\"infer\", None),\n-    )\n-\n-    return output\n-\n-\n-def read_csv_data(\n-    in_csv: str = None,\n-    train_folds: Optional[List[int]] = None,\n-    valid_folds: Optional[List[int]] = None,\n-    infer_folds: Optional[List[int]] = None,\n-    seed: int = 42,\n-    n_folds: int = 5,\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-    \"\"\"\n-    From giving path ``in_csv`` reads a dataframe\n-    and split it to train/valid/infer folds\n-    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n-    reads independent folds.\n-\n-    .. note::\n-       This function can be used with different combinations of params.\n-        First block is used to get dataset from one `csv`:\n-            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n-        Second includes paths to different csv for train/valid and infer parts:\n-            in_csv_train, in_csv_valid, in_csv_infer\n-        The other params (tag2class, tag_column, class_column) are optional\n-            for any previous block\n-\n-    Args:\n-        in_csv: paths to whole dataset\n-        train_folds: train folds\n-        valid_folds (List[int], optional): valid folds.\n-            If none takes all folds not included in ``train_folds``\n-        infer_folds (List[int], optional): infer folds.\n-            If none takes all folds not included in ``train_folds``\n-            and ``valid_folds``\n-        seed: seed for split\n-        n_folds: number of folds\n-\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-\n-        tag2class (Dict[str, int]): mapping from label names into ints\n-        tag_column: column with label names\n-        class_column: column to use for split\n-\n-    Returns:\n-        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-            tuple with 4 elements\n-            (whole dataframe,\n-            list with train data,\n-            list with valid data\n-            and list with infer data)\n-    \"\"\"\n-    from_one_df: bool = in_csv is not None\n-    from_multiple_df: bool = (\n-        in_csv_train is not None\n-        or in_csv_valid is not None\n-        or in_csv_infer is not None\n-    )\n-\n-    if from_one_df == from_multiple_df:\n-        raise ValueError(\n-            \"You should pass `in_csv` \"\n-            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n-        )\n-\n-    if from_one_df:\n-        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n-        dataframe, df_train, df_valid, df_infer = split_dataframe(\n-            dataframe,\n-            train_folds=train_folds,\n-            valid_folds=valid_folds,\n-            infer_folds=infer_folds,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-            seed=seed,\n-            n_folds=n_folds,\n-        )\n-    else:\n-        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n-            in_csv_train=in_csv_train,\n-            in_csv_valid=in_csv_valid,\n-            in_csv_infer=in_csv_infer,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-        )\n-\n-    for data in [df_train, df_valid, df_infer]:\n-        if data is not None and \"fold\" in data.columns:\n-            del data[\"fold\"]\n-\n-    result = (\n-        dataframe,\n-        dataframe_to_list(df_train) if df_train is not None else None,\n-        dataframe_to_list(df_valid) if df_valid is not None else None,\n-        dataframe_to_list(df_infer) if df_infer is not None else None,\n-    )\n-\n-    return result\n+    df_infer = df_all[fold_series.isin(infer_folds)]\n+\n+    return df_all, df_train, df_valid, df_infer\n+\n+\n+# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n+#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n+#\n+#     Args:\n+#         fold_name: current fold name\n+#         paths: paths to csv separated by commas\n+#\n+#     Returns:\n+#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n+#     \"\"\"\n+#     result = pd.DataFrame()\n+#     if paths is not None:\n+#         for csv_path in paths.split(\",\"):\n+#             dataframe = pd.read_csv(csv_path)\n+#             dataframe[\"fold\"] = fold_name\n+#             result = result.append(dataframe, ignore_index=True)\n+#\n+#     return result\n+\n+\n+# def read_multiple_dataframes(\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n+#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n+#\n+#     Args:\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#         tag2class (Dict[str, int], optional): mapping from label names into int\n+#         tag_column (str, optional): column with label names\n+#         class_column (str, optional): column to use for split\n+#\n+#     Returns:\n+#         tuple: tuple with 4 dataframes\n+#             whole dataframe, train part, valid part and infer part\n+#     \"\"\"\n+#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n+#\n+#     result_df = None\n+#     fold_dfs = {}\n+#     for fold_df, fold_name in zip(\n+#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n+#     ):\n+#         if fold_df is not None:\n+#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n+#             if args_are_not_none(tag2class, tag_column, class_column):\n+#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n+#             fold_dfs[fold_name] = fold_df\n+#\n+#             result_df = (\n+#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n+#             )\n+#\n+#     output = (\n+#         result_df,\n+#         fold_dfs.get(\"train\", None),\n+#         fold_dfs.get(\"valid\", None),\n+#         fold_dfs.get(\"infer\", None),\n+#     )\n+#\n+#     return output\n+\n+\n+# def read_csv_data(\n+#     in_csv: str = None,\n+#     train_folds: Optional[List[int]] = None,\n+#     valid_folds: Optional[List[int]] = None,\n+#     infer_folds: Optional[List[int]] = None,\n+#     seed: int = 42,\n+#     n_folds: int = 5,\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#     \"\"\"\n+#     From giving path ``in_csv`` reads a dataframe\n+#     and split it to train/valid/infer folds\n+#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n+#     reads independent folds.\n+#\n+#     .. note::\n+#        This function can be used with different combinations of params.\n+#         First block is used to get dataset from one `csv`:\n+#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n+#         Second includes paths to different csv for train/valid and infer parts:\n+#             in_csv_train, in_csv_valid, in_csv_infer\n+#         The other params (tag2class, tag_column, class_column) are optional\n+#             for any previous block\n+#\n+#     Args:\n+#         in_csv: paths to whole dataset\n+#         train_folds: train folds\n+#         valid_folds (List[int], optional): valid folds.\n+#             If none takes all folds not included in ``train_folds``\n+#         infer_folds (List[int], optional): infer folds.\n+#             If none takes all folds not included in ``train_folds``\n+#             and ``valid_folds``\n+#         seed: seed for split\n+#         n_folds: number of folds\n+#\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#\n+#         tag2class (Dict[str, int]): mapping from label names into ints\n+#         tag_column: column with label names\n+#         class_column: column to use for split\n+#\n+#     Returns:\n+#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#             tuple with 4 elements\n+#             (whole dataframe,\n+#             list with train data,\n+#             list with valid data\n+#             and list with infer data)\n+#     \"\"\"\n+#     from_one_df: bool = in_csv is not None\n+#     from_multiple_df: bool = (\n+#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n+#     )\n+#\n+#     if from_one_df == from_multiple_df:\n+#         raise ValueError(\n+#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n+#         )\n+#\n+#     if from_one_df:\n+#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n+#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n+#             dataframe,\n+#             train_folds=train_folds,\n+#             valid_folds=valid_folds,\n+#             infer_folds=infer_folds,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#             seed=seed,\n+#             n_folds=n_folds,\n+#         )\n+#     else:\n+#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n+#             in_csv_train=in_csv_train,\n+#             in_csv_valid=in_csv_valid,\n+#             in_csv_infer=in_csv_infer,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#         )\n+#\n+#     for data in [df_train, df_valid, df_infer]:\n+#         if data is not None and \"fold\" in data.columns:\n+#             del data[\"fold\"]\n+#\n+#     result = (\n+#         dataframe,\n+#         dataframe_to_list(df_train) if df_train is not None else None,\n+#         dataframe_to_list(df_valid) if df_valid is not None else None,\n+#         dataframe_to_list(df_infer) if df_infer is not None else None,\n+#     )\n+#\n+#     return result\n \n",
        "source_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\n<DED>def merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    <IND>\"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        <IND>for csv_path in paths.split(\",\"):\n            <IND>dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    <DED><DED>return result\n\n\n<DED>def read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    <IND>\"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        <IND>if fold_df is not None:\n            <IND>fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                <IND>fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            <DED>fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    <DED><DED>output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\n<DED>def read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    <IND>\"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        <IND>raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    <DED>if from_one_df:\n        <IND>dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    <DED>for data in [df_train, df_valid, df_infer]:\n        <IND>if data is not None and \"fold\" in data.columns:\n            <IND>del data[\"fold\"]\n\n    <DED><DED>result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/utils/pandas.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/utils/pandas.py",
    "file_hunks_size": 15,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/contrib/utils/pandas.py:537:4 Incompatible variable type [9]: in_csv_valid is declared to have type `str` but is used as type `None`.",
    "message": " in_csv_valid is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 537,
    "warning_line": "    in_csv_valid: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    if class_column is not None:\n        result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    else:\n        result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_len": 682,
        "target_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    if class_column is not None:\n        df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    else:\n        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_len": 562,
        "diff_format": "@@ -415,22 +394,15 @@\n     if args_are_not_none(tag2class, tag_column, class_column):\n-        dataframe = map_dataframe(\n-            dataframe, tag_column, class_column, tag2class\n-        )\n+        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n \n     if class_column is not None:\n-        result_dataframe = split_dataframe_on_stratified_folds(\n-            dataframe,\n-            class_column=class_column,\n-            random_state=seed,\n-            n_folds=n_folds,\n+        df_all = split_dataframe_on_stratified_folds(\n+            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n         )\n     else:\n-        result_dataframe = split_dataframe_on_folds(\n-            dataframe, random_state=seed, n_folds=n_folds\n-        )\n-\n-    fold_series = result_dataframe[\"fold\"]\n+        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n+\n+    fold_series = df_all[\"fold\"]\n \n     train_folds = folds_to_list(train_folds)\n-    df_train = result_dataframe[fold_series.isin(train_folds)]\n+    df_train = df_all[fold_series.isin(train_folds)]\n \n",
        "source_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    <DED>if class_column is not None:\n        <IND>result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    <DED>fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    <DED>if class_column is not None:\n        <IND>df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    <DED>fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\ndef merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    \"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        for csv_path in paths.split(\",\"):\n            dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    return result\n\n\ndef read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        if fold_df is not None:\n            fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\ndef read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    \"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    if from_one_df:\n        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    else:\n        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    for data in [df_train, df_valid, df_infer]:\n        if data is not None and \"fold\" in data.columns:\n            del data[\"fold\"]\n\n    result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_len": 6590,
        "target_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_len": 6715,
        "diff_format": "@@ -438,196 +410,183 @@\n         mask = ~fold_series.isin(train_folds)\n-        valid_folds = result_dataframe[mask][\"fold\"]\n+        valid_folds = df_all[mask][\"fold\"]\n \n     valid_folds = folds_to_list(valid_folds)\n-    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n+    df_valid = df_all[fold_series.isin(valid_folds)]\n \n     infer_folds = folds_to_list(infer_folds or [])\n-    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n-\n-    return result_dataframe, df_train, df_valid, df_infer\n-\n-\n-def merge_multiple_fold_csv(\n-    fold_name: str, paths: Optional[str]\n-) -> pd.DataFrame:\n-    \"\"\"Reads csv into one DataFrame with column ``fold``.\n-\n-    Args:\n-        fold_name: current fold name\n-        paths: paths to csv separated by commas\n-\n-    Returns:\n-         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n-    \"\"\"\n-    result = pd.DataFrame()\n-    if paths is not None:\n-        for csv_path in paths.split(\",\"):\n-            dataframe = pd.read_csv(csv_path)\n-            dataframe[\"fold\"] = fold_name\n-            result = result.append(dataframe, ignore_index=True)\n-\n-    return result\n-\n-\n-def read_multiple_dataframes(\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n-    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n-\n-    Args:\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-        tag2class (Dict[str, int], optional): mapping from label names into int\n-        tag_column (str, optional): column with label names\n-        class_column (str, optional): column to use for split\n-\n-    Returns:\n-        tuple: tuple with 4 dataframes\n-            whole dataframe, train part, valid part and infer part\n-    \"\"\"\n-    assert any(\n-        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n-    )\n-\n-    result_df = None\n-    fold_dfs = {}\n-    for fold_df, fold_name in zip(\n-        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n-    ):\n-        if fold_df is not None:\n-            fold_df = merge_multiple_fold_csv(\n-                fold_name=fold_name, paths=fold_df\n-            )\n-            if args_are_not_none(tag2class, tag_column, class_column):\n-                fold_df = map_dataframe(\n-                    fold_df, tag_column, class_column, tag2class\n-                )\n-            fold_dfs[fold_name] = fold_df\n-\n-            result_df = (\n-                fold_df\n-                if result_df is None\n-                else result_df.append(fold_df, ignore_index=True)\n-            )\n-\n-    output = (\n-        result_df,\n-        fold_dfs.get(\"train\", None),\n-        fold_dfs.get(\"valid\", None),\n-        fold_dfs.get(\"infer\", None),\n-    )\n-\n-    return output\n-\n-\n-def read_csv_data(\n-    in_csv: str = None,\n-    train_folds: Optional[List[int]] = None,\n-    valid_folds: Optional[List[int]] = None,\n-    infer_folds: Optional[List[int]] = None,\n-    seed: int = 42,\n-    n_folds: int = 5,\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-    \"\"\"\n-    From giving path ``in_csv`` reads a dataframe\n-    and split it to train/valid/infer folds\n-    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n-    reads independent folds.\n-\n-    .. note::\n-       This function can be used with different combinations of params.\n-        First block is used to get dataset from one `csv`:\n-            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n-        Second includes paths to different csv for train/valid and infer parts:\n-            in_csv_train, in_csv_valid, in_csv_infer\n-        The other params (tag2class, tag_column, class_column) are optional\n-            for any previous block\n-\n-    Args:\n-        in_csv: paths to whole dataset\n-        train_folds: train folds\n-        valid_folds (List[int], optional): valid folds.\n-            If none takes all folds not included in ``train_folds``\n-        infer_folds (List[int], optional): infer folds.\n-            If none takes all folds not included in ``train_folds``\n-            and ``valid_folds``\n-        seed: seed for split\n-        n_folds: number of folds\n-\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-\n-        tag2class (Dict[str, int]): mapping from label names into ints\n-        tag_column: column with label names\n-        class_column: column to use for split\n-\n-    Returns:\n-        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-            tuple with 4 elements\n-            (whole dataframe,\n-            list with train data,\n-            list with valid data\n-            and list with infer data)\n-    \"\"\"\n-    from_one_df: bool = in_csv is not None\n-    from_multiple_df: bool = (\n-        in_csv_train is not None\n-        or in_csv_valid is not None\n-        or in_csv_infer is not None\n-    )\n-\n-    if from_one_df == from_multiple_df:\n-        raise ValueError(\n-            \"You should pass `in_csv` \"\n-            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n-        )\n-\n-    if from_one_df:\n-        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n-        dataframe, df_train, df_valid, df_infer = split_dataframe(\n-            dataframe,\n-            train_folds=train_folds,\n-            valid_folds=valid_folds,\n-            infer_folds=infer_folds,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-            seed=seed,\n-            n_folds=n_folds,\n-        )\n-    else:\n-        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n-            in_csv_train=in_csv_train,\n-            in_csv_valid=in_csv_valid,\n-            in_csv_infer=in_csv_infer,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-        )\n-\n-    for data in [df_train, df_valid, df_infer]:\n-        if data is not None and \"fold\" in data.columns:\n-            del data[\"fold\"]\n-\n-    result = (\n-        dataframe,\n-        dataframe_to_list(df_train) if df_train is not None else None,\n-        dataframe_to_list(df_valid) if df_valid is not None else None,\n-        dataframe_to_list(df_infer) if df_infer is not None else None,\n-    )\n-\n-    return result\n+    df_infer = df_all[fold_series.isin(infer_folds)]\n+\n+    return df_all, df_train, df_valid, df_infer\n+\n+\n+# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n+#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n+#\n+#     Args:\n+#         fold_name: current fold name\n+#         paths: paths to csv separated by commas\n+#\n+#     Returns:\n+#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n+#     \"\"\"\n+#     result = pd.DataFrame()\n+#     if paths is not None:\n+#         for csv_path in paths.split(\",\"):\n+#             dataframe = pd.read_csv(csv_path)\n+#             dataframe[\"fold\"] = fold_name\n+#             result = result.append(dataframe, ignore_index=True)\n+#\n+#     return result\n+\n+\n+# def read_multiple_dataframes(\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n+#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n+#\n+#     Args:\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#         tag2class (Dict[str, int], optional): mapping from label names into int\n+#         tag_column (str, optional): column with label names\n+#         class_column (str, optional): column to use for split\n+#\n+#     Returns:\n+#         tuple: tuple with 4 dataframes\n+#             whole dataframe, train part, valid part and infer part\n+#     \"\"\"\n+#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n+#\n+#     result_df = None\n+#     fold_dfs = {}\n+#     for fold_df, fold_name in zip(\n+#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n+#     ):\n+#         if fold_df is not None:\n+#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n+#             if args_are_not_none(tag2class, tag_column, class_column):\n+#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n+#             fold_dfs[fold_name] = fold_df\n+#\n+#             result_df = (\n+#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n+#             )\n+#\n+#     output = (\n+#         result_df,\n+#         fold_dfs.get(\"train\", None),\n+#         fold_dfs.get(\"valid\", None),\n+#         fold_dfs.get(\"infer\", None),\n+#     )\n+#\n+#     return output\n+\n+\n+# def read_csv_data(\n+#     in_csv: str = None,\n+#     train_folds: Optional[List[int]] = None,\n+#     valid_folds: Optional[List[int]] = None,\n+#     infer_folds: Optional[List[int]] = None,\n+#     seed: int = 42,\n+#     n_folds: int = 5,\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#     \"\"\"\n+#     From giving path ``in_csv`` reads a dataframe\n+#     and split it to train/valid/infer folds\n+#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n+#     reads independent folds.\n+#\n+#     .. note::\n+#        This function can be used with different combinations of params.\n+#         First block is used to get dataset from one `csv`:\n+#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n+#         Second includes paths to different csv for train/valid and infer parts:\n+#             in_csv_train, in_csv_valid, in_csv_infer\n+#         The other params (tag2class, tag_column, class_column) are optional\n+#             for any previous block\n+#\n+#     Args:\n+#         in_csv: paths to whole dataset\n+#         train_folds: train folds\n+#         valid_folds (List[int], optional): valid folds.\n+#             If none takes all folds not included in ``train_folds``\n+#         infer_folds (List[int], optional): infer folds.\n+#             If none takes all folds not included in ``train_folds``\n+#             and ``valid_folds``\n+#         seed: seed for split\n+#         n_folds: number of folds\n+#\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#\n+#         tag2class (Dict[str, int]): mapping from label names into ints\n+#         tag_column: column with label names\n+#         class_column: column to use for split\n+#\n+#     Returns:\n+#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#             tuple with 4 elements\n+#             (whole dataframe,\n+#             list with train data,\n+#             list with valid data\n+#             and list with infer data)\n+#     \"\"\"\n+#     from_one_df: bool = in_csv is not None\n+#     from_multiple_df: bool = (\n+#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n+#     )\n+#\n+#     if from_one_df == from_multiple_df:\n+#         raise ValueError(\n+#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n+#         )\n+#\n+#     if from_one_df:\n+#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n+#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n+#             dataframe,\n+#             train_folds=train_folds,\n+#             valid_folds=valid_folds,\n+#             infer_folds=infer_folds,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#             seed=seed,\n+#             n_folds=n_folds,\n+#         )\n+#     else:\n+#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n+#             in_csv_train=in_csv_train,\n+#             in_csv_valid=in_csv_valid,\n+#             in_csv_infer=in_csv_infer,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#         )\n+#\n+#     for data in [df_train, df_valid, df_infer]:\n+#         if data is not None and \"fold\" in data.columns:\n+#             del data[\"fold\"]\n+#\n+#     result = (\n+#         dataframe,\n+#         dataframe_to_list(df_train) if df_train is not None else None,\n+#         dataframe_to_list(df_valid) if df_valid is not None else None,\n+#         dataframe_to_list(df_infer) if df_infer is not None else None,\n+#     )\n+#\n+#     return result\n \n",
        "source_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\n<DED>def merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    <IND>\"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        <IND>for csv_path in paths.split(\",\"):\n            <IND>dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    <DED><DED>return result\n\n\n<DED>def read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    <IND>\"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        <IND>if fold_df is not None:\n            <IND>fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                <IND>fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            <DED>fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    <DED><DED>output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\n<DED>def read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    <IND>\"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        <IND>raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    <DED>if from_one_df:\n        <IND>dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    <DED>for data in [df_train, df_valid, df_infer]:\n        <IND>if data is not None and \"fold\" in data.columns:\n            <IND>del data[\"fold\"]\n\n    <DED><DED>result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/utils/pandas.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/utils/pandas.py",
    "file_hunks_size": 15,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/contrib/utils/pandas.py:538:4 Incompatible variable type [9]: in_csv_infer is declared to have type `str` but is used as type `None`.",
    "message": " in_csv_infer is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 538,
    "warning_line": "    in_csv_infer: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    if class_column is not None:\n        result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    else:\n        result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_len": 682,
        "target_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    if class_column is not None:\n        df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    else:\n        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_len": 562,
        "diff_format": "@@ -415,22 +394,15 @@\n     if args_are_not_none(tag2class, tag_column, class_column):\n-        dataframe = map_dataframe(\n-            dataframe, tag_column, class_column, tag2class\n-        )\n+        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n \n     if class_column is not None:\n-        result_dataframe = split_dataframe_on_stratified_folds(\n-            dataframe,\n-            class_column=class_column,\n-            random_state=seed,\n-            n_folds=n_folds,\n+        df_all = split_dataframe_on_stratified_folds(\n+            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n         )\n     else:\n-        result_dataframe = split_dataframe_on_folds(\n-            dataframe, random_state=seed, n_folds=n_folds\n-        )\n-\n-    fold_series = result_dataframe[\"fold\"]\n+        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n+\n+    fold_series = df_all[\"fold\"]\n \n     train_folds = folds_to_list(train_folds)\n-    df_train = result_dataframe[fold_series.isin(train_folds)]\n+    df_train = df_all[fold_series.isin(train_folds)]\n \n",
        "source_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    <DED>if class_column is not None:\n        <IND>result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    <DED>fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    <DED>if class_column is not None:\n        <IND>df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    <DED>fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\ndef merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    \"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        for csv_path in paths.split(\",\"):\n            dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    return result\n\n\ndef read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        if fold_df is not None:\n            fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\ndef read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    \"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    if from_one_df:\n        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    else:\n        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    for data in [df_train, df_valid, df_infer]:\n        if data is not None and \"fold\" in data.columns:\n            del data[\"fold\"]\n\n    result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_len": 6590,
        "target_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_len": 6715,
        "diff_format": "@@ -438,196 +410,183 @@\n         mask = ~fold_series.isin(train_folds)\n-        valid_folds = result_dataframe[mask][\"fold\"]\n+        valid_folds = df_all[mask][\"fold\"]\n \n     valid_folds = folds_to_list(valid_folds)\n-    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n+    df_valid = df_all[fold_series.isin(valid_folds)]\n \n     infer_folds = folds_to_list(infer_folds or [])\n-    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n-\n-    return result_dataframe, df_train, df_valid, df_infer\n-\n-\n-def merge_multiple_fold_csv(\n-    fold_name: str, paths: Optional[str]\n-) -> pd.DataFrame:\n-    \"\"\"Reads csv into one DataFrame with column ``fold``.\n-\n-    Args:\n-        fold_name: current fold name\n-        paths: paths to csv separated by commas\n-\n-    Returns:\n-         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n-    \"\"\"\n-    result = pd.DataFrame()\n-    if paths is not None:\n-        for csv_path in paths.split(\",\"):\n-            dataframe = pd.read_csv(csv_path)\n-            dataframe[\"fold\"] = fold_name\n-            result = result.append(dataframe, ignore_index=True)\n-\n-    return result\n-\n-\n-def read_multiple_dataframes(\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n-    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n-\n-    Args:\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-        tag2class (Dict[str, int], optional): mapping from label names into int\n-        tag_column (str, optional): column with label names\n-        class_column (str, optional): column to use for split\n-\n-    Returns:\n-        tuple: tuple with 4 dataframes\n-            whole dataframe, train part, valid part and infer part\n-    \"\"\"\n-    assert any(\n-        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n-    )\n-\n-    result_df = None\n-    fold_dfs = {}\n-    for fold_df, fold_name in zip(\n-        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n-    ):\n-        if fold_df is not None:\n-            fold_df = merge_multiple_fold_csv(\n-                fold_name=fold_name, paths=fold_df\n-            )\n-            if args_are_not_none(tag2class, tag_column, class_column):\n-                fold_df = map_dataframe(\n-                    fold_df, tag_column, class_column, tag2class\n-                )\n-            fold_dfs[fold_name] = fold_df\n-\n-            result_df = (\n-                fold_df\n-                if result_df is None\n-                else result_df.append(fold_df, ignore_index=True)\n-            )\n-\n-    output = (\n-        result_df,\n-        fold_dfs.get(\"train\", None),\n-        fold_dfs.get(\"valid\", None),\n-        fold_dfs.get(\"infer\", None),\n-    )\n-\n-    return output\n-\n-\n-def read_csv_data(\n-    in_csv: str = None,\n-    train_folds: Optional[List[int]] = None,\n-    valid_folds: Optional[List[int]] = None,\n-    infer_folds: Optional[List[int]] = None,\n-    seed: int = 42,\n-    n_folds: int = 5,\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-    \"\"\"\n-    From giving path ``in_csv`` reads a dataframe\n-    and split it to train/valid/infer folds\n-    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n-    reads independent folds.\n-\n-    .. note::\n-       This function can be used with different combinations of params.\n-        First block is used to get dataset from one `csv`:\n-            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n-        Second includes paths to different csv for train/valid and infer parts:\n-            in_csv_train, in_csv_valid, in_csv_infer\n-        The other params (tag2class, tag_column, class_column) are optional\n-            for any previous block\n-\n-    Args:\n-        in_csv: paths to whole dataset\n-        train_folds: train folds\n-        valid_folds (List[int], optional): valid folds.\n-            If none takes all folds not included in ``train_folds``\n-        infer_folds (List[int], optional): infer folds.\n-            If none takes all folds not included in ``train_folds``\n-            and ``valid_folds``\n-        seed: seed for split\n-        n_folds: number of folds\n-\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-\n-        tag2class (Dict[str, int]): mapping from label names into ints\n-        tag_column: column with label names\n-        class_column: column to use for split\n-\n-    Returns:\n-        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-            tuple with 4 elements\n-            (whole dataframe,\n-            list with train data,\n-            list with valid data\n-            and list with infer data)\n-    \"\"\"\n-    from_one_df: bool = in_csv is not None\n-    from_multiple_df: bool = (\n-        in_csv_train is not None\n-        or in_csv_valid is not None\n-        or in_csv_infer is not None\n-    )\n-\n-    if from_one_df == from_multiple_df:\n-        raise ValueError(\n-            \"You should pass `in_csv` \"\n-            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n-        )\n-\n-    if from_one_df:\n-        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n-        dataframe, df_train, df_valid, df_infer = split_dataframe(\n-            dataframe,\n-            train_folds=train_folds,\n-            valid_folds=valid_folds,\n-            infer_folds=infer_folds,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-            seed=seed,\n-            n_folds=n_folds,\n-        )\n-    else:\n-        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n-            in_csv_train=in_csv_train,\n-            in_csv_valid=in_csv_valid,\n-            in_csv_infer=in_csv_infer,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-        )\n-\n-    for data in [df_train, df_valid, df_infer]:\n-        if data is not None and \"fold\" in data.columns:\n-            del data[\"fold\"]\n-\n-    result = (\n-        dataframe,\n-        dataframe_to_list(df_train) if df_train is not None else None,\n-        dataframe_to_list(df_valid) if df_valid is not None else None,\n-        dataframe_to_list(df_infer) if df_infer is not None else None,\n-    )\n-\n-    return result\n+    df_infer = df_all[fold_series.isin(infer_folds)]\n+\n+    return df_all, df_train, df_valid, df_infer\n+\n+\n+# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n+#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n+#\n+#     Args:\n+#         fold_name: current fold name\n+#         paths: paths to csv separated by commas\n+#\n+#     Returns:\n+#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n+#     \"\"\"\n+#     result = pd.DataFrame()\n+#     if paths is not None:\n+#         for csv_path in paths.split(\",\"):\n+#             dataframe = pd.read_csv(csv_path)\n+#             dataframe[\"fold\"] = fold_name\n+#             result = result.append(dataframe, ignore_index=True)\n+#\n+#     return result\n+\n+\n+# def read_multiple_dataframes(\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n+#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n+#\n+#     Args:\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#         tag2class (Dict[str, int], optional): mapping from label names into int\n+#         tag_column (str, optional): column with label names\n+#         class_column (str, optional): column to use for split\n+#\n+#     Returns:\n+#         tuple: tuple with 4 dataframes\n+#             whole dataframe, train part, valid part and infer part\n+#     \"\"\"\n+#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n+#\n+#     result_df = None\n+#     fold_dfs = {}\n+#     for fold_df, fold_name in zip(\n+#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n+#     ):\n+#         if fold_df is not None:\n+#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n+#             if args_are_not_none(tag2class, tag_column, class_column):\n+#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n+#             fold_dfs[fold_name] = fold_df\n+#\n+#             result_df = (\n+#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n+#             )\n+#\n+#     output = (\n+#         result_df,\n+#         fold_dfs.get(\"train\", None),\n+#         fold_dfs.get(\"valid\", None),\n+#         fold_dfs.get(\"infer\", None),\n+#     )\n+#\n+#     return output\n+\n+\n+# def read_csv_data(\n+#     in_csv: str = None,\n+#     train_folds: Optional[List[int]] = None,\n+#     valid_folds: Optional[List[int]] = None,\n+#     infer_folds: Optional[List[int]] = None,\n+#     seed: int = 42,\n+#     n_folds: int = 5,\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#     \"\"\"\n+#     From giving path ``in_csv`` reads a dataframe\n+#     and split it to train/valid/infer folds\n+#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n+#     reads independent folds.\n+#\n+#     .. note::\n+#        This function can be used with different combinations of params.\n+#         First block is used to get dataset from one `csv`:\n+#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n+#         Second includes paths to different csv for train/valid and infer parts:\n+#             in_csv_train, in_csv_valid, in_csv_infer\n+#         The other params (tag2class, tag_column, class_column) are optional\n+#             for any previous block\n+#\n+#     Args:\n+#         in_csv: paths to whole dataset\n+#         train_folds: train folds\n+#         valid_folds (List[int], optional): valid folds.\n+#             If none takes all folds not included in ``train_folds``\n+#         infer_folds (List[int], optional): infer folds.\n+#             If none takes all folds not included in ``train_folds``\n+#             and ``valid_folds``\n+#         seed: seed for split\n+#         n_folds: number of folds\n+#\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#\n+#         tag2class (Dict[str, int]): mapping from label names into ints\n+#         tag_column: column with label names\n+#         class_column: column to use for split\n+#\n+#     Returns:\n+#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#             tuple with 4 elements\n+#             (whole dataframe,\n+#             list with train data,\n+#             list with valid data\n+#             and list with infer data)\n+#     \"\"\"\n+#     from_one_df: bool = in_csv is not None\n+#     from_multiple_df: bool = (\n+#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n+#     )\n+#\n+#     if from_one_df == from_multiple_df:\n+#         raise ValueError(\n+#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n+#         )\n+#\n+#     if from_one_df:\n+#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n+#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n+#             dataframe,\n+#             train_folds=train_folds,\n+#             valid_folds=valid_folds,\n+#             infer_folds=infer_folds,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#             seed=seed,\n+#             n_folds=n_folds,\n+#         )\n+#     else:\n+#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n+#             in_csv_train=in_csv_train,\n+#             in_csv_valid=in_csv_valid,\n+#             in_csv_infer=in_csv_infer,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#         )\n+#\n+#     for data in [df_train, df_valid, df_infer]:\n+#         if data is not None and \"fold\" in data.columns:\n+#             del data[\"fold\"]\n+#\n+#     result = (\n+#         dataframe,\n+#         dataframe_to_list(df_train) if df_train is not None else None,\n+#         dataframe_to_list(df_valid) if df_valid is not None else None,\n+#         dataframe_to_list(df_infer) if df_infer is not None else None,\n+#     )\n+#\n+#     return result\n \n",
        "source_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\n<DED>def merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    <IND>\"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        <IND>for csv_path in paths.split(\",\"):\n            <IND>dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    <DED><DED>return result\n\n\n<DED>def read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    <IND>\"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        <IND>if fold_df is not None:\n            <IND>fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                <IND>fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            <DED>fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    <DED><DED>output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\n<DED>def read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    <IND>\"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        <IND>raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    <DED>if from_one_df:\n        <IND>dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    <DED>for data in [df_train, df_valid, df_infer]:\n        <IND>if data is not None and \"fold\" in data.columns:\n            <IND>del data[\"fold\"]\n\n    <DED><DED>result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/utils/pandas.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/utils/pandas.py",
    "file_hunks_size": 15,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/contrib/utils/pandas.py:540:4 Incompatible variable type [9]: class_column is declared to have type `str` but is used as type `None`.",
    "message": " class_column is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 540,
    "warning_line": "    class_column: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    if class_column is not None:\n        result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    else:\n        result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_len": 682,
        "target_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    if class_column is not None:\n        df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    else:\n        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_len": 562,
        "diff_format": "@@ -415,22 +394,15 @@\n     if args_are_not_none(tag2class, tag_column, class_column):\n-        dataframe = map_dataframe(\n-            dataframe, tag_column, class_column, tag2class\n-        )\n+        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n \n     if class_column is not None:\n-        result_dataframe = split_dataframe_on_stratified_folds(\n-            dataframe,\n-            class_column=class_column,\n-            random_state=seed,\n-            n_folds=n_folds,\n+        df_all = split_dataframe_on_stratified_folds(\n+            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n         )\n     else:\n-        result_dataframe = split_dataframe_on_folds(\n-            dataframe, random_state=seed, n_folds=n_folds\n-        )\n-\n-    fold_series = result_dataframe[\"fold\"]\n+        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n+\n+    fold_series = df_all[\"fold\"]\n \n     train_folds = folds_to_list(train_folds)\n-    df_train = result_dataframe[fold_series.isin(train_folds)]\n+    df_train = df_all[fold_series.isin(train_folds)]\n \n",
        "source_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    <DED>if class_column is not None:\n        <IND>result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    <DED>fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    <DED>if class_column is not None:\n        <IND>df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    <DED>fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\ndef merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    \"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        for csv_path in paths.split(\",\"):\n            dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    return result\n\n\ndef read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        if fold_df is not None:\n            fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\ndef read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    \"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    if from_one_df:\n        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    else:\n        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    for data in [df_train, df_valid, df_infer]:\n        if data is not None and \"fold\" in data.columns:\n            del data[\"fold\"]\n\n    result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_len": 6590,
        "target_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_len": 6715,
        "diff_format": "@@ -438,196 +410,183 @@\n         mask = ~fold_series.isin(train_folds)\n-        valid_folds = result_dataframe[mask][\"fold\"]\n+        valid_folds = df_all[mask][\"fold\"]\n \n     valid_folds = folds_to_list(valid_folds)\n-    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n+    df_valid = df_all[fold_series.isin(valid_folds)]\n \n     infer_folds = folds_to_list(infer_folds or [])\n-    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n-\n-    return result_dataframe, df_train, df_valid, df_infer\n-\n-\n-def merge_multiple_fold_csv(\n-    fold_name: str, paths: Optional[str]\n-) -> pd.DataFrame:\n-    \"\"\"Reads csv into one DataFrame with column ``fold``.\n-\n-    Args:\n-        fold_name: current fold name\n-        paths: paths to csv separated by commas\n-\n-    Returns:\n-         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n-    \"\"\"\n-    result = pd.DataFrame()\n-    if paths is not None:\n-        for csv_path in paths.split(\",\"):\n-            dataframe = pd.read_csv(csv_path)\n-            dataframe[\"fold\"] = fold_name\n-            result = result.append(dataframe, ignore_index=True)\n-\n-    return result\n-\n-\n-def read_multiple_dataframes(\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n-    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n-\n-    Args:\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-        tag2class (Dict[str, int], optional): mapping from label names into int\n-        tag_column (str, optional): column with label names\n-        class_column (str, optional): column to use for split\n-\n-    Returns:\n-        tuple: tuple with 4 dataframes\n-            whole dataframe, train part, valid part and infer part\n-    \"\"\"\n-    assert any(\n-        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n-    )\n-\n-    result_df = None\n-    fold_dfs = {}\n-    for fold_df, fold_name in zip(\n-        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n-    ):\n-        if fold_df is not None:\n-            fold_df = merge_multiple_fold_csv(\n-                fold_name=fold_name, paths=fold_df\n-            )\n-            if args_are_not_none(tag2class, tag_column, class_column):\n-                fold_df = map_dataframe(\n-                    fold_df, tag_column, class_column, tag2class\n-                )\n-            fold_dfs[fold_name] = fold_df\n-\n-            result_df = (\n-                fold_df\n-                if result_df is None\n-                else result_df.append(fold_df, ignore_index=True)\n-            )\n-\n-    output = (\n-        result_df,\n-        fold_dfs.get(\"train\", None),\n-        fold_dfs.get(\"valid\", None),\n-        fold_dfs.get(\"infer\", None),\n-    )\n-\n-    return output\n-\n-\n-def read_csv_data(\n-    in_csv: str = None,\n-    train_folds: Optional[List[int]] = None,\n-    valid_folds: Optional[List[int]] = None,\n-    infer_folds: Optional[List[int]] = None,\n-    seed: int = 42,\n-    n_folds: int = 5,\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-    \"\"\"\n-    From giving path ``in_csv`` reads a dataframe\n-    and split it to train/valid/infer folds\n-    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n-    reads independent folds.\n-\n-    .. note::\n-       This function can be used with different combinations of params.\n-        First block is used to get dataset from one `csv`:\n-            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n-        Second includes paths to different csv for train/valid and infer parts:\n-            in_csv_train, in_csv_valid, in_csv_infer\n-        The other params (tag2class, tag_column, class_column) are optional\n-            for any previous block\n-\n-    Args:\n-        in_csv: paths to whole dataset\n-        train_folds: train folds\n-        valid_folds (List[int], optional): valid folds.\n-            If none takes all folds not included in ``train_folds``\n-        infer_folds (List[int], optional): infer folds.\n-            If none takes all folds not included in ``train_folds``\n-            and ``valid_folds``\n-        seed: seed for split\n-        n_folds: number of folds\n-\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-\n-        tag2class (Dict[str, int]): mapping from label names into ints\n-        tag_column: column with label names\n-        class_column: column to use for split\n-\n-    Returns:\n-        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-            tuple with 4 elements\n-            (whole dataframe,\n-            list with train data,\n-            list with valid data\n-            and list with infer data)\n-    \"\"\"\n-    from_one_df: bool = in_csv is not None\n-    from_multiple_df: bool = (\n-        in_csv_train is not None\n-        or in_csv_valid is not None\n-        or in_csv_infer is not None\n-    )\n-\n-    if from_one_df == from_multiple_df:\n-        raise ValueError(\n-            \"You should pass `in_csv` \"\n-            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n-        )\n-\n-    if from_one_df:\n-        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n-        dataframe, df_train, df_valid, df_infer = split_dataframe(\n-            dataframe,\n-            train_folds=train_folds,\n-            valid_folds=valid_folds,\n-            infer_folds=infer_folds,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-            seed=seed,\n-            n_folds=n_folds,\n-        )\n-    else:\n-        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n-            in_csv_train=in_csv_train,\n-            in_csv_valid=in_csv_valid,\n-            in_csv_infer=in_csv_infer,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-        )\n-\n-    for data in [df_train, df_valid, df_infer]:\n-        if data is not None and \"fold\" in data.columns:\n-            del data[\"fold\"]\n-\n-    result = (\n-        dataframe,\n-        dataframe_to_list(df_train) if df_train is not None else None,\n-        dataframe_to_list(df_valid) if df_valid is not None else None,\n-        dataframe_to_list(df_infer) if df_infer is not None else None,\n-    )\n-\n-    return result\n+    df_infer = df_all[fold_series.isin(infer_folds)]\n+\n+    return df_all, df_train, df_valid, df_infer\n+\n+\n+# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n+#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n+#\n+#     Args:\n+#         fold_name: current fold name\n+#         paths: paths to csv separated by commas\n+#\n+#     Returns:\n+#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n+#     \"\"\"\n+#     result = pd.DataFrame()\n+#     if paths is not None:\n+#         for csv_path in paths.split(\",\"):\n+#             dataframe = pd.read_csv(csv_path)\n+#             dataframe[\"fold\"] = fold_name\n+#             result = result.append(dataframe, ignore_index=True)\n+#\n+#     return result\n+\n+\n+# def read_multiple_dataframes(\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n+#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n+#\n+#     Args:\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#         tag2class (Dict[str, int], optional): mapping from label names into int\n+#         tag_column (str, optional): column with label names\n+#         class_column (str, optional): column to use for split\n+#\n+#     Returns:\n+#         tuple: tuple with 4 dataframes\n+#             whole dataframe, train part, valid part and infer part\n+#     \"\"\"\n+#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n+#\n+#     result_df = None\n+#     fold_dfs = {}\n+#     for fold_df, fold_name in zip(\n+#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n+#     ):\n+#         if fold_df is not None:\n+#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n+#             if args_are_not_none(tag2class, tag_column, class_column):\n+#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n+#             fold_dfs[fold_name] = fold_df\n+#\n+#             result_df = (\n+#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n+#             )\n+#\n+#     output = (\n+#         result_df,\n+#         fold_dfs.get(\"train\", None),\n+#         fold_dfs.get(\"valid\", None),\n+#         fold_dfs.get(\"infer\", None),\n+#     )\n+#\n+#     return output\n+\n+\n+# def read_csv_data(\n+#     in_csv: str = None,\n+#     train_folds: Optional[List[int]] = None,\n+#     valid_folds: Optional[List[int]] = None,\n+#     infer_folds: Optional[List[int]] = None,\n+#     seed: int = 42,\n+#     n_folds: int = 5,\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#     \"\"\"\n+#     From giving path ``in_csv`` reads a dataframe\n+#     and split it to train/valid/infer folds\n+#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n+#     reads independent folds.\n+#\n+#     .. note::\n+#        This function can be used with different combinations of params.\n+#         First block is used to get dataset from one `csv`:\n+#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n+#         Second includes paths to different csv for train/valid and infer parts:\n+#             in_csv_train, in_csv_valid, in_csv_infer\n+#         The other params (tag2class, tag_column, class_column) are optional\n+#             for any previous block\n+#\n+#     Args:\n+#         in_csv: paths to whole dataset\n+#         train_folds: train folds\n+#         valid_folds (List[int], optional): valid folds.\n+#             If none takes all folds not included in ``train_folds``\n+#         infer_folds (List[int], optional): infer folds.\n+#             If none takes all folds not included in ``train_folds``\n+#             and ``valid_folds``\n+#         seed: seed for split\n+#         n_folds: number of folds\n+#\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#\n+#         tag2class (Dict[str, int]): mapping from label names into ints\n+#         tag_column: column with label names\n+#         class_column: column to use for split\n+#\n+#     Returns:\n+#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#             tuple with 4 elements\n+#             (whole dataframe,\n+#             list with train data,\n+#             list with valid data\n+#             and list with infer data)\n+#     \"\"\"\n+#     from_one_df: bool = in_csv is not None\n+#     from_multiple_df: bool = (\n+#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n+#     )\n+#\n+#     if from_one_df == from_multiple_df:\n+#         raise ValueError(\n+#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n+#         )\n+#\n+#     if from_one_df:\n+#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n+#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n+#             dataframe,\n+#             train_folds=train_folds,\n+#             valid_folds=valid_folds,\n+#             infer_folds=infer_folds,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#             seed=seed,\n+#             n_folds=n_folds,\n+#         )\n+#     else:\n+#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n+#             in_csv_train=in_csv_train,\n+#             in_csv_valid=in_csv_valid,\n+#             in_csv_infer=in_csv_infer,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#         )\n+#\n+#     for data in [df_train, df_valid, df_infer]:\n+#         if data is not None and \"fold\" in data.columns:\n+#             del data[\"fold\"]\n+#\n+#     result = (\n+#         dataframe,\n+#         dataframe_to_list(df_train) if df_train is not None else None,\n+#         dataframe_to_list(df_valid) if df_valid is not None else None,\n+#         dataframe_to_list(df_infer) if df_infer is not None else None,\n+#     )\n+#\n+#     return result\n \n",
        "source_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\n<DED>def merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    <IND>\"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        <IND>for csv_path in paths.split(\",\"):\n            <IND>dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    <DED><DED>return result\n\n\n<DED>def read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    <IND>\"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        <IND>if fold_df is not None:\n            <IND>fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                <IND>fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            <DED>fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    <DED><DED>output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\n<DED>def read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    <IND>\"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        <IND>raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    <DED>if from_one_df:\n        <IND>dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    <DED>for data in [df_train, df_valid, df_infer]:\n        <IND>if data is not None and \"fold\" in data.columns:\n            <IND>del data[\"fold\"]\n\n    <DED><DED>result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/utils/pandas.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/utils/pandas.py",
    "file_hunks_size": 15,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/contrib/utils/pandas.py:541:4 Incompatible variable type [9]: tag_column is declared to have type `str` but is used as type `None`.",
    "message": " tag_column is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 541,
    "warning_line": "    tag_column: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    if class_column is not None:\n        result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    else:\n        result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_len": 682,
        "target_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    if class_column is not None:\n        df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    else:\n        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_len": 562,
        "diff_format": "@@ -415,22 +394,15 @@\n     if args_are_not_none(tag2class, tag_column, class_column):\n-        dataframe = map_dataframe(\n-            dataframe, tag_column, class_column, tag2class\n-        )\n+        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n \n     if class_column is not None:\n-        result_dataframe = split_dataframe_on_stratified_folds(\n-            dataframe,\n-            class_column=class_column,\n-            random_state=seed,\n-            n_folds=n_folds,\n+        df_all = split_dataframe_on_stratified_folds(\n+            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n         )\n     else:\n-        result_dataframe = split_dataframe_on_folds(\n-            dataframe, random_state=seed, n_folds=n_folds\n-        )\n-\n-    fold_series = result_dataframe[\"fold\"]\n+        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n+\n+    fold_series = df_all[\"fold\"]\n \n     train_folds = folds_to_list(train_folds)\n-    df_train = result_dataframe[fold_series.isin(train_folds)]\n+    df_train = df_all[fold_series.isin(train_folds)]\n \n",
        "source_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    <DED>if class_column is not None:\n        <IND>result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    <DED>fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    <DED>if class_column is not None:\n        <IND>df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    <DED>fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\ndef merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    \"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        for csv_path in paths.split(\",\"):\n            dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    return result\n\n\ndef read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        if fold_df is not None:\n            fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\ndef read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    \"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    if from_one_df:\n        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    else:\n        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    for data in [df_train, df_valid, df_infer]:\n        if data is not None and \"fold\" in data.columns:\n            del data[\"fold\"]\n\n    result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_len": 6590,
        "target_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_len": 6715,
        "diff_format": "@@ -438,196 +410,183 @@\n         mask = ~fold_series.isin(train_folds)\n-        valid_folds = result_dataframe[mask][\"fold\"]\n+        valid_folds = df_all[mask][\"fold\"]\n \n     valid_folds = folds_to_list(valid_folds)\n-    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n+    df_valid = df_all[fold_series.isin(valid_folds)]\n \n     infer_folds = folds_to_list(infer_folds or [])\n-    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n-\n-    return result_dataframe, df_train, df_valid, df_infer\n-\n-\n-def merge_multiple_fold_csv(\n-    fold_name: str, paths: Optional[str]\n-) -> pd.DataFrame:\n-    \"\"\"Reads csv into one DataFrame with column ``fold``.\n-\n-    Args:\n-        fold_name: current fold name\n-        paths: paths to csv separated by commas\n-\n-    Returns:\n-         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n-    \"\"\"\n-    result = pd.DataFrame()\n-    if paths is not None:\n-        for csv_path in paths.split(\",\"):\n-            dataframe = pd.read_csv(csv_path)\n-            dataframe[\"fold\"] = fold_name\n-            result = result.append(dataframe, ignore_index=True)\n-\n-    return result\n-\n-\n-def read_multiple_dataframes(\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n-    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n-\n-    Args:\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-        tag2class (Dict[str, int], optional): mapping from label names into int\n-        tag_column (str, optional): column with label names\n-        class_column (str, optional): column to use for split\n-\n-    Returns:\n-        tuple: tuple with 4 dataframes\n-            whole dataframe, train part, valid part and infer part\n-    \"\"\"\n-    assert any(\n-        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n-    )\n-\n-    result_df = None\n-    fold_dfs = {}\n-    for fold_df, fold_name in zip(\n-        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n-    ):\n-        if fold_df is not None:\n-            fold_df = merge_multiple_fold_csv(\n-                fold_name=fold_name, paths=fold_df\n-            )\n-            if args_are_not_none(tag2class, tag_column, class_column):\n-                fold_df = map_dataframe(\n-                    fold_df, tag_column, class_column, tag2class\n-                )\n-            fold_dfs[fold_name] = fold_df\n-\n-            result_df = (\n-                fold_df\n-                if result_df is None\n-                else result_df.append(fold_df, ignore_index=True)\n-            )\n-\n-    output = (\n-        result_df,\n-        fold_dfs.get(\"train\", None),\n-        fold_dfs.get(\"valid\", None),\n-        fold_dfs.get(\"infer\", None),\n-    )\n-\n-    return output\n-\n-\n-def read_csv_data(\n-    in_csv: str = None,\n-    train_folds: Optional[List[int]] = None,\n-    valid_folds: Optional[List[int]] = None,\n-    infer_folds: Optional[List[int]] = None,\n-    seed: int = 42,\n-    n_folds: int = 5,\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-    \"\"\"\n-    From giving path ``in_csv`` reads a dataframe\n-    and split it to train/valid/infer folds\n-    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n-    reads independent folds.\n-\n-    .. note::\n-       This function can be used with different combinations of params.\n-        First block is used to get dataset from one `csv`:\n-            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n-        Second includes paths to different csv for train/valid and infer parts:\n-            in_csv_train, in_csv_valid, in_csv_infer\n-        The other params (tag2class, tag_column, class_column) are optional\n-            for any previous block\n-\n-    Args:\n-        in_csv: paths to whole dataset\n-        train_folds: train folds\n-        valid_folds (List[int], optional): valid folds.\n-            If none takes all folds not included in ``train_folds``\n-        infer_folds (List[int], optional): infer folds.\n-            If none takes all folds not included in ``train_folds``\n-            and ``valid_folds``\n-        seed: seed for split\n-        n_folds: number of folds\n-\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-\n-        tag2class (Dict[str, int]): mapping from label names into ints\n-        tag_column: column with label names\n-        class_column: column to use for split\n-\n-    Returns:\n-        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-            tuple with 4 elements\n-            (whole dataframe,\n-            list with train data,\n-            list with valid data\n-            and list with infer data)\n-    \"\"\"\n-    from_one_df: bool = in_csv is not None\n-    from_multiple_df: bool = (\n-        in_csv_train is not None\n-        or in_csv_valid is not None\n-        or in_csv_infer is not None\n-    )\n-\n-    if from_one_df == from_multiple_df:\n-        raise ValueError(\n-            \"You should pass `in_csv` \"\n-            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n-        )\n-\n-    if from_one_df:\n-        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n-        dataframe, df_train, df_valid, df_infer = split_dataframe(\n-            dataframe,\n-            train_folds=train_folds,\n-            valid_folds=valid_folds,\n-            infer_folds=infer_folds,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-            seed=seed,\n-            n_folds=n_folds,\n-        )\n-    else:\n-        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n-            in_csv_train=in_csv_train,\n-            in_csv_valid=in_csv_valid,\n-            in_csv_infer=in_csv_infer,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-        )\n-\n-    for data in [df_train, df_valid, df_infer]:\n-        if data is not None and \"fold\" in data.columns:\n-            del data[\"fold\"]\n-\n-    result = (\n-        dataframe,\n-        dataframe_to_list(df_train) if df_train is not None else None,\n-        dataframe_to_list(df_valid) if df_valid is not None else None,\n-        dataframe_to_list(df_infer) if df_infer is not None else None,\n-    )\n-\n-    return result\n+    df_infer = df_all[fold_series.isin(infer_folds)]\n+\n+    return df_all, df_train, df_valid, df_infer\n+\n+\n+# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n+#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n+#\n+#     Args:\n+#         fold_name: current fold name\n+#         paths: paths to csv separated by commas\n+#\n+#     Returns:\n+#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n+#     \"\"\"\n+#     result = pd.DataFrame()\n+#     if paths is not None:\n+#         for csv_path in paths.split(\",\"):\n+#             dataframe = pd.read_csv(csv_path)\n+#             dataframe[\"fold\"] = fold_name\n+#             result = result.append(dataframe, ignore_index=True)\n+#\n+#     return result\n+\n+\n+# def read_multiple_dataframes(\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n+#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n+#\n+#     Args:\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#         tag2class (Dict[str, int], optional): mapping from label names into int\n+#         tag_column (str, optional): column with label names\n+#         class_column (str, optional): column to use for split\n+#\n+#     Returns:\n+#         tuple: tuple with 4 dataframes\n+#             whole dataframe, train part, valid part and infer part\n+#     \"\"\"\n+#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n+#\n+#     result_df = None\n+#     fold_dfs = {}\n+#     for fold_df, fold_name in zip(\n+#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n+#     ):\n+#         if fold_df is not None:\n+#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n+#             if args_are_not_none(tag2class, tag_column, class_column):\n+#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n+#             fold_dfs[fold_name] = fold_df\n+#\n+#             result_df = (\n+#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n+#             )\n+#\n+#     output = (\n+#         result_df,\n+#         fold_dfs.get(\"train\", None),\n+#         fold_dfs.get(\"valid\", None),\n+#         fold_dfs.get(\"infer\", None),\n+#     )\n+#\n+#     return output\n+\n+\n+# def read_csv_data(\n+#     in_csv: str = None,\n+#     train_folds: Optional[List[int]] = None,\n+#     valid_folds: Optional[List[int]] = None,\n+#     infer_folds: Optional[List[int]] = None,\n+#     seed: int = 42,\n+#     n_folds: int = 5,\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#     \"\"\"\n+#     From giving path ``in_csv`` reads a dataframe\n+#     and split it to train/valid/infer folds\n+#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n+#     reads independent folds.\n+#\n+#     .. note::\n+#        This function can be used with different combinations of params.\n+#         First block is used to get dataset from one `csv`:\n+#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n+#         Second includes paths to different csv for train/valid and infer parts:\n+#             in_csv_train, in_csv_valid, in_csv_infer\n+#         The other params (tag2class, tag_column, class_column) are optional\n+#             for any previous block\n+#\n+#     Args:\n+#         in_csv: paths to whole dataset\n+#         train_folds: train folds\n+#         valid_folds (List[int], optional): valid folds.\n+#             If none takes all folds not included in ``train_folds``\n+#         infer_folds (List[int], optional): infer folds.\n+#             If none takes all folds not included in ``train_folds``\n+#             and ``valid_folds``\n+#         seed: seed for split\n+#         n_folds: number of folds\n+#\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#\n+#         tag2class (Dict[str, int]): mapping from label names into ints\n+#         tag_column: column with label names\n+#         class_column: column to use for split\n+#\n+#     Returns:\n+#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#             tuple with 4 elements\n+#             (whole dataframe,\n+#             list with train data,\n+#             list with valid data\n+#             and list with infer data)\n+#     \"\"\"\n+#     from_one_df: bool = in_csv is not None\n+#     from_multiple_df: bool = (\n+#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n+#     )\n+#\n+#     if from_one_df == from_multiple_df:\n+#         raise ValueError(\n+#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n+#         )\n+#\n+#     if from_one_df:\n+#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n+#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n+#             dataframe,\n+#             train_folds=train_folds,\n+#             valid_folds=valid_folds,\n+#             infer_folds=infer_folds,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#             seed=seed,\n+#             n_folds=n_folds,\n+#         )\n+#     else:\n+#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n+#             in_csv_train=in_csv_train,\n+#             in_csv_valid=in_csv_valid,\n+#             in_csv_infer=in_csv_infer,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#         )\n+#\n+#     for data in [df_train, df_valid, df_infer]:\n+#         if data is not None and \"fold\" in data.columns:\n+#             del data[\"fold\"]\n+#\n+#     result = (\n+#         dataframe,\n+#         dataframe_to_list(df_train) if df_train is not None else None,\n+#         dataframe_to_list(df_valid) if df_valid is not None else None,\n+#         dataframe_to_list(df_infer) if df_infer is not None else None,\n+#     )\n+#\n+#     return result\n \n",
        "source_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\n<DED>def merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    <IND>\"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        <IND>for csv_path in paths.split(\",\"):\n            <IND>dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    <DED><DED>return result\n\n\n<DED>def read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    <IND>\"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        <IND>if fold_df is not None:\n            <IND>fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                <IND>fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            <DED>fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    <DED><DED>output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\n<DED>def read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    <IND>\"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        <IND>raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    <DED>if from_one_df:\n        <IND>dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    <DED>for data in [df_train, df_valid, df_infer]:\n        <IND>if data is not None and \"fold\" in data.columns:\n            <IND>del data[\"fold\"]\n\n    <DED><DED>result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/utils/pandas.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/utils/pandas.py",
    "file_hunks_size": 15,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/contrib/utils/pandas.py:602:12 Incompatible parameter type [6]: Expected `List[int]` for 2nd parameter `train_folds` to call `split_dataframe` but got `Optional[List[int]]`.",
    "message": " Expected `List[int]` for 2nd parameter `train_folds` to call `split_dataframe` but got `Optional[List[int]]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 602,
    "warning_line": "            train_folds=train_folds,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    if class_column is not None:\n        result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    else:\n        result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_len": 682,
        "target_code": "    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    if class_column is not None:\n        df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    else:\n        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_len": 562,
        "diff_format": "@@ -415,22 +394,15 @@\n     if args_are_not_none(tag2class, tag_column, class_column):\n-        dataframe = map_dataframe(\n-            dataframe, tag_column, class_column, tag2class\n-        )\n+        dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n \n     if class_column is not None:\n-        result_dataframe = split_dataframe_on_stratified_folds(\n-            dataframe,\n-            class_column=class_column,\n-            random_state=seed,\n-            n_folds=n_folds,\n+        df_all = split_dataframe_on_stratified_folds(\n+            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n         )\n     else:\n-        result_dataframe = split_dataframe_on_folds(\n-            dataframe, random_state=seed, n_folds=n_folds\n-        )\n-\n-    fold_series = result_dataframe[\"fold\"]\n+        df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n+\n+    fold_series = df_all[\"fold\"]\n \n     train_folds = folds_to_list(train_folds)\n-    df_train = result_dataframe[fold_series.isin(train_folds)]\n+    df_train = df_all[fold_series.isin(train_folds)]\n \n",
        "source_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    <DED>if class_column is not None:\n        <IND>result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    <DED>fold_series = result_dataframe[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    if args_are_not_none(tag2class, tag_column, class_column):\n        <IND>dataframe = map_dataframe(dataframe, tag_column, class_column, tag2class)\n\n    <DED>if class_column is not None:\n        <IND>df_all = split_dataframe_on_stratified_folds(\n            dataframe, class_column=class_column, random_state=seed, n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>df_all = split_dataframe_on_folds(dataframe, random_state=seed, n_folds=n_folds)\n\n    <DED>fold_series = df_all[\"fold\"]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = df_all[fold_series.isin(train_folds)]\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\ndef merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    \"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        for csv_path in paths.split(\",\"):\n            dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    return result\n\n\ndef read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        if fold_df is not None:\n            fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\ndef read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    \"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    if from_one_df:\n        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    else:\n        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    for data in [df_train, df_valid, df_infer]:\n        if data is not None and \"fold\" in data.columns:\n            del data[\"fold\"]\n\n    result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_len": 6590,
        "target_code": "        mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_len": 6715,
        "diff_format": "@@ -438,196 +410,183 @@\n         mask = ~fold_series.isin(train_folds)\n-        valid_folds = result_dataframe[mask][\"fold\"]\n+        valid_folds = df_all[mask][\"fold\"]\n \n     valid_folds = folds_to_list(valid_folds)\n-    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n+    df_valid = df_all[fold_series.isin(valid_folds)]\n \n     infer_folds = folds_to_list(infer_folds or [])\n-    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n-\n-    return result_dataframe, df_train, df_valid, df_infer\n-\n-\n-def merge_multiple_fold_csv(\n-    fold_name: str, paths: Optional[str]\n-) -> pd.DataFrame:\n-    \"\"\"Reads csv into one DataFrame with column ``fold``.\n-\n-    Args:\n-        fold_name: current fold name\n-        paths: paths to csv separated by commas\n-\n-    Returns:\n-         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n-    \"\"\"\n-    result = pd.DataFrame()\n-    if paths is not None:\n-        for csv_path in paths.split(\",\"):\n-            dataframe = pd.read_csv(csv_path)\n-            dataframe[\"fold\"] = fold_name\n-            result = result.append(dataframe, ignore_index=True)\n-\n-    return result\n-\n-\n-def read_multiple_dataframes(\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n-    \"\"\"This function reads train/valid/infer dataframes from giving paths.\n-\n-    Args:\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-        tag2class (Dict[str, int], optional): mapping from label names into int\n-        tag_column (str, optional): column with label names\n-        class_column (str, optional): column to use for split\n-\n-    Returns:\n-        tuple: tuple with 4 dataframes\n-            whole dataframe, train part, valid part and infer part\n-    \"\"\"\n-    assert any(\n-        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n-    )\n-\n-    result_df = None\n-    fold_dfs = {}\n-    for fold_df, fold_name in zip(\n-        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n-    ):\n-        if fold_df is not None:\n-            fold_df = merge_multiple_fold_csv(\n-                fold_name=fold_name, paths=fold_df\n-            )\n-            if args_are_not_none(tag2class, tag_column, class_column):\n-                fold_df = map_dataframe(\n-                    fold_df, tag_column, class_column, tag2class\n-                )\n-            fold_dfs[fold_name] = fold_df\n-\n-            result_df = (\n-                fold_df\n-                if result_df is None\n-                else result_df.append(fold_df, ignore_index=True)\n-            )\n-\n-    output = (\n-        result_df,\n-        fold_dfs.get(\"train\", None),\n-        fold_dfs.get(\"valid\", None),\n-        fold_dfs.get(\"infer\", None),\n-    )\n-\n-    return output\n-\n-\n-def read_csv_data(\n-    in_csv: str = None,\n-    train_folds: Optional[List[int]] = None,\n-    valid_folds: Optional[List[int]] = None,\n-    infer_folds: Optional[List[int]] = None,\n-    seed: int = 42,\n-    n_folds: int = 5,\n-    in_csv_train: str = None,\n-    in_csv_valid: str = None,\n-    in_csv_infer: str = None,\n-    tag2class: Optional[Dict[str, int]] = None,\n-    class_column: str = None,\n-    tag_column: str = None,\n-) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-    \"\"\"\n-    From giving path ``in_csv`` reads a dataframe\n-    and split it to train/valid/infer folds\n-    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n-    reads independent folds.\n-\n-    .. note::\n-       This function can be used with different combinations of params.\n-        First block is used to get dataset from one `csv`:\n-            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n-        Second includes paths to different csv for train/valid and infer parts:\n-            in_csv_train, in_csv_valid, in_csv_infer\n-        The other params (tag2class, tag_column, class_column) are optional\n-            for any previous block\n-\n-    Args:\n-        in_csv: paths to whole dataset\n-        train_folds: train folds\n-        valid_folds (List[int], optional): valid folds.\n-            If none takes all folds not included in ``train_folds``\n-        infer_folds (List[int], optional): infer folds.\n-            If none takes all folds not included in ``train_folds``\n-            and ``valid_folds``\n-        seed: seed for split\n-        n_folds: number of folds\n-\n-        in_csv_train: paths to train csv separated by commas\n-        in_csv_valid: paths to valid csv separated by commas\n-        in_csv_infer: paths to infer csv separated by commas\n-\n-        tag2class (Dict[str, int]): mapping from label names into ints\n-        tag_column: column with label names\n-        class_column: column to use for split\n-\n-    Returns:\n-        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n-            tuple with 4 elements\n-            (whole dataframe,\n-            list with train data,\n-            list with valid data\n-            and list with infer data)\n-    \"\"\"\n-    from_one_df: bool = in_csv is not None\n-    from_multiple_df: bool = (\n-        in_csv_train is not None\n-        or in_csv_valid is not None\n-        or in_csv_infer is not None\n-    )\n-\n-    if from_one_df == from_multiple_df:\n-        raise ValueError(\n-            \"You should pass `in_csv` \"\n-            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n-        )\n-\n-    if from_one_df:\n-        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n-        dataframe, df_train, df_valid, df_infer = split_dataframe(\n-            dataframe,\n-            train_folds=train_folds,\n-            valid_folds=valid_folds,\n-            infer_folds=infer_folds,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-            seed=seed,\n-            n_folds=n_folds,\n-        )\n-    else:\n-        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n-            in_csv_train=in_csv_train,\n-            in_csv_valid=in_csv_valid,\n-            in_csv_infer=in_csv_infer,\n-            tag2class=tag2class,\n-            class_column=class_column,\n-            tag_column=tag_column,\n-        )\n-\n-    for data in [df_train, df_valid, df_infer]:\n-        if data is not None and \"fold\" in data.columns:\n-            del data[\"fold\"]\n-\n-    result = (\n-        dataframe,\n-        dataframe_to_list(df_train) if df_train is not None else None,\n-        dataframe_to_list(df_valid) if df_valid is not None else None,\n-        dataframe_to_list(df_infer) if df_infer is not None else None,\n-    )\n-\n-    return result\n+    df_infer = df_all[fold_series.isin(infer_folds)]\n+\n+    return df_all, df_train, df_valid, df_infer\n+\n+\n+# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n+#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n+#\n+#     Args:\n+#         fold_name: current fold name\n+#         paths: paths to csv separated by commas\n+#\n+#     Returns:\n+#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n+#     \"\"\"\n+#     result = pd.DataFrame()\n+#     if paths is not None:\n+#         for csv_path in paths.split(\",\"):\n+#             dataframe = pd.read_csv(csv_path)\n+#             dataframe[\"fold\"] = fold_name\n+#             result = result.append(dataframe, ignore_index=True)\n+#\n+#     return result\n+\n+\n+# def read_multiple_dataframes(\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n+#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n+#\n+#     Args:\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#         tag2class (Dict[str, int], optional): mapping from label names into int\n+#         tag_column (str, optional): column with label names\n+#         class_column (str, optional): column to use for split\n+#\n+#     Returns:\n+#         tuple: tuple with 4 dataframes\n+#             whole dataframe, train part, valid part and infer part\n+#     \"\"\"\n+#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n+#\n+#     result_df = None\n+#     fold_dfs = {}\n+#     for fold_df, fold_name in zip(\n+#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n+#     ):\n+#         if fold_df is not None:\n+#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n+#             if args_are_not_none(tag2class, tag_column, class_column):\n+#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n+#             fold_dfs[fold_name] = fold_df\n+#\n+#             result_df = (\n+#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n+#             )\n+#\n+#     output = (\n+#         result_df,\n+#         fold_dfs.get(\"train\", None),\n+#         fold_dfs.get(\"valid\", None),\n+#         fold_dfs.get(\"infer\", None),\n+#     )\n+#\n+#     return output\n+\n+\n+# def read_csv_data(\n+#     in_csv: str = None,\n+#     train_folds: Optional[List[int]] = None,\n+#     valid_folds: Optional[List[int]] = None,\n+#     infer_folds: Optional[List[int]] = None,\n+#     seed: int = 42,\n+#     n_folds: int = 5,\n+#     in_csv_train: str = None,\n+#     in_csv_valid: str = None,\n+#     in_csv_infer: str = None,\n+#     tag2class: Optional[Dict[str, int]] = None,\n+#     class_column: str = None,\n+#     tag_column: str = None,\n+# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#     \"\"\"\n+#     From giving path ``in_csv`` reads a dataframe\n+#     and split it to train/valid/infer folds\n+#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n+#     reads independent folds.\n+#\n+#     .. note::\n+#        This function can be used with different combinations of params.\n+#         First block is used to get dataset from one `csv`:\n+#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n+#         Second includes paths to different csv for train/valid and infer parts:\n+#             in_csv_train, in_csv_valid, in_csv_infer\n+#         The other params (tag2class, tag_column, class_column) are optional\n+#             for any previous block\n+#\n+#     Args:\n+#         in_csv: paths to whole dataset\n+#         train_folds: train folds\n+#         valid_folds (List[int], optional): valid folds.\n+#             If none takes all folds not included in ``train_folds``\n+#         infer_folds (List[int], optional): infer folds.\n+#             If none takes all folds not included in ``train_folds``\n+#             and ``valid_folds``\n+#         seed: seed for split\n+#         n_folds: number of folds\n+#\n+#         in_csv_train: paths to train csv separated by commas\n+#         in_csv_valid: paths to valid csv separated by commas\n+#         in_csv_infer: paths to infer csv separated by commas\n+#\n+#         tag2class (Dict[str, int]): mapping from label names into ints\n+#         tag_column: column with label names\n+#         class_column: column to use for split\n+#\n+#     Returns:\n+#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n+#             tuple with 4 elements\n+#             (whole dataframe,\n+#             list with train data,\n+#             list with valid data\n+#             and list with infer data)\n+#     \"\"\"\n+#     from_one_df: bool = in_csv is not None\n+#     from_multiple_df: bool = (\n+#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n+#     )\n+#\n+#     if from_one_df == from_multiple_df:\n+#         raise ValueError(\n+#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n+#         )\n+#\n+#     if from_one_df:\n+#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n+#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n+#             dataframe,\n+#             train_folds=train_folds,\n+#             valid_folds=valid_folds,\n+#             infer_folds=infer_folds,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#             seed=seed,\n+#             n_folds=n_folds,\n+#         )\n+#     else:\n+#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n+#             in_csv_train=in_csv_train,\n+#             in_csv_valid=in_csv_valid,\n+#             in_csv_infer=in_csv_infer,\n+#             tag2class=tag2class,\n+#             class_column=class_column,\n+#             tag_column=tag_column,\n+#         )\n+#\n+#     for data in [df_train, df_valid, df_infer]:\n+#         if data is not None and \"fold\" in data.columns:\n+#             del data[\"fold\"]\n+#\n+#     result = (\n+#         dataframe,\n+#         dataframe_to_list(df_train) if df_train is not None else None,\n+#         dataframe_to_list(df_valid) if df_valid is not None else None,\n+#         dataframe_to_list(df_infer) if df_infer is not None else None,\n+#     )\n+#\n+#     return result\n \n",
        "source_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\n<DED>def merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    <IND>\"\"\"Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name: current fold name\n        paths: paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    \"\"\"\n    result = pd.DataFrame()\n    if paths is not None:\n        <IND>for csv_path in paths.split(\",\"):\n            <IND>dataframe = pd.read_csv(csv_path)\n            dataframe[\"fold\"] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    <DED><DED>return result\n\n\n<DED>def read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    <IND>\"\"\"This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        tuple: tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    \"\"\"\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n    ):\n        <IND>if fold_df is not None:\n            <IND>fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                <IND>fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            <DED>fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    <DED><DED>output = (\n        result_df,\n        fold_dfs.get(\"train\", None),\n        fold_dfs.get(\"valid\", None),\n        fold_dfs.get(\"infer\", None),\n    )\n\n    return output\n\n\n<DED>def read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    <IND>\"\"\"\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv: paths to whole dataset\n        train_folds: train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed: seed for split\n        n_folds: number of folds\n\n        in_csv_train: paths to train csv separated by commas\n        in_csv_valid: paths to valid csv separated by commas\n        in_csv_infer: paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column: column with label names\n        class_column: column to use for split\n\n    Returns:\n        Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    \"\"\"\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        <IND>raise ValueError(\n            \"You should pass `in_csv` \"\n            \"or `in_csv_train` with `in_csv_valid` but not both!\"\n        )\n\n    <DED>if from_one_df:\n        <IND>dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    <DED>else:\n        <IND>dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    <DED>for data in [df_train, df_valid, df_infer]:\n        <IND>if data is not None and \"fold\" in data.columns:\n            <IND>del data[\"fold\"]\n\n    <DED><DED>result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <IND>mask = ~fold_series.isin(train_folds)\n        valid_folds = df_all[mask][\"fold\"]\n\n    <DED>valid_folds = folds_to_list(valid_folds)\n    df_valid = df_all[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = df_all[fold_series.isin(infer_folds)]\n\n    return df_all, df_train, df_valid, df_infer\n\n\n# def merge_multiple_fold_csv(fold_name: str, paths: Optional[str]) -> pd.DataFrame:\n#     \"\"\"Reads csv into one DataFrame with column ``fold``.\n#\n#     Args:\n#         fold_name: current fold name\n#         paths: paths to csv separated by commas\n#\n#     Returns:\n#          pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n#     \"\"\"\n#     result = pd.DataFrame()\n#     if paths is not None:\n#         for csv_path in paths.split(\",\"):\n#             dataframe = pd.read_csv(csv_path)\n#             dataframe[\"fold\"] = fold_name\n#             result = result.append(dataframe, ignore_index=True)\n#\n#     return result\n\n\n# def read_multiple_dataframes(\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n#     \"\"\"This function reads train/valid/infer dataframes from giving paths.\n#\n#     Args:\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#         tag2class (Dict[str, int], optional): mapping from label names into int\n#         tag_column (str, optional): column with label names\n#         class_column (str, optional): column to use for split\n#\n#     Returns:\n#         tuple: tuple with 4 dataframes\n#             whole dataframe, train part, valid part and infer part\n#     \"\"\"\n#     assert any(x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer))\n#\n#     result_df = None\n#     fold_dfs = {}\n#     for fold_df, fold_name in zip(\n#         (in_csv_train, in_csv_valid, in_csv_infer), (\"train\", \"valid\", \"infer\")\n#     ):\n#         if fold_df is not None:\n#             fold_df = merge_multiple_fold_csv(fold_name=fold_name, paths=fold_df)\n#             if args_are_not_none(tag2class, tag_column, class_column):\n#                 fold_df = map_dataframe(fold_df, tag_column, class_column, tag2class)\n#             fold_dfs[fold_name] = fold_df\n#\n#             result_df = (\n#                 fold_df if result_df is None else result_df.append(fold_df, ignore_index=True)\n#             )\n#\n#     output = (\n#         result_df,\n#         fold_dfs.get(\"train\", None),\n#         fold_dfs.get(\"valid\", None),\n#         fold_dfs.get(\"infer\", None),\n#     )\n#\n#     return output\n\n\n# def read_csv_data(\n#     in_csv: str = None,\n#     train_folds: Optional[List[int]] = None,\n#     valid_folds: Optional[List[int]] = None,\n#     infer_folds: Optional[List[int]] = None,\n#     seed: int = 42,\n#     n_folds: int = 5,\n#     in_csv_train: str = None,\n#     in_csv_valid: str = None,\n#     in_csv_infer: str = None,\n#     tag2class: Optional[Dict[str, int]] = None,\n#     class_column: str = None,\n#     tag_column: str = None,\n# ) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#     \"\"\"\n#     From giving path ``in_csv`` reads a dataframe\n#     and split it to train/valid/infer folds\n#     or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n#     reads independent folds.\n#\n#     .. note::\n#        This function can be used with different combinations of params.\n#         First block is used to get dataset from one `csv`:\n#             in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n#         Second includes paths to different csv for train/valid and infer parts:\n#             in_csv_train, in_csv_valid, in_csv_infer\n#         The other params (tag2class, tag_column, class_column) are optional\n#             for any previous block\n#\n#     Args:\n#         in_csv: paths to whole dataset\n#         train_folds: train folds\n#         valid_folds (List[int], optional): valid folds.\n#             If none takes all folds not included in ``train_folds``\n#         infer_folds (List[int], optional): infer folds.\n#             If none takes all folds not included in ``train_folds``\n#             and ``valid_folds``\n#         seed: seed for split\n#         n_folds: number of folds\n#\n#         in_csv_train: paths to train csv separated by commas\n#         in_csv_valid: paths to valid csv separated by commas\n#         in_csv_infer: paths to infer csv separated by commas\n#\n#         tag2class (Dict[str, int]): mapping from label names into ints\n#         tag_column: column with label names\n#         class_column: column to use for split\n#\n#     Returns:\n#         Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n#             tuple with 4 elements\n#             (whole dataframe,\n#             list with train data,\n#             list with valid data\n#             and list with infer data)\n#     \"\"\"\n#     from_one_df: bool = in_csv is not None\n#     from_multiple_df: bool = (\n#         in_csv_train is not None or in_csv_valid is not None or in_csv_infer is not None\n#     )\n#\n#     if from_one_df == from_multiple_df:\n#         raise ValueError(\n#             \"You should pass `in_csv` \" \"or `in_csv_train` with `in_csv_valid` but not both!\"\n#         )\n#\n#     if from_one_df:\n#         dataframe: pd.DataFrame = pd.read_csv(in_csv)\n#         dataframe, df_train, df_valid, df_infer = split_dataframe(\n#             dataframe,\n#             train_folds=train_folds,\n#             valid_folds=valid_folds,\n#             infer_folds=infer_folds,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#             seed=seed,\n#             n_folds=n_folds,\n#         )\n#     else:\n#         dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n#             in_csv_train=in_csv_train,\n#             in_csv_valid=in_csv_valid,\n#             in_csv_infer=in_csv_infer,\n#             tag2class=tag2class,\n#             class_column=class_column,\n#             tag_column=tag_column,\n#         )\n#\n#     for data in [df_train, df_valid, df_infer]:\n#         if data is not None and \"fold\" in data.columns:\n#             del data[\"fold\"]\n#\n#     result = (\n#         dataframe,\n#         dataframe_to_list(df_train) if df_train is not None else None,\n#         dataframe_to_list(df_valid) if df_valid is not None else None,\n#         dataframe_to_list(df_infer) if df_infer is not None else None,\n#     )\n#\n#     return result\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/utils/plotly.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/utils/plotly.py:56:61 Incompatible parameter type [6]: Expected `str` for 3rd positional only parameter to call `_get_tensorboard_scalars` but got `Optional[str]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/utils/plotly.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/contrib/utils/wizard.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/contrib/utils/wizard.py:107:14 Incompatible variable type [9]: step_name is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/contrib/utils/wizard.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/core/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/core/experiment.py:237:29 Incompatible variable type [9]: stage is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/core/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/core/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/core/experiment.py:237:48 Incompatible variable type [9]: dataset is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/core/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/core/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/core/experiment.py:259:26 Incompatible variable type [9]: epoch is declared to have type `int` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/core/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/core/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/core/experiment.py:304:26 Incompatible variable type [9]: epoch is declared to have type `int` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/core/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/experiments/config.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/experiments/config.py:382:14 Incompatible variable type [9]: stage is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/experiments/config.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/experiments/config.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/experiments/config.py:382:33 Incompatible variable type [9]: dataset is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/experiments/config.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/experiments/config.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/experiments/config.py:418:26 Incompatible variable type [9]: epoch is declared to have type `int` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/experiments/config.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/experiments/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/experiments/experiment.py:48:8 Incompatible variable type [9]: datasets is declared to have type `OrderedDict[str, typing.Any]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/experiments/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/experiments/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/experiments/experiment.py:50:8 Incompatible variable type [9]: callbacks is declared to have type `Union[List[Callback], OrderedDict[str, Callback]]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/experiments/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/experiments/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/experiments/experiment.py:51:8 Incompatible variable type [9]: logdir is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/experiments/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/experiments/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/experiments/experiment.py:65:8 Incompatible variable type [9]: stage_kwargs is declared to have type `Dict[typing.Any, typing.Any]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/experiments/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/experiments/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/experiments/experiment.py:66:8 Incompatible variable type [9]: checkpoint_data is declared to have type `Dict[typing.Any, typing.Any]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/experiments/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/experiments/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/experiments/experiment.py:67:8 Incompatible variable type [9]: distributed_params is declared to have type `Dict[typing.Any, typing.Any]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/experiments/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/experiments/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/experiments/experiment.py:260:26 Incompatible variable type [9]: epoch is declared to have type `int` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/experiments/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/experiments/functional.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/experiments/functional.py:37:36 Incompatible variable type [9]: stage_index is declared to have type `int` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/experiments/functional.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/experiments/functional.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/experiments/functional.py:50:8 Incompatible variable type [9]: stage_index is declared to have type `int` but is used as type `float`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/experiments/functional.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/experiments/hydra_config.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/experiments/hydra_config.py:236:14 Incompatible variable type [9]: stage is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/experiments/hydra_config.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/experiments/hydra_config.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/experiments/hydra_config.py:236:33 Incompatible variable type [9]: dataset is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/experiments/hydra_config.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/experiments/hydra_config.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/experiments/hydra_config.py:265:26 Incompatible variable type [9]: epoch is declared to have type `int` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/experiments/hydra_config.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/experiments/hydra_config.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/experiments/hydra_config.py:361:26 Incompatible variable type [9]: epoch is declared to have type `int` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/experiments/hydra_config.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/experiments/hydra_config.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/experiments/hydra_config.py:399:4 Inconsistent override [15]: `catalyst.experiments.hydra_config.HydraConfigExperiment.get_callbacks` overrides method defined in `IExperiment` inconsistently. Returned type `Dict[str, Callback]` is not a subtype of the overridden return `OrderedDict[str, Callback]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/experiments/hydra_config.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/metrics/accuracy.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/metrics/functional/_accuracy.py",
    "file_hunks_size": 8,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/metrics/accuracy.py:71:38 Incompatible parameter type [6]: Expected `str` for 1st positional only parameter to call `get_activation_fn` but got `Optional[str]`.",
    "message": " Expected `str` for 1st positional only parameter to call `get_activation_fn` but got `Optional[str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 71,
    "warning_line": "    activation_fn = get_activation_fn(activation)",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "    \"\"\"\n    activation_fn = get_activation_fn(activation)\n    outputs = activation_fn(outputs)\n\n    max_k = max(topk)\n",
        "source_code_len": 118,
        "target_code": "    \"\"\"\n    max_k = max(topk)\n",
        "target_code_len": 30,
        "diff_format": "@@ -70,5 +64,2 @@\n     \"\"\"\n-    activation_fn = get_activation_fn(activation)\n-    outputs = activation_fn(outputs)\n-\n     max_k = max(topk)\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n    activation_fn = get_activation_fn(activation)\n    outputs = activation_fn(outputs)\n\n    max_k = max(topk)\n",
        "target_code_with_indent": "\n    max_k = max(topk)\n"
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/metrics/dice.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/metrics/dice.py:12:4 Incompatible variable type [9]: threshold is declared to have type `float` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/metrics/dice.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/metrics/functional.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/metrics/functional/_misc.py",
    "file_hunks_size": 23,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/metrics/functional.py:395:25 Incompatible variable type [9]: activation is declared to have type `str` but is used as type `None`.",
    "message": " activation is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 395,
    "warning_line": "    metric_fn: Callable, activation: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        raise ValueError(\"Inconsistent numbers of samples\")\n\n\ndef wrap_metric_fn_with_activation(\n    metric_fn: Callable, activation: str = None,\n):\n    \"\"\"Wraps model outputs for ``metric_fn` with specified ``activation``.\n\n    Args:\n        metric_fn: metric function to compute\n        activation: activation name to use\n\n    Returns:\n        wrapped metric function with wrapped model outputs\n\n    .. note::\n        Works only with ``metric_fn`` like\n        ``metric_fn(outputs, targets, *args, **kwargs)``.\n    \"\"\"\n    activation_fn = get_activation_fn(activation)\n\n    def wrapped_metric_fn(\n        outputs: torch.Tensor, targets: torch.Tensor, *args, **kwargs\n    ):\n        outputs = activation_fn(outputs)\n        output = metric_fn(outputs, targets, *args, **kwargs)\n        return output\n\n    return wrapped_metric_fn\n\n\ndef wrap_class_metric2dict(\n    metric_fn: Callable,\n    per_class: bool = False,\n    class_args: Sequence[str] = None,\n) -> Callable:\n    \"\"\"# noqa: D202\n    Logging wrapper for metrics with torch.Tensor output\n    and [num_classes] shape.\n    Computes the metric and sync each element from the output Tensor\n    with passed `class` argument.\n\n    Args:\n        metric_fn: metric function to compute\n        per_class: boolean flag to log per class metrics,\n            or use mean/macro statistics otherwise\n        class_args: class names for logging,\n            default: None - class indexes will be used.\n\n    Returns:\n        wrapped metric function with List[Dict] output\n    \"\"\"\n    if per_class is False and class_args is not None:\n        logger.warning(\n            \"``per_class`` is disabled, but ``class_args`` are not None\"\n            \"check the experiment conditions.\"\n        )\n\n    if per_class:\n\n        def class_metric_with_dict_output(*args, **kwargs):\n            output = metric_fn(*args, **kwargs)\n            num_classes = len(output)\n            output_class_args = class_args or [\n                f\"/class_{i:02}\" for i in range(num_classes)\n            ]\n            mean_stats = torch.mean(output).item()\n            output = {\n                key: value.item()\n                for key, value in zip(output_class_args, output)\n            }\n            output[\"\"] = mean_stats\n            output[\"/mean\"] = mean_stats\n            return output\n\n    else:\n\n        def class_metric_with_dict_output(*args, **kwargs):\n            output = metric_fn(*args, **kwargs)\n            mean_stats = torch.mean(output).item()\n            output = {\"\": mean_stats}\n            return output\n\n    return class_metric_with_dict_output\n\n\ndef wrap_topk_metric2dict(\n    metric_fn: Callable, topk_args: Sequence[int]\n) -> Callable:  # noqa: DAR401\n    \"\"\"\n    Logging wrapper for metrics with\n    Sequence[Union[torch.Tensor, int, float, Dict]] output.\n    Computes the metric and sync each element from the output sequence\n    with passed `topk` argument.\n\n    Args:\n        metric_fn: metric function to compute\n        topk_args: topk args to sync outputs with\n\n    Returns:\n        wrapped metric function with List[Dict] output\n    \"\"\"\n    metric_fn = partial(metric_fn, topk=topk_args)\n\n    def topk_metric_with_dict_output(*args, **kwargs):\n        output: Sequence = metric_fn(*args, **kwargs)\n\n        if isinstance(output[0], (int, float, torch.Tensor)):\n            output = {\n                f\"{topk_key:02}\": metric_value\n                for topk_key, metric_value in zip(topk_args, output)\n            }\n        elif isinstance(output[0], Dict):\n            output = {\n                {\n                    f\"{metric_key}{topk_key:02}\": metric_value\n                    for metric_key, metric_value in metric_dict_value.items()\n                }\n                for topk_key, metric_dict_value in zip(topk_args, output)\n            }\n        else:\n            raise NotImplementedError()\n\n        return output\n\n    return topk_metric_with_dict_output\n\n",
        "source_code_len": 3915,
        "target_code": "        raise ValueError(\"Inconsistent numbers of samples\")\n\n",
        "target_code_len": 61,
        "diff_format": "@@ -391,127 +356,2 @@\n         raise ValueError(\"Inconsistent numbers of samples\")\n-\n-\n-def wrap_metric_fn_with_activation(\n-    metric_fn: Callable, activation: str = None,\n-):\n-    \"\"\"Wraps model outputs for ``metric_fn` with specified ``activation``.\n-\n-    Args:\n-        metric_fn: metric function to compute\n-        activation: activation name to use\n-\n-    Returns:\n-        wrapped metric function with wrapped model outputs\n-\n-    .. note::\n-        Works only with ``metric_fn`` like\n-        ``metric_fn(outputs, targets, *args, **kwargs)``.\n-    \"\"\"\n-    activation_fn = get_activation_fn(activation)\n-\n-    def wrapped_metric_fn(\n-        outputs: torch.Tensor, targets: torch.Tensor, *args, **kwargs\n-    ):\n-        outputs = activation_fn(outputs)\n-        output = metric_fn(outputs, targets, *args, **kwargs)\n-        return output\n-\n-    return wrapped_metric_fn\n-\n-\n-def wrap_class_metric2dict(\n-    metric_fn: Callable,\n-    per_class: bool = False,\n-    class_args: Sequence[str] = None,\n-) -> Callable:\n-    \"\"\"# noqa: D202\n-    Logging wrapper for metrics with torch.Tensor output\n-    and [num_classes] shape.\n-    Computes the metric and sync each element from the output Tensor\n-    with passed `class` argument.\n-\n-    Args:\n-        metric_fn: metric function to compute\n-        per_class: boolean flag to log per class metrics,\n-            or use mean/macro statistics otherwise\n-        class_args: class names for logging,\n-            default: None - class indexes will be used.\n-\n-    Returns:\n-        wrapped metric function with List[Dict] output\n-    \"\"\"\n-    if per_class is False and class_args is not None:\n-        logger.warning(\n-            \"``per_class`` is disabled, but ``class_args`` are not None\"\n-            \"check the experiment conditions.\"\n-        )\n-\n-    if per_class:\n-\n-        def class_metric_with_dict_output(*args, **kwargs):\n-            output = metric_fn(*args, **kwargs)\n-            num_classes = len(output)\n-            output_class_args = class_args or [\n-                f\"/class_{i:02}\" for i in range(num_classes)\n-            ]\n-            mean_stats = torch.mean(output).item()\n-            output = {\n-                key: value.item()\n-                for key, value in zip(output_class_args, output)\n-            }\n-            output[\"\"] = mean_stats\n-            output[\"/mean\"] = mean_stats\n-            return output\n-\n-    else:\n-\n-        def class_metric_with_dict_output(*args, **kwargs):\n-            output = metric_fn(*args, **kwargs)\n-            mean_stats = torch.mean(output).item()\n-            output = {\"\": mean_stats}\n-            return output\n-\n-    return class_metric_with_dict_output\n-\n-\n-def wrap_topk_metric2dict(\n-    metric_fn: Callable, topk_args: Sequence[int]\n-) -> Callable:  # noqa: DAR401\n-    \"\"\"\n-    Logging wrapper for metrics with\n-    Sequence[Union[torch.Tensor, int, float, Dict]] output.\n-    Computes the metric and sync each element from the output sequence\n-    with passed `topk` argument.\n-\n-    Args:\n-        metric_fn: metric function to compute\n-        topk_args: topk args to sync outputs with\n-\n-    Returns:\n-        wrapped metric function with List[Dict] output\n-    \"\"\"\n-    metric_fn = partial(metric_fn, topk=topk_args)\n-\n-    def topk_metric_with_dict_output(*args, **kwargs):\n-        output: Sequence = metric_fn(*args, **kwargs)\n-\n-        if isinstance(output[0], (int, float, torch.Tensor)):\n-            output = {\n-                f\"{topk_key:02}\": metric_value\n-                for topk_key, metric_value in zip(topk_args, output)\n-            }\n-        elif isinstance(output[0], Dict):\n-            output = {\n-                {\n-                    f\"{metric_key}{topk_key:02}\": metric_value\n-                    for metric_key, metric_value in metric_dict_value.items()\n-                }\n-                for topk_key, metric_dict_value in zip(topk_args, output)\n-            }\n-        else:\n-            raise NotImplementedError()\n-\n-        return output\n-\n-    return topk_metric_with_dict_output\n \n",
        "source_code_with_indent": "        <IND>raise ValueError(\"Inconsistent numbers of samples\")\n\n\n<DED><DED>def wrap_metric_fn_with_activation(\n    metric_fn: Callable, activation: str = None,\n):\n    <IND>\"\"\"Wraps model outputs for ``metric_fn` with specified ``activation``.\n\n    Args:\n        metric_fn: metric function to compute\n        activation: activation name to use\n\n    Returns:\n        wrapped metric function with wrapped model outputs\n\n    .. note::\n        Works only with ``metric_fn`` like\n        ``metric_fn(outputs, targets, *args, **kwargs)``.\n    \"\"\"\n    activation_fn = get_activation_fn(activation)\n\n    def wrapped_metric_fn(\n        outputs: torch.Tensor, targets: torch.Tensor, *args, **kwargs\n    ):\n        <IND>outputs = activation_fn(outputs)\n        output = metric_fn(outputs, targets, *args, **kwargs)\n        return output\n\n    <DED>return wrapped_metric_fn\n\n\n<DED>def wrap_class_metric2dict(\n    metric_fn: Callable,\n    per_class: bool = False,\n    class_args: Sequence[str] = None,\n) -> Callable:\n    <IND>\"\"\"# noqa: D202\n    Logging wrapper for metrics with torch.Tensor output\n    and [num_classes] shape.\n    Computes the metric and sync each element from the output Tensor\n    with passed `class` argument.\n\n    Args:\n        metric_fn: metric function to compute\n        per_class: boolean flag to log per class metrics,\n            or use mean/macro statistics otherwise\n        class_args: class names for logging,\n            default: None - class indexes will be used.\n\n    Returns:\n        wrapped metric function with List[Dict] output\n    \"\"\"\n    if per_class is False and class_args is not None:\n        <IND>logger.warning(\n            \"``per_class`` is disabled, but ``class_args`` are not None\"\n            \"check the experiment conditions.\"\n        )\n\n    <DED>if per_class:\n\n        <IND>def class_metric_with_dict_output(*args, **kwargs):\n            <IND>output = metric_fn(*args, **kwargs)\n            num_classes = len(output)\n            output_class_args = class_args or [\n                f\"/class_{i:02}\" for i in range(num_classes)\n            ]\n            mean_stats = torch.mean(output).item()\n            output = {\n                key: value.item()\n                for key, value in zip(output_class_args, output)\n            }\n            output[\"\"] = mean_stats\n            output[\"/mean\"] = mean_stats\n            return output\n\n    <DED><DED>else:\n\n        <IND>def class_metric_with_dict_output(*args, **kwargs):\n            <IND>output = metric_fn(*args, **kwargs)\n            mean_stats = torch.mean(output).item()\n            output = {\"\": mean_stats}\n            return output\n\n    <DED><DED>return class_metric_with_dict_output\n\n\n<DED>def wrap_topk_metric2dict(\n    metric_fn: Callable, topk_args: Sequence[int]\n) -> Callable:  # noqa: DAR401\n    <IND>\"\"\"\n    Logging wrapper for metrics with\n    Sequence[Union[torch.Tensor, int, float, Dict]] output.\n    Computes the metric and sync each element from the output sequence\n    with passed `topk` argument.\n\n    Args:\n        metric_fn: metric function to compute\n        topk_args: topk args to sync outputs with\n\n    Returns:\n        wrapped metric function with List[Dict] output\n    \"\"\"\n    metric_fn = partial(metric_fn, topk=topk_args)\n\n    def topk_metric_with_dict_output(*args, **kwargs):\n        <IND>output: Sequence = metric_fn(*args, **kwargs)\n\n        if isinstance(output[0], (int, float, torch.Tensor)):\n            <IND>output = {\n                f\"{topk_key:02}\": metric_value\n                for topk_key, metric_value in zip(topk_args, output)\n            }\n        <DED>elif isinstance(output[0], Dict):\n            <IND>output = {\n                {\n                    f\"{metric_key}{topk_key:02}\": metric_value\n                    for metric_key, metric_value in metric_dict_value.items()\n                }\n                for topk_key, metric_dict_value in zip(topk_args, output)\n            }\n        <DED>else:\n            <IND>raise NotImplementedError()\n\n        <DED>return output\n\n    <DED>return topk_metric_with_dict_output\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <IND>raise ValueError(\"Inconsistent numbers of samples\")\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/metrics/functional.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/metrics/functional/_misc.py",
    "file_hunks_size": 23,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/metrics/functional.py:425:4 Incompatible variable type [9]: class_args is declared to have type `Sequence[str]` but is used as type `None`.",
    "message": " class_args is declared to have type `Sequence[str]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 425,
    "warning_line": "    class_args: Sequence[str] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        raise ValueError(\"Inconsistent numbers of samples\")\n\n\ndef wrap_metric_fn_with_activation(\n    metric_fn: Callable, activation: str = None,\n):\n    \"\"\"Wraps model outputs for ``metric_fn` with specified ``activation``.\n\n    Args:\n        metric_fn: metric function to compute\n        activation: activation name to use\n\n    Returns:\n        wrapped metric function with wrapped model outputs\n\n    .. note::\n        Works only with ``metric_fn`` like\n        ``metric_fn(outputs, targets, *args, **kwargs)``.\n    \"\"\"\n    activation_fn = get_activation_fn(activation)\n\n    def wrapped_metric_fn(\n        outputs: torch.Tensor, targets: torch.Tensor, *args, **kwargs\n    ):\n        outputs = activation_fn(outputs)\n        output = metric_fn(outputs, targets, *args, **kwargs)\n        return output\n\n    return wrapped_metric_fn\n\n\ndef wrap_class_metric2dict(\n    metric_fn: Callable,\n    per_class: bool = False,\n    class_args: Sequence[str] = None,\n) -> Callable:\n    \"\"\"# noqa: D202\n    Logging wrapper for metrics with torch.Tensor output\n    and [num_classes] shape.\n    Computes the metric and sync each element from the output Tensor\n    with passed `class` argument.\n\n    Args:\n        metric_fn: metric function to compute\n        per_class: boolean flag to log per class metrics,\n            or use mean/macro statistics otherwise\n        class_args: class names for logging,\n            default: None - class indexes will be used.\n\n    Returns:\n        wrapped metric function with List[Dict] output\n    \"\"\"\n    if per_class is False and class_args is not None:\n        logger.warning(\n            \"``per_class`` is disabled, but ``class_args`` are not None\"\n            \"check the experiment conditions.\"\n        )\n\n    if per_class:\n\n        def class_metric_with_dict_output(*args, **kwargs):\n            output = metric_fn(*args, **kwargs)\n            num_classes = len(output)\n            output_class_args = class_args or [\n                f\"/class_{i:02}\" for i in range(num_classes)\n            ]\n            mean_stats = torch.mean(output).item()\n            output = {\n                key: value.item()\n                for key, value in zip(output_class_args, output)\n            }\n            output[\"\"] = mean_stats\n            output[\"/mean\"] = mean_stats\n            return output\n\n    else:\n\n        def class_metric_with_dict_output(*args, **kwargs):\n            output = metric_fn(*args, **kwargs)\n            mean_stats = torch.mean(output).item()\n            output = {\"\": mean_stats}\n            return output\n\n    return class_metric_with_dict_output\n\n\ndef wrap_topk_metric2dict(\n    metric_fn: Callable, topk_args: Sequence[int]\n) -> Callable:  # noqa: DAR401\n    \"\"\"\n    Logging wrapper for metrics with\n    Sequence[Union[torch.Tensor, int, float, Dict]] output.\n    Computes the metric and sync each element from the output sequence\n    with passed `topk` argument.\n\n    Args:\n        metric_fn: metric function to compute\n        topk_args: topk args to sync outputs with\n\n    Returns:\n        wrapped metric function with List[Dict] output\n    \"\"\"\n    metric_fn = partial(metric_fn, topk=topk_args)\n\n    def topk_metric_with_dict_output(*args, **kwargs):\n        output: Sequence = metric_fn(*args, **kwargs)\n\n        if isinstance(output[0], (int, float, torch.Tensor)):\n            output = {\n                f\"{topk_key:02}\": metric_value\n                for topk_key, metric_value in zip(topk_args, output)\n            }\n        elif isinstance(output[0], Dict):\n            output = {\n                {\n                    f\"{metric_key}{topk_key:02}\": metric_value\n                    for metric_key, metric_value in metric_dict_value.items()\n                }\n                for topk_key, metric_dict_value in zip(topk_args, output)\n            }\n        else:\n            raise NotImplementedError()\n\n        return output\n\n    return topk_metric_with_dict_output\n\n",
        "source_code_len": 3915,
        "target_code": "        raise ValueError(\"Inconsistent numbers of samples\")\n\n",
        "target_code_len": 61,
        "diff_format": "@@ -391,127 +356,2 @@\n         raise ValueError(\"Inconsistent numbers of samples\")\n-\n-\n-def wrap_metric_fn_with_activation(\n-    metric_fn: Callable, activation: str = None,\n-):\n-    \"\"\"Wraps model outputs for ``metric_fn` with specified ``activation``.\n-\n-    Args:\n-        metric_fn: metric function to compute\n-        activation: activation name to use\n-\n-    Returns:\n-        wrapped metric function with wrapped model outputs\n-\n-    .. note::\n-        Works only with ``metric_fn`` like\n-        ``metric_fn(outputs, targets, *args, **kwargs)``.\n-    \"\"\"\n-    activation_fn = get_activation_fn(activation)\n-\n-    def wrapped_metric_fn(\n-        outputs: torch.Tensor, targets: torch.Tensor, *args, **kwargs\n-    ):\n-        outputs = activation_fn(outputs)\n-        output = metric_fn(outputs, targets, *args, **kwargs)\n-        return output\n-\n-    return wrapped_metric_fn\n-\n-\n-def wrap_class_metric2dict(\n-    metric_fn: Callable,\n-    per_class: bool = False,\n-    class_args: Sequence[str] = None,\n-) -> Callable:\n-    \"\"\"# noqa: D202\n-    Logging wrapper for metrics with torch.Tensor output\n-    and [num_classes] shape.\n-    Computes the metric and sync each element from the output Tensor\n-    with passed `class` argument.\n-\n-    Args:\n-        metric_fn: metric function to compute\n-        per_class: boolean flag to log per class metrics,\n-            or use mean/macro statistics otherwise\n-        class_args: class names for logging,\n-            default: None - class indexes will be used.\n-\n-    Returns:\n-        wrapped metric function with List[Dict] output\n-    \"\"\"\n-    if per_class is False and class_args is not None:\n-        logger.warning(\n-            \"``per_class`` is disabled, but ``class_args`` are not None\"\n-            \"check the experiment conditions.\"\n-        )\n-\n-    if per_class:\n-\n-        def class_metric_with_dict_output(*args, **kwargs):\n-            output = metric_fn(*args, **kwargs)\n-            num_classes = len(output)\n-            output_class_args = class_args or [\n-                f\"/class_{i:02}\" for i in range(num_classes)\n-            ]\n-            mean_stats = torch.mean(output).item()\n-            output = {\n-                key: value.item()\n-                for key, value in zip(output_class_args, output)\n-            }\n-            output[\"\"] = mean_stats\n-            output[\"/mean\"] = mean_stats\n-            return output\n-\n-    else:\n-\n-        def class_metric_with_dict_output(*args, **kwargs):\n-            output = metric_fn(*args, **kwargs)\n-            mean_stats = torch.mean(output).item()\n-            output = {\"\": mean_stats}\n-            return output\n-\n-    return class_metric_with_dict_output\n-\n-\n-def wrap_topk_metric2dict(\n-    metric_fn: Callable, topk_args: Sequence[int]\n-) -> Callable:  # noqa: DAR401\n-    \"\"\"\n-    Logging wrapper for metrics with\n-    Sequence[Union[torch.Tensor, int, float, Dict]] output.\n-    Computes the metric and sync each element from the output sequence\n-    with passed `topk` argument.\n-\n-    Args:\n-        metric_fn: metric function to compute\n-        topk_args: topk args to sync outputs with\n-\n-    Returns:\n-        wrapped metric function with List[Dict] output\n-    \"\"\"\n-    metric_fn = partial(metric_fn, topk=topk_args)\n-\n-    def topk_metric_with_dict_output(*args, **kwargs):\n-        output: Sequence = metric_fn(*args, **kwargs)\n-\n-        if isinstance(output[0], (int, float, torch.Tensor)):\n-            output = {\n-                f\"{topk_key:02}\": metric_value\n-                for topk_key, metric_value in zip(topk_args, output)\n-            }\n-        elif isinstance(output[0], Dict):\n-            output = {\n-                {\n-                    f\"{metric_key}{topk_key:02}\": metric_value\n-                    for metric_key, metric_value in metric_dict_value.items()\n-                }\n-                for topk_key, metric_dict_value in zip(topk_args, output)\n-            }\n-        else:\n-            raise NotImplementedError()\n-\n-        return output\n-\n-    return topk_metric_with_dict_output\n \n",
        "source_code_with_indent": "        <IND>raise ValueError(\"Inconsistent numbers of samples\")\n\n\n<DED><DED>def wrap_metric_fn_with_activation(\n    metric_fn: Callable, activation: str = None,\n):\n    <IND>\"\"\"Wraps model outputs for ``metric_fn` with specified ``activation``.\n\n    Args:\n        metric_fn: metric function to compute\n        activation: activation name to use\n\n    Returns:\n        wrapped metric function with wrapped model outputs\n\n    .. note::\n        Works only with ``metric_fn`` like\n        ``metric_fn(outputs, targets, *args, **kwargs)``.\n    \"\"\"\n    activation_fn = get_activation_fn(activation)\n\n    def wrapped_metric_fn(\n        outputs: torch.Tensor, targets: torch.Tensor, *args, **kwargs\n    ):\n        <IND>outputs = activation_fn(outputs)\n        output = metric_fn(outputs, targets, *args, **kwargs)\n        return output\n\n    <DED>return wrapped_metric_fn\n\n\n<DED>def wrap_class_metric2dict(\n    metric_fn: Callable,\n    per_class: bool = False,\n    class_args: Sequence[str] = None,\n) -> Callable:\n    <IND>\"\"\"# noqa: D202\n    Logging wrapper for metrics with torch.Tensor output\n    and [num_classes] shape.\n    Computes the metric and sync each element from the output Tensor\n    with passed `class` argument.\n\n    Args:\n        metric_fn: metric function to compute\n        per_class: boolean flag to log per class metrics,\n            or use mean/macro statistics otherwise\n        class_args: class names for logging,\n            default: None - class indexes will be used.\n\n    Returns:\n        wrapped metric function with List[Dict] output\n    \"\"\"\n    if per_class is False and class_args is not None:\n        <IND>logger.warning(\n            \"``per_class`` is disabled, but ``class_args`` are not None\"\n            \"check the experiment conditions.\"\n        )\n\n    <DED>if per_class:\n\n        <IND>def class_metric_with_dict_output(*args, **kwargs):\n            <IND>output = metric_fn(*args, **kwargs)\n            num_classes = len(output)\n            output_class_args = class_args or [\n                f\"/class_{i:02}\" for i in range(num_classes)\n            ]\n            mean_stats = torch.mean(output).item()\n            output = {\n                key: value.item()\n                for key, value in zip(output_class_args, output)\n            }\n            output[\"\"] = mean_stats\n            output[\"/mean\"] = mean_stats\n            return output\n\n    <DED><DED>else:\n\n        <IND>def class_metric_with_dict_output(*args, **kwargs):\n            <IND>output = metric_fn(*args, **kwargs)\n            mean_stats = torch.mean(output).item()\n            output = {\"\": mean_stats}\n            return output\n\n    <DED><DED>return class_metric_with_dict_output\n\n\n<DED>def wrap_topk_metric2dict(\n    metric_fn: Callable, topk_args: Sequence[int]\n) -> Callable:  # noqa: DAR401\n    <IND>\"\"\"\n    Logging wrapper for metrics with\n    Sequence[Union[torch.Tensor, int, float, Dict]] output.\n    Computes the metric and sync each element from the output sequence\n    with passed `topk` argument.\n\n    Args:\n        metric_fn: metric function to compute\n        topk_args: topk args to sync outputs with\n\n    Returns:\n        wrapped metric function with List[Dict] output\n    \"\"\"\n    metric_fn = partial(metric_fn, topk=topk_args)\n\n    def topk_metric_with_dict_output(*args, **kwargs):\n        <IND>output: Sequence = metric_fn(*args, **kwargs)\n\n        if isinstance(output[0], (int, float, torch.Tensor)):\n            <IND>output = {\n                f\"{topk_key:02}\": metric_value\n                for topk_key, metric_value in zip(topk_args, output)\n            }\n        <DED>elif isinstance(output[0], Dict):\n            <IND>output = {\n                {\n                    f\"{metric_key}{topk_key:02}\": metric_value\n                    for metric_key, metric_value in metric_dict_value.items()\n                }\n                for topk_key, metric_dict_value in zip(topk_args, output)\n            }\n        <DED>else:\n            <IND>raise NotImplementedError()\n\n        <DED>return output\n\n    <DED>return topk_metric_with_dict_output\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <IND>raise ValueError(\"Inconsistent numbers of samples\")\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/metrics/iou.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/metrics/iou.py:10:4 Incompatible variable type [9]: threshold is declared to have type `float` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/metrics/iou.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/registry/registries.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/registry/registries.py:102:32 Incompatible parameter type [6]: Expected `typing.Callable[[typing.Union[typing.Callable[..., typing.Any], typing.Type[typing.Any]], typing.Tuple[typing.Any, ...], typing.Mapping[typing.Any, typing.Any]], typing.Any]` for 2nd parameter `default_meta_factory` to call `Registry.__init__` but got `typing.Type[_GradClipperWrap]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/registry/registries.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/registry/registry.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/tools/registry.py",
    "file_hunks_size": 10,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/registry/registry.py:26:11 Call error [29]: `Union[typing.Callable[..., typing.Any], Type[typing.Any]]` is not a function.",
    "message": " `Union[typing.Callable[..., typing.Any], Type[typing.Any]]` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 26,
    "warning_line": "    return factory(*args, **kwargs)"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/registry/registry.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/tools/registry.py",
    "file_hunks_size": 10,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/registry/registry.py:86:8 Incompatible variable type [9]: factory is declared to have type `Union[typing.Callable[..., typing.Any], Type[typing.Any]]` but is used as type `None`.",
    "message": " factory is declared to have type `Union[typing.Callable[..., typing.Any], Type[typing.Any]]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 86,
    "warning_line": "        factory: Factory = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/registry/registry.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/tools/registry.py",
    "file_hunks_size": 10,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/registry/registry.py:88:8 Incompatible variable type [9]: name is declared to have type `str` but is used as type `None`.",
    "message": " name is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 88,
    "warning_line": "        name: str = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/registry/registry.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/tools/registry.py",
    "file_hunks_size": 10,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/registry/registry.py:149:22 Incompatible variable type [9]: prefix is declared to have type `Union[List[str], str]` but is used as type `None`.",
    "message": " prefix is declared to have type `Union[List[str], str]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 149,
    "warning_line": "        self, module, prefix: Union[str, List[str]] = None"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/registry/registry.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/tools/registry.py",
    "file_hunks_size": 10,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/registry/registry.py:190:19 Incompatible parameter type [6]: Expected `str` for 1st positional only parameter to call `Registry.add` but got `Union[Type[typing.Any], types.FunctionType]`.",
    "message": " Expected `str` for 1st positional only parameter to call `Registry.add` but got `Union[Type[typing.Any], types.FunctionType]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 190,
    "warning_line": "        self.add(**to_add)"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/registry/registry.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/tools/registry.py",
    "file_hunks_size": 10,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/registry/registry.py:317:4 Inconsistent override [14]: `catalyst.registry.registry.Registry.__contains__` overrides method defined in `Mapping` inconsistently. Could not find parameter `o` in overriding signature.",
    "message": " `catalyst.registry.registry.Registry.__contains__` overrides method defined in `Mapping` inconsistently. Could not find parameter `o` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 317,
    "warning_line": "    def __contains__(self, name: str):"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/runners/multi_supervised.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/runners/multi_supervised.py:20:8 Incompatible variable type [9]: models_keys is declared to have type `Mapping[str, typing.Any]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/runners/multi_supervised.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/runners/multi_supervised.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/runners/multi_supervised.py:129:8 Incompatible variable type [9]: output is declared to have type `Union[List[typing.Any], typing.Tuple[typing.Any, ...]]` but is used as type `typing.Dict[typing.Any, typing.Any]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/runners/multi_supervised.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/runners/runner.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/runners/runner.py",
    "file_hunks_size": 14,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/runners/runner.py:84:8 Incompatible variable type [9]: datasets is declared to have type `OrderedDict[str, typing.Any]` but is used as type `None`.",
    "message": " datasets is declared to have type `OrderedDict[str, typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 84,
    "warning_line": "        datasets: \"OrderedDict[str, Union[Dataset, Dict, Any]]\" = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/runners/runner.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/runners/runner.py",
    "file_hunks_size": 14,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/runners/runner.py:88:8 Incompatible variable type [9]: resume is declared to have type `str` but is used as type `None`.",
    "message": " resume is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 88,
    "warning_line": "        resume: str = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/runners/runner.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/runners/runner.py",
    "file_hunks_size": 14,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/runners/runner.py:94:8 Incompatible variable type [9]: stage_kwargs is declared to have type `Dict[typing.Any, typing.Any]` but is used as type `None`.",
    "message": " stage_kwargs is declared to have type `Dict[typing.Any, typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 94,
    "warning_line": "        stage_kwargs: Dict = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/runners/runner.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/runners/runner.py",
    "file_hunks_size": 14,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/runners/runner.py:95:8 Incompatible variable type [9]: checkpoint_data is declared to have type `Dict[typing.Any, typing.Any]` but is used as type `None`.",
    "message": " checkpoint_data is declared to have type `Dict[typing.Any, typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 95,
    "warning_line": "        checkpoint_data: Dict = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/runners/runner.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/runners/runner.py",
    "file_hunks_size": 14,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/runners/runner.py:96:8 Incompatible variable type [9]: fp16 is declared to have type `Union[Dict[typing.Any, typing.Any], bool]` but is used as type `None`.",
    "message": " fp16 is declared to have type `Union[Dict[typing.Any, typing.Any], bool]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 96,
    "warning_line": "        fp16: Union[Dict, bool] = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/runners/runner.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/runners/runner.py",
    "file_hunks_size": 14,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/runners/runner.py:103:8 Incompatible variable type [9]: state_kwargs is declared to have type `Dict[typing.Any, typing.Any]` but is used as type `None`.",
    "message": " state_kwargs is declared to have type `Dict[typing.Any, typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 103,
    "warning_line": "        state_kwargs: Dict = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/runners/runner.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/runners/runner.py",
    "file_hunks_size": 14,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/runners/runner.py:185:35 Incompatible parameter type [6]: Expected `Union[Dict[str, str], str]` for 2nd parameter `load_on_stage_end` to call `CheckpointCallback.__init__` but got `typing.Optional[str]`.",
    "message": " Expected `Union[Dict[str, str], str]` for 2nd parameter `load_on_stage_end` to call `CheckpointCallback.__init__` but got `typing.Optional[str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 185,
    "warning_line": "                    resume=resume, load_on_stage_end=load_on_stage_end,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/runners/runner.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/runners/runner.py",
    "file_hunks_size": 14,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/runners/runner.py:220:8 Incompatible variable type [9]: datasets is declared to have type `OrderedDict[str, typing.Any]` but is used as type `None`.",
    "message": " datasets is declared to have type `OrderedDict[str, typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 220,
    "warning_line": "        datasets: \"OrderedDict[str, Union[Dataset, Dict, Any]]\" = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/runners/runner.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/runners/runner.py",
    "file_hunks_size": 14,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/runners/runner.py:222:8 Incompatible variable type [9]: callbacks is declared to have type `Union[List[Callback], OrderedDict[str, Callback]]` but is used as type `None`.",
    "message": " callbacks is declared to have type `Union[List[Callback], OrderedDict[str, Callback]]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 222,
    "warning_line": "        callbacks: \"Union[List[Callback], OrderedDict[str, Callback]]\" = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/runners/runner.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/runners/runner.py",
    "file_hunks_size": 14,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/runners/runner.py:223:8 Incompatible variable type [9]: logdir is declared to have type `str` but is used as type `None`.",
    "message": " logdir is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 223,
    "warning_line": "        logdir: str = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/runners/runner.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/runners/runner.py",
    "file_hunks_size": 14,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/runners/runner.py:224:8 Incompatible variable type [9]: resume is declared to have type `str` but is used as type `None`.",
    "message": " resume is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 224,
    "warning_line": "        resume: str = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/runners/runner.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/runners/runner.py",
    "file_hunks_size": 14,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/runners/runner.py:226:8 Incompatible variable type [9]: stage_kwargs is declared to have type `Dict[typing.Any, typing.Any]` but is used as type `None`.",
    "message": " stage_kwargs is declared to have type `Dict[typing.Any, typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 226,
    "warning_line": "        stage_kwargs: Dict = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/runners/runner.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/runners/runner.py",
    "file_hunks_size": 14,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/runners/runner.py:227:8 Incompatible variable type [9]: fp16 is declared to have type `Union[Dict[typing.Any, typing.Any], bool]` but is used as type `None`.",
    "message": " fp16 is declared to have type `Union[Dict[typing.Any, typing.Any], bool]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 227,
    "warning_line": "        fp16: Union[Dict, bool] = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/runners/runner.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/runners/runner.py",
    "file_hunks_size": 14,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/runners/runner.py:231:8 Incompatible variable type [9]: state_kwargs is declared to have type `Dict[typing.Any, typing.Any]` but is used as type `None`.",
    "message": " state_kwargs is declared to have type `Dict[typing.Any, typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 231,
    "warning_line": "        state_kwargs: Dict = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/runners/runner.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/runners/runner.py",
    "file_hunks_size": 14,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/runners/runner.py:324:8 Incompatible variable type [9]: resume is declared to have type `str` but is used as type `None`.",
    "message": " resume is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 324,
    "warning_line": "        resume: str = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/runners/runner.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/runners/runner.py",
    "file_hunks_size": 14,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/runners/runner.py:325:8 Incompatible variable type [9]: fp16 is declared to have type `Union[Dict[typing.Any, typing.Any], bool]` but is used as type `None`.",
    "message": " fp16 is declared to have type `Union[Dict[typing.Any, typing.Any], bool]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 325,
    "warning_line": "        fp16: Union[Dict, bool] = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/runners/runner.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/runners/runner.py",
    "file_hunks_size": 14,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/runners/runner.py:355:8 Incompatible attribute type [8]: Attribute `experiment` declared in class `catalyst.core.runner.IRunner` has type `catalyst.core.experiment.IExperiment` but is used as type `None`.",
    "message": " Attribute `experiment` declared in class `catalyst.core.runner.IRunner` has type `catalyst.core.experiment.IExperiment` but is used as type `None`.",
    "rule_id": "Incompatible attribute type [8]",
    "warning_line_no": 355,
    "warning_line": "        self.experiment = None"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/runners/runner.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/runners/runner.py",
    "file_hunks_size": 14,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/runners/runner.py:379:8 Incompatible variable type [9]: logdir is declared to have type `str` but is used as type `None`.",
    "message": " logdir is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 379,
    "warning_line": "        logdir: str = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/runners/runner.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/runners/runner.py",
    "file_hunks_size": 14,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/runners/runner.py:384:8 Incompatible variable type [9]: fp16 is declared to have type `Union[Dict[typing.Any, typing.Any], bool]` but is used as type `None`.",
    "message": " fp16 is declared to have type `Union[Dict[typing.Any, typing.Any], bool]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 384,
    "warning_line": "        fp16: Union[Dict, bool] = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/runners/runner.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/runners/runner.py",
    "file_hunks_size": 14,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/runners/runner.py:386:8 Incompatible variable type [9]: predict_params is declared to have type `Dict[typing.Any, typing.Any]` but is used as type `None`.",
    "message": " predict_params is declared to have type `Dict[typing.Any, typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 386,
    "warning_line": "        predict_params: dict = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/tools/meters/confusionmeter.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/tools/meters/confusionmeter.py:35:4 Inconsistent override [14]: `catalyst.tools.meters.confusionmeter.ConfusionMeter.add` overrides method defined in `meter.Meter` inconsistently. Could not find parameter `value` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/tools/meters/confusionmeter.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/tools/meters/ppv_tpr_f1_meter.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/tools/meters/ppv_tpr_f1_meter.py:92:4 Inconsistent override [14]: `catalyst.tools.meters.ppv_tpr_f1_meter.PrecisionRecallF1ScoreMeter.add` overrides method defined in `meter.Meter` inconsistently. Could not find parameter `value` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/tools/meters/ppv_tpr_f1_meter.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/components.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/utils/components.py:104:4 Incompatible variable type [9]: distributed_params is declared to have type `Dict[typing.Any, typing.Any]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/components.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/loaders.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/data.py",
    "file_hunks_size": 13,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/loaders.py:18:4 Incompatible variable type [9]: dict_transform is declared to have type `typing.Callable[..., typing.Any]` but is used as type `None`.",
    "message": " dict_transform is declared to have type `typing.Callable[..., typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 18,
    "warning_line": "    dict_transform: Callable = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/loaders.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/data.py",
    "file_hunks_size": 13,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/loaders.py:61:8 Incompatible parameter type [6]: Expected `typing.List[Dict[typing.Any, typing.Any]]` for 1st parameter `list_data` to call `catalyst.data.dataset.torch.ListDataset.__init__` but got `Iterable[Dict[typing.Any, typing.Any]]`.",
    "message": " Expected `typing.List[Dict[typing.Any, typing.Any]]` for 1st parameter `list_data` to call `catalyst.data.dataset.torch.ListDataset.__init__` but got `Iterable[Dict[typing.Any, typing.Any]]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 61,
    "warning_line": "        list_data=data_source, open_fn=open_fn, dict_transform=dict_transform,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/loaders.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/data.py",
    "file_hunks_size": 13,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/loaders.py:191:4 Incompatible variable type [9]: loaders_params is declared to have type `Dict[str, typing.Any]` but is used as type `None`.",
    "message": " loaders_params is declared to have type `Dict[str, typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 191,
    "warning_line": "    loaders_params: Dict[str, Any] = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/loaders.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/data.py",
    "file_hunks_size": 13,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/loaders.py:192:4 Incompatible variable type [9]: samplers_params is declared to have type `Dict[str, typing.Any]` but is used as type `None`.",
    "message": " samplers_params is declared to have type `Dict[str, typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 192,
    "warning_line": "    samplers_params: Dict[str, Any] = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/loaders.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/data.py",
    "file_hunks_size": 13,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/loaders.py:194:4 Incompatible variable type [9]: get_datasets_fn is declared to have type `typing.Callable[..., typing.Any]` but is used as type `None`.",
    "message": " get_datasets_fn is declared to have type `typing.Callable[..., typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 194,
    "warning_line": "    get_datasets_fn: Callable = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/quantization.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/quantization.py",
    "file_hunks_size": 2,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/quantization.py:26:4 Incompatible variable type [9]: logdir is declared to have type `Union[Path, str]` but is used as type `None`.",
    "message": " logdir is declared to have type `Union[Path, str]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 26,
    "warning_line": "    logdir: Union[str, Path] = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/quantization.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/quantization.py",
    "file_hunks_size": 2,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/quantization.py:27:4 Incompatible variable type [9]: checkpoint_name is declared to have type `str` but is used as type `None`.",
    "message": " checkpoint_name is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 27,
    "warning_line": "    checkpoint_name: str = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/quantization.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/quantization.py",
    "file_hunks_size": 2,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/quantization.py:28:4 Incompatible variable type [9]: out_dir is declared to have type `Union[Path, str]` but is used as type `None`.",
    "message": " out_dir is declared to have type `Union[Path, str]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 28,
    "warning_line": "    out_dir: Union[str, Path] = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/quantization.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/quantization.py",
    "file_hunks_size": 2,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/quantization.py:29:4 Incompatible variable type [9]: out_model is declared to have type `Union[Path, str]` but is used as type `None`.",
    "message": " out_model is declared to have type `Union[Path, str]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 29,
    "warning_line": "    out_model: Union[str, Path] = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/quantization.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/quantization.py",
    "file_hunks_size": 2,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/quantization.py:49:8 Incompatible variable type [9]: output is declared to have type `Path` but is used as type `Union[Path, str]`.",
    "message": " output is declared to have type `Path` but is used as type `Union[Path, str]`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 49,
    "warning_line": "        output: Path = out_dir"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/quantization.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/quantization.py",
    "file_hunks_size": 2,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/quantization.py:70:4 Incompatible variable type [9]: stage is declared to have type `str` but is used as type `None`.",
    "message": " stage is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 70,
    "warning_line": "    stage: str = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/quantization.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/quantization.py",
    "file_hunks_size": 2,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/quantization.py:73:4 Incompatible variable type [9]: backend is declared to have type `str` but is used as type `None`.",
    "message": " backend is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 73,
    "warning_line": "    backend: str = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/quantization.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/quantization.py",
    "file_hunks_size": 2,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/quantization.py:96:4 Incompatible variable type [9]: config is declared to have type `Dict[str, Dict[typing.Any, typing.Any]]` but is used as type `Union[Dict[typing.Any, typing.Any], typing.List[typing.Any]]`.",
    "message": " config is declared to have type `Dict[str, Dict[typing.Any, typing.Any]]` but is used as type `Union[Dict[typing.Any, typing.Any], typing.List[typing.Any]]`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 96,
    "warning_line": "    config: Dict[str, dict] = load_config(config_path)"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/quantization.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/quantization.py",
    "file_hunks_size": 2,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/quantization.py:104:4 Incompatible variable type [9]: experiment is declared to have type `catalyst.experiments.config.ConfigExperiment` but is used as type `None`.",
    "message": " experiment is declared to have type `catalyst.experiments.config.ConfigExperiment` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 104,
    "warning_line": "    experiment: ConfigExperiment = None"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/quantization.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/quantization.py",
    "file_hunks_size": 2,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/quantization.py:114:33 Incompatible parameter type [6]: Expected `str` for 1st positional only parameter to call `catalyst.utils.checkpoint.load_checkpoint` but got `Path`.",
    "message": " Expected `str` for 1st positional only parameter to call `catalyst.utils.checkpoint.load_checkpoint` but got `Path`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 114,
    "warning_line": "    checkpoint = load_checkpoint(checkpoint_path)"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/scripts.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/utils/scripts.py:39:8 Incompatible parameter type [6]: Expected `typing.Optional[typing.List[str]]` for 3rd parameter `submodule_search_locations` to call `spec_from_file_location` but got `typing.List[pathlib.Path]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/scripts.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/scripts.py",
    "min_patch_found": false,
    "full_warning_msg": "catalyst/utils/scripts.py:41:25 Incompatible parameter type [6]: Expected `importlib.machinery.ModuleSpec` for 1st positional only parameter to call `module_from_spec` but got `typing.Optional[importlib.machinery.ModuleSpec]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/scripts.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/tracing.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/tracing.py:36:28 Incompatible variable type [9]: exclude is declared to have type `List[str]` but is used as type `None`.",
    "message": " exclude is declared to have type `List[str]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 36,
    "warning_line": "    fn: Callable[..., Any], exclude: List[str] = None"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/tracing.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/tracing.py:109:4 Incompatible variable type [9]: opt_level is declared to have type `str` but is used as type `None`.",
    "message": " opt_level is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 109,
    "warning_line": "    opt_level: str = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/tracing.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/tracing.py:111:4 Incompatible variable type [9]: predict_params is declared to have type `Dict[typing.Any, typing.Any]` but is used as type `None`.",
    "message": " predict_params is declared to have type `Dict[typing.Any, typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 111,
    "warning_line": "    predict_params: dict = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/tracing.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/tracing.py:166:4 Incompatible variable type [9]: stage is declared to have type `str` but is used as type `None`.",
    "message": " stage is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 166,
    "warning_line": "    stage: str = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/tracing.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/tracing.py:167:4 Incompatible variable type [9]: loader is declared to have type `Union[int, str]` but is used as type `None`.",
    "message": " loader is declared to have type `Union[int, str]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 167,
    "warning_line": "    loader: Union[str, int] = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/tracing.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/tracing.py:170:4 Incompatible variable type [9]: opt_level is declared to have type `str` but is used as type `None`.",
    "message": " opt_level is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 170,
    "warning_line": "    opt_level: str = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/tracing.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/tracing.py:194:4 Incompatible variable type [9]: config is declared to have type `Dict[str, Dict[typing.Any, typing.Any]]` but is used as type `Union[Dict[typing.Any, typing.Any], List[typing.Any]]`.",
    "message": " config is declared to have type `Dict[str, Dict[typing.Any, typing.Any]]` but is used as type `Union[Dict[typing.Any, typing.Any], List[typing.Any]]`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 194,
    "warning_line": "    config: Dict[str, dict] = load_config(config_path)"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/tracing.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/tracing.py:202:4 Incompatible variable type [9]: experiment is declared to have type `catalyst.experiments.config.ConfigExperiment` but is used as type `None`.",
    "message": " experiment is declared to have type `catalyst.experiments.config.ConfigExperiment` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 202,
    "warning_line": "    experiment: ConfigExperiment = None"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/tracing.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/tracing.py:212:33 Incompatible parameter type [6]: Expected `str` for 1st positional only parameter to call `load_checkpoint` but got `Path`.",
    "message": " Expected `str` for 1st positional only parameter to call `load_checkpoint` but got `Path`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 212,
    "warning_line": "    checkpoint = load_checkpoint(checkpoint_path)"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/tracing.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/tracing.py:248:4 Incompatible variable type [9]: checkpoint_name is declared to have type `str` but is used as type `None`.",
    "message": " checkpoint_name is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 248,
    "warning_line": "    checkpoint_name: str = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/tracing.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/tracing.py:252:4 Incompatible variable type [9]: opt_level is declared to have type `str` but is used as type `None`.",
    "message": " opt_level is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 252,
    "warning_line": "    opt_level: str = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/tracing.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/tracing.py:278:37 Incompatible parameter type [6]: Expected `str` for 1st parameter `filepath` to call `load_checkpoint` but got `Path`.",
    "message": " Expected `str` for 1st parameter `filepath` to call `load_checkpoint` but got `Path`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 278,
    "warning_line": "        checkpoint = load_checkpoint(filepath=checkpoint_path)"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/tracing.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/tracing.py:336:4 Incompatible variable type [9]: opt_level is declared to have type `str` but is used as type `None`.",
    "message": " opt_level is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 336,
    "warning_line": "    opt_level: str = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/tracing.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/tracing.py:337:4 Incompatible variable type [9]: additional_string is declared to have type `str` but is used as type `None`.",
    "message": " additional_string is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 337,
    "warning_line": "    additional_string: str = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/tracing.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/tracing.py:372:4 Incompatible variable type [9]: logdir is declared to have type `Union[Path, str]` but is used as type `None`.",
    "message": " logdir is declared to have type `Union[Path, str]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 372,
    "warning_line": "    logdir: Union[str, Path] = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/tracing.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/tracing.py:376:4 Incompatible variable type [9]: opt_level is declared to have type `str` but is used as type `None`.",
    "message": " opt_level is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 376,
    "warning_line": "    opt_level: str = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/tracing.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/tracing.py:377:4 Incompatible variable type [9]: out_dir is declared to have type `Union[Path, str]` but is used as type `None`.",
    "message": " out_dir is declared to have type `Union[Path, str]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 377,
    "warning_line": "    out_dir: Union[str, Path] = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/tracing.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/tracing.py:378:4 Incompatible variable type [9]: out_model is declared to have type `Union[Path, str]` but is used as type `None`.",
    "message": " out_model is declared to have type `Union[Path, str]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 378,
    "warning_line": "    out_model: Union[str, Path] = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/tracing.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/tracing.py:379:4 Incompatible variable type [9]: checkpoint_name is declared to have type `str` but is used as type `None`.",
    "message": " checkpoint_name is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 379,
    "warning_line": "    checkpoint_name: str = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/tracing.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/tracing.py:409:8 Incompatible variable type [9]: output is declared to have type `Path` but is used as type `Union[Path, str]`.",
    "message": " output is declared to have type `Path` but is used as type `Union[Path, str]`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 409,
    "warning_line": "        output: Path = out_dir"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "catalyst/utils/tracing.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/catalyst/utils/tracing.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "catalyst/utils/tracing.py:430:4 Incompatible variable type [9]: opt_level is declared to have type `str` but is used as type `None`.",
    "message": " opt_level is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 430,
    "warning_line": "    opt_level: str = None,"
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "examples/_empty/src/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "examples/_empty/src/experiment.py:12:4 Inconsistent override [14]: `examples._empty.src.experiment.Experiment.get_transforms` overrides method defined in `catalyst.experiments.config.ConfigExperiment` inconsistently. Could not find parameter `dataset` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/examples/_empty/src/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "examples/_empty/src/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "examples/_empty/src/experiment.py:12:4 Inconsistent override [14]: `examples._empty.src.experiment.Experiment.get_transforms` overrides method defined in `catalyst.experiments.config.ConfigExperiment` inconsistently. Could not find parameter `self` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/examples/_empty/src/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "examples/_empty/src/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "examples/_empty/src/experiment.py:12:23 Incompatible variable type [9]: stage is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/examples/_empty/src/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "examples/_empty/src/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "examples/_empty/src/experiment.py:12:42 Incompatible variable type [9]: mode is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/examples/_empty/src/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "examples/_empty/src/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "examples/_empty/src/experiment.py:24:4 Inconsistent override [14]: `examples._empty.src.experiment.Experiment.get_datasets` overrides method defined in `catalyst.core.experiment.IExperiment` inconsistently. Could not find parameter `epoch` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/examples/_empty/src/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "examples/_empty/src/model.py",
    "min_patch_found": false,
    "full_warning_msg": "examples/_empty/src/model.py:46:16 Call error [29]: `typing.Type[Model]` is not a function.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/examples/_empty/src/model.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "examples/cifar_simple/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "examples/cifar_simple/experiment.py:14:4 Inconsistent override [14]: `examples.cifar_simple.experiment.Experiment.get_transforms` overrides method defined in `catalyst.experiments.config.ConfigExperiment` inconsistently. Could not find parameter `dataset` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/examples/cifar_simple/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "examples/cifar_simple/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "examples/cifar_simple/experiment.py:14:4 Inconsistent override [14]: `examples.cifar_simple.experiment.Experiment.get_transforms` overrides method defined in `catalyst.experiments.config.ConfigExperiment` inconsistently. Could not find parameter `self` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/examples/cifar_simple/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "examples/cifar_simple/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "examples/cifar_simple/experiment.py:14:23 Incompatible variable type [9]: stage is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/examples/cifar_simple/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "examples/cifar_simple/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "examples/cifar_simple/experiment.py:14:42 Incompatible variable type [9]: mode is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/examples/cifar_simple/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "examples/cifar_simple/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "examples/cifar_simple/experiment.py:23:4 Inconsistent override [14]: `examples.cifar_simple.experiment.Experiment.get_datasets` overrides method defined in `catalyst.core.experiment.IExperiment` inconsistently. Could not find parameter `epoch` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/examples/cifar_simple/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "examples/cifar_simple/experiments/simple_experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "examples/cifar_simple/experiments/simple_experiment.py:14:4 Inconsistent override [14]: `examples.cifar_simple.experiments.simple_experiment.SimpleExperiment.get_transforms` overrides method defined in `catalyst.experiments.config.ConfigExperiment` inconsistently. Could not find parameter `dataset` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/examples/cifar_simple/experiments/simple_experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "examples/cifar_simple/experiments/simple_experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "examples/cifar_simple/experiments/simple_experiment.py:14:4 Inconsistent override [14]: `examples.cifar_simple.experiments.simple_experiment.SimpleExperiment.get_transforms` overrides method defined in `catalyst.experiments.config.ConfigExperiment` inconsistently. Could not find parameter `self` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/examples/cifar_simple/experiments/simple_experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "examples/cifar_simple/experiments/simple_experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "examples/cifar_simple/experiments/simple_experiment.py:14:23 Incompatible variable type [9]: stage is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/examples/cifar_simple/experiments/simple_experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "examples/cifar_simple/experiments/simple_experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "examples/cifar_simple/experiments/simple_experiment.py:14:42 Incompatible variable type [9]: mode is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/examples/cifar_simple/experiments/simple_experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "examples/cifar_simple/experiments/simple_experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "examples/cifar_simple/experiments/simple_experiment.py:23:4 Inconsistent override [14]: `examples.cifar_simple.experiments.simple_experiment.SimpleExperiment.get_datasets` overrides method defined in `catalyst.core.experiment.IExperiment` inconsistently. Could not find parameter `epoch` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/examples/cifar_simple/experiments/simple_experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "examples/cifar_stages/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "examples/cifar_stages/experiment.py:56:4 Inconsistent override [14]: `examples.cifar_stages.experiment.Experiment.get_datasets` overrides method defined in `catalyst.core.experiment.IExperiment` inconsistently. Could not find parameter `epoch` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/examples/cifar_stages/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "examples/cifar_stages_optuna/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "examples/cifar_stages_optuna/experiment.py:56:4 Inconsistent override [14]: `examples.cifar_stages_optuna.experiment.Experiment.get_datasets` overrides method defined in `catalyst.core.experiment.IExperiment` inconsistently. Could not find parameter `epoch` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/examples/cifar_stages_optuna/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_contrib_dl_callbacks/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_contrib_dl_callbacks/experiment.py:15:4 Inconsistent override [14]: `tests._tests_contrib_dl_callbacks.experiment.Experiment.get_transforms` overrides method defined in `catalyst.experiments.config.ConfigExperiment` inconsistently. Could not find parameter `dataset` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_contrib_dl_callbacks/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_contrib_dl_callbacks/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_contrib_dl_callbacks/experiment.py:15:4 Inconsistent override [14]: `tests._tests_contrib_dl_callbacks.experiment.Experiment.get_transforms` overrides method defined in `catalyst.experiments.config.ConfigExperiment` inconsistently. Could not find parameter `self` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_contrib_dl_callbacks/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_contrib_dl_callbacks/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_contrib_dl_callbacks/experiment.py:15:23 Incompatible variable type [9]: stage is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_contrib_dl_callbacks/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_contrib_dl_callbacks/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_contrib_dl_callbacks/experiment.py:15:42 Incompatible variable type [9]: mode is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_contrib_dl_callbacks/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_contrib_dl_callbacks/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_contrib_dl_callbacks/experiment.py:19:4 Inconsistent override [14]: `tests._tests_contrib_dl_callbacks.experiment.Experiment.get_datasets` overrides method defined in `catalyst.core.experiment.IExperiment` inconsistently. Could not find parameter `epoch` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_contrib_dl_callbacks/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification/experiment.py:13:4 Inconsistent override [14]: `tests._tests_cv_classification.experiment.Experiment.get_transforms` overrides method defined in `catalyst.experiments.config.ConfigExperiment` inconsistently. Could not find parameter `dataset` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification/experiment.py:13:4 Inconsistent override [14]: `tests._tests_cv_classification.experiment.Experiment.get_transforms` overrides method defined in `catalyst.experiments.config.ConfigExperiment` inconsistently. Could not find parameter `self` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification/experiment.py:13:23 Incompatible variable type [9]: stage is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification/experiment.py:13:42 Incompatible variable type [9]: mode is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification/experiment.py:17:4 Inconsistent override [14]: `tests._tests_cv_classification.experiment.Experiment.get_datasets` overrides method defined in `catalyst.core.experiment.IExperiment` inconsistently. Could not find parameter `epoch` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification_registry/test1/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification_registry/test1/experiment.py:13:4 Inconsistent override [14]: `tests._tests_cv_classification_registry.test1.experiment.SimpleExperiment.get_transforms` overrides method defined in `catalyst.experiments.config.ConfigExperiment` inconsistently. Could not find parameter `dataset` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification_registry/test1/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification_registry/test1/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification_registry/test1/experiment.py:13:4 Inconsistent override [14]: `tests._tests_cv_classification_registry.test1.experiment.SimpleExperiment.get_transforms` overrides method defined in `catalyst.experiments.config.ConfigExperiment` inconsistently. Could not find parameter `self` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification_registry/test1/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification_registry/test1/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification_registry/test1/experiment.py:13:23 Incompatible variable type [9]: stage is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification_registry/test1/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification_registry/test1/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification_registry/test1/experiment.py:13:42 Incompatible variable type [9]: mode is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification_registry/test1/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification_registry/test1/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification_registry/test1/experiment.py:17:4 Inconsistent override [14]: `tests._tests_cv_classification_registry.test1.experiment.SimpleExperiment.get_datasets` overrides method defined in `catalyst.core.experiment.IExperiment` inconsistently. Could not find parameter `epoch` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification_registry/test1/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment1.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment1.py:13:4 Inconsistent override [14]: `tests._tests_cv_classification_registry.test2.experiments.SimpleExperiment1.SimpleExperiment1.get_transforms` overrides method defined in `catalyst.experiments.config.ConfigExperiment` inconsistently. Could not find parameter `dataset` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment1.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment1.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment1.py:13:4 Inconsistent override [14]: `tests._tests_cv_classification_registry.test2.experiments.SimpleExperiment1.SimpleExperiment1.get_transforms` overrides method defined in `catalyst.experiments.config.ConfigExperiment` inconsistently. Could not find parameter `self` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment1.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment1.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment1.py:13:23 Incompatible variable type [9]: stage is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment1.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment1.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment1.py:13:42 Incompatible variable type [9]: mode is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment1.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment1.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment1.py:17:4 Inconsistent override [14]: `tests._tests_cv_classification_registry.test2.experiments.SimpleExperiment1.SimpleExperiment1.get_datasets` overrides method defined in `catalyst.core.experiment.IExperiment` inconsistently. Could not find parameter `epoch` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment1.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment2.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment2.py:13:4 Inconsistent override [14]: `tests._tests_cv_classification_registry.test2.experiments.SimpleExperiment2.SimpleExperiment2.get_transforms` overrides method defined in `catalyst.experiments.config.ConfigExperiment` inconsistently. Could not find parameter `dataset` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment2.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment2.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment2.py:13:4 Inconsistent override [14]: `tests._tests_cv_classification_registry.test2.experiments.SimpleExperiment2.SimpleExperiment2.get_transforms` overrides method defined in `catalyst.experiments.config.ConfigExperiment` inconsistently. Could not find parameter `self` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment2.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment2.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment2.py:13:23 Incompatible variable type [9]: stage is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment2.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment2.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment2.py:13:42 Incompatible variable type [9]: mode is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment2.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment2.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment2.py:17:4 Inconsistent override [14]: `tests._tests_cv_classification_registry.test2.experiments.SimpleExperiment2.SimpleExperiment2.get_datasets` overrides method defined in `catalyst.core.experiment.IExperiment` inconsistently. Could not find parameter `epoch` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification_registry/test2/experiments/SimpleExperiment2.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification_registry/test3/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification_registry/test3/experiment.py:13:4 Inconsistent override [14]: `tests._tests_cv_classification_registry.test3.experiment.SimpleExperiment.get_transforms` overrides method defined in `catalyst.experiments.config.ConfigExperiment` inconsistently. Could not find parameter `dataset` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification_registry/test3/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification_registry/test3/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification_registry/test3/experiment.py:13:4 Inconsistent override [14]: `tests._tests_cv_classification_registry.test3.experiment.SimpleExperiment.get_transforms` overrides method defined in `catalyst.experiments.config.ConfigExperiment` inconsistently. Could not find parameter `self` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification_registry/test3/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification_registry/test3/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification_registry/test3/experiment.py:13:23 Incompatible variable type [9]: stage is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification_registry/test3/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification_registry/test3/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification_registry/test3/experiment.py:13:42 Incompatible variable type [9]: mode is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification_registry/test3/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification_registry/test3/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification_registry/test3/experiment.py:17:4 Inconsistent override [14]: `tests._tests_cv_classification_registry.test3.experiment.SimpleExperiment.get_datasets` overrides method defined in `catalyst.core.experiment.IExperiment` inconsistently. Could not find parameter `epoch` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification_registry/test3/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_classification_transforms/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_classification_transforms/experiment.py:39:4 Inconsistent override [14]: `tests._tests_cv_classification_transforms.experiment.Experiment.get_datasets` overrides method defined in `catalyst.core.experiment.IExperiment` inconsistently. Could not find parameter `epoch` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_classification_transforms/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_segmentation/dataset.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_segmentation/dataset.py:17:34 Incompatible variable type [9]: masks is declared to have type `List[Path]` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_segmentation/dataset.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_cv_segmentation/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_cv_segmentation/experiment.py:17:4 Inconsistent override [14]: `tests._tests_cv_segmentation.experiment.Experiment.get_datasets` overrides method defined in `catalyst.core.experiment.IExperiment` inconsistently. Could not find parameter `epoch` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_cv_segmentation/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_dl_callbacks/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_dl_callbacks/experiment.py:15:4 Inconsistent override [14]: `tests._tests_dl_callbacks.experiment.Experiment.get_transforms` overrides method defined in `catalyst.experiments.config.ConfigExperiment` inconsistently. Could not find parameter `dataset` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_dl_callbacks/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_dl_callbacks/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_dl_callbacks/experiment.py:15:4 Inconsistent override [14]: `tests._tests_dl_callbacks.experiment.Experiment.get_transforms` overrides method defined in `catalyst.experiments.config.ConfigExperiment` inconsistently. Could not find parameter `self` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_dl_callbacks/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_dl_callbacks/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_dl_callbacks/experiment.py:15:23 Incompatible variable type [9]: stage is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_dl_callbacks/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_dl_callbacks/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_dl_callbacks/experiment.py:15:42 Incompatible variable type [9]: mode is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_dl_callbacks/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_dl_callbacks/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_dl_callbacks/experiment.py:19:4 Inconsistent override [14]: `tests._tests_dl_callbacks.experiment.Experiment.get_datasets` overrides method defined in `catalyst.core.experiment.IExperiment` inconsistently. Could not find parameter `epoch` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_dl_callbacks/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_ml_cmcscore/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_ml_cmcscore/experiment.py:14:4 Inconsistent override [14]: `tests._tests_ml_cmcscore.experiment.Experiment.get_transforms` overrides method defined in `catalyst.experiments.config.ConfigExperiment` inconsistently. Could not find parameter `dataset` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_ml_cmcscore/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_ml_cmcscore/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_ml_cmcscore/experiment.py:14:4 Inconsistent override [14]: `tests._tests_ml_cmcscore.experiment.Experiment.get_transforms` overrides method defined in `catalyst.experiments.config.ConfigExperiment` inconsistently. Could not find parameter `self` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_ml_cmcscore/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_ml_cmcscore/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_ml_cmcscore/experiment.py:14:23 Incompatible variable type [9]: stage is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_ml_cmcscore/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_ml_cmcscore/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_ml_cmcscore/experiment.py:14:42 Incompatible variable type [9]: mode is declared to have type `str` but is used as type `None`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_ml_cmcscore/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_ml_cmcscore/experiment.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_ml_cmcscore/experiment.py:18:4 Inconsistent override [14]: `tests._tests_ml_cmcscore.experiment.Experiment.get_datasets` overrides method defined in `catalyst.core.experiment.IExperiment` inconsistently. Could not find parameter `epoch` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_ml_cmcscore/experiment.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_scripts/core_runner_onecyle_lr_scheduler.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_scripts/core_runner_onecyle_lr_scheduler.py:75:4 Incompatible parameter type [6]: Expected `typing.Union[collections.OrderedDict[str, catalyst.core.callback.Callback], typing.List[catalyst.core.callback.Callback]]` for 9th parameter `callbacks` to call `catalyst.runners.runner.Runner.train` but got `typing.List[LRCheckerCallback]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_scripts/core_runner_onecyle_lr_scheduler.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_scripts/dl_z_docs_distributed_2.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_scripts/dl_z_docs_distributed_2.py:32:4 Incompatible parameter type [6]: Expected `collections.OrderedDict[str, typing.Any]` for 2nd parameter `datasets` to call `catalyst.runners.runner.Runner.train` but got `typing.Dict[str, typing.Any]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_scripts/dl_z_docs_distributed_2.py'",
    "dd_fail": true
  },
  {
    "project": "catalyst-team/catalyst",
    "commit": "e43c62e9cafdd703d5d35779c7aa90e2be761a34",
    "filename": "tests/_tests_scripts/dl_z_mvp_mnist_metric_learning.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/_tests_scripts/dl_z_mvp_mnist_metric_learning.py:66:8 Incompatible parameter type [6]: Expected `typing.Union[collections.OrderedDict[str, catalyst.core.callback.Callback], typing.List[catalyst.core.callback.Callback]]` for 4th parameter `callbacks` to call `catalyst.runners.runner.Runner.train` but got `typing.List[typing.Union[catalyst.callbacks.control_flow.ControlFlowCallback, catalyst.callbacks.periodic_loader.PeriodicLoaderCallback]]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/catalyst-team-catalyst/tests/_tests_scripts/dl_z_mvp_mnist_metric_learning.py'",
    "dd_fail": true
  }
]