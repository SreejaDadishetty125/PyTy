[
  {
    "project": "asyml/forte",
    "commit": "51d8425e1e663254dacb01891b7eda9927f0015f",
    "filename": "examples/NER/main_predict.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asyml-forte/examples/NER/main_predict.py",
    "file_hunks_size": 3,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "examples/NER/main_predict.py:24:12 Incompatible parameter type [6]: Expected `typing.Type[forte.data.ontology.top.Annotation]` for 1st parameter `context_type` to call `forte.data.data_pack.DataPack.get_data` but got `str`.",
    "message": " Expected `typing.Type[forte.data.ontology.top.Annotation]` for 1st parameter `context_type` to call `forte.data.data_pack.DataPack.get_data` but got `str`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 24,
    "warning_line": "            context_type=\"sentence\","
  },
  {
    "project": "asyml/forte",
    "commit": "51d8425e1e663254dacb01891b7eda9927f0015f",
    "filename": "forte/data/batchers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asyml-forte/forte/data/batchers.py",
    "file_hunks_size": 6,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "forte/data/batchers.py:112:12 Incompatible variable type [9]: requests is declared to have type `Dict[Type[forte.data.ontology.top.Entry], Union[Dict[typing.Any, typing.Any], List[typing.Any]]]` but is used as type `None`.",
    "message": " requests is declared to have type `Dict[Type[forte.data.ontology.top.Entry], Union[Dict[typing.Any, typing.Any], List[typing.Any]]]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 112,
    "warning_line": "            requests: Dict[Type[Entry], Union[Dict, List]] = None,"
  },
  {
    "project": "asyml/forte",
    "commit": "51d8425e1e663254dacb01891b7eda9927f0015f",
    "filename": "forte/data/batchers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asyml-forte/forte/data/batchers.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "forte/data/batchers.py:206:12 Incompatible variable type [9]: requests is declared to have type `Dict[Type[forte.data.ontology.top.Entry], Union[Dict[typing.Any, typing.Any], List[typing.Any]]]` but is used as type `None`.",
    "message": " requests is declared to have type `Dict[Type[forte.data.ontology.top.Entry], Union[Dict[typing.Any, typing.Any], List[typing.Any]]]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 206,
    "warning_line": "            requests: Dict[Type[Entry], Union[Dict, List]] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n\nclass TxtgenMultiPackProcessingBatcher(Batcher):\n    \"\"\"\n    A Batcher used in ``MultiPackBatchProcessors``.\n    The Batcher calles the ProcessingBatcher inherently on each specified\n    data pack in the MultiPack.\n\n    It's flexible to query MultiPack so we delegate the task to the subclasses.\n    Such as:\n    - query all packs with the same ``context`` and ``input_info``.\n    - query different packs with different ``context``s and ``input_info``s.\n    Since the batcher will save the data_pack_pool on the fly, it's not trivial\n        to batching and slicing multiple data packs in the same time\n    \"\"\"\n\n    def __init__(self, input_pack_name, batch_size: int,\n                 hard_batch: bool = False,\n                 ):\n        super().__init__(batch_size)\n        self.hard_batch = hard_batch\n\n        self.current_batch: Dict = {}\n        self.instance_num_in_current_batch = 0\n\n        self.data_pack_pool: List[MultiPack] = []\n        self.current_batch_sources: List[int] = []\n\n        self.input_pack_name = input_pack_name\n\n    def get_batch(  # type: ignore\n            self,\n            input_pack: Optional[MultiPack],\n            context_type: Type[Annotation],\n            requests: Dict[Type[Entry], Union[Dict, List]] = None,\n            tail_instances: bool = False):\n\n        if input_pack is None:  # No more packs, return the tail instances\n            if self.current_batch:\n                yield self.current_batch\n                self.current_batch = {}\n                self.instance_num_in_current_batch = 0\n                self.current_batch_sources = []\n        else:  # cache the new pack and generate batches\n            self.data_pack_pool.append(input_pack)\n            for (data_batch, instance_num) in self._get_data_batch_by_need(\n                    input_pack, context_type, requests):\n\n                self.current_batch = merge_batches(\n                    [self.current_batch, data_batch])\n\n                self.instance_num_in_current_batch += instance_num\n                self.current_batch_sources.append(instance_num)\n\n                if (tail_instances or not self.hard_batch or\n                        self.instance_num_in_current_batch == self.batch_size):\n                    yield self.current_batch\n\n                    self.current_batch = {}\n                    self.instance_num_in_current_batch = 0\n                    self.current_batch_sources = []\n\n    def _get_data_batch_by_need(\n            self,\n            data_pack: MultiPack,\n            context_type: Type[Annotation],\n            requests: Optional[Dict[Type[Entry], Union[Dict, List]]] = None,\n            offset: int = 0) -> Iterable[Tuple[Dict, int]]:\n        \"\"\"\n        Try to get batches of size ``batch_size``. If the tail instances cannot\n        make up a full batch, will generate a small batch with the tail\n        instances.\n\n        Returns:\n            An iterator of tuples ``(batch, cnt)``, ``batch`` is a dict\n            containing the required annotations and context, and ``cnt`` is\n            the number of instances in the batch.\n        \"\"\"\n\n        instances: List[Dict] = []\n        input_pack = data_pack.packs[self.input_pack_name]\n        for data in input_pack.get_data(context_type, requests, offset):\n\n            instances.append(data)\n            if (len(instances) ==\n                    self.batch_size - self.instance_num_in_current_batch):\n                batch = batch_instances(instances)\n                yield (batch, len(instances))\n                instances = []\n\n        if len(instances):\n            batch = batch_instances(instances)\n            yield (batch, len(instances))\n",
        "source_code_len": 3651,
        "target_code": "\n    @staticmethod\n    def default_hparams() -> Dict:\n        return {\n            'input_pack_name': 'source',\n        }\n",
        "target_code_len": 122,
        "diff_format": "@@ -172,93 +240,6 @@\n \n-\n-class TxtgenMultiPackProcessingBatcher(Batcher):\n-    \"\"\"\n-    A Batcher used in ``MultiPackBatchProcessors``.\n-    The Batcher calles the ProcessingBatcher inherently on each specified\n-    data pack in the MultiPack.\n-\n-    It's flexible to query MultiPack so we delegate the task to the subclasses.\n-    Such as:\n-    - query all packs with the same ``context`` and ``input_info``.\n-    - query different packs with different ``context``s and ``input_info``s.\n-    Since the batcher will save the data_pack_pool on the fly, it's not trivial\n-        to batching and slicing multiple data packs in the same time\n-    \"\"\"\n-\n-    def __init__(self, input_pack_name, batch_size: int,\n-                 hard_batch: bool = False,\n-                 ):\n-        super().__init__(batch_size)\n-        self.hard_batch = hard_batch\n-\n-        self.current_batch: Dict = {}\n-        self.instance_num_in_current_batch = 0\n-\n-        self.data_pack_pool: List[MultiPack] = []\n-        self.current_batch_sources: List[int] = []\n-\n-        self.input_pack_name = input_pack_name\n-\n-    def get_batch(  # type: ignore\n-            self,\n-            input_pack: Optional[MultiPack],\n-            context_type: Type[Annotation],\n-            requests: Dict[Type[Entry], Union[Dict, List]] = None,\n-            tail_instances: bool = False):\n-\n-        if input_pack is None:  # No more packs, return the tail instances\n-            if self.current_batch:\n-                yield self.current_batch\n-                self.current_batch = {}\n-                self.instance_num_in_current_batch = 0\n-                self.current_batch_sources = []\n-        else:  # cache the new pack and generate batches\n-            self.data_pack_pool.append(input_pack)\n-            for (data_batch, instance_num) in self._get_data_batch_by_need(\n-                    input_pack, context_type, requests):\n-\n-                self.current_batch = merge_batches(\n-                    [self.current_batch, data_batch])\n-\n-                self.instance_num_in_current_batch += instance_num\n-                self.current_batch_sources.append(instance_num)\n-\n-                if (tail_instances or not self.hard_batch or\n-                        self.instance_num_in_current_batch == self.batch_size):\n-                    yield self.current_batch\n-\n-                    self.current_batch = {}\n-                    self.instance_num_in_current_batch = 0\n-                    self.current_batch_sources = []\n-\n-    def _get_data_batch_by_need(\n-            self,\n-            data_pack: MultiPack,\n-            context_type: Type[Annotation],\n-            requests: Optional[Dict[Type[Entry], Union[Dict, List]]] = None,\n-            offset: int = 0) -> Iterable[Tuple[Dict, int]]:\n-        \"\"\"\n-        Try to get batches of size ``batch_size``. If the tail instances cannot\n-        make up a full batch, will generate a small batch with the tail\n-        instances.\n-\n-        Returns:\n-            An iterator of tuples ``(batch, cnt)``, ``batch`` is a dict\n-            containing the required annotations and context, and ``cnt`` is\n-            the number of instances in the batch.\n-        \"\"\"\n-\n-        instances: List[Dict] = []\n-        input_pack = data_pack.packs[self.input_pack_name]\n-        for data in input_pack.get_data(context_type, requests, offset):\n-\n-            instances.append(data)\n-            if (len(instances) ==\n-                    self.batch_size - self.instance_num_in_current_batch):\n-                batch = batch_instances(instances)\n-                yield (batch, len(instances))\n-                instances = []\n-\n-        if len(instances):\n-            batch = batch_instances(instances)\n-            yield (batch, len(instances))\n+    @staticmethod\n+    def default_hparams() -> Dict:\n+        return {\n+            'input_pack_name': 'source',\n+        }\n",
        "source_code_with_indent": "\n\n<DED><DED><DED>class TxtgenMultiPackProcessingBatcher(Batcher):\n    <IND>\"\"\"\n    A Batcher used in ``MultiPackBatchProcessors``.\n    The Batcher calles the ProcessingBatcher inherently on each specified\n    data pack in the MultiPack.\n\n    It's flexible to query MultiPack so we delegate the task to the subclasses.\n    Such as:\n    - query all packs with the same ``context`` and ``input_info``.\n    - query different packs with different ``context``s and ``input_info``s.\n    Since the batcher will save the data_pack_pool on the fly, it's not trivial\n        to batching and slicing multiple data packs in the same time\n    \"\"\"\n\n    def __init__(self, input_pack_name, batch_size: int,\n                 hard_batch: bool = False,\n                 ):\n        <IND>super().__init__(batch_size)\n        self.hard_batch = hard_batch\n\n        self.current_batch: Dict = {}\n        self.instance_num_in_current_batch = 0\n\n        self.data_pack_pool: List[MultiPack] = []\n        self.current_batch_sources: List[int] = []\n\n        self.input_pack_name = input_pack_name\n\n    <DED>def get_batch(  # type: ignore\n            self,\n            input_pack: Optional[MultiPack],\n            context_type: Type[Annotation],\n            requests: Dict[Type[Entry], Union[Dict, List]] = None,\n            tail_instances: bool = False):\n\n        <IND>if input_pack is None:  # No more packs, return the tail instances\n            <IND>if self.current_batch:\n                <IND>yield self.current_batch\n                self.current_batch = {}\n                self.instance_num_in_current_batch = 0\n                self.current_batch_sources = []\n        <DED><DED>else:  # cache the new pack and generate batches\n            <IND>self.data_pack_pool.append(input_pack)\n            for (data_batch, instance_num) in self._get_data_batch_by_need(\n                    input_pack, context_type, requests):\n\n                <IND>self.current_batch = merge_batches(\n                    [self.current_batch, data_batch])\n\n                self.instance_num_in_current_batch += instance_num\n                self.current_batch_sources.append(instance_num)\n\n                if (tail_instances or not self.hard_batch or\n                        self.instance_num_in_current_batch == self.batch_size):\n                    <IND>yield self.current_batch\n\n                    self.current_batch = {}\n                    self.instance_num_in_current_batch = 0\n                    self.current_batch_sources = []\n\n    <DED><DED><DED><DED>def _get_data_batch_by_need(\n            self,\n            data_pack: MultiPack,\n            context_type: Type[Annotation],\n            requests: Optional[Dict[Type[Entry], Union[Dict, List]]] = None,\n            offset: int = 0) -> Iterable[Tuple[Dict, int]]:\n        <IND>\"\"\"\n        Try to get batches of size ``batch_size``. If the tail instances cannot\n        make up a full batch, will generate a small batch with the tail\n        instances.\n\n        Returns:\n            An iterator of tuples ``(batch, cnt)``, ``batch`` is a dict\n            containing the required annotations and context, and ``cnt`` is\n            the number of instances in the batch.\n        \"\"\"\n\n        instances: List[Dict] = []\n        input_pack = data_pack.packs[self.input_pack_name]\n        for data in input_pack.get_data(context_type, requests, offset):\n\n            <IND>instances.append(data)\n            if (len(instances) ==\n                    self.batch_size - self.instance_num_in_current_batch):\n                <IND>batch = batch_instances(instances)\n                yield (batch, len(instances))\n                instances = []\n\n        <DED><DED>if len(instances):\n            <IND>batch = batch_instances(instances)\n            yield (batch, len(instances))\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED><DED>@staticmethod\n    def default_hparams() -> Dict:\n        <IND>return {\n            'input_pack_name': 'source',\n        }\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "asyml/forte",
    "commit": "51d8425e1e663254dacb01891b7eda9927f0015f",
    "filename": "forte/data/readers/file_reader.py",
    "min_patch_found": false,
    "full_warning_msg": "forte/data/readers/file_reader.py:28:4 Inconsistent override [14]: `forte.data.readers.file_reader.MonoFileReader._cache_key_function` overrides method defined in `forte.data.readers.base_reader.BaseReader` inconsistently. Could not find parameter `collection` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/asyml-forte/forte/data/readers/file_reader.py'",
    "dd_fail": true
  },
  {
    "project": "asyml/forte",
    "commit": "51d8425e1e663254dacb01891b7eda9927f0015f",
    "filename": "forte/data/readers/file_reader.py",
    "min_patch_found": false,
    "full_warning_msg": "forte/data/readers/file_reader.py:41:4 Inconsistent override [14]: `forte.data.readers.file_reader.MonoFileReader.parse_pack` overrides method defined in `forte.data.readers.base_reader.BaseReader` inconsistently. Could not find parameter `collection` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/asyml-forte/forte/data/readers/file_reader.py'",
    "dd_fail": true
  },
  {
    "project": "asyml/forte",
    "commit": "51d8425e1e663254dacb01891b7eda9927f0015f",
    "filename": "forte/processors/dummy_processor.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asyml-forte/forte/processors/dummy_batch_processor.py",
    "file_hunks_size": 7,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "forte/processors/dummy_processor.py:78:4 Inconsistent override [14]: `forte.processors.dummy_processor.DummyRelationExtractor.pack` overrides method defined in `forte.processors.base.batch_processor.BaseBatchProcessor` inconsistently. Could not find parameter `inputs` in overriding signature.",
    "message": " `forte.processors.dummy_processor.DummyRelationExtractor.pack` overrides method defined in `forte.processors.base.batch_processor.BaseBatchProcessor` inconsistently. Could not find parameter `inputs` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 78,
    "warning_line": "    def pack(self, data_pack: DataPack, output_dict: Optional[Dict] = None):"
  },
  {
    "project": "asyml/forte",
    "commit": "51d8425e1e663254dacb01891b7eda9927f0015f",
    "filename": "forte/processors/dummy_processor.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asyml-forte/forte/processors/dummy_batch_processor.py",
    "file_hunks_size": 7,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "forte/processors/dummy_processor.py:78:4 Inconsistent override [14]: `forte.processors.dummy_processor.DummyRelationExtractor.pack` overrides method defined in `forte.processors.base.batch_processor.BaseBatchProcessor` inconsistently. Could not find parameter `pack` in overriding signature.",
    "message": " `forte.processors.dummy_processor.DummyRelationExtractor.pack` overrides method defined in `forte.processors.base.batch_processor.BaseBatchProcessor` inconsistently. Could not find parameter `pack` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 78,
    "warning_line": "    def pack(self, data_pack: DataPack, output_dict: Optional[Dict] = None):"
  },
  {
    "project": "asyml/forte",
    "commit": "51d8425e1e663254dacb01891b7eda9927f0015f",
    "filename": "forte/processors/txtgen_predictor.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asyml-forte/forte/processors/txtgen_predictor.py",
    "file_hunks_size": 9,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "forte/processors/txtgen_predictor.py:208:4 Inconsistent override [14]: `forte.processors.txtgen_predictor.TxtgenPredictor._record_fields` overrides method defined in `forte.processors.base.base_processor.BaseProcessor` inconsistently. Could not find parameter `input_pack` in overriding signature.",
    "message": " `forte.processors.txtgen_predictor.TxtgenPredictor._record_fields` overrides method defined in `forte.processors.base.base_processor.BaseProcessor` inconsistently. Could not find parameter `input_pack` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 208,
    "warning_line": "    def _record_fields(self, data_pack: MultiPack):"
  }
]