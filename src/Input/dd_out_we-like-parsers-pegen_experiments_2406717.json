[
  {
    "project": "we-like-parsers/pegen_experiments",
    "commit": "2406717bb6958166e42baf543214e1c464995190",
    "filename": "pegen/tokenizer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/we-like-parsers-pegen_experiments/pegen/tokenizer.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pegen/tokenizer.py:45:43 Incompatible parameter type [6]: Expected `tokenize.TokenError` for 1st positional only parameter to call `Tokenizer.fix_token_error` but got `typing.Union[IndentationError, tokenize.TokenError]`.",
    "message": " Expected `tokenize.TokenError` for 1st positional only parameter to call `Tokenizer.fix_token_error` but got `typing.Union[IndentationError, tokenize.TokenError]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 45,
    "warning_line": "                tok = self.fix_token_error(err)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    def fix_token_error(self, err: tokenize.TokenError) -> tokenize.TokenInfo:\n        msg = err.args[0]\n",
        "source_code_len": 106,
        "target_code": "\n    def fix_token_error(self, err: Exception) -> tokenize.TokenInfo:\n        msg = err.args[0]\n",
        "target_code_len": 96,
        "diff_format": "@@ -73,3 +73,3 @@\n \n-    def fix_token_error(self, err: tokenize.TokenError) -> tokenize.TokenInfo:\n+    def fix_token_error(self, err: Exception) -> tokenize.TokenInfo:\n         msg = err.args[0]\n",
        "source_code_with_indent": "\n    <DED>def fix_token_error(self, err: tokenize.TokenError) -> tokenize.TokenInfo:\n        <IND>msg = err.args[0]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def fix_token_error(self, err: Exception) -> tokenize.TokenInfo:\n        <IND>msg = err.args[0]\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "we-like-parsers/pegen_experiments",
    "commit": "2406717bb6958166e42baf543214e1c464995190",
    "filename": "pegen/tokenizer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/we-like-parsers-pegen_experiments/pegen/tokenizer.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "pegen/tokenizer.py:65:43 Incompatible parameter type [6]: Expected `tokenize.TokenError` for 1st positional only parameter to call `Tokenizer.fix_token_error` but got `typing.Union[IndentationError, tokenize.TokenError]`.",
    "message": " Expected `tokenize.TokenError` for 1st positional only parameter to call `Tokenizer.fix_token_error` but got `typing.Union[IndentationError, tokenize.TokenError]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 65,
    "warning_line": "                tok = self.fix_token_error(err)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    def fix_token_error(self, err: tokenize.TokenError) -> tokenize.TokenInfo:\n        msg = err.args[0]\n",
        "source_code_len": 106,
        "target_code": "\n    def fix_token_error(self, err: Exception) -> tokenize.TokenInfo:\n        msg = err.args[0]\n",
        "target_code_len": 96,
        "diff_format": "@@ -73,3 +73,3 @@\n \n-    def fix_token_error(self, err: tokenize.TokenError) -> tokenize.TokenInfo:\n+    def fix_token_error(self, err: Exception) -> tokenize.TokenInfo:\n         msg = err.args[0]\n",
        "source_code_with_indent": "\n    <DED>def fix_token_error(self, err: tokenize.TokenError) -> tokenize.TokenInfo:\n        <IND>msg = err.args[0]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def fix_token_error(self, err: Exception) -> tokenize.TokenInfo:\n        <IND>msg = err.args[0]\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  }
]