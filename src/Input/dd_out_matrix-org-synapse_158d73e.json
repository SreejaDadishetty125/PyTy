[
  {
    "project": "matrix-org/synapse",
    "commit": "158d73ebdd61eef33831ae5f6990acf07244fc55",
    "filename": "synapse/config/cache.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/matrix-org-synapse/synapse/config/cache.py",
    "file_hunks_size": 8,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "synapse/config/cache.py:88:8 Call error [29]: `Optional[typing.Callable[[], None]]` is not a function.",
    "message": " `Optional[typing.Callable[[], None]]` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 88,
    "warning_line": "        properties.resize_all_caches_func()",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n@attr.s(slots=True, auto_attribs=True)\nclass CacheProperties:\n    # The default factor size for all caches\n    default_factor_size: float = float(\n        os.environ.get(_CACHE_PREFIX, _DEFAULT_FACTOR_SIZE)\n    )\n    resize_all_caches_func: Optional[Callable[[], None]] = None\n\n",
        "source_code_len": 279,
        "target_code": "\nclass CacheProperties:\n    def __init__(self):\n        # The default factor size for all caches\n        self.default_factor_size = float(\n            os.environ.get(_CACHE_PREFIX, _DEFAULT_FACTOR_SIZE)\n        )\n        self.resize_all_caches_func = None\n\n",
        "target_code_len": 257,
        "diff_format": "@@ -38,9 +36,9 @@\n \n-@attr.s(slots=True, auto_attribs=True)\n class CacheProperties:\n-    # The default factor size for all caches\n-    default_factor_size: float = float(\n-        os.environ.get(_CACHE_PREFIX, _DEFAULT_FACTOR_SIZE)\n-    )\n-    resize_all_caches_func: Optional[Callable[[], None]] = None\n+    def __init__(self):\n+        # The default factor size for all caches\n+        self.default_factor_size = float(\n+            os.environ.get(_CACHE_PREFIX, _DEFAULT_FACTOR_SIZE)\n+        )\n+        self.resize_all_caches_func = None\n \n",
        "source_code_with_indent": "\n@attr.s(slots=True, auto_attribs=True)\nclass CacheProperties:\n    # The default factor size for all caches\n    <IND>default_factor_size: float = float(\n        os.environ.get(_CACHE_PREFIX, _DEFAULT_FACTOR_SIZE)\n    )\n    resize_all_caches_func: Optional[Callable[[], None]] = None\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\nclass CacheProperties:\n    <IND>def __init__(self):\n        # The default factor size for all caches\n        <IND>self.default_factor_size = float(\n            os.environ.get(_CACHE_PREFIX, _DEFAULT_FACTOR_SIZE)\n        )\n        self.resize_all_caches_func = None\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "matrix-org/synapse",
    "commit": "158d73ebdd61eef33831ae5f6990acf07244fc55",
    "filename": "synapse/handlers/auth.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/matrix-org-synapse/synapse/handlers/auth.py",
    "file_hunks_size": 24,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "synapse/handlers/auth.py:753:36 Call error [29]: `Variable[_VT]` is not a function.",
    "message": " `Variable[_VT]` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 753,
    "warning_line": "                    params[stage] = get_params[stage]()"
  },
  {
    "project": "matrix-org/synapse",
    "commit": "158d73ebdd61eef33831ae5f6990acf07244fc55",
    "filename": "synapse/storage/background_updates.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/matrix-org-synapse/synapse/storage/background_updates.py",
    "file_hunks_size": 17,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "synapse/storage/background_updates.py:217:8 Incompatible return type [7]: Expected `AsyncContextManager[int]` but got `_BackgroundUpdateContextManager`.",
    "message": " Expected `AsyncContextManager[int]` but got `_BackgroundUpdateContextManager`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 217,
    "warning_line": "        return _BackgroundUpdateContextManager(sleep, self._clock)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        self._on_update_callback: Optional[ON_UPDATE_CALLBACK] = None\n        self._default_batch_size_callback: Optional[DEFAULT_BATCH_SIZE_CALLBACK] = None\n        self._min_batch_size_callback: Optional[MIN_BATCH_SIZE_CALLBACK] = None\n\n        self._background_update_performance: Dict[str, BackgroundUpdatePerformance] = {}\n        self._background_update_handlers: Dict[str, _BackgroundUpdateHandler] = {}\n        self._all_done = False\n",
        "source_code_len": 443,
        "target_code": "\n        self._background_update_performance: Dict[str, BackgroundUpdatePerformance] = {}\n        self._background_update_handlers: Dict[\n            str, Callable[[JsonDict, int], Awaitable[int]]\n        ] = {}\n        self._all_done = False\n",
        "target_code_len": 243,
        "diff_format": "@@ -145,8 +96,6 @@\n \n-        self._on_update_callback: Optional[ON_UPDATE_CALLBACK] = None\n-        self._default_batch_size_callback: Optional[DEFAULT_BATCH_SIZE_CALLBACK] = None\n-        self._min_batch_size_callback: Optional[MIN_BATCH_SIZE_CALLBACK] = None\n-\n         self._background_update_performance: Dict[str, BackgroundUpdatePerformance] = {}\n-        self._background_update_handlers: Dict[str, _BackgroundUpdateHandler] = {}\n+        self._background_update_handlers: Dict[\n+            str, Callable[[JsonDict, int], Awaitable[int]]\n+        ] = {}\n         self._all_done = False\n",
        "source_code_with_indent": "\n        self._on_update_callback: Optional[ON_UPDATE_CALLBACK] = None\n        self._default_batch_size_callback: Optional[DEFAULT_BATCH_SIZE_CALLBACK] = None\n        self._min_batch_size_callback: Optional[MIN_BATCH_SIZE_CALLBACK] = None\n\n        self._background_update_performance: Dict[str, BackgroundUpdatePerformance] = {}\n        self._background_update_handlers: Dict[str, _BackgroundUpdateHandler] = {}\n        self._all_done = False\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        self._background_update_performance: Dict[str, BackgroundUpdatePerformance] = {}\n        self._background_update_handlers: Dict[\n            str, Callable[[JsonDict, int], Awaitable[int]]\n        ] = {}\n        self._all_done = False\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        self.enabled = True\n\n    def register_update_controller_callbacks(\n        self,\n        on_update: ON_UPDATE_CALLBACK,\n        default_batch_size: Optional[DEFAULT_BATCH_SIZE_CALLBACK] = None,\n        min_batch_size: Optional[DEFAULT_BATCH_SIZE_CALLBACK] = None,\n    ) -> None:\n        \"\"\"Register callbacks from a module for each hook.\"\"\"\n        if self._on_update_callback is not None:\n            logger.warning(\n                \"More than one module tried to register callbacks for controlling\"\n                \" background updates. Only the callbacks registered by the first module\"\n                \" (in order of appearance in Synapse's configuration file) that tried to\"\n                \" do so will be called.\"\n            )\n\n            return\n\n        self._on_update_callback = on_update\n\n        if default_batch_size is not None:\n            self._default_batch_size_callback = default_batch_size\n\n        if min_batch_size is not None:\n            self._min_batch_size_callback = min_batch_size\n\n    def _get_context_manager_for_update(\n        self,\n        sleep: bool,\n        update_name: str,\n        database_name: str,\n        oneshot: bool,\n    ) -> AsyncContextManager[int]:\n        \"\"\"Get a context manager to run a background update with.\n\n        If a module has registered a `update_handler` callback, use the context manager\n        it returns.\n\n        Otherwise, returns a context manager that will return a default value, optionally\n        sleeping if needed.\n\n        Args:\n            sleep: Whether we can sleep between updates.\n            update_name: The name of the update.\n            database_name: The name of the database the update is being run on.\n            oneshot: Whether the update will complete all in one go, e.g. index creation.\n                In such cases the returned target duration is ignored.\n\n        Returns:\n            The target duration in milliseconds that the background update should run for.\n\n            Note: this is a *target*, and an iteration may take substantially longer or\n            shorter.\n        \"\"\"\n        if self._on_update_callback is not None:\n            return self._on_update_callback(update_name, database_name, oneshot)\n\n        return _BackgroundUpdateContextManager(sleep, self._clock)\n\n    async def _default_batch_size(self, update_name: str, database_name: str) -> int:\n        \"\"\"The batch size to use for the first iteration of a new background\n        update.\n        \"\"\"\n        if self._default_batch_size_callback is not None:\n            return await self._default_batch_size_callback(update_name, database_name)\n\n        return self.DEFAULT_BACKGROUND_BATCH_SIZE\n\n    async def _min_batch_size(self, update_name: str, database_name: str) -> int:\n        \"\"\"A lower bound on the batch size of a new background update.\n\n        Used to ensure that progress is always made. Must be greater than 0.\n        \"\"\"\n        if self._min_batch_size_callback is not None:\n            return await self._min_batch_size_callback(update_name, database_name)\n\n        return self.MINIMUM_BACKGROUND_BATCH_SIZE\n\n",
        "source_code_len": 3114,
        "target_code": "        self.enabled = True\n\n",
        "target_code_len": 29,
        "diff_format": "@@ -159,79 +108,2 @@\n         self.enabled = True\n-\n-    def register_update_controller_callbacks(\n-        self,\n-        on_update: ON_UPDATE_CALLBACK,\n-        default_batch_size: Optional[DEFAULT_BATCH_SIZE_CALLBACK] = None,\n-        min_batch_size: Optional[DEFAULT_BATCH_SIZE_CALLBACK] = None,\n-    ) -> None:\n-        \"\"\"Register callbacks from a module for each hook.\"\"\"\n-        if self._on_update_callback is not None:\n-            logger.warning(\n-                \"More than one module tried to register callbacks for controlling\"\n-                \" background updates. Only the callbacks registered by the first module\"\n-                \" (in order of appearance in Synapse's configuration file) that tried to\"\n-                \" do so will be called.\"\n-            )\n-\n-            return\n-\n-        self._on_update_callback = on_update\n-\n-        if default_batch_size is not None:\n-            self._default_batch_size_callback = default_batch_size\n-\n-        if min_batch_size is not None:\n-            self._min_batch_size_callback = min_batch_size\n-\n-    def _get_context_manager_for_update(\n-        self,\n-        sleep: bool,\n-        update_name: str,\n-        database_name: str,\n-        oneshot: bool,\n-    ) -> AsyncContextManager[int]:\n-        \"\"\"Get a context manager to run a background update with.\n-\n-        If a module has registered a `update_handler` callback, use the context manager\n-        it returns.\n-\n-        Otherwise, returns a context manager that will return a default value, optionally\n-        sleeping if needed.\n-\n-        Args:\n-            sleep: Whether we can sleep between updates.\n-            update_name: The name of the update.\n-            database_name: The name of the database the update is being run on.\n-            oneshot: Whether the update will complete all in one go, e.g. index creation.\n-                In such cases the returned target duration is ignored.\n-\n-        Returns:\n-            The target duration in milliseconds that the background update should run for.\n-\n-            Note: this is a *target*, and an iteration may take substantially longer or\n-            shorter.\n-        \"\"\"\n-        if self._on_update_callback is not None:\n-            return self._on_update_callback(update_name, database_name, oneshot)\n-\n-        return _BackgroundUpdateContextManager(sleep, self._clock)\n-\n-    async def _default_batch_size(self, update_name: str, database_name: str) -> int:\n-        \"\"\"The batch size to use for the first iteration of a new background\n-        update.\n-        \"\"\"\n-        if self._default_batch_size_callback is not None:\n-            return await self._default_batch_size_callback(update_name, database_name)\n-\n-        return self.DEFAULT_BACKGROUND_BATCH_SIZE\n-\n-    async def _min_batch_size(self, update_name: str, database_name: str) -> int:\n-        \"\"\"A lower bound on the batch size of a new background update.\n-\n-        Used to ensure that progress is always made. Must be greater than 0.\n-        \"\"\"\n-        if self._min_batch_size_callback is not None:\n-            return await self._min_batch_size_callback(update_name, database_name)\n-\n-        return self.MINIMUM_BACKGROUND_BATCH_SIZE\n \n",
        "source_code_with_indent": "        self.enabled = True\n\n    <DED>def register_update_controller_callbacks(\n        self,\n        on_update: ON_UPDATE_CALLBACK,\n        default_batch_size: Optional[DEFAULT_BATCH_SIZE_CALLBACK] = None,\n        min_batch_size: Optional[DEFAULT_BATCH_SIZE_CALLBACK] = None,\n    ) -> None:\n        <IND>\"\"\"Register callbacks from a module for each hook.\"\"\"\n        if self._on_update_callback is not None:\n            <IND>logger.warning(\n                \"More than one module tried to register callbacks for controlling\"\n                \" background updates. Only the callbacks registered by the first module\"\n                \" (in order of appearance in Synapse's configuration file) that tried to\"\n                \" do so will be called.\"\n            )\n\n            return\n\n        <DED>self._on_update_callback = on_update\n\n        if default_batch_size is not None:\n            <IND>self._default_batch_size_callback = default_batch_size\n\n        <DED>if min_batch_size is not None:\n            <IND>self._min_batch_size_callback = min_batch_size\n\n    <DED><DED>def _get_context_manager_for_update(\n        self,\n        sleep: bool,\n        update_name: str,\n        database_name: str,\n        oneshot: bool,\n    ) -> AsyncContextManager[int]:\n        <IND>\"\"\"Get a context manager to run a background update with.\n\n        If a module has registered a `update_handler` callback, use the context manager\n        it returns.\n\n        Otherwise, returns a context manager that will return a default value, optionally\n        sleeping if needed.\n\n        Args:\n            sleep: Whether we can sleep between updates.\n            update_name: The name of the update.\n            database_name: The name of the database the update is being run on.\n            oneshot: Whether the update will complete all in one go, e.g. index creation.\n                In such cases the returned target duration is ignored.\n\n        Returns:\n            The target duration in milliseconds that the background update should run for.\n\n            Note: this is a *target*, and an iteration may take substantially longer or\n            shorter.\n        \"\"\"\n        if self._on_update_callback is not None:\n            <IND>return self._on_update_callback(update_name, database_name, oneshot)\n\n        <DED>return _BackgroundUpdateContextManager(sleep, self._clock)\n\n    <DED>async def _default_batch_size(self, update_name: str, database_name: str) -> int:\n        <IND>\"\"\"The batch size to use for the first iteration of a new background\n        update.\n        \"\"\"\n        if self._default_batch_size_callback is not None:\n            <IND>return await self._default_batch_size_callback(update_name, database_name)\n\n        <DED>return self.DEFAULT_BACKGROUND_BATCH_SIZE\n\n    <DED>async def _min_batch_size(self, update_name: str, database_name: str) -> int:\n        <IND>\"\"\"A lower bound on the batch size of a new background update.\n\n        Used to ensure that progress is always made. Must be greater than 0.\n        \"\"\"\n        if self._min_batch_size_callback is not None:\n            <IND>return await self._min_batch_size_callback(update_name, database_name)\n\n        <DED>return self.MINIMUM_BACKGROUND_BATCH_SIZE\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        self.enabled = True\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    async def do_next_background_update(self, sleep: bool = True) -> bool:\n        \"\"\"Does some amount of work on the next queued background update\n",
        "source_code_len": 149,
        "target_code": "\n    async def do_next_background_update(self, desired_duration_ms: float) -> bool:\n        \"\"\"Does some amount of work on the next queued background update\n",
        "target_code_len": 157,
        "diff_format": "@@ -328,3 +205,3 @@\n \n-    async def do_next_background_update(self, sleep: bool = True) -> bool:\n+    async def do_next_background_update(self, desired_duration_ms: float) -> bool:\n         \"\"\"Does some amount of work on the next queued background update\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n    <DED>async def do_next_background_update(self, sleep: bool = True) -> bool:\n        <IND>",
        "target_code_with_indent": "\n    <DED>async def do_next_background_update(self, desired_duration_ms: float) -> bool:\n        <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        # We have a background update to run, otherwise we would have returned\n        # early.\n        assert self._current_background_update is not None\n        update_info = self._background_update_handlers[self._current_background_update]\n\n        async with self._get_context_manager_for_update(\n            sleep=sleep,\n            update_name=self._current_background_update,\n            database_name=self._database_name,\n            oneshot=update_info.oneshot,\n        ) as desired_duration_ms:\n            await self._do_background_update(desired_duration_ms)\n\n        return False\n",
        "source_code_len": 594,
        "target_code": "\n        await self._do_background_update(desired_duration_ms)\n        return False\n",
        "target_code_len": 84,
        "diff_format": "@@ -379,15 +254,3 @@\n \n-        # We have a background update to run, otherwise we would have returned\n-        # early.\n-        assert self._current_background_update is not None\n-        update_info = self._background_update_handlers[self._current_background_update]\n-\n-        async with self._get_context_manager_for_update(\n-            sleep=sleep,\n-            update_name=self._current_background_update,\n-            database_name=self._database_name,\n-            oneshot=update_info.oneshot,\n-        ) as desired_duration_ms:\n-            await self._do_background_update(desired_duration_ms)\n-\n+        await self._do_background_update(desired_duration_ms)\n         return False\n",
        "source_code_with_indent": "\n        # We have a background update to run, otherwise we would have returned\n        # early.\n        <DED>assert self._current_background_update is not None\n        update_info = self._background_update_handlers[self._current_background_update]\n\n        async with self._get_context_manager_for_update(\n            sleep=sleep,\n            update_name=self._current_background_update,\n            database_name=self._database_name,\n            oneshot=update_info.oneshot,\n        ) as desired_duration_ms:\n            <IND>await self._do_background_update(desired_duration_ms)\n\n        <DED>return False\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        <DED>await self._do_background_update(desired_duration_ms)\n        return False\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        update_handler = self._background_update_handlers[update_name].callback\n\n",
        "source_code_len": 82,
        "target_code": "\n        update_handler = self._background_update_handlers[update_name]\n\n",
        "target_code_len": 73,
        "diff_format": "@@ -399,3 +262,3 @@\n \n-        update_handler = self._background_update_handlers[update_name].callback\n+        update_handler = self._background_update_handlers[update_name]\n \n",
        "source_code_with_indent": "\n        update_handler = self._background_update_handlers[update_name].callback\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        update_handler = self._background_update_handlers[update_name]\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "            # Clamp the batch size so that we always make progress\n            batch_size = max(\n                batch_size,\n                await self._min_batch_size(update_name, self._database_name),\n            )\n        else:\n            batch_size = await self._default_batch_size(\n                update_name, self._database_name\n            )\n\n",
        "source_code_len": 352,
        "target_code": "            # Clamp the batch size so that we always make progress\n            batch_size = max(batch_size, self.MINIMUM_BACKGROUND_BATCH_SIZE)\n        else:\n            batch_size = self.DEFAULT_BACKGROUND_BATCH_SIZE\n\n",
        "target_code_len": 219,
        "diff_format": "@@ -412,10 +275,5 @@\n             # Clamp the batch size so that we always make progress\n-            batch_size = max(\n-                batch_size,\n-                await self._min_batch_size(update_name, self._database_name),\n-            )\n+            batch_size = max(batch_size, self.MINIMUM_BACKGROUND_BATCH_SIZE)\n         else:\n-            batch_size = await self._default_batch_size(\n-                update_name, self._database_name\n-            )\n+            batch_size = self.DEFAULT_BACKGROUND_BATCH_SIZE\n \n",
        "source_code_with_indent": "            # Clamp the batch size so that we always make progress\n            batch_size = max(\n                batch_size,\n                await self._min_batch_size(update_name, self._database_name),\n            )\n        <DED>else:\n            <IND>batch_size = await self._default_batch_size(\n                update_name, self._database_name\n            )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "            # Clamp the batch size so that we always make progress\n            batch_size = max(batch_size, self.MINIMUM_BACKGROUND_BATCH_SIZE)\n        <DED>else:\n            <IND>batch_size = self.DEFAULT_BACKGROUND_BATCH_SIZE\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        \"\"\"\n        self._background_update_handlers[update_name] = _BackgroundUpdateHandler(\n            update_handler\n        )\n\n",
        "source_code_len": 132,
        "target_code": "        \"\"\"\n        self._background_update_handlers[update_name] = update_handler\n\n",
        "target_code_len": 84,
        "diff_format": "@@ -475,5 +333,3 @@\n         \"\"\"\n-        self._background_update_handlers[update_name] = _BackgroundUpdateHandler(\n-            update_handler\n-        )\n+        self._background_update_handlers[update_name] = update_handler\n \n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n        self._background_update_handlers[update_name] = _BackgroundUpdateHandler(\n            update_handler\n        )\n\n",
        "target_code_with_indent": "\n        self._background_update_handlers[update_name] = update_handler\n\n"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        self._background_update_handlers[update_name] = _BackgroundUpdateHandler(\n            updater, oneshot=True\n        )\n\n",
        "source_code_len": 128,
        "target_code": "\n        self.register_background_update_handler(update_name, updater)\n\n",
        "target_code_len": 72,
        "diff_format": "@@ -599,5 +455,3 @@\n \n-        self._background_update_handlers[update_name] = _BackgroundUpdateHandler(\n-            updater, oneshot=True\n-        )\n+        self.register_background_update_handler(update_name, updater)\n \n",
        "source_code_with_indent": "\n        <DED>self._background_update_handlers[update_name] = _BackgroundUpdateHandler(\n            updater, oneshot=True\n        )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        <DED>self.register_background_update_handler(update_name, updater)\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "matrix-org/synapse",
    "commit": "158d73ebdd61eef33831ae5f6990acf07244fc55",
    "filename": "synapse/storage/background_updates.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/matrix-org-synapse/synapse/storage/background_updates.py",
    "file_hunks_size": 17,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "synapse/storage/background_updates.py:387:12 Incompatible parameter type [6]: Expected `str` for 2nd parameter `update_name` to call `BackgroundUpdater._get_context_manager_for_update` but got `Optional[str]`.",
    "message": " Expected `str` for 2nd parameter `update_name` to call `BackgroundUpdater._get_context_manager_for_update` but got `Optional[str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 387,
    "warning_line": "            update_name=self._current_background_update,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        self._on_update_callback: Optional[ON_UPDATE_CALLBACK] = None\n        self._default_batch_size_callback: Optional[DEFAULT_BATCH_SIZE_CALLBACK] = None\n        self._min_batch_size_callback: Optional[MIN_BATCH_SIZE_CALLBACK] = None\n\n        self._background_update_performance: Dict[str, BackgroundUpdatePerformance] = {}\n        self._background_update_handlers: Dict[str, _BackgroundUpdateHandler] = {}\n        self._all_done = False\n",
        "source_code_len": 443,
        "target_code": "\n        self._background_update_performance: Dict[str, BackgroundUpdatePerformance] = {}\n        self._background_update_handlers: Dict[\n            str, Callable[[JsonDict, int], Awaitable[int]]\n        ] = {}\n        self._all_done = False\n",
        "target_code_len": 243,
        "diff_format": "@@ -145,8 +96,6 @@\n \n-        self._on_update_callback: Optional[ON_UPDATE_CALLBACK] = None\n-        self._default_batch_size_callback: Optional[DEFAULT_BATCH_SIZE_CALLBACK] = None\n-        self._min_batch_size_callback: Optional[MIN_BATCH_SIZE_CALLBACK] = None\n-\n         self._background_update_performance: Dict[str, BackgroundUpdatePerformance] = {}\n-        self._background_update_handlers: Dict[str, _BackgroundUpdateHandler] = {}\n+        self._background_update_handlers: Dict[\n+            str, Callable[[JsonDict, int], Awaitable[int]]\n+        ] = {}\n         self._all_done = False\n",
        "source_code_with_indent": "\n        self._on_update_callback: Optional[ON_UPDATE_CALLBACK] = None\n        self._default_batch_size_callback: Optional[DEFAULT_BATCH_SIZE_CALLBACK] = None\n        self._min_batch_size_callback: Optional[MIN_BATCH_SIZE_CALLBACK] = None\n\n        self._background_update_performance: Dict[str, BackgroundUpdatePerformance] = {}\n        self._background_update_handlers: Dict[str, _BackgroundUpdateHandler] = {}\n        self._all_done = False\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        self._background_update_performance: Dict[str, BackgroundUpdatePerformance] = {}\n        self._background_update_handlers: Dict[\n            str, Callable[[JsonDict, int], Awaitable[int]]\n        ] = {}\n        self._all_done = False\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        self.enabled = True\n\n    def register_update_controller_callbacks(\n        self,\n        on_update: ON_UPDATE_CALLBACK,\n        default_batch_size: Optional[DEFAULT_BATCH_SIZE_CALLBACK] = None,\n        min_batch_size: Optional[DEFAULT_BATCH_SIZE_CALLBACK] = None,\n    ) -> None:\n        \"\"\"Register callbacks from a module for each hook.\"\"\"\n        if self._on_update_callback is not None:\n            logger.warning(\n                \"More than one module tried to register callbacks for controlling\"\n                \" background updates. Only the callbacks registered by the first module\"\n                \" (in order of appearance in Synapse's configuration file) that tried to\"\n                \" do so will be called.\"\n            )\n\n            return\n\n        self._on_update_callback = on_update\n\n        if default_batch_size is not None:\n            self._default_batch_size_callback = default_batch_size\n\n        if min_batch_size is not None:\n            self._min_batch_size_callback = min_batch_size\n\n    def _get_context_manager_for_update(\n        self,\n        sleep: bool,\n        update_name: str,\n        database_name: str,\n        oneshot: bool,\n    ) -> AsyncContextManager[int]:\n        \"\"\"Get a context manager to run a background update with.\n\n        If a module has registered a `update_handler` callback, use the context manager\n        it returns.\n\n        Otherwise, returns a context manager that will return a default value, optionally\n        sleeping if needed.\n\n        Args:\n            sleep: Whether we can sleep between updates.\n            update_name: The name of the update.\n            database_name: The name of the database the update is being run on.\n            oneshot: Whether the update will complete all in one go, e.g. index creation.\n                In such cases the returned target duration is ignored.\n\n        Returns:\n            The target duration in milliseconds that the background update should run for.\n\n            Note: this is a *target*, and an iteration may take substantially longer or\n            shorter.\n        \"\"\"\n        if self._on_update_callback is not None:\n            return self._on_update_callback(update_name, database_name, oneshot)\n\n        return _BackgroundUpdateContextManager(sleep, self._clock)\n\n    async def _default_batch_size(self, update_name: str, database_name: str) -> int:\n        \"\"\"The batch size to use for the first iteration of a new background\n        update.\n        \"\"\"\n        if self._default_batch_size_callback is not None:\n            return await self._default_batch_size_callback(update_name, database_name)\n\n        return self.DEFAULT_BACKGROUND_BATCH_SIZE\n\n    async def _min_batch_size(self, update_name: str, database_name: str) -> int:\n        \"\"\"A lower bound on the batch size of a new background update.\n\n        Used to ensure that progress is always made. Must be greater than 0.\n        \"\"\"\n        if self._min_batch_size_callback is not None:\n            return await self._min_batch_size_callback(update_name, database_name)\n\n        return self.MINIMUM_BACKGROUND_BATCH_SIZE\n\n",
        "source_code_len": 3114,
        "target_code": "        self.enabled = True\n\n",
        "target_code_len": 29,
        "diff_format": "@@ -159,79 +108,2 @@\n         self.enabled = True\n-\n-    def register_update_controller_callbacks(\n-        self,\n-        on_update: ON_UPDATE_CALLBACK,\n-        default_batch_size: Optional[DEFAULT_BATCH_SIZE_CALLBACK] = None,\n-        min_batch_size: Optional[DEFAULT_BATCH_SIZE_CALLBACK] = None,\n-    ) -> None:\n-        \"\"\"Register callbacks from a module for each hook.\"\"\"\n-        if self._on_update_callback is not None:\n-            logger.warning(\n-                \"More than one module tried to register callbacks for controlling\"\n-                \" background updates. Only the callbacks registered by the first module\"\n-                \" (in order of appearance in Synapse's configuration file) that tried to\"\n-                \" do so will be called.\"\n-            )\n-\n-            return\n-\n-        self._on_update_callback = on_update\n-\n-        if default_batch_size is not None:\n-            self._default_batch_size_callback = default_batch_size\n-\n-        if min_batch_size is not None:\n-            self._min_batch_size_callback = min_batch_size\n-\n-    def _get_context_manager_for_update(\n-        self,\n-        sleep: bool,\n-        update_name: str,\n-        database_name: str,\n-        oneshot: bool,\n-    ) -> AsyncContextManager[int]:\n-        \"\"\"Get a context manager to run a background update with.\n-\n-        If a module has registered a `update_handler` callback, use the context manager\n-        it returns.\n-\n-        Otherwise, returns a context manager that will return a default value, optionally\n-        sleeping if needed.\n-\n-        Args:\n-            sleep: Whether we can sleep between updates.\n-            update_name: The name of the update.\n-            database_name: The name of the database the update is being run on.\n-            oneshot: Whether the update will complete all in one go, e.g. index creation.\n-                In such cases the returned target duration is ignored.\n-\n-        Returns:\n-            The target duration in milliseconds that the background update should run for.\n-\n-            Note: this is a *target*, and an iteration may take substantially longer or\n-            shorter.\n-        \"\"\"\n-        if self._on_update_callback is not None:\n-            return self._on_update_callback(update_name, database_name, oneshot)\n-\n-        return _BackgroundUpdateContextManager(sleep, self._clock)\n-\n-    async def _default_batch_size(self, update_name: str, database_name: str) -> int:\n-        \"\"\"The batch size to use for the first iteration of a new background\n-        update.\n-        \"\"\"\n-        if self._default_batch_size_callback is not None:\n-            return await self._default_batch_size_callback(update_name, database_name)\n-\n-        return self.DEFAULT_BACKGROUND_BATCH_SIZE\n-\n-    async def _min_batch_size(self, update_name: str, database_name: str) -> int:\n-        \"\"\"A lower bound on the batch size of a new background update.\n-\n-        Used to ensure that progress is always made. Must be greater than 0.\n-        \"\"\"\n-        if self._min_batch_size_callback is not None:\n-            return await self._min_batch_size_callback(update_name, database_name)\n-\n-        return self.MINIMUM_BACKGROUND_BATCH_SIZE\n \n",
        "source_code_with_indent": "        self.enabled = True\n\n    <DED>def register_update_controller_callbacks(\n        self,\n        on_update: ON_UPDATE_CALLBACK,\n        default_batch_size: Optional[DEFAULT_BATCH_SIZE_CALLBACK] = None,\n        min_batch_size: Optional[DEFAULT_BATCH_SIZE_CALLBACK] = None,\n    ) -> None:\n        <IND>\"\"\"Register callbacks from a module for each hook.\"\"\"\n        if self._on_update_callback is not None:\n            <IND>logger.warning(\n                \"More than one module tried to register callbacks for controlling\"\n                \" background updates. Only the callbacks registered by the first module\"\n                \" (in order of appearance in Synapse's configuration file) that tried to\"\n                \" do so will be called.\"\n            )\n\n            return\n\n        <DED>self._on_update_callback = on_update\n\n        if default_batch_size is not None:\n            <IND>self._default_batch_size_callback = default_batch_size\n\n        <DED>if min_batch_size is not None:\n            <IND>self._min_batch_size_callback = min_batch_size\n\n    <DED><DED>def _get_context_manager_for_update(\n        self,\n        sleep: bool,\n        update_name: str,\n        database_name: str,\n        oneshot: bool,\n    ) -> AsyncContextManager[int]:\n        <IND>\"\"\"Get a context manager to run a background update with.\n\n        If a module has registered a `update_handler` callback, use the context manager\n        it returns.\n\n        Otherwise, returns a context manager that will return a default value, optionally\n        sleeping if needed.\n\n        Args:\n            sleep: Whether we can sleep between updates.\n            update_name: The name of the update.\n            database_name: The name of the database the update is being run on.\n            oneshot: Whether the update will complete all in one go, e.g. index creation.\n                In such cases the returned target duration is ignored.\n\n        Returns:\n            The target duration in milliseconds that the background update should run for.\n\n            Note: this is a *target*, and an iteration may take substantially longer or\n            shorter.\n        \"\"\"\n        if self._on_update_callback is not None:\n            <IND>return self._on_update_callback(update_name, database_name, oneshot)\n\n        <DED>return _BackgroundUpdateContextManager(sleep, self._clock)\n\n    <DED>async def _default_batch_size(self, update_name: str, database_name: str) -> int:\n        <IND>\"\"\"The batch size to use for the first iteration of a new background\n        update.\n        \"\"\"\n        if self._default_batch_size_callback is not None:\n            <IND>return await self._default_batch_size_callback(update_name, database_name)\n\n        <DED>return self.DEFAULT_BACKGROUND_BATCH_SIZE\n\n    <DED>async def _min_batch_size(self, update_name: str, database_name: str) -> int:\n        <IND>\"\"\"A lower bound on the batch size of a new background update.\n\n        Used to ensure that progress is always made. Must be greater than 0.\n        \"\"\"\n        if self._min_batch_size_callback is not None:\n            <IND>return await self._min_batch_size_callback(update_name, database_name)\n\n        <DED>return self.MINIMUM_BACKGROUND_BATCH_SIZE\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        self.enabled = True\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    async def do_next_background_update(self, sleep: bool = True) -> bool:\n        \"\"\"Does some amount of work on the next queued background update\n",
        "source_code_len": 149,
        "target_code": "\n    async def do_next_background_update(self, desired_duration_ms: float) -> bool:\n        \"\"\"Does some amount of work on the next queued background update\n",
        "target_code_len": 157,
        "diff_format": "@@ -328,3 +205,3 @@\n \n-    async def do_next_background_update(self, sleep: bool = True) -> bool:\n+    async def do_next_background_update(self, desired_duration_ms: float) -> bool:\n         \"\"\"Does some amount of work on the next queued background update\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n    <DED>async def do_next_background_update(self, sleep: bool = True) -> bool:\n        <IND>",
        "target_code_with_indent": "\n    <DED>async def do_next_background_update(self, desired_duration_ms: float) -> bool:\n        <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        # We have a background update to run, otherwise we would have returned\n        # early.\n        assert self._current_background_update is not None\n        update_info = self._background_update_handlers[self._current_background_update]\n\n        async with self._get_context_manager_for_update(\n            sleep=sleep,\n            update_name=self._current_background_update,\n            database_name=self._database_name,\n            oneshot=update_info.oneshot,\n        ) as desired_duration_ms:\n            await self._do_background_update(desired_duration_ms)\n\n        return False\n",
        "source_code_len": 594,
        "target_code": "\n        await self._do_background_update(desired_duration_ms)\n        return False\n",
        "target_code_len": 84,
        "diff_format": "@@ -379,15 +254,3 @@\n \n-        # We have a background update to run, otherwise we would have returned\n-        # early.\n-        assert self._current_background_update is not None\n-        update_info = self._background_update_handlers[self._current_background_update]\n-\n-        async with self._get_context_manager_for_update(\n-            sleep=sleep,\n-            update_name=self._current_background_update,\n-            database_name=self._database_name,\n-            oneshot=update_info.oneshot,\n-        ) as desired_duration_ms:\n-            await self._do_background_update(desired_duration_ms)\n-\n+        await self._do_background_update(desired_duration_ms)\n         return False\n",
        "source_code_with_indent": "\n        # We have a background update to run, otherwise we would have returned\n        # early.\n        <DED>assert self._current_background_update is not None\n        update_info = self._background_update_handlers[self._current_background_update]\n\n        async with self._get_context_manager_for_update(\n            sleep=sleep,\n            update_name=self._current_background_update,\n            database_name=self._database_name,\n            oneshot=update_info.oneshot,\n        ) as desired_duration_ms:\n            <IND>await self._do_background_update(desired_duration_ms)\n\n        <DED>return False\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        <DED>await self._do_background_update(desired_duration_ms)\n        return False\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        update_handler = self._background_update_handlers[update_name].callback\n\n",
        "source_code_len": 82,
        "target_code": "\n        update_handler = self._background_update_handlers[update_name]\n\n",
        "target_code_len": 73,
        "diff_format": "@@ -399,3 +262,3 @@\n \n-        update_handler = self._background_update_handlers[update_name].callback\n+        update_handler = self._background_update_handlers[update_name]\n \n",
        "source_code_with_indent": "\n        update_handler = self._background_update_handlers[update_name].callback\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        update_handler = self._background_update_handlers[update_name]\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "            # Clamp the batch size so that we always make progress\n            batch_size = max(\n                batch_size,\n                await self._min_batch_size(update_name, self._database_name),\n            )\n        else:\n            batch_size = await self._default_batch_size(\n                update_name, self._database_name\n            )\n\n",
        "source_code_len": 352,
        "target_code": "            # Clamp the batch size so that we always make progress\n            batch_size = max(batch_size, self.MINIMUM_BACKGROUND_BATCH_SIZE)\n        else:\n            batch_size = self.DEFAULT_BACKGROUND_BATCH_SIZE\n\n",
        "target_code_len": 219,
        "diff_format": "@@ -412,10 +275,5 @@\n             # Clamp the batch size so that we always make progress\n-            batch_size = max(\n-                batch_size,\n-                await self._min_batch_size(update_name, self._database_name),\n-            )\n+            batch_size = max(batch_size, self.MINIMUM_BACKGROUND_BATCH_SIZE)\n         else:\n-            batch_size = await self._default_batch_size(\n-                update_name, self._database_name\n-            )\n+            batch_size = self.DEFAULT_BACKGROUND_BATCH_SIZE\n \n",
        "source_code_with_indent": "            # Clamp the batch size so that we always make progress\n            batch_size = max(\n                batch_size,\n                await self._min_batch_size(update_name, self._database_name),\n            )\n        <DED>else:\n            <IND>batch_size = await self._default_batch_size(\n                update_name, self._database_name\n            )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "            # Clamp the batch size so that we always make progress\n            batch_size = max(batch_size, self.MINIMUM_BACKGROUND_BATCH_SIZE)\n        <DED>else:\n            <IND>batch_size = self.DEFAULT_BACKGROUND_BATCH_SIZE\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        \"\"\"\n        self._background_update_handlers[update_name] = _BackgroundUpdateHandler(\n            update_handler\n        )\n\n",
        "source_code_len": 132,
        "target_code": "        \"\"\"\n        self._background_update_handlers[update_name] = update_handler\n\n",
        "target_code_len": 84,
        "diff_format": "@@ -475,5 +333,3 @@\n         \"\"\"\n-        self._background_update_handlers[update_name] = _BackgroundUpdateHandler(\n-            update_handler\n-        )\n+        self._background_update_handlers[update_name] = update_handler\n \n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n        self._background_update_handlers[update_name] = _BackgroundUpdateHandler(\n            update_handler\n        )\n\n",
        "target_code_with_indent": "\n        self._background_update_handlers[update_name] = update_handler\n\n"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        self._background_update_handlers[update_name] = _BackgroundUpdateHandler(\n            updater, oneshot=True\n        )\n\n",
        "source_code_len": 128,
        "target_code": "\n        self.register_background_update_handler(update_name, updater)\n\n",
        "target_code_len": 72,
        "diff_format": "@@ -599,5 +455,3 @@\n \n-        self._background_update_handlers[update_name] = _BackgroundUpdateHandler(\n-            updater, oneshot=True\n-        )\n+        self.register_background_update_handler(update_name, updater)\n \n",
        "source_code_with_indent": "\n        <DED>self._background_update_handlers[update_name] = _BackgroundUpdateHandler(\n            updater, oneshot=True\n        )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        <DED>self.register_background_update_handler(update_name, updater)\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "matrix-org/synapse",
    "commit": "158d73ebdd61eef33831ae5f6990acf07244fc55",
    "filename": "synapse/storage/databases/main/event_federation.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/matrix-org-synapse/synapse/storage/databases/main/event_federation.py",
    "file_hunks_size": 1,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "synapse/storage/databases/main/event_federation.py:74:8 Incompatible attribute type [8]: Attribute `_event_auth_cache` declared in class `EventFederationWorkerStore` has type `LruCache[str, List[Tuple[str, int]]]` but is used as type `LruCache[Variable[synapse.util.caches.lrucache.KT], typing.Sized]`.",
    "message": " Attribute `_event_auth_cache` declared in class `EventFederationWorkerStore` has type `LruCache[str, List[Tuple[str, int]]]` but is used as type `LruCache[Variable[synapse.util.caches.lrucache.KT], typing.Sized]`.",
    "rule_id": "Incompatible attribute type [8]",
    "warning_line_no": 74,
    "warning_line": "        self._event_auth_cache: LruCache[str, List[Tuple[str, int]]] = LruCache("
  },
  {
    "project": "matrix-org/synapse",
    "commit": "158d73ebdd61eef33831ae5f6990acf07244fc55",
    "filename": "synapse/storage/databases/main/event_push_actions.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/matrix-org-synapse/synapse/storage/databases/main/event_push_actions.py",
    "file_hunks_size": 4,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "synapse/storage/databases/main/event_push_actions.py:336:8 Incompatible return type [7]: Expected `List[HttpPushAction]` but got `List[Dict[str, typing.Any]]`.",
    "message": " Expected `List[HttpPushAction]` but got `List[Dict[str, typing.Any]]`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 336,
    "warning_line": "        return notifs[:limit]",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        limit: int = 20,\n    ) -> List[HttpPushAction]:\n        \"\"\"Get a list of the most recent unread push actions for a given user,\n",
        "source_code_len": 135,
        "target_code": "        limit: int = 20,\n    ) -> List[dict]:\n        \"\"\"Get a list of the most recent unread push actions for a given user,\n",
        "target_code_len": 125,
        "diff_format": "@@ -238,3 +223,3 @@\n         limit: int = 20,\n-    ) -> List[HttpPushAction]:\n+    ) -> List[dict]:\n         \"\"\"Get a list of the most recent unread push actions for a given user,\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "        limit: int = 20,\n    ) -> List[HttpPushAction]:\n        <IND>",
        "target_code_with_indent": "        limit: int = 20,\n    ) -> List[dict]:\n        <IND>"
      }
    ]
  },
  {
    "project": "matrix-org/synapse",
    "commit": "158d73ebdd61eef33831ae5f6990acf07244fc55",
    "filename": "synapse/storage/databases/main/event_push_actions.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/matrix-org-synapse/synapse/storage/databases/main/event_push_actions.py",
    "file_hunks_size": 4,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "synapse/storage/databases/main/event_push_actions.py:441:8 Incompatible return type [7]: Expected `List[EmailPushAction]` but got `List[Dict[str, typing.Any]]`.",
    "message": " Expected `List[EmailPushAction]` but got `List[Dict[str, typing.Any]]`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 441,
    "warning_line": "        return notifs[:limit]",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        limit: int = 20,\n    ) -> List[EmailPushAction]:\n        \"\"\"Get a list of the most recent unread push actions for a given user,\n",
        "source_code_len": 136,
        "target_code": "        limit: int = 20,\n    ) -> List[dict]:\n        \"\"\"Get a list of the most recent unread push actions for a given user,\n",
        "target_code_len": 125,
        "diff_format": "@@ -343,3 +328,3 @@\n         limit: int = 20,\n-    ) -> List[EmailPushAction]:\n+    ) -> List[dict]:\n         \"\"\"Get a list of the most recent unread push actions for a given user,\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "        limit: int = 20,\n    ) -> List[EmailPushAction]:\n        <IND>",
        "target_code_with_indent": "        limit: int = 20,\n    ) -> List[dict]:\n        <IND>"
      }
    ]
  },
  {
    "project": "matrix-org/synapse",
    "commit": "158d73ebdd61eef33831ae5f6990acf07244fc55",
    "filename": "synapse/storage/databases/main/events_worker.py",
    "min_patch_found": false,
    "full_warning_msg": "synapse/storage/databases/main/events_worker.py:178:35 Incompatible parameter type [6]: Expected `synapse.storage.types.Connection` for 2nd positional only parameter to call `SQLBaseStore.__init__` but got `LoggingDatabaseConnection`.",
    "exception": "Cannot have more than 64 hunks in a file",
    "dd_fail": true
  },
  {
    "project": "matrix-org/synapse",
    "commit": "158d73ebdd61eef33831ae5f6990acf07244fc55",
    "filename": "synapse/storage/databases/main/events_worker.py",
    "min_patch_found": false,
    "full_warning_msg": "synapse/storage/databases/main/events_worker.py:1529:42 Incompatible parameter type [6]: Expected `Tuple[int, Tuple[str, str, str, str, str, str]]` for 1st positional only parameter to call `list.append` but got `Tuple[int, typing.Tuple[typing.Union[int, str], ...]]`.",
    "exception": "Cannot have more than 64 hunks in a file",
    "dd_fail": true
  },
  {
    "project": "matrix-org/synapse",
    "commit": "158d73ebdd61eef33831ae5f6990acf07244fc55",
    "filename": "synapse/storage/databases/main/events_worker.py",
    "min_patch_found": false,
    "full_warning_msg": "synapse/storage/databases/main/events_worker.py:1556:42 Incompatible parameter type [6]: Expected `Tuple[int, Tuple[str, str, str, str, str, str]]` for 1st positional only parameter to call `list.append` but got `Tuple[int, typing.Tuple[typing.Union[int, str], ...]]`.",
    "exception": "Cannot have more than 64 hunks in a file",
    "dd_fail": true
  },
  {
    "project": "matrix-org/synapse",
    "commit": "158d73ebdd61eef33831ae5f6990acf07244fc55",
    "filename": "synapse/storage/databases/main/transactions.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/matrix-org-synapse/synapse/storage/databases/main/transactions.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "synapse/storage/databases/main/transactions.py:547:55 Incompatible parameter type [6]: Expected `synapse.storage.types.Cursor` for 1st positional only parameter to call `DatabasePool.cursor_to_dict` but got `LoggingTransaction`.",
    "message": " Expected `synapse.storage.types.Cursor` for 1st positional only parameter to call `DatabasePool.cursor_to_dict` but got `LoggingTransaction`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 547,
    "warning_line": "            destinations = self.db_pool.cursor_to_dict(txn)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        return destinations\n\n    async def get_destinations_paginate(\n        self,\n        start: int,\n        limit: int,\n        destination: Optional[str] = None,\n        order_by: str = DestinationSortOrder.DESTINATION.value,\n        direction: str = \"f\",\n    ) -> Tuple[List[JsonDict], int]:\n        \"\"\"Function to retrieve a paginated list of destinations.\n        This will return a json list of destinations and the\n        total number of destinations matching the filter criteria.\n\n        Args:\n            start: start number to begin the query from\n            limit: number of rows to retrieve\n            destination: search string in destination\n            order_by: the sort order of the returned list\n            direction: sort ascending or descending\n        Returns:\n            A tuple of a list of mappings from destination to information\n            and a count of total destinations.\n        \"\"\"\n\n        def get_destinations_paginate_txn(\n            txn: LoggingTransaction,\n        ) -> Tuple[List[JsonDict], int]:\n            order_by_column = DestinationSortOrder(order_by).value\n\n            if direction == \"b\":\n                order = \"DESC\"\n            else:\n                order = \"ASC\"\n\n            args = []\n            where_statement = \"\"\n            if destination:\n                args.extend([\"%\" + destination.lower() + \"%\"])\n                where_statement = \"WHERE LOWER(destination) LIKE ?\"\n\n            sql_base = f\"FROM destinations {where_statement} \"\n            sql = f\"SELECT COUNT(*) as total_destinations {sql_base}\"\n            txn.execute(sql, args)\n            count = txn.fetchone()[0]\n\n            sql = f\"\"\"\n                SELECT destination, retry_last_ts, retry_interval, failure_ts,\n                last_successful_stream_ordering\n                {sql_base}\n                ORDER BY {order_by_column} {order}, destination ASC\n                LIMIT ? OFFSET ?\n            \"\"\"\n            txn.execute(sql, args + [limit, start])\n            destinations = self.db_pool.cursor_to_dict(txn)\n            return destinations, count\n\n        return await self.db_pool.runInteraction(\n            \"get_destinations_paginate_txn\", get_destinations_paginate_txn\n        )\n",
        "source_code_len": 2229,
        "target_code": "        return destinations\n",
        "target_code_len": 28,
        "diff_format": "@@ -493,60 +482,1 @@\n         return destinations\n-\n-    async def get_destinations_paginate(\n-        self,\n-        start: int,\n-        limit: int,\n-        destination: Optional[str] = None,\n-        order_by: str = DestinationSortOrder.DESTINATION.value,\n-        direction: str = \"f\",\n-    ) -> Tuple[List[JsonDict], int]:\n-        \"\"\"Function to retrieve a paginated list of destinations.\n-        This will return a json list of destinations and the\n-        total number of destinations matching the filter criteria.\n-\n-        Args:\n-            start: start number to begin the query from\n-            limit: number of rows to retrieve\n-            destination: search string in destination\n-            order_by: the sort order of the returned list\n-            direction: sort ascending or descending\n-        Returns:\n-            A tuple of a list of mappings from destination to information\n-            and a count of total destinations.\n-        \"\"\"\n-\n-        def get_destinations_paginate_txn(\n-            txn: LoggingTransaction,\n-        ) -> Tuple[List[JsonDict], int]:\n-            order_by_column = DestinationSortOrder(order_by).value\n-\n-            if direction == \"b\":\n-                order = \"DESC\"\n-            else:\n-                order = \"ASC\"\n-\n-            args = []\n-            where_statement = \"\"\n-            if destination:\n-                args.extend([\"%\" + destination.lower() + \"%\"])\n-                where_statement = \"WHERE LOWER(destination) LIKE ?\"\n-\n-            sql_base = f\"FROM destinations {where_statement} \"\n-            sql = f\"SELECT COUNT(*) as total_destinations {sql_base}\"\n-            txn.execute(sql, args)\n-            count = txn.fetchone()[0]\n-\n-            sql = f\"\"\"\n-                SELECT destination, retry_last_ts, retry_interval, failure_ts,\n-                last_successful_stream_ordering\n-                {sql_base}\n-                ORDER BY {order_by_column} {order}, destination ASC\n-                LIMIT ? OFFSET ?\n-            \"\"\"\n-            txn.execute(sql, args + [limit, start])\n-            destinations = self.db_pool.cursor_to_dict(txn)\n-            return destinations, count\n-\n-        return await self.db_pool.runInteraction(\n-            \"get_destinations_paginate_txn\", get_destinations_paginate_txn\n-        )\n",
        "source_code_with_indent": "        return destinations\n\n    <DED>async def get_destinations_paginate(\n        self,\n        start: int,\n        limit: int,\n        destination: Optional[str] = None,\n        order_by: str = DestinationSortOrder.DESTINATION.value,\n        direction: str = \"f\",\n    ) -> Tuple[List[JsonDict], int]:\n        <IND>\"\"\"Function to retrieve a paginated list of destinations.\n        This will return a json list of destinations and the\n        total number of destinations matching the filter criteria.\n\n        Args:\n            start: start number to begin the query from\n            limit: number of rows to retrieve\n            destination: search string in destination\n            order_by: the sort order of the returned list\n            direction: sort ascending or descending\n        Returns:\n            A tuple of a list of mappings from destination to information\n            and a count of total destinations.\n        \"\"\"\n\n        def get_destinations_paginate_txn(\n            txn: LoggingTransaction,\n        ) -> Tuple[List[JsonDict], int]:\n            <IND>order_by_column = DestinationSortOrder(order_by).value\n\n            if direction == \"b\":\n                <IND>order = \"DESC\"\n            <DED>else:\n                <IND>order = \"ASC\"\n\n            <DED>args = []\n            where_statement = \"\"\n            if destination:\n                <IND>args.extend([\"%\" + destination.lower() + \"%\"])\n                where_statement = \"WHERE LOWER(destination) LIKE ?\"\n\n            <DED>sql_base = f\"FROM destinations {where_statement} \"\n            sql = f\"SELECT COUNT(*) as total_destinations {sql_base}\"\n            txn.execute(sql, args)\n            count = txn.fetchone()[0]\n\n            sql = f\"\"\"\n                SELECT destination, retry_last_ts, retry_interval, failure_ts,\n                last_successful_stream_ordering\n                {sql_base}\n                ORDER BY {order_by_column} {order}, destination ASC\n                LIMIT ? OFFSET ?\n            \"\"\"\n            txn.execute(sql, args + [limit, start])\n            destinations = self.db_pool.cursor_to_dict(txn)\n            return destinations, count\n\n        <DED>return await self.db_pool.runInteraction(\n            \"get_destinations_paginate_txn\", get_destinations_paginate_txn\n        )\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        return destinations\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "matrix-org/synapse",
    "commit": "158d73ebdd61eef33831ae5f6990acf07244fc55",
    "filename": "synapse/util/caches/dictionary_cache.py",
    "min_patch_found": false,
    "full_warning_msg": "synapse/util/caches/dictionary_cache.py:69:8 Incompatible attribute type [8]: Attribute `cache` declared in class `DictionaryCache` has type `LruCache[Variable[KT], DictionaryEntry]` but is used as type `LruCache[Variable[synapse.util.caches.lrucache.KT], typing.Sized]`.",
    "exception": "PatchSet should only have 1 Patch, but it has 0",
    "dd_fail": true
  },
  {
    "project": "matrix-org/synapse",
    "commit": "158d73ebdd61eef33831ae5f6990acf07244fc55",
    "filename": "tests/rest/admin/test_federation.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/rest/admin/test_federation.py:29:4 Inconsistent override [15]: `servlets` overrides attribute defined in `unittest.HomeserverTestCase` inconsistently. Type `List[typing.Union[typing.Callable(login.register_servlets)[[Named(hs, HomeServer), Named(http_server, synapse.http.server.HttpServer)], None], typing.Callable(synapse.rest.admin.register_servlets)[[Named(hs, HomeServer), Named(http_server, synapse.http.server.HttpServer)], None]]]` is not a subtype of the overridden attribute `List[typing.Callable[[HomeServer, synapse.http.server.HttpServer], None]]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/matrix-org-synapse/tests/rest/admin/test_federation.py'",
    "dd_fail": true
  },
  {
    "project": "matrix-org/synapse",
    "commit": "158d73ebdd61eef33831ae5f6990acf07244fc55",
    "filename": "tests/rest/admin/test_federation.py",
    "min_patch_found": false,
    "full_warning_msg": "tests/rest/admin/test_federation.py:34:4 Inconsistent override [14]: `tests.rest.admin.test_federation.FederationTestCase.prepare` overrides method defined in `unittest.HomeserverTestCase` inconsistently. Could not find parameter `homeserver` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/matrix-org-synapse/tests/rest/admin/test_federation.py'",
    "dd_fail": true
  },
  {
    "project": "matrix-org/synapse",
    "commit": "158d73ebdd61eef33831ae5f6990acf07244fc55",
    "filename": "tests/rest/admin/test_statistics.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/matrix-org-synapse/tests/rest/admin/test_statistics.py",
    "file_hunks_size": 41,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "tests/rest/admin/test_statistics.py:32:4 Inconsistent override [15]: `servlets` overrides attribute defined in `unittest.HomeserverTestCase` inconsistently. Type `List[typing.Union[typing.Callable(login.register_servlets)[[Named(hs, HomeServer), Named(http_server, synapse.http.server.HttpServer)], None], typing.Callable(synapse.rest.admin.register_servlets)[[Named(hs, HomeServer), Named(http_server, synapse.http.server.HttpServer)], None]]]` is not a subtype of the overridden attribute `List[typing.Callable[[HomeServer, synapse.http.server.HttpServer], None]]`.",
    "message": " `servlets` overrides attribute defined in `unittest.HomeserverTestCase` inconsistently. Type `List[typing.Union[typing.Callable(login.register_servlets)[[Named(hs, HomeServer), Named(http_server, synapse.http.server.HttpServer)], None], typing.Callable(synapse.rest.admin.register_servlets)[[Named(hs, HomeServer), Named(http_server, synapse.http.server.HttpServer)], None]]]` is not a subtype of the overridden attribute `List[typing.Callable[[HomeServer, synapse.http.server.HttpServer], None]]`.",
    "rule_id": "Inconsistent override [15]",
    "warning_line_no": 32,
    "warning_line": "    servlets = ["
  },
  {
    "project": "matrix-org/synapse",
    "commit": "158d73ebdd61eef33831ae5f6990acf07244fc55",
    "filename": "tests/rest/admin/test_statistics.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/matrix-org-synapse/tests/rest/admin/test_statistics.py",
    "file_hunks_size": 41,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "tests/rest/admin/test_statistics.py:37:4 Inconsistent override [14]: `tests.rest.admin.test_statistics.UserMediaStatisticsTestCase.prepare` overrides method defined in `unittest.HomeserverTestCase` inconsistently. Could not find parameter `homeserver` in overriding signature.",
    "message": " `tests.rest.admin.test_statistics.UserMediaStatisticsTestCase.prepare` overrides method defined in `unittest.HomeserverTestCase` inconsistently. Could not find parameter `homeserver` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 37,
    "warning_line": "    def prepare(self, reactor: MemoryReactor, clock: Clock, hs: HomeServer) -> None:",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    def prepare(self, reactor: MemoryReactor, clock: Clock, hs: HomeServer) -> None:\n        self.media_repo = hs.get_media_repository_resource()\n",
        "source_code_len": 147,
        "target_code": "\n    def prepare(self, reactor, clock, hs):\n        self.media_repo = hs.get_media_repository_resource()\n",
        "target_code_len": 105,
        "diff_format": "@@ -36,3 +32,3 @@\n \n-    def prepare(self, reactor: MemoryReactor, clock: Clock, hs: HomeServer) -> None:\n+    def prepare(self, reactor, clock, hs):\n         self.media_repo = hs.get_media_repository_resource()\n",
        "source_code_with_indent": "\n    def prepare(self, reactor: MemoryReactor, clock: Clock, hs: HomeServer) -> None:\n        <IND>self.media_repo = hs.get_media_repository_resource()\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    def prepare(self, reactor, clock, hs):\n        <IND>self.media_repo = hs.get_media_repository_resource()\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "matrix-org/synapse",
    "commit": "158d73ebdd61eef33831ae5f6990acf07244fc55",
    "filename": "tests/storage/databases/main/test_events_worker.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/matrix-org-synapse/tests/storage/databases/main/test_events_worker.py",
    "file_hunks_size": 3,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "tests/storage/databases/main/test_events_worker.py:118:4 Inconsistent override [15]: `servlets` overrides attribute defined in `unittest.HomeserverTestCase` inconsistently. Type `typing.List[typing.Union[typing.Callable(admin.register_servlets)[[Named(hs, HomeServer), Named(http_server, synapse.http.server.HttpServer)], None], typing.Callable(login.register_servlets)[[Named(hs, HomeServer), Named(http_server, synapse.http.server.HttpServer)], None], typing.Callable(room.register_servlets)[[Named(hs, HomeServer), Named(http_server, synapse.http.server.HttpServer), Named(is_worker, bool, default)], None]]]` is not a subtype of the overridden attribute `typing.List[typing.Callable[[HomeServer, synapse.http.server.HttpServer], None]]`.",
    "message": " `servlets` overrides attribute defined in `unittest.HomeserverTestCase` inconsistently. Type `typing.List[typing.Union[typing.Callable(admin.register_servlets)[[Named(hs, HomeServer), Named(http_server, synapse.http.server.HttpServer)], None], typing.Callable(login.register_servlets)[[Named(hs, HomeServer), Named(http_server, synapse.http.server.HttpServer)], None], typing.Callable(room.register_servlets)[[Named(hs, HomeServer), Named(http_server, synapse.http.server.HttpServer), Named(is_worker, bool, default)], None]]]` is not a subtype of the overridden attribute `typing.List[typing.Callable[[HomeServer, synapse.http.server.HttpServer], None]]`.",
    "rule_id": "Inconsistent override [15]",
    "warning_line_no": 118,
    "warning_line": "    servlets = ["
  },
  {
    "project": "matrix-org/synapse",
    "commit": "158d73ebdd61eef33831ae5f6990acf07244fc55",
    "filename": "tests/storage/databases/main/test_events_worker.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/matrix-org-synapse/tests/storage/databases/main/test_events_worker.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "tests/storage/databases/main/test_events_worker.py:165:4 Inconsistent override [14]: `tests.storage.databases.main.test_events_worker.DatabaseOutageTestCase.prepare` overrides method defined in `unittest.HomeserverTestCase` inconsistently. Could not find parameter `homeserver` in overriding signature.",
    "message": " `tests.storage.databases.main.test_events_worker.DatabaseOutageTestCase.prepare` overrides method defined in `unittest.HomeserverTestCase` inconsistently. Could not find parameter `homeserver` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 165,
    "warning_line": "    def prepare(self, reactor: MemoryReactor, clock: Clock, hs: HomeServer):",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "            self.assertEqual(ctx.get_resource_usage().evt_db_fetch_count, 1)\n\n\nclass DatabaseOutageTestCase(unittest.HomeserverTestCase):\n    \"\"\"Test event fetching during a database outage.\"\"\"\n\n    def prepare(self, reactor: MemoryReactor, clock: Clock, hs: HomeServer):\n        self.store: EventsWorkerStore = hs.get_datastore()\n\n        self.room_id = f\"!room:{hs.hostname}\"\n        self.event_ids = [f\"event{i}\" for i in range(20)]\n\n        self._populate_events()\n\n    def _populate_events(self) -> None:\n        \"\"\"Ensure that there are test events in the database.\n\n        When testing with the in-memory SQLite database, all the events are lost during\n        the simulated outage.\n\n        To ensure consistency between `room_id`s and `event_id`s before and after the\n        outage, rows are built and inserted manually.\n\n        Upserts are used to handle the non-SQLite case where events are not lost.\n        \"\"\"\n        self.get_success(\n            self.store.db_pool.simple_upsert(\n                \"rooms\",\n                {\"room_id\": self.room_id},\n                {\"room_version\": RoomVersions.V4.identifier},\n            )\n        )\n\n        self.event_ids = [f\"event{i}\" for i in range(20)]\n        for idx, event_id in enumerate(self.event_ids):\n            self.get_success(\n                self.store.db_pool.simple_upsert(\n                    \"events\",\n                    {\"event_id\": event_id},\n                    {\n                        \"event_id\": event_id,\n                        \"room_id\": self.room_id,\n                        \"topological_ordering\": idx,\n                        \"stream_ordering\": idx,\n                        \"type\": \"test\",\n                        \"processed\": True,\n                        \"outlier\": False,\n                    },\n                )\n            )\n            self.get_success(\n                self.store.db_pool.simple_upsert(\n                    \"event_json\",\n                    {\"event_id\": event_id},\n                    {\n                        \"room_id\": self.room_id,\n                        \"json\": json.dumps({\"type\": \"test\", \"room_id\": self.room_id}),\n                        \"internal_metadata\": \"{}\",\n                        \"format_version\": EventFormatVersions.V3,\n                    },\n                )\n            )\n\n    @contextmanager\n    def _outage(self) -> Generator[None, None, None]:\n        \"\"\"Simulate a database outage.\n\n        Returns:\n            A context manager. While the context is active, any attempts to connect to\n            the database will fail.\n        \"\"\"\n        connection_pool = self.store.db_pool._db_pool\n\n        # Close all connections and shut down the database `ThreadPool`.\n        connection_pool.close()\n\n        # Restart the database `ThreadPool`.\n        connection_pool.start()\n\n        original_connection_factory = connection_pool.connectionFactory\n\n        def connection_factory(_pool: ConnectionPool) -> Connection:\n            raise Exception(\"Could not connect to the database.\")\n\n        connection_pool.connectionFactory = connection_factory  # type: ignore[assignment]\n        try:\n            yield\n        finally:\n            connection_pool.connectionFactory = original_connection_factory\n\n            # If the in-memory SQLite database is being used, all the events are gone.\n            # Restore the test data.\n            self._populate_events()\n\n    def test_failure(self) -> None:\n        \"\"\"Test that event fetches do not get stuck during a database outage.\"\"\"\n        with self._outage():\n            failure = self.get_failure(\n                self.store.get_event(self.event_ids[0]), Exception\n            )\n            self.assertEqual(str(failure.value), \"Could not connect to the database.\")\n\n    def test_recovery(self) -> None:\n        \"\"\"Test that event fetchers recover after a database outage.\"\"\"\n        with self._outage():\n            # Kick off a bunch of event fetches but do not pump the reactor\n            event_deferreds = []\n            for event_id in self.event_ids:\n                event_deferreds.append(ensureDeferred(self.store.get_event(event_id)))\n\n            # We should have maxed out on event fetcher threads\n            self.assertEqual(self.store._event_fetch_ongoing, EVENT_QUEUE_THREADS)\n\n            # All the event fetchers will fail\n            self.pump()\n            self.assertEqual(self.store._event_fetch_ongoing, 0)\n\n            for event_deferred in event_deferreds:\n                failure = self.get_failure(event_deferred, Exception)\n                self.assertEqual(\n                    str(failure.value), \"Could not connect to the database.\"\n                )\n\n        # This next event fetch should succeed\n        self.get_success(self.store.get_event(self.event_ids[0]))\n",
        "source_code_len": 4783,
        "target_code": "            self.assertEqual(ctx.get_resource_usage().evt_db_fetch_count, 1)\n",
        "target_code_len": 77,
        "diff_format": "@@ -159,125 +146,1 @@\n             self.assertEqual(ctx.get_resource_usage().evt_db_fetch_count, 1)\n-\n-\n-class DatabaseOutageTestCase(unittest.HomeserverTestCase):\n-    \"\"\"Test event fetching during a database outage.\"\"\"\n-\n-    def prepare(self, reactor: MemoryReactor, clock: Clock, hs: HomeServer):\n-        self.store: EventsWorkerStore = hs.get_datastore()\n-\n-        self.room_id = f\"!room:{hs.hostname}\"\n-        self.event_ids = [f\"event{i}\" for i in range(20)]\n-\n-        self._populate_events()\n-\n-    def _populate_events(self) -> None:\n-        \"\"\"Ensure that there are test events in the database.\n-\n-        When testing with the in-memory SQLite database, all the events are lost during\n-        the simulated outage.\n-\n-        To ensure consistency between `room_id`s and `event_id`s before and after the\n-        outage, rows are built and inserted manually.\n-\n-        Upserts are used to handle the non-SQLite case where events are not lost.\n-        \"\"\"\n-        self.get_success(\n-            self.store.db_pool.simple_upsert(\n-                \"rooms\",\n-                {\"room_id\": self.room_id},\n-                {\"room_version\": RoomVersions.V4.identifier},\n-            )\n-        )\n-\n-        self.event_ids = [f\"event{i}\" for i in range(20)]\n-        for idx, event_id in enumerate(self.event_ids):\n-            self.get_success(\n-                self.store.db_pool.simple_upsert(\n-                    \"events\",\n-                    {\"event_id\": event_id},\n-                    {\n-                        \"event_id\": event_id,\n-                        \"room_id\": self.room_id,\n-                        \"topological_ordering\": idx,\n-                        \"stream_ordering\": idx,\n-                        \"type\": \"test\",\n-                        \"processed\": True,\n-                        \"outlier\": False,\n-                    },\n-                )\n-            )\n-            self.get_success(\n-                self.store.db_pool.simple_upsert(\n-                    \"event_json\",\n-                    {\"event_id\": event_id},\n-                    {\n-                        \"room_id\": self.room_id,\n-                        \"json\": json.dumps({\"type\": \"test\", \"room_id\": self.room_id}),\n-                        \"internal_metadata\": \"{}\",\n-                        \"format_version\": EventFormatVersions.V3,\n-                    },\n-                )\n-            )\n-\n-    @contextmanager\n-    def _outage(self) -> Generator[None, None, None]:\n-        \"\"\"Simulate a database outage.\n-\n-        Returns:\n-            A context manager. While the context is active, any attempts to connect to\n-            the database will fail.\n-        \"\"\"\n-        connection_pool = self.store.db_pool._db_pool\n-\n-        # Close all connections and shut down the database `ThreadPool`.\n-        connection_pool.close()\n-\n-        # Restart the database `ThreadPool`.\n-        connection_pool.start()\n-\n-        original_connection_factory = connection_pool.connectionFactory\n-\n-        def connection_factory(_pool: ConnectionPool) -> Connection:\n-            raise Exception(\"Could not connect to the database.\")\n-\n-        connection_pool.connectionFactory = connection_factory  # type: ignore[assignment]\n-        try:\n-            yield\n-        finally:\n-            connection_pool.connectionFactory = original_connection_factory\n-\n-            # If the in-memory SQLite database is being used, all the events are gone.\n-            # Restore the test data.\n-            self._populate_events()\n-\n-    def test_failure(self) -> None:\n-        \"\"\"Test that event fetches do not get stuck during a database outage.\"\"\"\n-        with self._outage():\n-            failure = self.get_failure(\n-                self.store.get_event(self.event_ids[0]), Exception\n-            )\n-            self.assertEqual(str(failure.value), \"Could not connect to the database.\")\n-\n-    def test_recovery(self) -> None:\n-        \"\"\"Test that event fetchers recover after a database outage.\"\"\"\n-        with self._outage():\n-            # Kick off a bunch of event fetches but do not pump the reactor\n-            event_deferreds = []\n-            for event_id in self.event_ids:\n-                event_deferreds.append(ensureDeferred(self.store.get_event(event_id)))\n-\n-            # We should have maxed out on event fetcher threads\n-            self.assertEqual(self.store._event_fetch_ongoing, EVENT_QUEUE_THREADS)\n-\n-            # All the event fetchers will fail\n-            self.pump()\n-            self.assertEqual(self.store._event_fetch_ongoing, 0)\n-\n-            for event_deferred in event_deferreds:\n-                failure = self.get_failure(event_deferred, Exception)\n-                self.assertEqual(\n-                    str(failure.value), \"Could not connect to the database.\"\n-                )\n-\n-        # This next event fetch should succeed\n-        self.get_success(self.store.get_event(self.event_ids[0]))\n",
        "source_code_with_indent": "            self.assertEqual(ctx.get_resource_usage().evt_db_fetch_count, 1)\n\n\n<DED><DED><DED>class DatabaseOutageTestCase(unittest.HomeserverTestCase):\n    <IND>\"\"\"Test event fetching during a database outage.\"\"\"\n\n    def prepare(self, reactor: MemoryReactor, clock: Clock, hs: HomeServer):\n        <IND>self.store: EventsWorkerStore = hs.get_datastore()\n\n        self.room_id = f\"!room:{hs.hostname}\"\n        self.event_ids = [f\"event{i}\" for i in range(20)]\n\n        self._populate_events()\n\n    <DED>def _populate_events(self) -> None:\n        <IND>\"\"\"Ensure that there are test events in the database.\n\n        When testing with the in-memory SQLite database, all the events are lost during\n        the simulated outage.\n\n        To ensure consistency between `room_id`s and `event_id`s before and after the\n        outage, rows are built and inserted manually.\n\n        Upserts are used to handle the non-SQLite case where events are not lost.\n        \"\"\"\n        self.get_success(\n            self.store.db_pool.simple_upsert(\n                \"rooms\",\n                {\"room_id\": self.room_id},\n                {\"room_version\": RoomVersions.V4.identifier},\n            )\n        )\n\n        self.event_ids = [f\"event{i}\" for i in range(20)]\n        for idx, event_id in enumerate(self.event_ids):\n            <IND>self.get_success(\n                self.store.db_pool.simple_upsert(\n                    \"events\",\n                    {\"event_id\": event_id},\n                    {\n                        \"event_id\": event_id,\n                        \"room_id\": self.room_id,\n                        \"topological_ordering\": idx,\n                        \"stream_ordering\": idx,\n                        \"type\": \"test\",\n                        \"processed\": True,\n                        \"outlier\": False,\n                    },\n                )\n            )\n            self.get_success(\n                self.store.db_pool.simple_upsert(\n                    \"event_json\",\n                    {\"event_id\": event_id},\n                    {\n                        \"room_id\": self.room_id,\n                        \"json\": json.dumps({\"type\": \"test\", \"room_id\": self.room_id}),\n                        \"internal_metadata\": \"{}\",\n                        \"format_version\": EventFormatVersions.V3,\n                    },\n                )\n            )\n\n    <DED><DED>@contextmanager\n    def _outage(self) -> Generator[None, None, None]:\n        <IND>\"\"\"Simulate a database outage.\n\n        Returns:\n            A context manager. While the context is active, any attempts to connect to\n            the database will fail.\n        \"\"\"\n        connection_pool = self.store.db_pool._db_pool\n\n        # Close all connections and shut down the database `ThreadPool`.\n        connection_pool.close()\n\n        # Restart the database `ThreadPool`.\n        connection_pool.start()\n\n        original_connection_factory = connection_pool.connectionFactory\n\n        def connection_factory(_pool: ConnectionPool) -> Connection:\n            <IND>raise Exception(\"Could not connect to the database.\")\n\n        <DED>connection_pool.connectionFactory = connection_factory  # type: ignore[assignment]\n        try:\n            <IND>yield\n        <DED>finally:\n            <IND>connection_pool.connectionFactory = original_connection_factory\n\n            # If the in-memory SQLite database is being used, all the events are gone.\n            # Restore the test data.\n            self._populate_events()\n\n    <DED><DED>def test_failure(self) -> None:\n        <IND>\"\"\"Test that event fetches do not get stuck during a database outage.\"\"\"\n        with self._outage():\n            <IND>failure = self.get_failure(\n                self.store.get_event(self.event_ids[0]), Exception\n            )\n            self.assertEqual(str(failure.value), \"Could not connect to the database.\")\n\n    <DED><DED>def test_recovery(self) -> None:\n        <IND>\"\"\"Test that event fetchers recover after a database outage.\"\"\"\n        with self._outage():\n            # Kick off a bunch of event fetches but do not pump the reactor\n            <IND>event_deferreds = []\n            for event_id in self.event_ids:\n                <IND>event_deferreds.append(ensureDeferred(self.store.get_event(event_id)))\n\n            # We should have maxed out on event fetcher threads\n            <DED>self.assertEqual(self.store._event_fetch_ongoing, EVENT_QUEUE_THREADS)\n\n            # All the event fetchers will fail\n            self.pump()\n            self.assertEqual(self.store._event_fetch_ongoing, 0)\n\n            for event_deferred in event_deferreds:\n                <IND>failure = self.get_failure(event_deferred, Exception)\n                self.assertEqual(\n                    str(failure.value), \"Could not connect to the database.\"\n                )\n\n        # This next event fetch should succeed\n        <DED><DED>self.get_success(self.store.get_event(self.event_ids[0]))\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "            self.assertEqual(ctx.get_resource_usage().evt_db_fetch_count, 1)\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  }
]