[
  {
    "project": "Adapter-Hub/adapter-transformers",
    "commit": "c83fbc5f2d4355a13be52b086fe548f3c43e8ec8",
    "filename": "src/transformers/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Adapter-Hub-adapter-transformers/src/transformers/trainer.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "src/transformers/trainer.py:645:37 Incompatible parameter type [6]: Expected `bool` for 2nd positional only parameter to call `dict.__setitem__` but got `float`.",
    "message": " Expected `bool` for 2nd positional only parameter to call `dict.__setitem__` but got `float`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 645,
    "warning_line": "            optimizer_kwargs[\"lr\"] = self.args.learning_rate",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n        Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.\n",
        "source_code_len": 222,
        "target_code": "        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n        Trainer's init through :obj:`optimizers`, or subclass and override this method (or :obj:`create_optimizer`\n        and/or :obj:`create_scheduler`) in a subclass.\n        \"\"\"\n        self.create_optimizer()\n        self.create_scheduler(num_training_steps)\n\n    def create_optimizer(self):\n        \"\"\"\n        Setup the optimizer.\n\n        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n        Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.\n",
        "target_code_len": 681,
        "diff_format": "@@ -619,2 +621,13 @@\n         We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n+        Trainer's init through :obj:`optimizers`, or subclass and override this method (or :obj:`create_optimizer`\n+        and/or :obj:`create_scheduler`) in a subclass.\n+        \"\"\"\n+        self.create_optimizer()\n+        self.create_scheduler(num_training_steps)\n+\n+    def create_optimizer(self):\n+        \"\"\"\n+        Setup the optimizer.\n+\n+        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n         Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "",
        "target_code_with_indent": "\n        self.create_optimizer()\n        self.create_scheduler(num_training_steps)\n\n    <DED>def create_optimizer(self):\n        <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        if self.lr_scheduler is None:\n",
        "source_code_len": 39,
        "target_code": "\n    def create_scheduler(self, num_training_steps: int):\n        \"\"\"\n        Setup the scheduler. The optimizer of the trainer must have been set up before this method is called.\n\n        Args:\n            num_training_steps (int): The number of training steps to do.\n        \"\"\"\n        if self.lr_scheduler is None:\n",
        "target_code_len": 319,
        "diff_format": "@@ -654,2 +667,9 @@\n \n+    def create_scheduler(self, num_training_steps: int):\n+        \"\"\"\n+        Setup the scheduler. The optimizer of the trainer must have been set up before this method is called.\n+\n+        Args:\n+            num_training_steps (int): The number of training steps to do.\n+        \"\"\"\n         if self.lr_scheduler is None:\n",
        "source_code_with_indent": "\n        <DED><DED>if self.lr_scheduler is None:\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED><DED><DED>def create_scheduler(self, num_training_steps: int):\n        <IND>\"\"\"\n        Setup the scheduler. The optimizer of the trainer must have been set up before this method is called.\n\n        Args:\n            num_training_steps (int): The number of training steps to do.\n        \"\"\"\n        if self.lr_scheduler is None:\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "Adapter-Hub/adapter-transformers",
    "commit": "c83fbc5f2d4355a13be52b086fe548f3c43e8ec8",
    "filename": "src/transformers/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Adapter-Hub-adapter-transformers/src/transformers/trainer.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "src/transformers/trainer.py:653:79 Incompatible parameter type [6]: Expected `Tuple[float, float]` for 2nd positional only parameter to call `AdamW.__init__` but got `Union[Tuple[float, float], float]`.",
    "message": " Expected `Tuple[float, float]` for 2nd positional only parameter to call `AdamW.__init__` but got `Union[Tuple[float, float], float]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 653,
    "warning_line": "                self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n        Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.\n",
        "source_code_len": 222,
        "target_code": "        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n        Trainer's init through :obj:`optimizers`, or subclass and override this method (or :obj:`create_optimizer`\n        and/or :obj:`create_scheduler`) in a subclass.\n        \"\"\"\n        self.create_optimizer()\n        self.create_scheduler(num_training_steps)\n\n    def create_optimizer(self):\n        \"\"\"\n        Setup the optimizer.\n\n        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n        Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.\n",
        "target_code_len": 681,
        "diff_format": "@@ -619,2 +621,13 @@\n         We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n+        Trainer's init through :obj:`optimizers`, or subclass and override this method (or :obj:`create_optimizer`\n+        and/or :obj:`create_scheduler`) in a subclass.\n+        \"\"\"\n+        self.create_optimizer()\n+        self.create_scheduler(num_training_steps)\n+\n+    def create_optimizer(self):\n+        \"\"\"\n+        Setup the optimizer.\n+\n+        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n         Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "",
        "target_code_with_indent": "\n        self.create_optimizer()\n        self.create_scheduler(num_training_steps)\n\n    <DED>def create_optimizer(self):\n        <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        if self.lr_scheduler is None:\n",
        "source_code_len": 39,
        "target_code": "\n    def create_scheduler(self, num_training_steps: int):\n        \"\"\"\n        Setup the scheduler. The optimizer of the trainer must have been set up before this method is called.\n\n        Args:\n            num_training_steps (int): The number of training steps to do.\n        \"\"\"\n        if self.lr_scheduler is None:\n",
        "target_code_len": 319,
        "diff_format": "@@ -654,2 +667,9 @@\n \n+    def create_scheduler(self, num_training_steps: int):\n+        \"\"\"\n+        Setup the scheduler. The optimizer of the trainer must have been set up before this method is called.\n+\n+        Args:\n+            num_training_steps (int): The number of training steps to do.\n+        \"\"\"\n         if self.lr_scheduler is None:\n",
        "source_code_with_indent": "\n        <DED><DED>if self.lr_scheduler is None:\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED><DED><DED>def create_scheduler(self, num_training_steps: int):\n        <IND>\"\"\"\n        Setup the scheduler. The optimizer of the trainer must have been set up before this method is called.\n\n        Args:\n            num_training_steps (int): The number of training steps to do.\n        \"\"\"\n        if self.lr_scheduler is None:\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "Adapter-Hub/adapter-transformers",
    "commit": "c83fbc5f2d4355a13be52b086fe548f3c43e8ec8",
    "filename": "src/transformers/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Adapter-Hub-adapter-transformers/src/transformers/trainer.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "src/transformers/trainer.py:653:79 Incompatible parameter type [6]: Expected `bool` for 2nd positional only parameter to call `AdamW.__init__` but got `Union[Tuple[float, float], float]`.",
    "message": " Expected `bool` for 2nd positional only parameter to call `AdamW.__init__` but got `Union[Tuple[float, float], float]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 653,
    "warning_line": "                self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n        Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.\n",
        "source_code_len": 222,
        "target_code": "        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n        Trainer's init through :obj:`optimizers`, or subclass and override this method (or :obj:`create_optimizer`\n        and/or :obj:`create_scheduler`) in a subclass.\n        \"\"\"\n        self.create_optimizer()\n        self.create_scheduler(num_training_steps)\n\n    def create_optimizer(self):\n        \"\"\"\n        Setup the optimizer.\n\n        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n        Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.\n",
        "target_code_len": 681,
        "diff_format": "@@ -619,2 +621,13 @@\n         We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n+        Trainer's init through :obj:`optimizers`, or subclass and override this method (or :obj:`create_optimizer`\n+        and/or :obj:`create_scheduler`) in a subclass.\n+        \"\"\"\n+        self.create_optimizer()\n+        self.create_scheduler(num_training_steps)\n+\n+    def create_optimizer(self):\n+        \"\"\"\n+        Setup the optimizer.\n+\n+        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n         Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "",
        "target_code_with_indent": "\n        self.create_optimizer()\n        self.create_scheduler(num_training_steps)\n\n    <DED>def create_optimizer(self):\n        <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        if self.lr_scheduler is None:\n",
        "source_code_len": 39,
        "target_code": "\n    def create_scheduler(self, num_training_steps: int):\n        \"\"\"\n        Setup the scheduler. The optimizer of the trainer must have been set up before this method is called.\n\n        Args:\n            num_training_steps (int): The number of training steps to do.\n        \"\"\"\n        if self.lr_scheduler is None:\n",
        "target_code_len": 319,
        "diff_format": "@@ -654,2 +667,9 @@\n \n+    def create_scheduler(self, num_training_steps: int):\n+        \"\"\"\n+        Setup the scheduler. The optimizer of the trainer must have been set up before this method is called.\n+\n+        Args:\n+            num_training_steps (int): The number of training steps to do.\n+        \"\"\"\n         if self.lr_scheduler is None:\n",
        "source_code_with_indent": "\n        <DED><DED>if self.lr_scheduler is None:\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED><DED><DED>def create_scheduler(self, num_training_steps: int):\n        <IND>\"\"\"\n        Setup the scheduler. The optimizer of the trainer must have been set up before this method is called.\n\n        Args:\n            num_training_steps (int): The number of training steps to do.\n        \"\"\"\n        if self.lr_scheduler is None:\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "Adapter-Hub/adapter-transformers",
    "commit": "c83fbc5f2d4355a13be52b086fe548f3c43e8ec8",
    "filename": "src/transformers/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/Adapter-Hub-adapter-transformers/src/transformers/trainer.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "src/transformers/trainer.py:653:79 Incompatible parameter type [6]: Expected `float` for 2nd positional only parameter to call `AdamW.__init__` but got `Union[Tuple[float, float], float]`.",
    "message": " Expected `float` for 2nd positional only parameter to call `AdamW.__init__` but got `Union[Tuple[float, float], float]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 653,
    "warning_line": "                self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n        Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.\n",
        "source_code_len": 222,
        "target_code": "        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n        Trainer's init through :obj:`optimizers`, or subclass and override this method (or :obj:`create_optimizer`\n        and/or :obj:`create_scheduler`) in a subclass.\n        \"\"\"\n        self.create_optimizer()\n        self.create_scheduler(num_training_steps)\n\n    def create_optimizer(self):\n        \"\"\"\n        Setup the optimizer.\n\n        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n        Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.\n",
        "target_code_len": 681,
        "diff_format": "@@ -619,2 +621,13 @@\n         We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n+        Trainer's init through :obj:`optimizers`, or subclass and override this method (or :obj:`create_optimizer`\n+        and/or :obj:`create_scheduler`) in a subclass.\n+        \"\"\"\n+        self.create_optimizer()\n+        self.create_scheduler(num_training_steps)\n+\n+    def create_optimizer(self):\n+        \"\"\"\n+        Setup the optimizer.\n+\n+        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n         Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "",
        "target_code_with_indent": "\n        self.create_optimizer()\n        self.create_scheduler(num_training_steps)\n\n    <DED>def create_optimizer(self):\n        <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        if self.lr_scheduler is None:\n",
        "source_code_len": 39,
        "target_code": "\n    def create_scheduler(self, num_training_steps: int):\n        \"\"\"\n        Setup the scheduler. The optimizer of the trainer must have been set up before this method is called.\n\n        Args:\n            num_training_steps (int): The number of training steps to do.\n        \"\"\"\n        if self.lr_scheduler is None:\n",
        "target_code_len": 319,
        "diff_format": "@@ -654,2 +667,9 @@\n \n+    def create_scheduler(self, num_training_steps: int):\n+        \"\"\"\n+        Setup the scheduler. The optimizer of the trainer must have been set up before this method is called.\n+\n+        Args:\n+            num_training_steps (int): The number of training steps to do.\n+        \"\"\"\n         if self.lr_scheduler is None:\n",
        "source_code_with_indent": "\n        <DED><DED>if self.lr_scheduler is None:\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED><DED><DED>def create_scheduler(self, num_training_steps: int):\n        <IND>\"\"\"\n        Setup the scheduler. The optimizer of the trainer must have been set up before this method is called.\n\n        Args:\n            num_training_steps (int): The number of training steps to do.\n        \"\"\"\n        if self.lr_scheduler is None:\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  }
]