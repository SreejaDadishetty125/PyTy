[
  {
    "project": "facebookresearch/nevergrad",
    "commit": "c583fe339b4585a2b08da858e67947529cf05cbb",
    "filename": "nevergrad/optimization/test_special.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/facebookresearch-nevergrad/nevergrad/optimization/test_special.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "nevergrad/optimization/test_special.py:41:4 Incompatible variable type [9]: durations is declared to have type `tp.Dict[str, float]` but is used as type `tp.DefaultDict[Variable[collections._KT], int]`.",
    "message": " durations is declared to have type `tp.Dict[str, float]` but is used as type `tp.DefaultDict[Variable[collections._KT], int]`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 41,
    "warning_line": "    durations: tp.Dict[str, float] = collections.defaultdict(int)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\n@pytest.mark.parametrize(\"dimension\", (2, 4, 7, 77))\n@pytest.mark.parametrize(\"num_workers\", (1,))\n@pytest.mark.parametrize(\"scale\", (4.0,))\n@pytest.mark.parametrize(\"baseline\", [\"MetaModel\", \"CMA\", \"ECMA\"])\n@pytest.mark.parametrize(\"budget\", [400, 4000])\n@pytest.mark.parametrize(\"ellipsoid\", [True, False])\ndef test_metamodel_sqp_chaining(\n    dimension: int, num_workers: int, scale: float, budget: int, ellipsoid: bool, baseline: str\n) -> None:\n    \"\"\"The test can operate on the sphere or on an elliptic funciton.\"\"\"\n    target = test_optimizerlib.QuadFunction(scale=scale, ellipse=ellipsoid)\n    baseline = baseline if dimension > 1 else \"OnePlusOne\"\n    chaining = \"ChainMetaModelSQP\"\n\n    # In both cases we compare MetaModel and CMA for a same given budget.\n    # But we expect MetaModel to be clearly better only for a larger budget in the ellipsoid case.\n    contextual_budget = budget if ellipsoid else 3 * budget\n    contextual_budget *= 5 * int(max(1, np.sqrt(scale)))\n\n    num_trials = 27\n    successes = 0.0\n    durations: tp.Dict[str, float] = collections.defaultdict(int)\n    for _ in range(num_trials):\n        if successes >= num_trials / 2:\n            break\n        # Let us run the comparison.\n        recoms: tp.Dict[str, np.ndarray] = {}\n        for name in (chaining, baseline):\n            opt = registry[name](dimension, contextual_budget, num_workers=num_workers)\n            t0 = time.time()\n            recoms[name] = opt.minimize(target).value\n            durations[name] += time.time() - t0\n\n        if target(recoms[baseline]) < target(recoms[chaining]):\n            successes += 1\n        if target(recoms[baseline]) == target(recoms[chaining]):\n            successes += 0.5\n\n    if successes <= num_trials // 2:\n        print(\n            f\"ChainMetaModelSQP fails ({successes}/{num_trials}) for d={dimension}, scale={scale}, \"\n            f\"num_workers={num_workers}, ellipsoid={ellipsoid}, budget={budget}, vs {baseline}\"\n        )\n        raise AssertionError(\"ChaingMetaModelSQP fails by performance.\")\n    print(\n        f\"ChainMetaModelSQP wins for d={dimension}, scale={scale}, num_workers={num_workers}, \"\n        f\"ellipsoid={ellipsoid}, budget={budget}, vs {baseline}\"\n    )\n    assert durations[chaining] < 7 * durations[baseline], \"Computationally more than 7x more expensive.\"\n\n\n@pytest.mark.parametrize(\"args\", test_optimizerlib.get_metamodel_test_settings(special=True))\n",
        "source_code_len": 2423,
        "target_code": "\n@pytest.mark.parametrize(\"args\", test_optimizerlib.get_metamodel_test_settings(special=True))\n",
        "target_code_len": 95,
        "diff_format": "@@ -19,53 +15,2 @@\n \n-@pytest.mark.parametrize(\"dimension\", (2, 4, 7, 77))\n-@pytest.mark.parametrize(\"num_workers\", (1,))\n-@pytest.mark.parametrize(\"scale\", (4.0,))\n-@pytest.mark.parametrize(\"baseline\", [\"MetaModel\", \"CMA\", \"ECMA\"])\n-@pytest.mark.parametrize(\"budget\", [400, 4000])\n-@pytest.mark.parametrize(\"ellipsoid\", [True, False])\n-def test_metamodel_sqp_chaining(\n-    dimension: int, num_workers: int, scale: float, budget: int, ellipsoid: bool, baseline: str\n-) -> None:\n-    \"\"\"The test can operate on the sphere or on an elliptic funciton.\"\"\"\n-    target = test_optimizerlib.QuadFunction(scale=scale, ellipse=ellipsoid)\n-    baseline = baseline if dimension > 1 else \"OnePlusOne\"\n-    chaining = \"ChainMetaModelSQP\"\n-\n-    # In both cases we compare MetaModel and CMA for a same given budget.\n-    # But we expect MetaModel to be clearly better only for a larger budget in the ellipsoid case.\n-    contextual_budget = budget if ellipsoid else 3 * budget\n-    contextual_budget *= 5 * int(max(1, np.sqrt(scale)))\n-\n-    num_trials = 27\n-    successes = 0.0\n-    durations: tp.Dict[str, float] = collections.defaultdict(int)\n-    for _ in range(num_trials):\n-        if successes >= num_trials / 2:\n-            break\n-        # Let us run the comparison.\n-        recoms: tp.Dict[str, np.ndarray] = {}\n-        for name in (chaining, baseline):\n-            opt = registry[name](dimension, contextual_budget, num_workers=num_workers)\n-            t0 = time.time()\n-            recoms[name] = opt.minimize(target).value\n-            durations[name] += time.time() - t0\n-\n-        if target(recoms[baseline]) < target(recoms[chaining]):\n-            successes += 1\n-        if target(recoms[baseline]) == target(recoms[chaining]):\n-            successes += 0.5\n-\n-    if successes <= num_trials // 2:\n-        print(\n-            f\"ChainMetaModelSQP fails ({successes}/{num_trials}) for d={dimension}, scale={scale}, \"\n-            f\"num_workers={num_workers}, ellipsoid={ellipsoid}, budget={budget}, vs {baseline}\"\n-        )\n-        raise AssertionError(\"ChaingMetaModelSQP fails by performance.\")\n-    print(\n-        f\"ChainMetaModelSQP wins for d={dimension}, scale={scale}, num_workers={num_workers}, \"\n-        f\"ellipsoid={ellipsoid}, budget={budget}, vs {baseline}\"\n-    )\n-    assert durations[chaining] < 7 * durations[baseline], \"Computationally more than 7x more expensive.\"\n-\n-\n @pytest.mark.parametrize(\"args\", test_optimizerlib.get_metamodel_test_settings(special=True))\n",
        "source_code_with_indent": "\n<DED>@pytest.mark.parametrize(\"dimension\", (2, 4, 7, 77))\n@pytest.mark.parametrize(\"num_workers\", (1,))\n@pytest.mark.parametrize(\"scale\", (4.0,))\n@pytest.mark.parametrize(\"baseline\", [\"MetaModel\", \"CMA\", \"ECMA\"])\n@pytest.mark.parametrize(\"budget\", [400, 4000])\n@pytest.mark.parametrize(\"ellipsoid\", [True, False])\ndef test_metamodel_sqp_chaining(\n    dimension: int, num_workers: int, scale: float, budget: int, ellipsoid: bool, baseline: str\n) -> None:\n    <IND>\"\"\"The test can operate on the sphere or on an elliptic funciton.\"\"\"\n    target = test_optimizerlib.QuadFunction(scale=scale, ellipse=ellipsoid)\n    baseline = baseline if dimension > 1 else \"OnePlusOne\"\n    chaining = \"ChainMetaModelSQP\"\n\n    # In both cases we compare MetaModel and CMA for a same given budget.\n    # But we expect MetaModel to be clearly better only for a larger budget in the ellipsoid case.\n    contextual_budget = budget if ellipsoid else 3 * budget\n    contextual_budget *= 5 * int(max(1, np.sqrt(scale)))\n\n    num_trials = 27\n    successes = 0.0\n    durations: tp.Dict[str, float] = collections.defaultdict(int)\n    for _ in range(num_trials):\n        <IND>if successes >= num_trials / 2:\n            <IND>break\n        # Let us run the comparison.\n        <DED>recoms: tp.Dict[str, np.ndarray] = {}\n        for name in (chaining, baseline):\n            <IND>opt = registry[name](dimension, contextual_budget, num_workers=num_workers)\n            t0 = time.time()\n            recoms[name] = opt.minimize(target).value\n            durations[name] += time.time() - t0\n\n        <DED>if target(recoms[baseline]) < target(recoms[chaining]):\n            <IND>successes += 1\n        <DED>if target(recoms[baseline]) == target(recoms[chaining]):\n            <IND>successes += 0.5\n\n    <DED><DED>if successes <= num_trials // 2:\n        <IND>print(\n            f\"ChainMetaModelSQP fails ({successes}/{num_trials}) for d={dimension}, scale={scale}, \"\n            f\"num_workers={num_workers}, ellipsoid={ellipsoid}, budget={budget}, vs {baseline}\"\n        )\n        raise AssertionError(\"ChaingMetaModelSQP fails by performance.\")\n    <DED>print(\n        f\"ChainMetaModelSQP wins for d={dimension}, scale={scale}, num_workers={num_workers}, \"\n        f\"ellipsoid={ellipsoid}, budget={budget}, vs {baseline}\"\n    )\n    assert durations[chaining] < 7 * durations[baseline], \"Computationally more than 7x more expensive.\"\n\n\n<DED>@pytest.mark.parametrize(\"args\", test_optimizerlib.get_metamodel_test_settings(special=True))\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n<DED>@pytest.mark.parametrize(\"args\", test_optimizerlib.get_metamodel_test_settings(special=True))\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  }
]