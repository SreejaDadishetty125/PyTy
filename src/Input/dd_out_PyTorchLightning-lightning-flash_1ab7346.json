[
  {
    "project": "PyTorchLightning/lightning-flash",
    "commit": "1ab73463d10f00b8bb47c010c384d14dd2abd056",
    "filename": "flash/core/model.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-lightning-flash/flash/core/model.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "flash/core/model.py:83:8 Incompatible variable type [9]: preprocess is declared to have type `flash.data.process.Preprocess` but is used as type `None`.",
    "message": " preprocess is declared to have type `flash.data.process.Preprocess` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 83,
    "warning_line": "        preprocess: Preprocess = None,"
  },
  {
    "project": "PyTorchLightning/lightning-flash",
    "commit": "1ab73463d10f00b8bb47c010c384d14dd2abd056",
    "filename": "flash/core/model.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-lightning-flash/flash/core/model.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "flash/core/model.py:84:8 Incompatible variable type [9]: postprocess is declared to have type `flash.data.process.Postprocess` but is used as type `None`.",
    "message": " postprocess is declared to have type `flash.data.process.Postprocess` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 84,
    "warning_line": "        postprocess: Postprocess = None,"
  },
  {
    "project": "PyTorchLightning/lightning-flash",
    "commit": "1ab73463d10f00b8bb47c010c384d14dd2abd056",
    "filename": "flash/core/model.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-lightning-flash/flash/core/model.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "flash/core/model.py:279:8 Incompatible attribute type [8]: Attribute `_preprocess` declared in class `Task` has type `flash.data.process.Preprocess` but is used as type `Optional[flash.data.process.Preprocess]`.",
    "message": " Attribute `_preprocess` declared in class `Task` has type `flash.data.process.Preprocess` but is used as type `Optional[flash.data.process.Preprocess]`.",
    "rule_id": "Incompatible attribute type [8]",
    "warning_line_no": 279,
    "warning_line": "        self._preprocess, self._postprocess = Task._resolve("
  },
  {
    "project": "PyTorchLightning/lightning-flash",
    "commit": "1ab73463d10f00b8bb47c010c384d14dd2abd056",
    "filename": "flash/core/model.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-lightning-flash/flash/core/model.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "flash/core/model.py:279:26 Incompatible attribute type [8]: Attribute `_postprocess` declared in class `Task` has type `flash.data.process.Postprocess` but is used as type `Optional[flash.data.process.Postprocess]`.",
    "message": " Attribute `_postprocess` declared in class `Task` has type `flash.data.process.Postprocess` but is used as type `Optional[flash.data.process.Postprocess]`.",
    "rule_id": "Incompatible attribute type [8]",
    "warning_line_no": 279,
    "warning_line": "        self._preprocess, self._postprocess = Task._resolve("
  },
  {
    "project": "PyTorchLightning/lightning-flash",
    "commit": "1ab73463d10f00b8bb47c010c384d14dd2abd056",
    "filename": "flash/text/seq2seq/core/data.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-lightning-flash/flash/text/seq2seq/core/data.py",
    "file_hunks_size": 9,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "flash/text/seq2seq/core/data.py:211:12 Incompatible parameter type [6]: Expected `str` for 4th positional only parameter to call `Seq2SeqData.instantiate_preprocess` but got `Optional[str]`.",
    "message": " Expected `str` for 4th positional only parameter to call `Seq2SeqData.instantiate_preprocess` but got `Optional[str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 211,
    "warning_line": "            target,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        tokenizer = AutoTokenizer.from_pretrained(backbone, use_fast=True)\n        preprocess = cls.instantiate_preprocess(\n            tokenizer,\n",
        "source_code_len": 147,
        "target_code": "        tokenizer = AutoTokenizer.from_pretrained(backbone, use_fast=True)\n        preprocess = preprocess or cls.preprocess_cls(\n            tokenizer,\n",
        "target_code_len": 153,
        "diff_format": "@@ -206,3 +168,3 @@\n         tokenizer = AutoTokenizer.from_pretrained(backbone, use_fast=True)\n-        preprocess = cls.instantiate_preprocess(\n+        preprocess = preprocess or cls.preprocess_cls(\n             tokenizer,\n",
        "source_code_with_indent": "        tokenizer = AutoTokenizer.from_pretrained(backbone, use_fast=True)\n        preprocess = cls.instantiate_preprocess(\n            tokenizer,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        tokenizer = AutoTokenizer.from_pretrained(backbone, use_fast=True)\n        preprocess = preprocess or cls.preprocess_cls(\n            tokenizer,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "            padding,\n            preprocess_cls=preprocess_cls\n        )\n",
        "source_code_len": 73,
        "target_code": "            padding,\n        )\n",
        "target_code_len": 31,
        "diff_format": "@@ -214,3 +176,2 @@\n             padding,\n-            preprocess_cls=preprocess_cls\n         )\n",
        "source_code_with_indent": "            padding,\n            preprocess_cls=preprocess_cls\n        )\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "            padding,\n        )\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "PyTorchLightning/lightning-flash",
    "commit": "1ab73463d10f00b8bb47c010c384d14dd2abd056",
    "filename": "flash/text/seq2seq/core/data.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-lightning-flash/flash/text/seq2seq/core/data.py",
    "file_hunks_size": 9,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "flash/text/seq2seq/core/data.py:214:12 Incompatible parameter type [6]: Expected `int` for 7th positional only parameter to call `Seq2SeqData.instantiate_preprocess` but got `Union[bool, str]`.",
    "message": " Expected `int` for 7th positional only parameter to call `Seq2SeqData.instantiate_preprocess` but got `Union[bool, str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 214,
    "warning_line": "            padding,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        tokenizer = AutoTokenizer.from_pretrained(backbone, use_fast=True)\n        preprocess = cls.instantiate_preprocess(\n            tokenizer,\n",
        "source_code_len": 147,
        "target_code": "        tokenizer = AutoTokenizer.from_pretrained(backbone, use_fast=True)\n        preprocess = preprocess or cls.preprocess_cls(\n            tokenizer,\n",
        "target_code_len": 153,
        "diff_format": "@@ -206,3 +168,3 @@\n         tokenizer = AutoTokenizer.from_pretrained(backbone, use_fast=True)\n-        preprocess = cls.instantiate_preprocess(\n+        preprocess = preprocess or cls.preprocess_cls(\n             tokenizer,\n",
        "source_code_with_indent": "        tokenizer = AutoTokenizer.from_pretrained(backbone, use_fast=True)\n        preprocess = cls.instantiate_preprocess(\n            tokenizer,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        tokenizer = AutoTokenizer.from_pretrained(backbone, use_fast=True)\n        preprocess = preprocess or cls.preprocess_cls(\n            tokenizer,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "            padding,\n            preprocess_cls=preprocess_cls\n        )\n",
        "source_code_len": 73,
        "target_code": "            padding,\n        )\n",
        "target_code_len": 31,
        "diff_format": "@@ -214,3 +176,2 @@\n             padding,\n-            preprocess_cls=preprocess_cls\n         )\n",
        "source_code_with_indent": "            padding,\n            preprocess_cls=preprocess_cls\n        )\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "            padding,\n        )\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "PyTorchLightning/lightning-flash",
    "commit": "1ab73463d10f00b8bb47c010c384d14dd2abd056",
    "filename": "flash/text/seq2seq/summarization/data.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-lightning-flash/flash/text/seq2seq/summarization/data.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "flash/text/seq2seq/summarization/data.py:102:12 Incompatible parameter type [6]: Expected `str` for 4th positional only parameter to call `Seq2SeqData.instantiate_preprocess` but got `Optional[str]`.",
    "message": " Expected `str` for 4th positional only parameter to call `Seq2SeqData.instantiate_preprocess` but got `Optional[str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 102,
    "warning_line": "            target,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        tokenizer = AutoTokenizer.from_pretrained(backbone, use_fast=True)\n        preprocess = cls.instantiate_preprocess(\n            tokenizer,\n",
        "source_code_len": 147,
        "target_code": "        tokenizer = AutoTokenizer.from_pretrained(backbone, use_fast=True)\n\n        preprocess = preprocess or cls.preprocess_cls(\n            tokenizer,\n",
        "target_code_len": 154,
        "diff_format": "@@ -97,3 +90,4 @@\n         tokenizer = AutoTokenizer.from_pretrained(backbone, use_fast=True)\n-        preprocess = cls.instantiate_preprocess(\n+\n+        preprocess = preprocess or cls.preprocess_cls(\n             tokenizer,\n",
        "source_code_with_indent": "        tokenizer = AutoTokenizer.from_pretrained(backbone, use_fast=True)\n        preprocess = cls.instantiate_preprocess(\n            tokenizer,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        tokenizer = AutoTokenizer.from_pretrained(backbone, use_fast=True)\n\n        preprocess = preprocess or cls.preprocess_cls(\n            tokenizer,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "            padding,\n            preprocess_cls=preprocess_cls\n        )\n\n        postprocess = cls.instantiate_postprocess(tokenizer, postprocess_cls=postprocess_cls)\n\n",
        "source_code_len": 169,
        "target_code": "            padding,\n        )\n\n        postprocess = postprocess or cls.postprocess_cls(tokenizer)\n\n",
        "target_code_len": 101,
        "diff_format": "@@ -105,6 +99,5 @@\n             padding,\n-            preprocess_cls=preprocess_cls\n         )\n \n-        postprocess = cls.instantiate_postprocess(tokenizer, postprocess_cls=postprocess_cls)\n+        postprocess = postprocess or cls.postprocess_cls(tokenizer)\n \n",
        "source_code_with_indent": "            padding,\n            preprocess_cls=preprocess_cls\n        )\n\n        postprocess = cls.instantiate_postprocess(tokenizer, postprocess_cls=postprocess_cls)\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "            padding,\n        )\n\n        postprocess = postprocess or cls.postprocess_cls(tokenizer)\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "PyTorchLightning/lightning-flash",
    "commit": "1ab73463d10f00b8bb47c010c384d14dd2abd056",
    "filename": "flash/text/seq2seq/summarization/data.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-lightning-flash/flash/text/seq2seq/summarization/data.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "flash/text/seq2seq/summarization/data.py:105:12 Incompatible parameter type [6]: Expected `int` for 7th positional only parameter to call `Seq2SeqData.instantiate_preprocess` but got `Union[bool, str]`.",
    "message": " Expected `int` for 7th positional only parameter to call `Seq2SeqData.instantiate_preprocess` but got `Union[bool, str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 105,
    "warning_line": "            padding,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        tokenizer = AutoTokenizer.from_pretrained(backbone, use_fast=True)\n        preprocess = cls.instantiate_preprocess(\n            tokenizer,\n",
        "source_code_len": 147,
        "target_code": "        tokenizer = AutoTokenizer.from_pretrained(backbone, use_fast=True)\n\n        preprocess = preprocess or cls.preprocess_cls(\n            tokenizer,\n",
        "target_code_len": 154,
        "diff_format": "@@ -97,3 +90,4 @@\n         tokenizer = AutoTokenizer.from_pretrained(backbone, use_fast=True)\n-        preprocess = cls.instantiate_preprocess(\n+\n+        preprocess = preprocess or cls.preprocess_cls(\n             tokenizer,\n",
        "source_code_with_indent": "        tokenizer = AutoTokenizer.from_pretrained(backbone, use_fast=True)\n        preprocess = cls.instantiate_preprocess(\n            tokenizer,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        tokenizer = AutoTokenizer.from_pretrained(backbone, use_fast=True)\n\n        preprocess = preprocess or cls.preprocess_cls(\n            tokenizer,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "            padding,\n            preprocess_cls=preprocess_cls\n        )\n\n        postprocess = cls.instantiate_postprocess(tokenizer, postprocess_cls=postprocess_cls)\n\n",
        "source_code_len": 169,
        "target_code": "            padding,\n        )\n\n        postprocess = postprocess or cls.postprocess_cls(tokenizer)\n\n",
        "target_code_len": 101,
        "diff_format": "@@ -105,6 +99,5 @@\n             padding,\n-            preprocess_cls=preprocess_cls\n         )\n \n-        postprocess = cls.instantiate_postprocess(tokenizer, postprocess_cls=postprocess_cls)\n+        postprocess = postprocess or cls.postprocess_cls(tokenizer)\n \n",
        "source_code_with_indent": "            padding,\n            preprocess_cls=preprocess_cls\n        )\n\n        postprocess = cls.instantiate_postprocess(tokenizer, postprocess_cls=postprocess_cls)\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "            padding,\n        )\n\n        postprocess = postprocess or cls.postprocess_cls(tokenizer)\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "PyTorchLightning/lightning-flash",
    "commit": "1ab73463d10f00b8bb47c010c384d14dd2abd056",
    "filename": "flash/text/seq2seq/summarization/data.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-lightning-flash/flash/text/seq2seq/summarization/data.py",
    "file_hunks_size": 6,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "flash/text/seq2seq/summarization/data.py:123:4 Inconsistent override [14]: `flash.text.seq2seq.summarization.data.SummarizationData.from_file` overrides method defined in `Seq2SeqData` inconsistently. Could not find parameter `preprocess_cls` in overriding signature.",
    "message": " `flash.text.seq2seq.summarization.data.SummarizationData.from_file` overrides method defined in `Seq2SeqData` inconsistently. Could not find parameter `preprocess_cls` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 123,
    "warning_line": "    def from_file("
  },
  {
    "project": "PyTorchLightning/lightning-flash",
    "commit": "1ab73463d10f00b8bb47c010c384d14dd2abd056",
    "filename": "flash/text/seq2seq/translation/data.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-lightning-flash/flash/text/seq2seq/translation/data.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "flash/text/seq2seq/translation/data.py:89:4 Inconsistent override [14]: `flash.text.seq2seq.translation.data.TranslationData.from_file` overrides method defined in `Seq2SeqData` inconsistently. Could not find parameter `preprocess_cls` in overriding signature.",
    "message": " `flash.text.seq2seq.translation.data.TranslationData.from_file` overrides method defined in `Seq2SeqData` inconsistently. Could not find parameter `preprocess_cls` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 89,
    "warning_line": "    def from_file("
  },
  {
    "project": "PyTorchLightning/lightning-flash",
    "commit": "1ab73463d10f00b8bb47c010c384d14dd2abd056",
    "filename": "flash/vision/classification/model.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-lightning-flash/flash/vision/classification/model.py",
    "file_hunks_size": 4,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "flash/vision/classification/model.py:96:12 Incompatible parameter type [6]: Expected `Optional[flash.data.process.Preprocess]` for 6th parameter `postprocess` to call `ClassificationTask.__init__` but got `flash.core.classification.ClassificationPostprocess`.",
    "message": " Expected `Optional[flash.data.process.Preprocess]` for 6th parameter `postprocess` to call `ClassificationTask.__init__` but got `flash.core.classification.ClassificationPostprocess`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 96,
    "warning_line": "            postprocess=self.postprocess_cls(multi_label)"
  },
  {
    "project": "PyTorchLightning/lightning-flash",
    "commit": "1ab73463d10f00b8bb47c010c384d14dd2abd056",
    "filename": "flash/vision/detection/data.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-lightning-flash/flash/vision/detection/data.py",
    "file_hunks_size": 2,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "flash/vision/detection/data.py:190:8 Incompatible variable type [9]: preprocess_cls is declared to have type `Type[Preprocess]` but is used as type `None`.",
    "message": " preprocess_cls is declared to have type `Type[Preprocess]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 190,
    "warning_line": "        preprocess_cls: Type[Preprocess] = None,"
  },
  {
    "project": "PyTorchLightning/lightning-flash",
    "commit": "1ab73463d10f00b8bb47c010c384d14dd2abd056",
    "filename": "flash/vision/detection/data.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-lightning-flash/flash/vision/detection/data.py",
    "file_hunks_size": 2,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "flash/vision/detection/data.py:211:8 Incompatible variable type [9]: preprocess_cls is declared to have type `Type[Preprocess]` but is used as type `None`.",
    "message": " preprocess_cls is declared to have type `Type[Preprocess]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 211,
    "warning_line": "        preprocess_cls: Type[Preprocess] = None,"
  }
]