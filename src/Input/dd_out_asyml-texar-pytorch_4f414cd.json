[
  {
    "project": "asyml/texar-pytorch",
    "commit": "4f414cdf0b19c19f8f2ef778f2d14f1108e3a79a",
    "filename": "examples/xlnet/xlnet/data/utils.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asyml-texar-pytorch/examples/xlnet/xlnet/data/utils.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "examples/xlnet/xlnet/data/utils.py:56:27 Incompatible parameter type [6]: Expected `typing.Iterable[str]` for 1st positional only parameter to call `str.join` but got `Union[List[bytes], List[str]]`.",
    "message": " Expected `typing.Iterable[str]` for 1st positional only parameter to call `str.join` but got `Union[List[bytes], List[str]]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 56,
    "warning_line": "        outputs = ' '.join(inputs.strip().split())",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "                    keep_accents: bool = False) -> str:\n    if remove_space:\n",
        "source_code_len": 77,
        "target_code": "                    keep_accents: bool = False) -> str:\n    if isinstance(inputs, bytes):\n        inputs = inputs.decode('utf-8')\n\n    if remove_space:\n",
        "target_code_len": 152,
        "diff_format": "@@ -54,2 +43,5 @@\n                     keep_accents: bool = False) -> str:\n+    if isinstance(inputs, bytes):\n+        inputs = inputs.decode('utf-8')\n+\n     if remove_space:\n",
        "source_code_with_indent": "                    keep_accents: bool = False) -> str:\n    <IND>if remove_space:\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                    keep_accents: bool = False) -> str:\n    <IND>if isinstance(inputs, bytes):\n        <IND>inputs = inputs.decode('utf-8')\n\n    <DED>if remove_space:\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "asyml/texar-pytorch",
    "commit": "4f414cdf0b19c19f8f2ef778f2d14f1108e3a79a",
    "filename": "examples/xlnet/xlnet/data/utils.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asyml-texar-pytorch/examples/xlnet/xlnet/data/utils.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "examples/xlnet/xlnet/data/utils.py:59:30 Incompatible parameter type [6]: Expected `bytes` for 1st positional only parameter to call `bytes.replace` but got `str`.",
    "message": " Expected `bytes` for 1st positional only parameter to call `bytes.replace` but got `str`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 59,
    "warning_line": "    outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "                    keep_accents: bool = False) -> str:\n    if remove_space:\n",
        "source_code_len": 77,
        "target_code": "                    keep_accents: bool = False) -> str:\n    if isinstance(inputs, bytes):\n        inputs = inputs.decode('utf-8')\n\n    if remove_space:\n",
        "target_code_len": 152,
        "diff_format": "@@ -54,2 +43,5 @@\n                     keep_accents: bool = False) -> str:\n+    if isinstance(inputs, bytes):\n+        inputs = inputs.decode('utf-8')\n+\n     if remove_space:\n",
        "source_code_with_indent": "                    keep_accents: bool = False) -> str:\n    <IND>if remove_space:\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                    keep_accents: bool = False) -> str:\n    <IND>if isinstance(inputs, bytes):\n        <IND>inputs = inputs.decode('utf-8')\n\n    <DED>if remove_space:\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "asyml/texar-pytorch",
    "commit": "4f414cdf0b19c19f8f2ef778f2d14f1108e3a79a",
    "filename": "examples/xlnet/xlnet/data/utils.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asyml-texar-pytorch/examples/xlnet/xlnet/data/utils.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "examples/xlnet/xlnet/data/utils.py:59:36 Incompatible parameter type [6]: Expected `bytes` for 2nd positional only parameter to call `bytes.replace` but got `str`.",
    "message": " Expected `bytes` for 2nd positional only parameter to call `bytes.replace` but got `str`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 59,
    "warning_line": "    outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "                    keep_accents: bool = False) -> str:\n    if remove_space:\n",
        "source_code_len": 77,
        "target_code": "                    keep_accents: bool = False) -> str:\n    if isinstance(inputs, bytes):\n        inputs = inputs.decode('utf-8')\n\n    if remove_space:\n",
        "target_code_len": 152,
        "diff_format": "@@ -54,2 +43,5 @@\n                     keep_accents: bool = False) -> str:\n+    if isinstance(inputs, bytes):\n+        inputs = inputs.decode('utf-8')\n+\n     if remove_space:\n",
        "source_code_with_indent": "                    keep_accents: bool = False) -> str:\n    <IND>if remove_space:\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                    keep_accents: bool = False) -> str:\n    <IND>if isinstance(inputs, bytes):\n        <IND>inputs = inputs.decode('utf-8')\n\n    <DED>if remove_space:\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "asyml/texar-pytorch",
    "commit": "4f414cdf0b19c19f8f2ef778f2d14f1108e3a79a",
    "filename": "examples/xlnet/xlnet/data/utils.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asyml-texar-pytorch/examples/xlnet/xlnet/data/utils.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "examples/xlnet/xlnet/data/utils.py:59:49 Incompatible parameter type [6]: Expected `bytes` for 1st positional only parameter to call `bytes.replace` but got `str`.",
    "message": " Expected `bytes` for 1st positional only parameter to call `bytes.replace` but got `str`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 59,
    "warning_line": "    outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "                    keep_accents: bool = False) -> str:\n    if remove_space:\n",
        "source_code_len": 77,
        "target_code": "                    keep_accents: bool = False) -> str:\n    if isinstance(inputs, bytes):\n        inputs = inputs.decode('utf-8')\n\n    if remove_space:\n",
        "target_code_len": 152,
        "diff_format": "@@ -54,2 +43,5 @@\n                     keep_accents: bool = False) -> str:\n+    if isinstance(inputs, bytes):\n+        inputs = inputs.decode('utf-8')\n+\n     if remove_space:\n",
        "source_code_with_indent": "                    keep_accents: bool = False) -> str:\n    <IND>if remove_space:\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                    keep_accents: bool = False) -> str:\n    <IND>if isinstance(inputs, bytes):\n        <IND>inputs = inputs.decode('utf-8')\n\n    <DED>if remove_space:\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "asyml/texar-pytorch",
    "commit": "4f414cdf0b19c19f8f2ef778f2d14f1108e3a79a",
    "filename": "examples/xlnet/xlnet/data/utils.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asyml-texar-pytorch/examples/xlnet/xlnet/data/utils.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "examples/xlnet/xlnet/data/utils.py:59:55 Incompatible parameter type [6]: Expected `bytes` for 2nd positional only parameter to call `bytes.replace` but got `str`.",
    "message": " Expected `bytes` for 2nd positional only parameter to call `bytes.replace` but got `str`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 59,
    "warning_line": "    outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "                    keep_accents: bool = False) -> str:\n    if remove_space:\n",
        "source_code_len": 77,
        "target_code": "                    keep_accents: bool = False) -> str:\n    if isinstance(inputs, bytes):\n        inputs = inputs.decode('utf-8')\n\n    if remove_space:\n",
        "target_code_len": 152,
        "diff_format": "@@ -54,2 +43,5 @@\n                     keep_accents: bool = False) -> str:\n+    if isinstance(inputs, bytes):\n+        inputs = inputs.decode('utf-8')\n+\n     if remove_space:\n",
        "source_code_with_indent": "                    keep_accents: bool = False) -> str:\n    <IND>if remove_space:\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                    keep_accents: bool = False) -> str:\n    <IND>if isinstance(inputs, bytes):\n        <IND>inputs = inputs.decode('utf-8')\n\n    <DED>if remove_space:\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "asyml/texar-pytorch",
    "commit": "4f414cdf0b19c19f8f2ef778f2d14f1108e3a79a",
    "filename": "examples/xlnet/xlnet/model/modules.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asyml-texar-pytorch/examples/xlnet/xlnet/model/modules.py",
    "file_hunks_size": 14,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": true,
    "full_warning_msg": "examples/xlnet/xlnet/model/modules.py:63:4 Inconsistent override [14]: `examples.xlnet.xlnet.model.modules.PositionWiseFF.forward` overrides method defined in `tx.module_base.ModuleBase` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `examples.xlnet.xlnet.model.modules.PositionWiseFF.forward` overrides method defined in `tx.module_base.ModuleBase` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 63,
    "warning_line": "    def forward(self, input: Tensor) -> Tensor:",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": true,
        "source_code": "\n    def forward(self, input: Tensor) -> Tensor:\n        # position-wise feed-forward\n",
        "source_code_len": 86,
        "target_code": "\n    def forward(self, input: Tensor) -> Tensor:  # type: ignore\n        # position-wise feed-forward\n",
        "target_code_len": 102,
        "diff_format": "@@ -62,3 +62,3 @@\n \n-    def forward(self, input: Tensor) -> Tensor:\n+    def forward(self, input: Tensor) -> Tensor:  # type: ignore\n         # position-wise feed-forward\n",
        "source_code_with_indent": "\n    <DED>def forward(self, input: Tensor) -> Tensor:\n        # position-wise feed-forward\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def forward(self, input: Tensor) -> Tensor:  # type: ignore\n        # position-wise feed-forward\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "asyml/texar-pytorch",
    "commit": "4f414cdf0b19c19f8f2ef778f2d14f1108e3a79a",
    "filename": "examples/xlnet/xlnet/model/modules.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asyml-texar-pytorch/examples/xlnet/xlnet/model/modules.py",
    "file_hunks_size": 14,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": true,
    "full_warning_msg": "examples/xlnet/xlnet/model/modules.py:119:4 Inconsistent override [14]: `examples.xlnet.xlnet.model.modules.RelativePositionalEncoding.forward` overrides method defined in `tx.module_base.ModuleBase` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `examples.xlnet.xlnet.model.modules.RelativePositionalEncoding.forward` overrides method defined in `tx.module_base.ModuleBase` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 119,
    "warning_line": "    def forward(self, batch_size: int, seq_len: int, total_len: int,",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": true,
        "source_code": "\n    def forward(self, batch_size: int, seq_len: int, total_len: int,\n                clamp_len: Optional[int] = None, attn_type: str = 'bi',\n",
        "source_code_len": 142,
        "target_code": "\n    def forward(self,  # type: ignore\n                batch_size: int, seq_len: int, total_len: int,\n                clamp_len: Optional[int] = None, attn_type: str = 'bi',\n",
        "target_code_len": 174,
        "diff_format": "@@ -118,3 +118,4 @@\n \n-    def forward(self, batch_size: int, seq_len: int, total_len: int,\n+    def forward(self,  # type: ignore\n+                batch_size: int, seq_len: int, total_len: int,\n                 clamp_len: Optional[int] = None, attn_type: str = 'bi',\n",
        "source_code_with_indent": "\n    <DED>def forward(self, batch_size: int, seq_len: int, total_len: int,\n                clamp_len: Optional[int] = None, attn_type: str = 'bi',\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def forward(self,  # type: ignore\n                batch_size: int, seq_len: int, total_len: int,\n                clamp_len: Optional[int] = None, attn_type: str = 'bi',\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "asyml/texar-pytorch",
    "commit": "4f414cdf0b19c19f8f2ef778f2d14f1108e3a79a",
    "filename": "examples/xlnet/xlnet/model/modules.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asyml-texar-pytorch/examples/xlnet/xlnet/model/modules.py",
    "file_hunks_size": 14,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "examples/xlnet/xlnet/model/modules.py:238:4 Inconsistent override [14]: `examples.xlnet.xlnet.model.modules.RelativeMultiheadAttention.forward` overrides method defined in `tx.module_base.ModuleBase` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `examples.xlnet.xlnet.model.modules.RelativeMultiheadAttention.forward` overrides method defined in `tx.module_base.ModuleBase` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 238,
    "warning_line": "    def forward(self, states: Tensor, pos_embed: Tensor,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nfrom typing import Dict, Any, Optional\n\n",
        "source_code_len": 41,
        "target_code": "\nfrom typing import Any, Dict, Optional, Tuple\n\n",
        "target_code_len": 48,
        "diff_format": "@@ -19,3 +19,3 @@\n \n-from typing import Dict, Any, Optional\n+from typing import Any, Dict, Optional, Tuple\n \n",
        "source_code_with_indent": "\nfrom typing import Dict, Any, Optional\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\nfrom typing import Any, Dict, Optional, Tuple\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        self.layer_norm = nn.LayerNorm(hidden_dim, eps=1e-12)\n\n",
        "source_code_len": 64,
        "target_code": "\n        bias_shape = (self.num_heads, self.head_dim)\n        self.untie_r = r_r_bias is None\n        self.r_r_bias = (r_r_bias if r_r_bias is not None\n                         else nn.Parameter(torch.Tensor(*bias_shape)))\n        self.r_w_bias = (r_w_bias if r_w_bias is not None\n                         else nn.Parameter(torch.Tensor(*bias_shape)))\n\n",
        "target_code_len": 353,
        "diff_format": "@@ -160,3 +165,8 @@\n \n-        self.layer_norm = nn.LayerNorm(hidden_dim, eps=1e-12)\n+        bias_shape = (self.num_heads, self.head_dim)\n+        self.untie_r = r_r_bias is None\n+        self.r_r_bias = (r_r_bias if r_r_bias is not None\n+                         else nn.Parameter(torch.Tensor(*bias_shape)))\n+        self.r_w_bias = (r_w_bias if r_w_bias is not None\n+                         else nn.Parameter(torch.Tensor(*bias_shape)))\n \n",
        "source_code_with_indent": "\n        self.layer_norm = nn.LayerNorm(hidden_dim, eps=1e-12)\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        bias_shape = (self.num_heads, self.head_dim)\n        self.untie_r = r_r_bias is None\n        self.r_r_bias = (r_r_bias if r_r_bias is not None\n                         else nn.Parameter(torch.Tensor(*bias_shape)))\n        self.r_w_bias = (r_w_bias if r_w_bias is not None\n                         else nn.Parameter(torch.Tensor(*bias_shape)))\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "                2, self.num_heads, self.head_dim))\n\n",
        "source_code_len": 52,
        "target_code": "                2, self.num_heads, self.head_dim))\n            self.r_s_bias = (r_s_bias if r_s_bias is not None\n                             else nn.Parameter(torch.Tensor(*bias_shape)))\n\n        self.layer_norm = nn.LayerNorm(hidden_dim, eps=1e-12)\n\n",
        "target_code_len": 252,
        "diff_format": "@@ -165,2 +175,6 @@\n                 2, self.num_heads, self.head_dim))\n+            self.r_s_bias = (r_s_bias if r_s_bias is not None\n+                             else nn.Parameter(torch.Tensor(*bias_shape)))\n+\n+        self.layer_norm = nn.LayerNorm(hidden_dim, eps=1e-12)\n \n",
        "source_code_with_indent": "                2, self.num_heads, self.head_dim))\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                2, self.num_heads, self.head_dim))\n            self.r_s_bias = (r_s_bias if r_s_bias is not None\n                             else nn.Parameter(torch.Tensor(*bias_shape)))\n\n        <DED>self.layer_norm = nn.LayerNorm(hidden_dim, eps=1e-12)\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "            k_head_r: Tensor, segment_mat: Optional[Tensor],\n            r_w_bias: Tensor, r_r_bias: Tensor, r_s_bias: Optional[Tensor],\n            attn_mask: Optional[Tensor] = None) -> Tensor:\n        # Content based attention score.\n        q_head_rw = q_head + r_w_bias\n        # attn_ac: (seq_len, tot_len, batch_size, n_head)\n",
        "source_code_len": 333,
        "target_code": "            k_head_r: Tensor, segment_mat: Optional[Tensor],\n            attn_mask: Optional[Tensor] = None) -> Tensor:\n        # Content based attention score.\n        q_head_rw = q_head + self.r_w_bias\n        # attn_ac: (seq_len, tot_len, batch_size, n_head)\n",
        "target_code_len": 262,
        "diff_format": "@@ -194,6 +213,5 @@\n             k_head_r: Tensor, segment_mat: Optional[Tensor],\n-            r_w_bias: Tensor, r_r_bias: Tensor, r_s_bias: Optional[Tensor],\n             attn_mask: Optional[Tensor] = None) -> Tensor:\n         # Content based attention score.\n-        q_head_rw = q_head + r_w_bias\n+        q_head_rw = q_head + self.r_w_bias\n         # attn_ac: (seq_len, tot_len, batch_size, n_head)\n",
        "source_code_with_indent": "            k_head_r: Tensor, segment_mat: Optional[Tensor],\n            r_w_bias: Tensor, r_r_bias: Tensor, r_s_bias: Optional[Tensor],\n            attn_mask: Optional[Tensor] = None) -> Tensor:\n        # Content based attention score.\n        <IND>q_head_rw = q_head + r_w_bias\n        # attn_ac: (seq_len, tot_len, batch_size, n_head)\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "            k_head_r: Tensor, segment_mat: Optional[Tensor],\n            attn_mask: Optional[Tensor] = None) -> Tensor:\n        # Content based attention score.\n        <IND>q_head_rw = q_head + self.r_w_bias\n        # attn_ac: (seq_len, tot_len, batch_size, n_head)\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        # Position based attention score.\n        q_head_rr = q_head + r_r_bias\n        # attn_bd: (seq_len, tot_len, batch_size, n_head)\n",
        "source_code_len": 138,
        "target_code": "        # Position based attention score.\n        q_head_rr = q_head + self.r_r_bias\n        # attn_bd: (seq_len, tot_len, batch_size, n_head)\n",
        "target_code_len": 143,
        "diff_format": "@@ -202,3 +220,3 @@\n         # Position based attention score.\n-        q_head_rr = q_head + r_r_bias\n+        q_head_rr = q_head + self.r_r_bias\n         # attn_bd: (seq_len, tot_len, batch_size, n_head)\n",
        "source_code_with_indent": "        # Position based attention score.\n        q_head_rr = q_head + r_r_bias\n        # attn_bd: (seq_len, tot_len, batch_size, n_head)\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        # Position based attention score.\n        q_head_rr = q_head + self.r_r_bias\n        # attn_bd: (seq_len, tot_len, batch_size, n_head)\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        else:\n            q_head_rs = q_head + r_s_bias\n            attn_ef = torch.einsum(\n",
        "source_code_len": 92,
        "target_code": "        else:\n            q_head_rs = q_head + self.r_s_bias\n            attn_ef = torch.einsum(\n",
        "target_code_len": 97,
        "diff_format": "@@ -211,3 +229,3 @@\n         else:\n-            q_head_rs = q_head + r_s_bias\n+            q_head_rs = q_head + self.r_s_bias\n             attn_ef = torch.einsum(\n",
        "source_code_with_indent": "        <DED>else:\n            <IND>q_head_rs = q_head + r_s_bias\n            attn_ef = torch.einsum(\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <DED>else:\n            <IND>q_head_rs = q_head + self.r_s_bias\n            attn_ef = torch.einsum(\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": true,
        "source_code": "\n    def forward(self, states: Tensor, pos_embed: Tensor,\n                segment_mat: Optional[Tensor],\n                r_w_bias: Tensor, r_r_bias: Tensor, r_s_bias: Optional[Tensor],\n                attn_mask: Optional[Tensor] = None,\n                memory: Optional[Tensor] = None) -> Tensor:\n        seq_len, batch_size = states.size()[:2]\n        pos_len = pos_embed.size(0)\n",
        "source_code_len": 381,
        "target_code": "\n    def _post_attention(self, attn_vec: Tensor) -> Tensor:\n        attn_vec = attn_vec.view(*attn_vec.size()[:2], -1)\n        attn_out = self.output_projection(attn_vec)\n        attn_out = self.dropout(attn_out)\n        return attn_out\n\n    def forward(self,  # type: ignore\n                states_h: Tensor, states_g: Optional[Tensor],\n                pos_embed: Tensor, segment_mat: Optional[Tensor],\n                attn_mask_h: Optional[Tensor] = None,\n                attn_mask_g: Optional[Tensor] = None,\n                target_mapping: Optional[Tensor] = None,\n                memory: Optional[Tensor] = None) \\\n            -> Tuple[Tensor, Optional[Tensor]]:\n        seq_len, batch_size = states_h.size()[:2]\n        pos_len = pos_embed.size(0)\n",
        "target_code_len": 754,
        "diff_format": "@@ -237,8 +255,17 @@\n \n-    def forward(self, states: Tensor, pos_embed: Tensor,\n-                segment_mat: Optional[Tensor],\n-                r_w_bias: Tensor, r_r_bias: Tensor, r_s_bias: Optional[Tensor],\n-                attn_mask: Optional[Tensor] = None,\n-                memory: Optional[Tensor] = None) -> Tensor:\n-        seq_len, batch_size = states.size()[:2]\n+    def _post_attention(self, attn_vec: Tensor) -> Tensor:\n+        attn_vec = attn_vec.view(*attn_vec.size()[:2], -1)\n+        attn_out = self.output_projection(attn_vec)\n+        attn_out = self.dropout(attn_out)\n+        return attn_out\n+\n+    def forward(self,  # type: ignore\n+                states_h: Tensor, states_g: Optional[Tensor],\n+                pos_embed: Tensor, segment_mat: Optional[Tensor],\n+                attn_mask_h: Optional[Tensor] = None,\n+                attn_mask_g: Optional[Tensor] = None,\n+                target_mapping: Optional[Tensor] = None,\n+                memory: Optional[Tensor] = None) \\\n+            -> Tuple[Tensor, Optional[Tensor]]:\n+        seq_len, batch_size = states_h.size()[:2]\n         pos_len = pos_embed.size(0)\n",
        "source_code_with_indent": "\n    <DED>def forward(self, states: Tensor, pos_embed: Tensor,\n                segment_mat: Optional[Tensor],\n                r_w_bias: Tensor, r_r_bias: Tensor, r_s_bias: Optional[Tensor],\n                attn_mask: Optional[Tensor] = None,\n                memory: Optional[Tensor] = None) -> Tensor:\n        <IND>seq_len, batch_size = states.size()[:2]\n        pos_len = pos_embed.size(0)\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent_exact_match": false,
        "target_code_with_indent": "\n    <DED>def _post_attention(self, attn_vec: Tensor) -> Tensor:\n        <IND>attn_vec = attn_vec.view(*attn_vec.size()[:2], -1)\n        attn_out = self.output_projection(attn_vec)\n        attn_out = self.dropout(attn_out)\n        return attn_out\n\n    <DED>def forward(self,  # type: ignore\n                states_h: Tensor, states_g: Optional[Tensor],\n                pos_embed: Tensor, segment_mat: Optional[Tensor],\n                attn_mask_h: Optional[Tensor] = None,\n                attn_mask_g: Optional[Tensor] = None,\n                target_mapping: Optional[Tensor] = None,\n                memory: Optional[Tensor] = None)            -> Tuple[Tensor, Optional[Tensor]]:\n        <IND>seq_len, batch_size = states_h.size()[:2]\n        pos_len = pos_embed.size(0)\n"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        if memory is not None and memory.dim() > 1:\n            concat_input = torch.cat([memory, states], dim=0)\n        else:\n            concat_input = states\n\n",
        "source_code_len": 163,
        "target_code": "        if memory is not None and memory.dim() > 1:\n            concat_input = torch.cat([memory, states_h], dim=0)\n        else:\n            concat_input = states_h\n\n",
        "target_code_len": 167,
        "diff_format": "@@ -246,5 +273,5 @@\n         if memory is not None and memory.dim() > 1:\n-            concat_input = torch.cat([memory, states], dim=0)\n-        else:\n-            concat_input = states\n+            concat_input = torch.cat([memory, states_h], dim=0)\n+        else:\n+            concat_input = states_h\n \n",
        "source_code_with_indent": "        if memory is not None and memory.dim() > 1:\n            <IND>concat_input = torch.cat([memory, states], dim=0)\n        <DED>else:\n            <IND>concat_input = states\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        if memory is not None and memory.dim() > 1:\n            <IND>concat_input = torch.cat([memory, states_h], dim=0)\n        <DED>else:\n            <IND>concat_input = states_h\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        # Core attention ops.\n        attn_vec = self._compute_attention_score(\n            q_head_h, k_head_h, v_head_h, k_head_r, segment_mat,\n            r_w_bias, r_r_bias, r_s_bias, attn_mask)\n\n        # Post attention processing.\n        attn_vec = attn_vec.view(*attn_vec.size()[:2], -1)\n        attn_out = self.output_projection(attn_vec)\n        attn_out = self.dropout(attn_out)\n        # residual + layer norm\n        output = self.layer_norm(states + attn_out)\n\n        return output\n",
        "source_code_len": 496,
        "target_code": "        # Core attention ops.\n        attn_vec_h = self._compute_attention_score(\n            q_head_h, k_head_h, v_head_h, k_head_r,\n            segment_mat, attn_mask_h)\n\n        # Post attention processing.\n        attn_out_h = self._post_attention(attn_vec_h)\n        # residual + layer norm\n        output_h = self.layer_norm(states_h + attn_out_h)\n\n        if states_g is not None:\n            proj_dim = self.num_heads * self.head_dim\n            proj_weight = self.head_projection.weight[:proj_dim]\n            q_head_g = F.linear(states_g, proj_weight)\n            q_head_g = q_head_g.view(\n                q_head_g.size(0), batch_size, self.num_heads, self.head_dim)\n            if target_mapping is not None:\n                q_head_g = torch.einsum(\n                    'mbnd,mlb->lbnd', [q_head_g, target_mapping])\n            attn_vec_g = self._compute_attention_score(\n                q_head_g, k_head_h, v_head_h, k_head_r,\n                segment_mat, attn_mask_g)\n            if target_mapping is not None:\n                attn_vec_g = torch.einsum(\n                    'lbnd,mlb->mbnd', [attn_vec_g, target_mapping])\n            attn_out_g = self._post_attention(attn_vec_g)\n            output_g = self.layer_norm(states_g + attn_out_g)\n        else:\n            output_g = None\n\n        return output_h, output_g\n",
        "target_code_len": 1332,
        "diff_format": "@@ -269,13 +296,31 @@\n         # Core attention ops.\n-        attn_vec = self._compute_attention_score(\n-            q_head_h, k_head_h, v_head_h, k_head_r, segment_mat,\n-            r_w_bias, r_r_bias, r_s_bias, attn_mask)\n+        attn_vec_h = self._compute_attention_score(\n+            q_head_h, k_head_h, v_head_h, k_head_r,\n+            segment_mat, attn_mask_h)\n \n         # Post attention processing.\n-        attn_vec = attn_vec.view(*attn_vec.size()[:2], -1)\n-        attn_out = self.output_projection(attn_vec)\n-        attn_out = self.dropout(attn_out)\n+        attn_out_h = self._post_attention(attn_vec_h)\n         # residual + layer norm\n-        output = self.layer_norm(states + attn_out)\n-\n-        return output\n+        output_h = self.layer_norm(states_h + attn_out_h)\n+\n+        if states_g is not None:\n+            proj_dim = self.num_heads * self.head_dim\n+            proj_weight = self.head_projection.weight[:proj_dim]\n+            q_head_g = F.linear(states_g, proj_weight)\n+            q_head_g = q_head_g.view(\n+                q_head_g.size(0), batch_size, self.num_heads, self.head_dim)\n+            if target_mapping is not None:\n+                q_head_g = torch.einsum(\n+                    'mbnd,mlb->lbnd', [q_head_g, target_mapping])\n+            attn_vec_g = self._compute_attention_score(\n+                q_head_g, k_head_h, v_head_h, k_head_r,\n+                segment_mat, attn_mask_g)\n+            if target_mapping is not None:\n+                attn_vec_g = torch.einsum(\n+                    'lbnd,mlb->mbnd', [attn_vec_g, target_mapping])\n+            attn_out_g = self._post_attention(attn_vec_g)\n+            output_g = self.layer_norm(states_g + attn_out_g)\n+        else:\n+            output_g = None\n+\n+        return output_h, output_g\n",
        "source_code_with_indent": "        # Core attention ops.\n        attn_vec = self._compute_attention_score(\n            q_head_h, k_head_h, v_head_h, k_head_r, segment_mat,\n            r_w_bias, r_r_bias, r_s_bias, attn_mask)\n\n        # Post attention processing.\n        attn_vec = attn_vec.view(*attn_vec.size()[:2], -1)\n        attn_out = self.output_projection(attn_vec)\n        attn_out = self.dropout(attn_out)\n        # residual + layer norm\n        output = self.layer_norm(states + attn_out)\n\n        return output\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        # Core attention ops.\n        attn_vec_h = self._compute_attention_score(\n            q_head_h, k_head_h, v_head_h, k_head_r,\n            segment_mat, attn_mask_h)\n\n        # Post attention processing.\n        attn_out_h = self._post_attention(attn_vec_h)\n        # residual + layer norm\n        output_h = self.layer_norm(states_h + attn_out_h)\n\n        if states_g is not None:\n            <IND>proj_dim = self.num_heads * self.head_dim\n            proj_weight = self.head_projection.weight[:proj_dim]\n            q_head_g = F.linear(states_g, proj_weight)\n            q_head_g = q_head_g.view(\n                q_head_g.size(0), batch_size, self.num_heads, self.head_dim)\n            if target_mapping is not None:\n                <IND>q_head_g = torch.einsum(\n                    'mbnd,mlb->lbnd', [q_head_g, target_mapping])\n            <DED>attn_vec_g = self._compute_attention_score(\n                q_head_g, k_head_h, v_head_h, k_head_r,\n                segment_mat, attn_mask_g)\n            if target_mapping is not None:\n                <IND>attn_vec_g = torch.einsum(\n                    'lbnd,mlb->mbnd', [attn_vec_g, target_mapping])\n            <DED>attn_out_g = self._post_attention(attn_vec_g)\n            output_g = self.layer_norm(states_g + attn_out_g)\n        <DED>else:\n            <IND>output_g = None\n\n        <DED>return output_h, output_g\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  }
]