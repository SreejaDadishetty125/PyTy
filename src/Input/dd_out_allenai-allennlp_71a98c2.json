[
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/__main__.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/__main__.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/__main__.py:10:37 Incompatible parameter type [6]: Expected `str` for 1st positional only parameter to call `typing.Mapping.get` but got `typing.Optional[str]`.",
    "message": " Expected `str` for 1st positional only parameter to call `typing.Mapping.get` but got `typing.Optional[str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 10,
    "warning_line": "    LEVEL = logging._nameToLevel.get(level_name, logging.INFO)",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "else:\n    level_name = os.environ.get(\"ALLENNLP_LOG_LEVEL\")\n    LEVEL = logging._nameToLevel.get(level_name, logging.INFO)\n",
        "source_code_len": 123,
        "target_code": "else:\n    level_name = os.environ.get(\"ALLENNLP_LOG_LEVEL\", \"INFO\")\n    LEVEL = logging._nameToLevel.get(level_name, logging.INFO)\n",
        "target_code_len": 131,
        "diff_format": "@@ -8,3 +8,3 @@\n else:\n-    level_name = os.environ.get(\"ALLENNLP_LOG_LEVEL\")\n+    level_name = os.environ.get(\"ALLENNLP_LOG_LEVEL\", \"INFO\")\n     LEVEL = logging._nameToLevel.get(level_name, logging.INFO)\n",
        "source_code_with_indent": "<DED>else:\n    <IND>level_name = os.environ.get(\"ALLENNLP_LOG_LEVEL\")\n    LEVEL = logging._nameToLevel.get(level_name, logging.INFO)\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "<DED>else:\n    <IND>level_name = os.environ.get(\"ALLENNLP_LOG_LEVEL\", \"INFO\")\n    LEVEL = logging._nameToLevel.get(level_name, logging.INFO)\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/commands/train.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/commands/train.py",
    "file_hunks_size": 7,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/commands/train.py:559:8 Incompatible variable type [9]: vocabulary is declared to have type `allennlp.common.lazy.Lazy[allennlp.data.vocabulary.Vocabulary]` but is used as type `None`.",
    "message": " vocabulary is declared to have type `allennlp.common.lazy.Lazy[allennlp.data.vocabulary.Vocabulary]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 559,
    "warning_line": "        vocabulary: Lazy[Vocabulary] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        trainer: Lazy[Trainer],\n        vocabulary: Lazy[Vocabulary] = None,\n        datasets_for_vocab_creation: List[str] = None,\n",
        "source_code_len": 132,
        "target_code": "        trainer: Lazy[Trainer],\n        vocabulary: Lazy[Vocabulary] = Lazy(Vocabulary),\n        datasets_for_vocab_creation: List[str] = None,\n",
        "target_code_len": 144,
        "diff_format": "@@ -558,3 +560,3 @@\n         trainer: Lazy[Trainer],\n-        vocabulary: Lazy[Vocabulary] = None,\n+        vocabulary: Lazy[Vocabulary] = Lazy(Vocabulary),\n         datasets_for_vocab_creation: List[str] = None,\n",
        "source_code_with_indent": "        trainer: Lazy[Trainer],\n        vocabulary: Lazy[Vocabulary] = None,\n        datasets_for_vocab_creation: List[str] = None,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        trainer: Lazy[Trainer],\n        vocabulary: Lazy[Vocabulary] = Lazy(Vocabulary),\n        datasets_for_vocab_creation: List[str] = None,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/commands/train.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/commands/train.py",
    "file_hunks_size": 7,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/commands/train.py:723:12 Incompatible parameter type [6]: Expected `Model` for 2nd parameter `model` to call `TrainModel.__init__` but got `Optional[Model]`.",
    "message": " Expected `Model` for 2nd parameter `model` to call `TrainModel.__init__` but got `Optional[Model]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 723,
    "warning_line": "            model=model_,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/commands/train.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/commands/train.py",
    "file_hunks_size": 7,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/commands/train.py:724:12 Incompatible parameter type [6]: Expected `Trainer` for 3rd parameter `trainer` to call `TrainModel.__init__` but got `Optional[Trainer]`.",
    "message": " Expected `Trainer` for 3rd parameter `trainer` to call `TrainModel.__init__` but got `Optional[Trainer]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 724,
    "warning_line": "            trainer=trainer_,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        )\n\n",
        "source_code_len": 11,
        "target_code": "        )\n        assert trainer_ is not None\n\n",
        "target_code_len": 47,
        "diff_format": "@@ -719,2 +716,3 @@\n         )\n+        assert trainer_ is not None\n \n",
        "source_code_with_indent": "        )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        )\n        assert trainer_ is not None\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/common/file_utils.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/common/file_utils.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/common/file_utils.py:448:4 Incompatible attribute type [8]: Attribute `size` declared in class `_Meta` has type `int` but is used as type `None`.",
    "message": " Attribute `size` declared in class `_Meta` has type `int` but is used as type `None`.",
    "rule_id": "Incompatible attribute type [8]",
    "warning_line_no": 448,
    "warning_line": "    size: int = None",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    size: int = None\n    \"\"\"\n",
        "source_code_len": 30,
        "target_code": "\n    size: int = 0\n    \"\"\"\n",
        "target_code_len": 27,
        "diff_format": "@@ -447,3 +447,3 @@\n \n-    size: int = None\n+    size: int = 0\n     \"\"\"\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n    size: int = None\n",
        "target_code_with_indent": "\n    size: int = 0\n"
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/common/from_params.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/common/from_params.py",
    "file_hunks_size": 6,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/common/from_params.py:115:31 Incompatible variable type [9]: constructor is declared to have type `typing.Callable[..., Variable[T (bound to FromParams)]]` but is used as type `None`.",
    "message": " constructor is declared to have type `typing.Callable[..., Variable[T (bound to FromParams)]]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 115,
    "warning_line": "def infer_params(cls: Type[T], constructor: Callable[..., T] = None) -> Dict[str, Any]:"
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/common/from_params.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/common/from_params.py",
    "file_hunks_size": 6,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/common/from_params.py:117:8 Incompatible variable type [9]: constructor is declared to have type `typing.Callable[..., Variable[T (bound to FromParams)]]` but is used as type `typing.Callable(object.__init__)[[Named(self, object)], None]`.",
    "message": " constructor is declared to have type `typing.Callable[..., Variable[T (bound to FromParams)]]` but is used as type `typing.Callable(object.__init__)[[Named(self, object)], None]`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 117,
    "warning_line": "        constructor = cls.__init__"
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/common/from_params.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/common/from_params.py",
    "file_hunks_size": 6,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/common/from_params.py:512:8 Incompatible variable type [9]: constructor_to_inspect is declared to have type `typing.Callable[..., Variable[T (bound to FromParams)]]` but is used as type `None`.",
    "message": " constructor_to_inspect is declared to have type `typing.Callable[..., Variable[T (bound to FromParams)]]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 512,
    "warning_line": "        constructor_to_inspect: Callable[..., T] = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/common/from_params.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/common/from_params.py",
    "file_hunks_size": 6,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/common/from_params.py:584:16 Incompatible variable type [9]: constructor_to_inspect is declared to have type `typing.Callable[..., Variable[T (bound to FromParams)]]` but is used as type `typing.Callable(object.__init__)[[Named(self, object)], None]`.",
    "message": " constructor_to_inspect is declared to have type `typing.Callable[..., Variable[T (bound to FromParams)]]` but is used as type `typing.Callable(object.__init__)[[Named(self, object)], None]`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 584,
    "warning_line": "                constructor_to_inspect = subclass.__init__"
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/common/from_params.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/common/from_params.py",
    "file_hunks_size": 6,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/common/from_params.py:614:16 Incompatible variable type [9]: constructor_to_inspect is declared to have type `typing.Callable[..., Variable[T (bound to FromParams)]]` but is used as type `typing.Callable(object.__init__)[[Named(self, object)], None]`.",
    "message": " constructor_to_inspect is declared to have type `typing.Callable[..., Variable[T (bound to FromParams)]]` but is used as type `typing.Callable(object.__init__)[[Named(self, object)], None]`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 614,
    "warning_line": "                constructor_to_inspect = cls.__init__"
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/common/logging.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/common/logging.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/common/logging.py:103:41 Incompatible parameter type [6]: Expected `str` for 1st positional only parameter to call `typing.Mapping.get` but got `typing.Optional[str]`.",
    "message": " Expected `str` for 1st positional only parameter to call `typing.Mapping.get` but got `typing.Optional[str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 103,
    "warning_line": "        LEVEL = logging._nameToLevel.get(level_name, logging.INFO)",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    else:\n        level_name = os.environ.get(\"ALLENNLP_LOG_LEVEL\")\n        LEVEL = logging._nameToLevel.get(level_name, logging.INFO)\n",
        "source_code_len": 135,
        "target_code": "    else:\n        level_name = os.environ.get(\"ALLENNLP_LOG_LEVEL\", \"INFO\")\n        LEVEL = logging._nameToLevel.get(level_name, logging.INFO)\n",
        "target_code_len": 143,
        "diff_format": "@@ -101,3 +101,3 @@\n     else:\n-        level_name = os.environ.get(\"ALLENNLP_LOG_LEVEL\")\n+        level_name = os.environ.get(\"ALLENNLP_LOG_LEVEL\", \"INFO\")\n         LEVEL = logging._nameToLevel.get(level_name, logging.INFO)\n",
        "source_code_with_indent": "    <DED>else:\n        <IND>level_name = os.environ.get(\"ALLENNLP_LOG_LEVEL\")\n        LEVEL = logging._nameToLevel.get(level_name, logging.INFO)\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    <DED>else:\n        <IND>level_name = os.environ.get(\"ALLENNLP_LOG_LEVEL\", \"INFO\")\n        LEVEL = logging._nameToLevel.get(level_name, logging.INFO)\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/common/params.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/common/params.py",
    "file_hunks_size": 4,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/common/params.py:259:12 Incompatible return type [7]: Expected `int` but got `None`.",
    "message": " Expected `int` but got `None`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 259,
    "warning_line": "            return None",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "from os import PathLike\nfrom typing import Any, Dict, List, Union\n\n",
        "source_code_len": 67,
        "target_code": "from os import PathLike\nfrom typing import Any, Dict, List, Union, Optional\n\n",
        "target_code_len": 77,
        "diff_format": "@@ -8,3 +8,3 @@\n from os import PathLike\n-from typing import Any, Dict, List, Union\n+from typing import Any, Dict, List, Union, Optional\n \n",
        "source_code_with_indent": "from os import PathLike\nfrom typing import Any, Dict, List, Union\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "from os import PathLike\nfrom typing import Any, Dict, List, Union, Optional\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    def pop_int(self, key: str, default: Any = DEFAULT) -> int:\n        \"\"\"\n",
        "source_code_len": 77,
        "target_code": "\n    def pop_int(self, key: str, default: Any = DEFAULT) -> Optional[int]:\n        \"\"\"\n",
        "target_code_len": 87,
        "diff_format": "@@ -252,3 +252,3 @@\n \n-    def pop_int(self, key: str, default: Any = DEFAULT) -> int:\n+    def pop_int(self, key: str, default: Any = DEFAULT) -> Optional[int]:\n         \"\"\"\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n    <DED><DED>def pop_int(self, key: str, default: Any = DEFAULT) -> int:\n        <IND>",
        "target_code_with_indent": "\n    <DED><DED>def pop_int(self, key: str, default: Any = DEFAULT) -> Optional[int]:\n        <IND>"
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/common/params.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/common/params.py",
    "file_hunks_size": 4,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/common/params.py:269:12 Incompatible return type [7]: Expected `float` but got `None`.",
    "message": " Expected `float` but got `None`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 269,
    "warning_line": "            return None",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "from os import PathLike\nfrom typing import Any, Dict, List, Union\n\n",
        "source_code_len": 67,
        "target_code": "from os import PathLike\nfrom typing import Any, Dict, List, Union, Optional\n\n",
        "target_code_len": 77,
        "diff_format": "@@ -8,3 +8,3 @@\n from os import PathLike\n-from typing import Any, Dict, List, Union\n+from typing import Any, Dict, List, Union, Optional\n \n",
        "source_code_with_indent": "from os import PathLike\nfrom typing import Any, Dict, List, Union\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "from os import PathLike\nfrom typing import Any, Dict, List, Union, Optional\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    def pop_float(self, key: str, default: Any = DEFAULT) -> float:\n        \"\"\"\n",
        "source_code_len": 81,
        "target_code": "\n    def pop_float(self, key: str, default: Any = DEFAULT) -> Optional[float]:\n        \"\"\"\n",
        "target_code_len": 91,
        "diff_format": "@@ -262,3 +262,3 @@\n \n-    def pop_float(self, key: str, default: Any = DEFAULT) -> float:\n+    def pop_float(self, key: str, default: Any = DEFAULT) -> Optional[float]:\n         \"\"\"\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n    <DED><DED>def pop_float(self, key: str, default: Any = DEFAULT) -> float:\n        <IND>",
        "target_code_with_indent": "\n    <DED><DED>def pop_float(self, key: str, default: Any = DEFAULT) -> Optional[float]:\n        <IND>"
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/common/params.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/common/params.py",
    "file_hunks_size": 4,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/common/params.py:279:12 Incompatible return type [7]: Expected `bool` but got `None`.",
    "message": " Expected `bool` but got `None`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 279,
    "warning_line": "            return None",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "from os import PathLike\nfrom typing import Any, Dict, List, Union\n\n",
        "source_code_len": 67,
        "target_code": "from os import PathLike\nfrom typing import Any, Dict, List, Union, Optional\n\n",
        "target_code_len": 77,
        "diff_format": "@@ -8,3 +8,3 @@\n from os import PathLike\n-from typing import Any, Dict, List, Union\n+from typing import Any, Dict, List, Union, Optional\n \n",
        "source_code_with_indent": "from os import PathLike\nfrom typing import Any, Dict, List, Union\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "from os import PathLike\nfrom typing import Any, Dict, List, Union, Optional\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    def pop_bool(self, key: str, default: Any = DEFAULT) -> bool:\n        \"\"\"\n",
        "source_code_len": 79,
        "target_code": "\n    def pop_bool(self, key: str, default: Any = DEFAULT) -> Optional[bool]:\n        \"\"\"\n",
        "target_code_len": 89,
        "diff_format": "@@ -272,3 +272,3 @@\n \n-    def pop_bool(self, key: str, default: Any = DEFAULT) -> bool:\n+    def pop_bool(self, key: str, default: Any = DEFAULT) -> Optional[bool]:\n         \"\"\"\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n    <DED><DED>def pop_bool(self, key: str, default: Any = DEFAULT) -> bool:\n        <IND>",
        "target_code_with_indent": "\n    <DED><DED>def pop_bool(self, key: str, default: Any = DEFAULT) -> Optional[bool]:\n        <IND>"
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/common/registrable.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/common/registrable.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/common/registrable.py:42:4 Incompatible attribute type [8]: Attribute `default_implementation` declared in class `Registrable` has type `str` but is used as type `None`.",
    "message": " Attribute `default_implementation` declared in class `Registrable` has type `str` but is used as type `None`.",
    "rule_id": "Incompatible attribute type [8]",
    "warning_line_no": 42,
    "warning_line": "    default_implementation: str = None",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    _registry: Dict[Type, Dict[str, Tuple[Type, str]]] = defaultdict(dict)\n    default_implementation: str = None\n\n",
        "source_code_len": 116,
        "target_code": "\n    _registry: Dict[Type, Dict[str, Tuple[Type, Optional[str]]]] = defaultdict(dict)\n    default_implementation: Optional[str] = None\n\n",
        "target_code_len": 136,
        "diff_format": "@@ -40,4 +40,4 @@\n \n-    _registry: Dict[Type, Dict[str, Tuple[Type, str]]] = defaultdict(dict)\n-    default_implementation: str = None\n+    _registry: Dict[Type, Dict[str, Tuple[Type, Optional[str]]]] = defaultdict(dict)\n+    default_implementation: Optional[str] = None\n \n",
        "source_code_with_indent": "\n    _registry: Dict[Type, Dict[str, Tuple[Type, str]]] = defaultdict(dict)\n    default_implementation: str = None\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    _registry: Dict[Type, Dict[str, Tuple[Type, Optional[str]]]] = defaultdict(dict)\n    default_implementation: Optional[str] = None\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        if name in Registrable._registry[cls]:\n            subclass, constructor = Registrable._registry[cls].get(name)\n            return subclass, constructor\n",
        "source_code_len": 161,
        "target_code": "        if name in Registrable._registry[cls]:\n            subclass, constructor = Registrable._registry[cls][name]\n            return subclass, constructor\n",
        "target_code_len": 157,
        "diff_format": "@@ -154,3 +154,3 @@\n         if name in Registrable._registry[cls]:\n-            subclass, constructor = Registrable._registry[cls].get(name)\n+            subclass, constructor = Registrable._registry[cls][name]\n             return subclass, constructor\n",
        "source_code_with_indent": "        if name in Registrable._registry[cls]:\n            <IND>subclass, constructor = Registrable._registry[cls].get(name)\n            return subclass, constructor\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        if name in Registrable._registry[cls]:\n            <IND>subclass, constructor = Registrable._registry[cls][name]\n            return subclass, constructor\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/common/testing/distributed_test.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/common/testing/distributed_test.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/common/testing/distributed_test.py:12:4 Incompatible variable type [9]: distributed_device_ids is declared to have type `List[int]` but is used as type `None`.",
    "message": " distributed_device_ids is declared to have type `List[int]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 12,
    "warning_line": "    distributed_device_ids: List[int] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    process_rank: int,\n    distributed_device_ids: List[int] = None,\n    world_size: int = 1,\n    func: Callable = None,\n    func_args: Tuple = None,\n",
        "source_code_len": 150,
        "target_code": "    process_rank: int,\n    world_size: int,\n    distributed_device_ids: List[int],\n    func: Callable,\n    func_args: Tuple = None,\n",
        "target_code_len": 132,
        "diff_format": "@@ -11,5 +11,5 @@\n     process_rank: int,\n-    distributed_device_ids: List[int] = None,\n-    world_size: int = 1,\n-    func: Callable = None,\n+    world_size: int,\n+    distributed_device_ids: List[int],\n+    func: Callable,\n     func_args: Tuple = None,\n",
        "source_code_with_indent": "    process_rank: int,\n    distributed_device_ids: List[int] = None,\n    world_size: int = 1,\n    func: Callable = None,\n    func_args: Tuple = None,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    process_rank: int,\n    world_size: int,\n    distributed_device_ids: List[int],\n    func: Callable,\n    func_args: Tuple = None,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/common/testing/distributed_test.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/common/testing/distributed_test.py",
    "file_hunks_size": 5,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/common/testing/distributed_test.py:50:4 Incompatible variable type [9]: func is declared to have type `typing.Callable[..., typing.Any]` but is used as type `None`.",
    "message": " func is declared to have type `typing.Callable[..., typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 50,
    "warning_line": "    func: Callable = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/common/testing/model_test_case.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/common/testing/model_test_case.py",
    "file_hunks_size": 1,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/common/testing/model_test_case.py:152:12 Incompatible parameter type [6]: Expected `allennlp.models.model.Model` for 1st positional only parameter to call `ModelTestCase.check_model_computes_gradients_correctly` but got `typing.Optional[allennlp.models.model.Model]`.",
    "message": " Expected `allennlp.models.model.Model` for 1st positional only parameter to call `ModelTestCase.check_model_computes_gradients_correctly` but got `typing.Optional[allennlp.models.model.Model]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 152,
    "warning_line": "            model, model_batch, gradients_to_ignore, disable_dropout"
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/common/testing/model_test_case.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/common/testing/model_test_case.py",
    "file_hunks_size": 1,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/common/testing/model_test_case.py:170:28 Call error [29]: `typing.Optional[allennlp.models.model.Model]` is not a function.",
    "message": " `typing.Optional[allennlp.models.model.Model]` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 170,
    "warning_line": "        model_predictions = model(**model_batch)"
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/data/dataset_readers/dataset_reader.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/dataset_readers/dataset_reader.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/data/dataset_readers/dataset_reader.py:269:19 Unsupported operand [58]: `/` is not supported for operand types `Optional[Path]` and `str`.",
    "message": " `/` is not supported for operand types `Optional[Path]` and `str`.",
    "rule_id": "Unsupported operand [58]",
    "warning_line_no": 269,
    "warning_line": "        return str(self._cache_directory / util.flatten_filename(str(file_path)))",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    def _get_cache_location_for_file_path(self, file_path: str) -> str:\n        return str(self._cache_directory / util.flatten_filename(str(file_path)))\n",
        "source_code_len": 154,
        "target_code": "    def _get_cache_location_for_file_path(self, file_path: str) -> str:\n        assert self._cache_directory is not None\n        return str(self._cache_directory / util.flatten_filename(str(file_path)))\n",
        "target_code_len": 203,
        "diff_format": "@@ -268,2 +268,3 @@\n     def _get_cache_location_for_file_path(self, file_path: str) -> str:\n+        assert self._cache_directory is not None\n         return str(self._cache_directory / util.flatten_filename(str(file_path)))\n",
        "source_code_with_indent": "    <DED><DED>def _get_cache_location_for_file_path(self, file_path: str) -> str:\n        <IND>return str(self._cache_directory / util.flatten_filename(str(file_path)))\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    <DED><DED>def _get_cache_location_for_file_path(self, file_path: str) -> str:\n        <IND>assert self._cache_directory is not None\n        return str(self._cache_directory / util.flatten_filename(str(file_path)))\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/data/fields/adjacency_field.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/fields/adjacency_field.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/data/fields/adjacency_field.py:71:8 Incompatible attribute type [8]: Attribute `_indexed_labels` declared in class `AdjacencyField` has type `List[int]` but is used as type `None`.",
    "message": " Attribute `_indexed_labels` declared in class `AdjacencyField` has type `List[int]` but is used as type `None`.",
    "rule_id": "Incompatible attribute type [8]",
    "warning_line_no": 71,
    "warning_line": "        self._indexed_labels: List[int] = None",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "from typing import Dict, List, Set, Tuple\nimport logging\n",
        "source_code_len": 57,
        "target_code": "from typing import Dict, List, Set, Tuple, Optional\nimport logging\n",
        "target_code_len": 67,
        "diff_format": "@@ -1,2 +1,2 @@\n-from typing import Dict, List, Set, Tuple\n+from typing import Dict, List, Set, Tuple, Optional\n import logging\n",
        "source_code_with_indent": "from typing import Dict, List, Set, Tuple\nimport logging\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "from typing import Dict, List, Set, Tuple, Optional\nimport logging\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        self._padding_value = padding_value\n        self._indexed_labels: List[int] = None\n\n",
        "source_code_len": 92,
        "target_code": "        self._padding_value = padding_value\n        self._indexed_labels: Optional[List[int]] = None\n\n",
        "target_code_len": 102,
        "diff_format": "@@ -70,3 +70,3 @@\n         self._padding_value = padding_value\n-        self._indexed_labels: List[int] = None\n+        self._indexed_labels: Optional[List[int]] = None\n \n",
        "source_code_with_indent": "        self._padding_value = padding_value\n        self._indexed_labels: List[int] = None\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        self._padding_value = padding_value\n        self._indexed_labels: Optional[List[int]] = None\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/data/fields/namespace_swapping_field.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/fields/namespace_swapping_field.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/data/fields/namespace_swapping_field.py:31:8 Incompatible attribute type [8]: Attribute `_mapping_array` declared in class `NamespaceSwappingField` has type `List[int]` but is used as type `None`.",
    "message": " Attribute `_mapping_array` declared in class `NamespaceSwappingField` has type `List[int]` but is used as type `None`.",
    "rule_id": "Incompatible attribute type [8]",
    "warning_line_no": 31,
    "warning_line": "        self._mapping_array: List[int] = None",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        self._target_namespace = target_namespace\n        self._mapping_array: List[int] = None\n\n",
        "source_code_len": 97,
        "target_code": "        self._target_namespace = target_namespace\n        self._mapping_array: List[int] = []\n\n",
        "target_code_len": 95,
        "diff_format": "@@ -30,3 +30,3 @@\n         self._target_namespace = target_namespace\n-        self._mapping_array: List[int] = None\n+        self._mapping_array: List[int] = []\n \n",
        "source_code_with_indent": "        self._target_namespace = target_namespace\n        self._mapping_array: List[int] = None\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        self._target_namespace = target_namespace\n        self._mapping_array: List[int] = []\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/data/samplers/bucket_batch_sampler.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/samplers/bucket_batch_sampler.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/data/samplers/bucket_batch_sampler.py:145:8 Incompatible variable type [9]: longest_field is declared to have type `str` but is used as type `None`.",
    "message": " longest_field is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 145,
    "warning_line": "        longest_field: str = None",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "import logging\nfrom typing import List, Iterable, Tuple\nimport random\n",
        "source_code_len": 70,
        "target_code": "import logging\nfrom typing import List, Iterable, Tuple, Optional\nimport random\n",
        "target_code_len": 80,
        "diff_format": "@@ -1,3 +1,3 @@\n import logging\n-from typing import List, Iterable, Tuple\n+from typing import List, Iterable, Tuple, Optional\n import random\n",
        "source_code_with_indent": "import logging\nfrom typing import List, Iterable, Tuple\nimport random\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "import logging\nfrom typing import List, Iterable, Tuple, Optional\nimport random\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        max_length = 0.0\n        longest_field: str = None\n        for i, instance in enumerate(instances):\n",
        "source_code_len": 108,
        "target_code": "        max_length = 0.0\n        longest_field: Optional[str] = None\n        for i, instance in enumerate(instances):\n",
        "target_code_len": 118,
        "diff_format": "@@ -144,3 +144,3 @@\n         max_length = 0.0\n-        longest_field: str = None\n+        longest_field: Optional[str] = None\n         for i, instance in enumerate(instances):\n",
        "source_code_with_indent": "        max_length = 0.0\n        longest_field: str = None\n        for i, instance in enumerate(instances):\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        max_length = 0.0\n        longest_field: Optional[str] = None\n        for i, instance in enumerate(instances):\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/data/samplers/max_tokens_batch_sampler.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/samplers/max_tokens_batch_sampler.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/data/samplers/max_tokens_batch_sampler.py:90:28 Unsupported operand [58]: `>` is not supported for operand types `int` and `Optional[int]`.",
    "message": " `>` is not supported for operand types `int` and `Optional[int]`.",
    "rule_id": "Unsupported operand [58]",
    "warning_line_no": 90,
    "warning_line": "            if group_size > self.max_tokens:",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        data_source: data.Dataset,\n        max_tokens: Optional[int] = None,\n        sorting_keys: List[str] = None,\n",
        "source_code_len": 117,
        "target_code": "        data_source: data.Dataset,\n        max_tokens: int,\n        sorting_keys: List[str] = None,\n",
        "target_code_len": 100,
        "diff_format": "@@ -56,3 +56,3 @@\n         data_source: data.Dataset,\n-        max_tokens: Optional[int] = None,\n+        max_tokens: int,\n         sorting_keys: List[str] = None,\n",
        "source_code_with_indent": "        data_source: data.Dataset,\n        max_tokens: Optional[int] = None,\n        sorting_keys: List[str] = None,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        data_source: data.Dataset,\n        max_tokens: int,\n        sorting_keys: List[str] = None,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/data/token_indexers/elmo_indexer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/token_indexers/elmo_indexer.py",
    "file_hunks_size": 2,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/data/token_indexers/elmo_indexer.py:147:70 Incompatible parameter type [6]: Expected `str` for 1st positional only parameter to call `ELMoCharacterMapper.convert_word_to_char_ids` but got `typing.Optional[str]`.",
    "message": " Expected `str` for 1st positional only parameter to call `ELMoCharacterMapper.convert_word_to_char_ids` but got `typing.Optional[str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 147,
    "warning_line": "        return {\"elmo_tokens\": [self._mapper.convert_word_to_char_ids(text) for text in texts]}"
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/data/token_indexers/pretrained_transformer_indexer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/token_indexers/pretrained_transformer_indexer.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/data/token_indexers/pretrained_transformer_indexer.py:108:8 Incompatible variable type [9]: output is declared to have type `Dict[str, List[typing.Any]]` but is used as type `Dict[str, typing.Union[None, List[bool], List[int]]]`.",
    "message": " output is declared to have type `Dict[str, List[typing.Any]]` but is used as type `Dict[str, typing.Union[None, List[bool], List[int]]]`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 108,
    "warning_line": "        output: IndexedTokenList = {",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "            \"mask\": [True] * len(indices),\n            \"type_ids\": type_ids,\n        }\n",
        "source_code_len": 87,
        "target_code": "            \"mask\": [True] * len(indices),\n            \"type_ids\": type_ids or [0] * len(indices),\n        }\n",
        "target_code_len": 109,
        "diff_format": "@@ -110,3 +110,3 @@\n             \"mask\": [True] * len(indices),\n-            \"type_ids\": type_ids,\n+            \"type_ids\": type_ids or [0] * len(indices),\n         }\n",
        "source_code_with_indent": "            \"mask\": [True] * len(indices),\n            \"type_ids\": type_ids,\n        }\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "            \"mask\": [True] * len(indices),\n            \"type_ids\": type_ids or [0] * len(indices),\n        }\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/data/token_indexers/pretrained_transformer_indexer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/token_indexers/pretrained_transformer_indexer.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/data/token_indexers/pretrained_transformer_indexer.py:148:31 Incompatible parameter type [6]: Expected `int` for 1st positional only parameter to call `list.append` but got `Optional[int]`.",
    "message": " Expected `int` for 1st positional only parameter to call `list.append` but got `Optional[int]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 148,
    "warning_line": "                indices.append(token.text_id)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        for token in tokens:\n            if getattr(token, \"text_id\", None) is not None:\n                # `text_id` being set on the token means that we aren't using the vocab, we just use\n                # this id instead. Id comes from the pretrained vocab.\n                # It is computed in PretrainedTransformerTokenizer.\n                indices.append(token.text_id)\n            else:\n                raise KeyError(\n                    \"Using PretrainedTransformerIndexer but field text_id is not set\"\n                    f\" for the following token: {token.text}\"\n                )\n\n            if type_ids is not None and getattr(token, \"type_id\", None) is not None:\n                type_ids.append(token.type_id)\n            else:\n                type_ids.append(0)\n\n        return indices, type_ids\n",
        "source_code_len": 811,
        "target_code": "        for token in tokens:\n            indices.append(\n                token.text_id\n                if token.text_id is not None\n                else self._tokenizer.convert_tokens_to_ids(token.text)\n            )\n            type_ids.append(token.type_id if token.type_id is not None else 0)\n        return indices, type_ids\n",
        "target_code_len": 329,
        "diff_format": "@@ -143,18 +141,8 @@\n         for token in tokens:\n-            if getattr(token, \"text_id\", None) is not None:\n-                # `text_id` being set on the token means that we aren't using the vocab, we just use\n-                # this id instead. Id comes from the pretrained vocab.\n-                # It is computed in PretrainedTransformerTokenizer.\n-                indices.append(token.text_id)\n-            else:\n-                raise KeyError(\n-                    \"Using PretrainedTransformerIndexer but field text_id is not set\"\n-                    f\" for the following token: {token.text}\"\n-                )\n-\n-            if type_ids is not None and getattr(token, \"type_id\", None) is not None:\n-                type_ids.append(token.type_id)\n-            else:\n-                type_ids.append(0)\n-\n+            indices.append(\n+                token.text_id\n+                if token.text_id is not None\n+                else self._tokenizer.convert_tokens_to_ids(token.text)\n+            )\n+            type_ids.append(token.type_id if token.type_id is not None else 0)\n         return indices, type_ids\n",
        "source_code_with_indent": "        for token in tokens:\n            <IND>if getattr(token, \"text_id\", None) is not None:\n                # `text_id` being set on the token means that we aren't using the vocab, we just use\n                # this id instead. Id comes from the pretrained vocab.\n                # It is computed in PretrainedTransformerTokenizer.\n                <IND>indices.append(token.text_id)\n            <DED>else:\n                <IND>raise KeyError(\n                    \"Using PretrainedTransformerIndexer but field text_id is not set\"\n                    f\" for the following token: {token.text}\"\n                )\n\n            <DED>if type_ids is not None and getattr(token, \"type_id\", None) is not None:\n                <IND>type_ids.append(token.type_id)\n            <DED>else:\n                <IND>type_ids.append(0)\n\n        <DED><DED>return indices, type_ids\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        for token in tokens:\n            <IND>indices.append(\n                token.text_id\n                if token.text_id is not None\n                else self._tokenizer.convert_tokens_to_ids(token.text)\n            )\n            type_ids.append(token.type_id if token.type_id is not None else 0)\n        <DED>return indices, type_ids\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/data/token_indexers/pretrained_transformer_indexer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/token_indexers/pretrained_transformer_indexer.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/data/token_indexers/pretrained_transformer_indexer.py:156:32 Incompatible parameter type [6]: Expected `int` for 1st positional only parameter to call `list.append` but got `Optional[int]`.",
    "message": " Expected `int` for 1st positional only parameter to call `list.append` but got `Optional[int]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 156,
    "warning_line": "                type_ids.append(token.type_id)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        for token in tokens:\n            if getattr(token, \"text_id\", None) is not None:\n                # `text_id` being set on the token means that we aren't using the vocab, we just use\n                # this id instead. Id comes from the pretrained vocab.\n                # It is computed in PretrainedTransformerTokenizer.\n                indices.append(token.text_id)\n            else:\n                raise KeyError(\n                    \"Using PretrainedTransformerIndexer but field text_id is not set\"\n                    f\" for the following token: {token.text}\"\n                )\n\n            if type_ids is not None and getattr(token, \"type_id\", None) is not None:\n                type_ids.append(token.type_id)\n            else:\n                type_ids.append(0)\n\n        return indices, type_ids\n",
        "source_code_len": 811,
        "target_code": "        for token in tokens:\n            indices.append(\n                token.text_id\n                if token.text_id is not None\n                else self._tokenizer.convert_tokens_to_ids(token.text)\n            )\n            type_ids.append(token.type_id if token.type_id is not None else 0)\n        return indices, type_ids\n",
        "target_code_len": 329,
        "diff_format": "@@ -143,18 +141,8 @@\n         for token in tokens:\n-            if getattr(token, \"text_id\", None) is not None:\n-                # `text_id` being set on the token means that we aren't using the vocab, we just use\n-                # this id instead. Id comes from the pretrained vocab.\n-                # It is computed in PretrainedTransformerTokenizer.\n-                indices.append(token.text_id)\n-            else:\n-                raise KeyError(\n-                    \"Using PretrainedTransformerIndexer but field text_id is not set\"\n-                    f\" for the following token: {token.text}\"\n-                )\n-\n-            if type_ids is not None and getattr(token, \"type_id\", None) is not None:\n-                type_ids.append(token.type_id)\n-            else:\n-                type_ids.append(0)\n-\n+            indices.append(\n+                token.text_id\n+                if token.text_id is not None\n+                else self._tokenizer.convert_tokens_to_ids(token.text)\n+            )\n+            type_ids.append(token.type_id if token.type_id is not None else 0)\n         return indices, type_ids\n",
        "source_code_with_indent": "        for token in tokens:\n            <IND>if getattr(token, \"text_id\", None) is not None:\n                # `text_id` being set on the token means that we aren't using the vocab, we just use\n                # this id instead. Id comes from the pretrained vocab.\n                # It is computed in PretrainedTransformerTokenizer.\n                <IND>indices.append(token.text_id)\n            <DED>else:\n                <IND>raise KeyError(\n                    \"Using PretrainedTransformerIndexer but field text_id is not set\"\n                    f\" for the following token: {token.text}\"\n                )\n\n            <DED>if type_ids is not None and getattr(token, \"type_id\", None) is not None:\n                <IND>type_ids.append(token.type_id)\n            <DED>else:\n                <IND>type_ids.append(0)\n\n        <DED><DED>return indices, type_ids\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        for token in tokens:\n            <IND>indices.append(\n                token.text_id\n                if token.text_id is not None\n                else self._tokenizer.convert_tokens_to_ids(token.text)\n            )\n            type_ids.append(token.type_id if token.type_id is not None else 0)\n        <DED>return indices, type_ids\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/data/tokenizers/pretrained_transformer_tokenizer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/tokenizers/pretrained_transformer_tokenizer.py",
    "file_hunks_size": 4,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/data/tokenizers/pretrained_transformer_tokenizer.py:415:8 Incompatible return type [7]: Expected `Tuple[List[Token], List[Tuple[int, int]], List[Tuple[int, int]]]` but got `Tuple[List[Token], List[Optional[Tuple[int, int]]], List[Optional[Tuple[int, int]]]]`.",
    "message": " Expected `Tuple[List[Token], List[Tuple[int, int]], List[Tuple[int, int]]]` but got `Tuple[List[Token], List[Optional[Tuple[int, int]]], List[Optional[Tuple[int, int]]]]`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 415,
    "warning_line": "        return tokens_a, offsets_a, offsets_b",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        self, string_tokens_a: List[str], string_tokens_b: List[str]\n    ) -> Tuple[List[Token], List[Tuple[int, int]], List[Tuple[int, int]]]:\n        \"\"\"\n",
        "source_code_len": 156,
        "target_code": "        self, string_tokens_a: List[str], string_tokens_b: List[str]\n    ) -> Tuple[List[Token], List[Optional[Tuple[int, int]]], List[Optional[Tuple[int, int]]]]:\n        \"\"\"\n",
        "target_code_len": 176,
        "diff_format": "@@ -393,3 +393,3 @@\n         self, string_tokens_a: List[str], string_tokens_b: List[str]\n-    ) -> Tuple[List[Token], List[Tuple[int, int]], List[Tuple[int, int]]]:\n+    ) -> Tuple[List[Token], List[Optional[Tuple[int, int]]], List[Optional[Tuple[int, int]]]]:\n         \"\"\"\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "        self, string_tokens_a: List[str], string_tokens_b: List[str]\n    ) -> Tuple[List[Token], List[Tuple[int, int]], List[Tuple[int, int]]]:\n        <IND>",
        "target_code_with_indent": "        self, string_tokens_a: List[str], string_tokens_b: List[str]\n    ) -> Tuple[List[Token], List[Optional[Tuple[int, int]]], List[Optional[Tuple[int, int]]]]:\n        <IND>"
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/interpret/attackers/input_reduction.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/interpret/attackers/input_reduction.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/interpret/attackers/input_reduction.py:36:8 Incompatible variable type [9]: inputs is declared to have type `typing.Dict[str, typing.Any]` but is used as type `None`.",
    "message": " inputs is declared to have type `typing.Dict[str, typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 36,
    "warning_line": "        inputs: JsonDict = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        self,\n        inputs: JsonDict = None,\n        input_field_to_attack: str = \"tokens\",\n",
        "source_code_len": 94,
        "target_code": "        self,\n        inputs: JsonDict,\n        input_field_to_attack: str = \"tokens\",\n",
        "target_code_len": 87,
        "diff_format": "@@ -35,3 +35,3 @@\n         self,\n-        inputs: JsonDict = None,\n+        inputs: JsonDict,\n         input_field_to_attack: str = \"tokens\",\n",
        "source_code_with_indent": "        self,\n        inputs: JsonDict = None,\n        input_field_to_attack: str = \"tokens\",\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        self,\n        inputs: JsonDict,\n        input_field_to_attack: str = \"tokens\",\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/modules/seq2vec_encoders/cls_pooler.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/modules/seq2vec_encoders/cls_pooler.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/modules/seq2vec_encoders/cls_pooler.py:31:23 Incompatible variable type [9]: embedding_dim is declared to have type `int` but is used as type `None`.",
    "message": " embedding_dim is declared to have type `int` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 31,
    "warning_line": "    def __init__(self, embedding_dim: int = None, cls_is_last_token: bool = False):",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    def __init__(self, embedding_dim: int = None, cls_is_last_token: bool = False):\n        super().__init__()\n",
        "source_code_len": 112,
        "target_code": "\n    def __init__(self, embedding_dim: int, cls_is_last_token: bool = False):\n        super().__init__()\n",
        "target_code_len": 105,
        "diff_format": "@@ -30,3 +30,3 @@\n \n-    def __init__(self, embedding_dim: int = None, cls_is_last_token: bool = False):\n+    def __init__(self, embedding_dim: int, cls_is_last_token: bool = False):\n         super().__init__()\n",
        "source_code_with_indent": "\n    def __init__(self, embedding_dim: int = None, cls_is_last_token: bool = False):\n        <IND>super().__init__()\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    def __init__(self, embedding_dim: int, cls_is_last_token: bool = False):\n        <IND>super().__init__()\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/modules/seq2vec_encoders/cnn_encoder.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/modules/seq2vec_encoders/cnn_encoder.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/modules/seq2vec_encoders/cnn_encoder.py:92:8 Incompatible return type [7]: Expected `int` but got `Optional[int]`.",
    "message": " Expected `int` but got `Optional[int]`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 92,
    "warning_line": "        return self._output_dim",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        self._activation = conv_layer_activation or Activation.by_name(\"relu\")()\n        self._output_dim = output_dim\n\n",
        "source_code_len": 120,
        "target_code": "        self._activation = conv_layer_activation or Activation.by_name(\"relu\")()\n\n",
        "target_code_len": 82,
        "diff_format": "@@ -65,3 +65,2 @@\n         self._activation = conv_layer_activation or Activation.by_name(\"relu\")()\n-        self._output_dim = output_dim\n \n",
        "source_code_with_indent": "        self._activation = conv_layer_activation or Activation.by_name(\"relu\")()\n        self._output_dim = output_dim\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        self._activation = conv_layer_activation or Activation.by_name(\"relu\")()\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/modules/token_embedders/embedding.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/modules/token_embedders/embedding.py",
    "file_hunks_size": 4,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/modules/token_embedders/embedding.py:218:8 Incompatible variable type [9]: vocab_namespace is declared to have type `str` but is used as type `None`.",
    "message": " vocab_namespace is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 218,
    "warning_line": "        vocab_namespace: str = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/training/checkpointer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/checkpointer.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/checkpointer.py:51:8 Incompatible variable type [9]: serialization_dir is declared to have type `str` but is used as type `None`.",
    "message": " serialization_dir is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 51,
    "warning_line": "        serialization_dir: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        self,\n        serialization_dir: str = None,\n        keep_serialized_model_every_num_seconds: int = None,\n",
        "source_code_len": 114,
        "target_code": "        self,\n        serialization_dir: str,\n        keep_serialized_model_every_num_seconds: int = None,\n",
        "target_code_len": 107,
        "diff_format": "@@ -50,3 +50,3 @@\n         self,\n-        serialization_dir: str = None,\n+        serialization_dir: str,\n         keep_serialized_model_every_num_seconds: int = None,\n",
        "source_code_with_indent": "        self,\n        serialization_dir: str = None,\n        keep_serialized_model_every_num_seconds: int = None,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        self,\n        serialization_dir: str,\n        keep_serialized_model_every_num_seconds: int = None,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/training/learning_rate_schedulers/linear_with_warmup.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/learning_rate_schedulers/linear_with_warmup.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/learning_rate_schedulers/linear_with_warmup.py:18:8 Incompatible variable type [9]: num_steps_per_epoch is declared to have type `int` but is used as type `None`.",
    "message": " num_steps_per_epoch is declared to have type `int` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 18,
    "warning_line": "        num_steps_per_epoch: int = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        num_epochs: int,\n        num_steps_per_epoch: int = None,\n        warmup_steps: int = 100,\n",
        "source_code_len": 99,
        "target_code": "        num_epochs: int,\n        num_steps_per_epoch: int,\n        warmup_steps: int = 100,\n",
        "target_code_len": 92,
        "diff_format": "@@ -17,3 +17,3 @@\n         num_epochs: int,\n-        num_steps_per_epoch: int = None,\n+        num_steps_per_epoch: int,\n         warmup_steps: int = 100,\n",
        "source_code_with_indent": "        num_epochs: int,\n        num_steps_per_epoch: int = None,\n        warmup_steps: int = 100,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        num_epochs: int,\n        num_steps_per_epoch: int,\n        warmup_steps: int = 100,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/training/metric_tracker.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/metric_tracker.py",
    "file_hunks_size": 4,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/metric_tracker.py:35:8 Incompatible attribute type [8]: Attribute `_best_so_far` declared in class `MetricTracker` has type `float` but is used as type `None`.",
    "message": " Attribute `_best_so_far` declared in class `MetricTracker` has type `float` but is used as type `None`.",
    "rule_id": "Incompatible attribute type [8]",
    "warning_line_no": 35,
    "warning_line": "        self._best_so_far: float = None"
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/training/metric_tracker.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/metric_tracker.py",
    "file_hunks_size": 4,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/metric_tracker.py:41:8 Incompatible attribute type [8]: Attribute `best_epoch` declared in class `MetricTracker` has type `int` but is used as type `None`.",
    "message": " Attribute `best_epoch` declared in class `MetricTracker` has type `int` but is used as type `None`.",
    "rule_id": "Incompatible attribute type [8]",
    "warning_line_no": 41,
    "warning_line": "        self.best_epoch: int = None",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        self._epoch_number = 0\n        self.best_epoch: int = None\n\n",
        "source_code_len": 68,
        "target_code": "        self._epoch_number = 0\n        self.best_epoch: Optional[int] = None\n\n",
        "target_code_len": 78,
        "diff_format": "@@ -40,3 +40,3 @@\n         self._epoch_number = 0\n-        self.best_epoch: int = None\n+        self.best_epoch: Optional[int] = None\n \n",
        "source_code_with_indent": "        self._epoch_number = 0\n        self.best_epoch: int = None\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        self._epoch_number = 0\n        self.best_epoch: Optional[int] = None\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/training/metric_tracker.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/metric_tracker.py",
    "file_hunks_size": 4,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/metric_tracker.py:68:8 Incompatible attribute type [8]: Attribute `_best_so_far` declared in class `MetricTracker` has type `float` but is used as type `None`.",
    "message": " Attribute `_best_so_far` declared in class `MetricTracker` has type `float` but is used as type `None`.",
    "rule_id": "Incompatible attribute type [8]",
    "warning_line_no": 68,
    "warning_line": "        self._best_so_far = None"
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/training/metric_tracker.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/metric_tracker.py",
    "file_hunks_size": 4,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/metric_tracker.py:72:8 Incompatible attribute type [8]: Attribute `best_epoch` declared in class `MetricTracker` has type `int` but is used as type `None`.",
    "message": " Attribute `best_epoch` declared in class `MetricTracker` has type `int` but is used as type `None`.",
    "rule_id": "Incompatible attribute type [8]",
    "warning_line_no": 72,
    "warning_line": "        self.best_epoch = None",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        self._epoch_number = 0\n        self.best_epoch: int = None\n\n",
        "source_code_len": 68,
        "target_code": "        self._epoch_number = 0\n        self.best_epoch: Optional[int] = None\n\n",
        "target_code_len": 78,
        "diff_format": "@@ -40,3 +40,3 @@\n         self._epoch_number = 0\n-        self.best_epoch: int = None\n+        self.best_epoch: Optional[int] = None\n \n",
        "source_code_with_indent": "        self._epoch_number = 0\n        self.best_epoch: int = None\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        self._epoch_number = 0\n        self.best_epoch: Optional[int] = None\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 18,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:525:46 Incompatible parameter type [6]: Expected `str` for 1st positional only parameter to call `Checkpointer.__init__` but got `Optional[str]`.",
    "message": " Expected `str` for 1st positional only parameter to call `Checkpointer.__init__` but got `Optional[str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 525,
    "warning_line": "            self._checkpointer = Checkpointer(serialization_dir)"
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 18,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:990:8 Incompatible variable type [9]: this_epoch_val_metric is declared to have type `float` but is used as type `None`.",
    "message": " this_epoch_val_metric is declared to have type `float` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 990,
    "warning_line": "        this_epoch_val_metric: float = None",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        val_metrics: Dict[str, float] = {}\n        this_epoch_val_metric: float = None\n        metrics: Dict[str, Any] = {}\n",
        "source_code_len": 124,
        "target_code": "        val_metrics: Dict[str, float] = {}\n        this_epoch_val_metric: float\n        metrics: Dict[str, Any] = {}\n",
        "target_code_len": 117,
        "diff_format": "@@ -989,3 +981,3 @@\n         val_metrics: Dict[str, float] = {}\n-        this_epoch_val_metric: float = None\n+        this_epoch_val_metric: float\n         metrics: Dict[str, Any] = {}\n",
        "source_code_with_indent": "        val_metrics: Dict[str, float] = {}\n        this_epoch_val_metric: float = None\n        metrics: Dict[str, Any] = {}\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        val_metrics: Dict[str, float] = {}\n        this_epoch_val_metric: float\n        metrics: Dict[str, Any] = {}\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 18,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:1225:8 Incompatible variable type [9]: distributed is declared to have type `bool` but is used as type `None`.",
    "message": " distributed is declared to have type `bool` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 1225,
    "warning_line": "        distributed: bool = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        grad_clipping: float = None,\n        distributed: bool = None,\n        world_size: int = 1,\n",
        "source_code_len": 100,
        "target_code": "        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n",
        "target_code_len": 101,
        "diff_format": "@@ -1224,3 +1221,3 @@\n         grad_clipping: float = None,\n-        distributed: bool = None,\n+        distributed: bool = False,\n         world_size: int = 1,\n",
        "source_code_with_indent": "        grad_clipping: float = None,\n        distributed: bool = None,\n        world_size: int = 1,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 18,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:1230:8 Incompatible variable type [9]: optimizer is declared to have type `allennlp.common.lazy.Lazy[Optimizer]` but is used as type `None`.",
    "message": " optimizer is declared to have type `allennlp.common.lazy.Lazy[Optimizer]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 1230,
    "warning_line": "        optimizer: Lazy[Optimizer] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = None,\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        tensorboard_writer: Lazy[TensorboardWriter] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = None,\n        batch_callbacks: List[BatchCallback] = None,\n",
        "source_code_len": 421,
        "target_code": "        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        tensorboard_writer: Lazy[TensorboardWriter] = Lazy(TensorboardWriter),\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        batch_callbacks: List[BatchCallback] = None,\n",
        "target_code_len": 473,
        "diff_format": "@@ -1229,8 +1226,8 @@\n         no_grad: List[str] = None,\n-        optimizer: Lazy[Optimizer] = None,\n+        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n         learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n         momentum_scheduler: Lazy[MomentumScheduler] = None,\n-        tensorboard_writer: Lazy[TensorboardWriter] = None,\n+        tensorboard_writer: Lazy[TensorboardWriter] = Lazy(TensorboardWriter),\n         moving_average: Lazy[MovingAverage] = None,\n-        checkpointer: Lazy[Checkpointer] = None,\n+        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n         batch_callbacks: List[BatchCallback] = None,\n",
        "source_code_with_indent": "        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = None,\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        tensorboard_writer: Lazy[TensorboardWriter] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = None,\n        batch_callbacks: List[BatchCallback] = None,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        tensorboard_writer: Lazy[TensorboardWriter] = Lazy(TensorboardWriter),\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        batch_callbacks: List[BatchCallback] = None,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 18,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:1233:8 Incompatible variable type [9]: tensorboard_writer is declared to have type `allennlp.common.lazy.Lazy[TensorboardWriter]` but is used as type `None`.",
    "message": " tensorboard_writer is declared to have type `allennlp.common.lazy.Lazy[TensorboardWriter]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 1233,
    "warning_line": "        tensorboard_writer: Lazy[TensorboardWriter] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = None,\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        tensorboard_writer: Lazy[TensorboardWriter] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = None,\n        batch_callbacks: List[BatchCallback] = None,\n",
        "source_code_len": 421,
        "target_code": "        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        tensorboard_writer: Lazy[TensorboardWriter] = Lazy(TensorboardWriter),\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        batch_callbacks: List[BatchCallback] = None,\n",
        "target_code_len": 473,
        "diff_format": "@@ -1229,8 +1226,8 @@\n         no_grad: List[str] = None,\n-        optimizer: Lazy[Optimizer] = None,\n+        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n         learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n         momentum_scheduler: Lazy[MomentumScheduler] = None,\n-        tensorboard_writer: Lazy[TensorboardWriter] = None,\n+        tensorboard_writer: Lazy[TensorboardWriter] = Lazy(TensorboardWriter),\n         moving_average: Lazy[MovingAverage] = None,\n-        checkpointer: Lazy[Checkpointer] = None,\n+        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n         batch_callbacks: List[BatchCallback] = None,\n",
        "source_code_with_indent": "        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = None,\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        tensorboard_writer: Lazy[TensorboardWriter] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = None,\n        batch_callbacks: List[BatchCallback] = None,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        tensorboard_writer: Lazy[TensorboardWriter] = Lazy(TensorboardWriter),\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        batch_callbacks: List[BatchCallback] = None,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "71a98c2a5fd014867188b8d81a05129cbc64268d",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 18,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:1235:8 Incompatible variable type [9]: checkpointer is declared to have type `allennlp.common.lazy.Lazy[Checkpointer]` but is used as type `None`.",
    "message": " checkpointer is declared to have type `allennlp.common.lazy.Lazy[Checkpointer]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 1235,
    "warning_line": "        checkpointer: Lazy[Checkpointer] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = None,\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        tensorboard_writer: Lazy[TensorboardWriter] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = None,\n        batch_callbacks: List[BatchCallback] = None,\n",
        "source_code_len": 421,
        "target_code": "        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        tensorboard_writer: Lazy[TensorboardWriter] = Lazy(TensorboardWriter),\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        batch_callbacks: List[BatchCallback] = None,\n",
        "target_code_len": 473,
        "diff_format": "@@ -1229,8 +1226,8 @@\n         no_grad: List[str] = None,\n-        optimizer: Lazy[Optimizer] = None,\n+        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n         learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n         momentum_scheduler: Lazy[MomentumScheduler] = None,\n-        tensorboard_writer: Lazy[TensorboardWriter] = None,\n+        tensorboard_writer: Lazy[TensorboardWriter] = Lazy(TensorboardWriter),\n         moving_average: Lazy[MovingAverage] = None,\n-        checkpointer: Lazy[Checkpointer] = None,\n+        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n         batch_callbacks: List[BatchCallback] = None,\n",
        "source_code_with_indent": "        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = None,\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        tensorboard_writer: Lazy[TensorboardWriter] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = None,\n        batch_callbacks: List[BatchCallback] = None,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        tensorboard_writer: Lazy[TensorboardWriter] = Lazy(TensorboardWriter),\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        batch_callbacks: List[BatchCallback] = None,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  }
]