[
  {
    "project": "lebrice/Sequoia",
    "commit": "e98dbb8003432c13e09a2cd7212381ada8ae047e",
    "filename": "sequoia/common/metrics/rl_metrics.py",
    "min_patch_found": false,
    "full_warning_msg": "sequoia/common/metrics/rl_metrics.py:45:4 Incompatible attribute type [8]: Attribute `average_episode_length` declared in class `RLMetrics` has type `int` but is used as type `float`.",
    "exception": "'bool' object has no attribute 'items'",
    "dd_fail": true
  },
  {
    "project": "lebrice/Sequoia",
    "commit": "e98dbb8003432c13e09a2cd7212381ada8ae047e",
    "filename": "sequoia/common/transforms/resize.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/lebrice-Sequoia/sequoia/common/transforms/resize.py",
    "file_hunks_size": 3,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "sequoia/common/transforms/resize.py:47:32 Incompatible parameter type [6]: Expected `Variable[sequoia.common.transforms.transform.InputType (bound to typing.Sized)]` for 1st positional only parameter to call `Transform.__call__` but got `Image.Image`.",
    "message": " Expected `Variable[sequoia.common.transforms.transform.InputType (bound to typing.Sized)]` for 1st positional only parameter to call `Transform.__call__` but got `Image.Image`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 47,
    "warning_line": "        return super().__call__(img)"
  },
  {
    "project": "lebrice/Sequoia",
    "commit": "e98dbb8003432c13e09a2cd7212381ada8ae047e",
    "filename": "sequoia/common/transforms/to_tensor.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/lebrice-Sequoia/sequoia/common/transforms/to_tensor.py",
    "file_hunks_size": 10,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "sequoia/common/transforms/to_tensor.py:118:4 Inconsistent override [14]: `sequoia.common.transforms.to_tensor.ToTensor.shape_change` overrides method defined in `Transform` inconsistently. Could not find parameter `self` in overriding signature.",
    "message": " `sequoia.common.transforms.to_tensor.ToTensor.shape_change` overrides method defined in `Transform` inconsistently. Could not find parameter `self` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 118,
    "warning_line": "    def shape_change(cls, input_shape: Union[Tuple[int, ...], torch.Size]) -> Tuple[int, ...]:",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        is always in channels_last format (as is usually the case with PIL\n        images) and always returns images with the channels last:\n        \n            Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n            [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n            if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1)\n            or if the numpy.ndarray has dtype = np.uint8\n        \"\"\"\n        t = to_tensor(pic)\n        assert isinstance(t, Tensor), type(t)\n        return t\n\n    @classmethod\n    def shape_change(cls, input_shape: Union[Tuple[int, ...], torch.Size]) -> Tuple[int, ...]:\n        from .channels import ChannelsFirstIfNeeded\n        return ChannelsFirstIfNeeded.shape_change(input_shape)\n\n    @classmethod\n    def space_change(cls, input_space: gym.Space) -> gym.Space:\n        if not isinstance(input_space, spaces.Box):\n            logger.warning(UserWarning(f\"Transform {cls} is only meant for Box spaces, not {input_space}\"))\n            return input_space\n        return spaces.Box(\n            low=0.,\n            high=1.,\n            shape=cls.shape_change(input_space.shape),\n            dtype=np.float32,\n        )\n        ",
        "source_code_len": 1246,
        "target_code": "        is always in channels_last format (as is usually the case with PIL\n        images) and always returns images with the channels *first*!\n        \n            Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n            [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range\n            [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P,\n            I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has\n            dtype = np.uint8\n        \"\"\"\n        return image_to_tensor(image)\n\n    # @classmethod\n    # def shape_change(cls, input_shape: Union[Tuple[int, ...], torch.Size]) -> Tuple[int, ...]:\n    #     from .channels import ChannelsFirstIfNeeded\n    #     return ChannelsFirstIfNeeded.shape_change(input_shape)\n\n    # @classmethod\n    # def space_change(cls, input_space: gym.Space) -> gym.Space:\n    #     if not isinstance(input_space, spaces.Box):\n    #         logger.warning(UserWarning(f\"Transform {cls} is only meant for Box spaces, not {input_space}\"))\n    #         return input_space\n    #     return spaces.Box(\n    #         low=0.,\n    #         high=1.,\n    #         shape=cls.shape_change(input_space.shape),\n    #         dtype=np.float32,\n    #     )\n        ",
        "target_code_len": 1239,
        "diff_format": "@@ -105,29 +170,28 @@\n         is always in channels_last format (as is usually the case with PIL\n-        images) and always returns images with the channels last:\n+        images) and always returns images with the channels *first*!\n         \n             Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n-            [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n-            if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1)\n-            or if the numpy.ndarray has dtype = np.uint8\n+            [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range\n+            [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P,\n+            I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has\n+            dtype = np.uint8\n         \"\"\"\n-        t = to_tensor(pic)\n-        assert isinstance(t, Tensor), type(t)\n-        return t\n+        return image_to_tensor(image)\n \n-    @classmethod\n-    def shape_change(cls, input_shape: Union[Tuple[int, ...], torch.Size]) -> Tuple[int, ...]:\n-        from .channels import ChannelsFirstIfNeeded\n-        return ChannelsFirstIfNeeded.shape_change(input_shape)\n+    # @classmethod\n+    # def shape_change(cls, input_shape: Union[Tuple[int, ...], torch.Size]) -> Tuple[int, ...]:\n+    #     from .channels import ChannelsFirstIfNeeded\n+    #     return ChannelsFirstIfNeeded.shape_change(input_shape)\n \n-    @classmethod\n-    def space_change(cls, input_space: gym.Space) -> gym.Space:\n-        if not isinstance(input_space, spaces.Box):\n-            logger.warning(UserWarning(f\"Transform {cls} is only meant for Box spaces, not {input_space}\"))\n-            return input_space\n-        return spaces.Box(\n-            low=0.,\n-            high=1.,\n-            shape=cls.shape_change(input_space.shape),\n-            dtype=np.float32,\n-        )\n+    # @classmethod\n+    # def space_change(cls, input_space: gym.Space) -> gym.Space:\n+    #     if not isinstance(input_space, spaces.Box):\n+    #         logger.warning(UserWarning(f\"Transform {cls} is only meant for Box spaces, not {input_space}\"))\n+    #         return input_space\n+    #     return spaces.Box(\n+    #         low=0.,\n+    #         high=1.,\n+    #         shape=cls.shape_change(input_space.shape),\n+    #         dtype=np.float32,\n+    #     )\n         ",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n        t = to_tensor(pic)\n        assert isinstance(t, Tensor), type(t)\n        return t\n\n    <DED>@classmethod\n    def shape_change(cls, input_shape: Union[Tuple[int, ...], torch.Size]) -> Tuple[int, ...]:\n        <IND>from .channels import ChannelsFirstIfNeeded\n        return ChannelsFirstIfNeeded.shape_change(input_shape)\n\n    <DED>@classmethod\n    def space_change(cls, input_space: gym.Space) -> gym.Space:\n        <IND>if not isinstance(input_space, spaces.Box):\n            <IND>logger.warning(UserWarning(f\"Transform {cls} is only meant for Box spaces, not {input_space}\"))\n            return input_space\n        <DED>return spaces.Box(\n            low=0.,\n            high=1.,\n            shape=cls.shape_change(input_space.shape),\n            dtype=np.float32,\n        )\n<DED><DED>",
        "target_code_with_indent": "\n        return image_to_tensor(image)\n\n    # @classmethod\n    # def shape_change(cls, input_shape: Union[Tuple[int, ...], torch.Size]) -> Tuple[int, ...]:\n    #     from .channels import ChannelsFirstIfNeeded\n    #     return ChannelsFirstIfNeeded.shape_change(input_shape)\n\n    # @classmethod\n    # def space_change(cls, input_space: gym.Space) -> gym.Space:\n    #     if not isinstance(input_space, spaces.Box):\n    #         logger.warning(UserWarning(f\"Transform {cls} is only meant for Box spaces, not {input_space}\"))\n    #         return input_space\n    #     return spaces.Box(\n    #         low=0.,\n    #         high=1.,\n    #         shape=cls.shape_change(input_space.shape),\n    #         dtype=np.float32,\n    #     )\n<DED><DED>"
      }
    ]
  },
  {
    "project": "lebrice/Sequoia",
    "commit": "e98dbb8003432c13e09a2cd7212381ada8ae047e",
    "filename": "sequoia/common/transforms/to_tensor.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/lebrice-Sequoia/sequoia/common/transforms/to_tensor.py",
    "file_hunks_size": 10,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "sequoia/common/transforms/to_tensor.py:123:4 Inconsistent override [14]: `sequoia.common.transforms.to_tensor.ToTensor.space_change` overrides method defined in `Transform` inconsistently. Could not find parameter `self` in overriding signature.",
    "message": " `sequoia.common.transforms.to_tensor.ToTensor.space_change` overrides method defined in `Transform` inconsistently. Could not find parameter `self` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 123,
    "warning_line": "    def space_change(cls, input_space: gym.Space) -> gym.Space:",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        is always in channels_last format (as is usually the case with PIL\n        images) and always returns images with the channels last:\n        \n            Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n            [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n            if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1)\n            or if the numpy.ndarray has dtype = np.uint8\n        \"\"\"\n        t = to_tensor(pic)\n        assert isinstance(t, Tensor), type(t)\n        return t\n\n    @classmethod\n    def shape_change(cls, input_shape: Union[Tuple[int, ...], torch.Size]) -> Tuple[int, ...]:\n        from .channels import ChannelsFirstIfNeeded\n        return ChannelsFirstIfNeeded.shape_change(input_shape)\n\n    @classmethod\n    def space_change(cls, input_space: gym.Space) -> gym.Space:\n        if not isinstance(input_space, spaces.Box):\n            logger.warning(UserWarning(f\"Transform {cls} is only meant for Box spaces, not {input_space}\"))\n            return input_space\n        return spaces.Box(\n            low=0.,\n            high=1.,\n            shape=cls.shape_change(input_space.shape),\n            dtype=np.float32,\n        )\n        ",
        "source_code_len": 1246,
        "target_code": "        is always in channels_last format (as is usually the case with PIL\n        images) and always returns images with the channels *first*!\n        \n            Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n            [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range\n            [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P,\n            I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has\n            dtype = np.uint8\n        \"\"\"\n        return image_to_tensor(image)\n\n    # @classmethod\n    # def shape_change(cls, input_shape: Union[Tuple[int, ...], torch.Size]) -> Tuple[int, ...]:\n    #     from .channels import ChannelsFirstIfNeeded\n    #     return ChannelsFirstIfNeeded.shape_change(input_shape)\n\n    # @classmethod\n    # def space_change(cls, input_space: gym.Space) -> gym.Space:\n    #     if not isinstance(input_space, spaces.Box):\n    #         logger.warning(UserWarning(f\"Transform {cls} is only meant for Box spaces, not {input_space}\"))\n    #         return input_space\n    #     return spaces.Box(\n    #         low=0.,\n    #         high=1.,\n    #         shape=cls.shape_change(input_space.shape),\n    #         dtype=np.float32,\n    #     )\n        ",
        "target_code_len": 1239,
        "diff_format": "@@ -105,29 +170,28 @@\n         is always in channels_last format (as is usually the case with PIL\n-        images) and always returns images with the channels last:\n+        images) and always returns images with the channels *first*!\n         \n             Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n-            [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n-            if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1)\n-            or if the numpy.ndarray has dtype = np.uint8\n+            [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range\n+            [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P,\n+            I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has\n+            dtype = np.uint8\n         \"\"\"\n-        t = to_tensor(pic)\n-        assert isinstance(t, Tensor), type(t)\n-        return t\n+        return image_to_tensor(image)\n \n-    @classmethod\n-    def shape_change(cls, input_shape: Union[Tuple[int, ...], torch.Size]) -> Tuple[int, ...]:\n-        from .channels import ChannelsFirstIfNeeded\n-        return ChannelsFirstIfNeeded.shape_change(input_shape)\n+    # @classmethod\n+    # def shape_change(cls, input_shape: Union[Tuple[int, ...], torch.Size]) -> Tuple[int, ...]:\n+    #     from .channels import ChannelsFirstIfNeeded\n+    #     return ChannelsFirstIfNeeded.shape_change(input_shape)\n \n-    @classmethod\n-    def space_change(cls, input_space: gym.Space) -> gym.Space:\n-        if not isinstance(input_space, spaces.Box):\n-            logger.warning(UserWarning(f\"Transform {cls} is only meant for Box spaces, not {input_space}\"))\n-            return input_space\n-        return spaces.Box(\n-            low=0.,\n-            high=1.,\n-            shape=cls.shape_change(input_space.shape),\n-            dtype=np.float32,\n-        )\n+    # @classmethod\n+    # def space_change(cls, input_space: gym.Space) -> gym.Space:\n+    #     if not isinstance(input_space, spaces.Box):\n+    #         logger.warning(UserWarning(f\"Transform {cls} is only meant for Box spaces, not {input_space}\"))\n+    #         return input_space\n+    #     return spaces.Box(\n+    #         low=0.,\n+    #         high=1.,\n+    #         shape=cls.shape_change(input_space.shape),\n+    #         dtype=np.float32,\n+    #     )\n         ",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n        t = to_tensor(pic)\n        assert isinstance(t, Tensor), type(t)\n        return t\n\n    <DED>@classmethod\n    def shape_change(cls, input_shape: Union[Tuple[int, ...], torch.Size]) -> Tuple[int, ...]:\n        <IND>from .channels import ChannelsFirstIfNeeded\n        return ChannelsFirstIfNeeded.shape_change(input_shape)\n\n    <DED>@classmethod\n    def space_change(cls, input_space: gym.Space) -> gym.Space:\n        <IND>if not isinstance(input_space, spaces.Box):\n            <IND>logger.warning(UserWarning(f\"Transform {cls} is only meant for Box spaces, not {input_space}\"))\n            return input_space\n        <DED>return spaces.Box(\n            low=0.,\n            high=1.,\n            shape=cls.shape_change(input_space.shape),\n            dtype=np.float32,\n        )\n<DED><DED>",
        "target_code_with_indent": "\n        return image_to_tensor(image)\n\n    # @classmethod\n    # def shape_change(cls, input_shape: Union[Tuple[int, ...], torch.Size]) -> Tuple[int, ...]:\n    #     from .channels import ChannelsFirstIfNeeded\n    #     return ChannelsFirstIfNeeded.shape_change(input_shape)\n\n    # @classmethod\n    # def space_change(cls, input_space: gym.Space) -> gym.Space:\n    #     if not isinstance(input_space, spaces.Box):\n    #         logger.warning(UserWarning(f\"Transform {cls} is only meant for Box spaces, not {input_space}\"))\n    #         return input_space\n    #     return spaces.Box(\n    #         low=0.,\n    #         high=1.,\n    #         shape=cls.shape_change(input_space.shape),\n    #         dtype=np.float32,\n    #     )\n<DED><DED>"
      }
    ]
  },
  {
    "project": "lebrice/Sequoia",
    "commit": "e98dbb8003432c13e09a2cd7212381ada8ae047e",
    "filename": "sequoia/methods/models/baseline_model/class_incremental_model.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/lebrice-Sequoia/sequoia/methods/models/baseline_model/class_incremental_model.py",
    "file_hunks_size": 10,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "sequoia/methods/models/baseline_model/class_incremental_model.py:46:8 Incompatible attribute type [8]: Attribute `_output_head` declared in class `ClassIncrementalModel` has type `sequoia.methods.models.output_heads.output_head.OutputHead` but is used as type `None`.",
    "message": " Attribute `_output_head` declared in class `ClassIncrementalModel` has type `sequoia.methods.models.output_heads.output_head.OutputHead` but is used as type `None`.",
    "rule_id": "Incompatible attribute type [8]",
    "warning_line_no": 46,
    "warning_line": "        self._output_head: OutputHead = None",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        # have access to task labels. Need to figure out how to manage this between TaskIncremental and Classifier.\n        multihead: bool = False\n\n    def __init__(self, setting: IncrementalSetting, hparams: HParams, config: Config):\n        self._output_head: OutputHead = None\n        super().__init__(setting=setting, hparams=hparams, config=config)\n        \n\n        self.hp: ClassIncrementalModel.HParams\n",
        "source_code_len": 412,
        "target_code": "        # have access to task labels. Need to figure out how to manage this between TaskIncremental and Classifier.\n        multihead: Optional[bool] = None\n\n    def __init__(self, setting: IncrementalSetting, hparams: HParams, config: Config):\n        super().__init__(setting=setting, hparams=hparams, config=config)\n        self.output_heads: Dict[str, OutputHead] = nn.ModuleDict()\n        self.hp: ClassIncrementalModel.HParams\n",
        "target_code_len": 433,
        "diff_format": "@@ -42,9 +41,7 @@\n         # have access to task labels. Need to figure out how to manage this between TaskIncremental and Classifier.\n-        multihead: bool = False\n+        multihead: Optional[bool] = None\n \n     def __init__(self, setting: IncrementalSetting, hparams: HParams, config: Config):\n-        self._output_head: OutputHead = None\n         super().__init__(setting=setting, hparams=hparams, config=config)\n-        \n-\n+        self.output_heads: Dict[str, OutputHead] = nn.ModuleDict()\n         self.hp: ClassIncrementalModel.HParams\n",
        "source_code_with_indent": "        # have access to task labels. Need to figure out how to manage this between TaskIncremental and Classifier.\n        multihead: bool = False\n\n    <DED>def __init__(self, setting: IncrementalSetting, hparams: HParams, config: Config):\n        <IND>self._output_head: OutputHead = None\n        super().__init__(setting=setting, hparams=hparams, config=config)\n        \n\n        self.hp: ClassIncrementalModel.HParams\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        # have access to task labels. Need to figure out how to manage this between TaskIncremental and Classifier.\n        multihead: Optional[bool] = None\n\n    <DED>def __init__(self, setting: IncrementalSetting, hparams: HParams, config: Config):\n        <IND>super().__init__(setting=setting, hparams=hparams, config=config)\n        self.output_heads: Dict[str, OutputHead] = nn.ModuleDict()\n        self.hp: ClassIncrementalModel.HParams\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        self.output_heads: Dict[str, OutputHead] = nn.ModuleDict()\n        if self.hp.multihead:\n            output_head = self.create_output_head(self.setting)\n            self.output_head = output_head\n            self.output_heads[str(self.setting.current_task_id)] = output_head\n\n    @property\n    def output_head(self) -> OutputHead:\n        \"\"\" Get the output head for the current task.\n\n        FIXME: It's generally bad practice to do heavy computation on a property\n        so we should probably add something like a get_output_head(task) method.\n        \"\"\"\n        if self.hp.multihead:\n            if ((self.training and self.setting.task_labels_at_train_time) or\n                (not self.training and self.setting.task_labels_at_test_time)):\n                current_task_id = self.current_task\n                # current_task_id = self.setting.current_task_id\n\n            elif self.task_inference_module is not None:\n                # current_task_id = self.task_inference_module(...)\n                raise NotImplementedError(\"TODO\")\n            \n            # TODO: Look into this, seems a bit weird.\n            elif self._output_head is not None:\n                # Just return the current output head.\n                return self._output_head\n            else:\n                raise RuntimeError(\"No way of determining the task id and output head is None!\")\n\n            key = str(current_task_id)\n            if key not in self.output_heads:\n                # Create the output head, since it's not already in there.\n                output_head = self.create_output_head(self.setting)\n                self.output_heads[key] = output_head\n            else:\n                output_head = self.output_heads[key]\n            self._output_head = output_head\n            # Return the output head for the current task.\n            return output_head\n\n        if self._output_head is None:\n            self._output_head = self.create_output_head(self.setting)\n        return self._output_head\n\n    @output_head.setter\n    def output_head(self, value: OutputHead) -> None:\n        # logger.debug(f\"Setting output head to {value}\")\n        self._output_head = value\n\n",
        "source_code_len": 2177,
        "target_code": "\n    @property\n    def default_output_head(self) -> OutputHead:\n        return self.output_heads[str(None)]\n\n    # @property\n    # def output_head(self) -> OutputHead:\n    #     \"\"\" Get the output head for the current task.\n\n    #     FIXME: It's generally bad practice to do heavy computation on a property\n    #     so we should probably add something like a get_output_head(task) method.\n    #     \"\"\"\n    #     if self.setting.nb_tasks == 1 or not self.hp.multihead:\n    #         return self.output_heads[str(None)]\n        \n    #     # We have a multi-headed model (often means we have task labels, but not\n    #     # necessarily).\n    #     key = str(self.current_task)\n    #     if key not in self.output_heads:\n    #         self.output_heads[key] = self.create_output_head(self.setting)\n    #     return self.output_heads[key]\n\n    # @output_head.setter\n    # def output_head(self, value: OutputHead) -> None:\n    #     # logger.debug(f\"Setting output head to {value}\")\n    #     # TODO: There's a problem here with multiheaded models. This setter gets\n    #     # 'bypassed' somehow.\n    #     assert False, value\n    #     self._output_head = value\n\n",
        "target_code_len": 1163,
        "diff_format": "@@ -59,51 +56,30 @@\n \n-        self.output_heads: Dict[str, OutputHead] = nn.ModuleDict()\n-        if self.hp.multihead:\n-            output_head = self.create_output_head(self.setting)\n-            self.output_head = output_head\n-            self.output_heads[str(self.setting.current_task_id)] = output_head\n-\n     @property\n-    def output_head(self) -> OutputHead:\n-        \"\"\" Get the output head for the current task.\n-\n-        FIXME: It's generally bad practice to do heavy computation on a property\n-        so we should probably add something like a get_output_head(task) method.\n-        \"\"\"\n-        if self.hp.multihead:\n-            if ((self.training and self.setting.task_labels_at_train_time) or\n-                (not self.training and self.setting.task_labels_at_test_time)):\n-                current_task_id = self.current_task\n-                # current_task_id = self.setting.current_task_id\n-\n-            elif self.task_inference_module is not None:\n-                # current_task_id = self.task_inference_module(...)\n-                raise NotImplementedError(\"TODO\")\n-            \n-            # TODO: Look into this, seems a bit weird.\n-            elif self._output_head is not None:\n-                # Just return the current output head.\n-                return self._output_head\n-            else:\n-                raise RuntimeError(\"No way of determining the task id and output head is None!\")\n-\n-            key = str(current_task_id)\n-            if key not in self.output_heads:\n-                # Create the output head, since it's not already in there.\n-                output_head = self.create_output_head(self.setting)\n-                self.output_heads[key] = output_head\n-            else:\n-                output_head = self.output_heads[key]\n-            self._output_head = output_head\n-            # Return the output head for the current task.\n-            return output_head\n-\n-        if self._output_head is None:\n-            self._output_head = self.create_output_head(self.setting)\n-        return self._output_head\n-\n-    @output_head.setter\n-    def output_head(self, value: OutputHead) -> None:\n-        # logger.debug(f\"Setting output head to {value}\")\n-        self._output_head = value\n+    def default_output_head(self) -> OutputHead:\n+        return self.output_heads[str(None)]\n+\n+    # @property\n+    # def output_head(self) -> OutputHead:\n+    #     \"\"\" Get the output head for the current task.\n+\n+    #     FIXME: It's generally bad practice to do heavy computation on a property\n+    #     so we should probably add something like a get_output_head(task) method.\n+    #     \"\"\"\n+    #     if self.setting.nb_tasks == 1 or not self.hp.multihead:\n+    #         return self.output_heads[str(None)]\n+        \n+    #     # We have a multi-headed model (often means we have task labels, but not\n+    #     # necessarily).\n+    #     key = str(self.current_task)\n+    #     if key not in self.output_heads:\n+    #         self.output_heads[key] = self.create_output_head(self.setting)\n+    #     return self.output_heads[key]\n+\n+    # @output_head.setter\n+    # def output_head(self, value: OutputHead) -> None:\n+    #     # logger.debug(f\"Setting output head to {value}\")\n+    #     # TODO: There's a problem here with multiheaded models. This setter gets\n+    #     # 'bypassed' somehow.\n+    #     assert False, value\n+    #     self._output_head = value\n \n",
        "source_code_with_indent": "\n        self.output_heads: Dict[str, OutputHead] = nn.ModuleDict()\n        if self.hp.multihead:\n            <IND>output_head = self.create_output_head(self.setting)\n            self.output_head = output_head\n            self.output_heads[str(self.setting.current_task_id)] = output_head\n\n    <DED><DED>@property\n    def output_head(self) -> OutputHead:\n        <IND>\"\"\" Get the output head for the current task.\n\n        FIXME: It's generally bad practice to do heavy computation on a property\n        so we should probably add something like a get_output_head(task) method.\n        \"\"\"\n        if self.hp.multihead:\n            <IND>if ((self.training and self.setting.task_labels_at_train_time) or\n                (not self.training and self.setting.task_labels_at_test_time)):\n                <IND>current_task_id = self.current_task\n                # current_task_id = self.setting.current_task_id\n\n            <DED>elif self.task_inference_module is not None:\n                # current_task_id = self.task_inference_module(...)\n                <IND>raise NotImplementedError(\"TODO\")\n            \n            # TODO: Look into this, seems a bit weird.\n            <DED>elif self._output_head is not None:\n                # Just return the current output head.\n                <IND>return self._output_head\n            <DED>else:\n                <IND>raise RuntimeError(\"No way of determining the task id and output head is None!\")\n\n            <DED>key = str(current_task_id)\n            if key not in self.output_heads:\n                # Create the output head, since it's not already in there.\n                <IND>output_head = self.create_output_head(self.setting)\n                self.output_heads[key] = output_head\n            <DED>else:\n                <IND>output_head = self.output_heads[key]\n            <DED>self._output_head = output_head\n            # Return the output head for the current task.\n            return output_head\n\n        <DED>if self._output_head is None:\n            <IND>self._output_head = self.create_output_head(self.setting)\n        <DED>return self._output_head\n\n    <DED>@output_head.setter\n    def output_head(self, value: OutputHead) -> None:\n        # logger.debug(f\"Setting output head to {value}\")\n        <IND>self._output_head = value\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>@property\n    def default_output_head(self) -> OutputHead:\n        <IND>return self.output_heads[str(None)]\n\n    # @property\n    # def output_head(self) -> OutputHead:\n    #     \"\"\" Get the output head for the current task.\n\n    #     FIXME: It's generally bad practice to do heavy computation on a property\n    #     so we should probably add something like a get_output_head(task) method.\n    #     \"\"\"\n    #     if self.setting.nb_tasks == 1 or not self.hp.multihead:\n    #         return self.output_heads[str(None)]\n        \n    #     # We have a multi-headed model (often means we have task labels, but not\n    #     # necessarily).\n    #     key = str(self.current_task)\n    #     if key not in self.output_heads:\n    #         self.output_heads[key] = self.create_output_head(self.setting)\n    #     return self.output_heads[key]\n\n    # @output_head.setter\n    # def output_head(self, value: OutputHead) -> None:\n    #     # logger.debug(f\"Setting output head to {value}\")\n    #     # TODO: There's a problem here with multiheaded models. This setter gets\n    #     # 'bypassed' somehow.\n    #     assert False, value\n    #     self._output_head = value\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "lebrice/Sequoia",
    "commit": "e98dbb8003432c13e09a2cd7212381ada8ae047e",
    "filename": "sequoia/methods/models/baseline_model/class_incremental_model.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/lebrice-Sequoia/sequoia/methods/models/baseline_model/class_incremental_model.py",
    "file_hunks_size": 10,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "sequoia/methods/models/baseline_model/class_incremental_model.py:247:4 Inconsistent override [14]: `sequoia.methods.models.baseline_model.class_incremental_model.ClassIncrementalModel.on_task_switch` overrides method defined in `BaseModel` inconsistently. Could not find parameter `training` in overriding signature.",
    "message": " `sequoia.methods.models.baseline_model.class_incremental_model.ClassIncrementalModel.on_task_switch` overrides method defined in `BaseModel` inconsistently. Could not find parameter `training` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 247,
    "warning_line": "    def on_task_switch(self, task_id: Optional[int]) -> None:"
  },
  {
    "project": "lebrice/Sequoia",
    "commit": "e98dbb8003432c13e09a2cd7212381ada8ae047e",
    "filename": "sequoia/methods/models/baseline_model/class_incremental_model.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/lebrice-Sequoia/sequoia/methods/models/baseline_model/class_incremental_model.py",
    "file_hunks_size": 10,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "sequoia/methods/models/baseline_model/class_incremental_model.py:255:31 Incompatible parameter type [6]: Expected `int` for 1st parameter `task_id` to call `BaseModel.on_task_switch` but got `Optional[int]`.",
    "message": " Expected `int` for 1st parameter `task_id` to call `BaseModel.on_task_switch` but got `Optional[int]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 255,
    "warning_line": "        super().on_task_switch(task_id=task_id)"
  },
  {
    "project": "lebrice/Sequoia",
    "commit": "e98dbb8003432c13e09a2cd7212381ada8ae047e",
    "filename": "sequoia/methods/models/baseline_model/self_supervised_model.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/lebrice-Sequoia/sequoia/methods/models/baseline_model/self_supervised_model.py",
    "file_hunks_size": 4,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "sequoia/methods/models/baseline_model/self_supervised_model.py:71:24 Incompatible parameter type [6]: Expected `Loss` for 1st positional only parameter to call `Loss.__iadd__` but got `float`.",
    "message": " Expected `Loss` for 1st positional only parameter to call `Loss.__iadd__` but got `float`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 71,
    "warning_line": "                loss += aux_task.coefficient * aux_loss.to(self.device)"
  },
  {
    "project": "lebrice/Sequoia",
    "commit": "e98dbb8003432c13e09a2cd7212381ada8ae047e",
    "filename": "sequoia/methods/models/baseline_model/self_supervised_model.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/lebrice-Sequoia/sequoia/methods/models/baseline_model/self_supervised_model.py",
    "file_hunks_size": 4,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "sequoia/methods/models/baseline_model/self_supervised_model.py:105:48 Incompatible parameter type [6]: Expected `sequoia.methods.aux_tasks.simclr.simclr_task.SimCLRTask.Options` for 1st parameter `options` to call `sequoia.methods.aux_tasks.simclr.simclr_task.SimCLRTask.__init__` but got `Optional[sequoia.methods.aux_tasks.simclr.simclr_task.SimCLRTask.Options]`.",
    "message": " Expected `sequoia.methods.aux_tasks.simclr.simclr_task.SimCLRTask.Options` for 1st parameter `options` to call `sequoia.methods.aux_tasks.simclr.simclr_task.SimCLRTask.__init__` but got `Optional[sequoia.methods.aux_tasks.simclr.simclr_task.SimCLRTask.Options]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 105,
    "warning_line": "            tasks[SimCLRTask.name] = SimCLRTask(options=self.hp.simclr)"
  },
  {
    "project": "lebrice/Sequoia",
    "commit": "e98dbb8003432c13e09a2cd7212381ada8ae047e",
    "filename": "sequoia/methods/models/baseline_model/self_supervised_model.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/lebrice-Sequoia/sequoia/methods/models/baseline_model/self_supervised_model.py",
    "file_hunks_size": 4,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "sequoia/methods/models/baseline_model/self_supervised_model.py:107:70 Incompatible parameter type [6]: Expected `sequoia.methods.aux_tasks.reconstruction.vae.VAEReconstructionTask.Options` for 1st parameter `options` to call `sequoia.methods.aux_tasks.reconstruction.vae.VAEReconstructionTask.__init__` but got `Optional[sequoia.methods.aux_tasks.reconstruction.vae.VAEReconstructionTask.Options]`.",
    "message": " Expected `sequoia.methods.aux_tasks.reconstruction.vae.VAEReconstructionTask.Options` for 1st parameter `options` to call `sequoia.methods.aux_tasks.reconstruction.vae.VAEReconstructionTask.__init__` but got `Optional[sequoia.methods.aux_tasks.reconstruction.vae.VAEReconstructionTask.Options]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 107,
    "warning_line": "            tasks[VAEReconstructionTask.name] = VAEReconstructionTask(options=self.hp.vae)"
  },
  {
    "project": "lebrice/Sequoia",
    "commit": "e98dbb8003432c13e09a2cd7212381ada8ae047e",
    "filename": "sequoia/methods/models/baseline_model/self_supervised_model.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/lebrice-Sequoia/sequoia/methods/models/baseline_model/self_supervised_model.py",
    "file_hunks_size": 4,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "sequoia/methods/models/baseline_model/self_supervised_model.py:115:4 Inconsistent override [14]: `sequoia.methods.models.baseline_model.self_supervised_model.SelfSupervisedModel.on_task_switch` overrides method defined in `BaseModel` inconsistently. Could not find parameter `training` in overriding signature.",
    "message": " `sequoia.methods.models.baseline_model.self_supervised_model.SelfSupervisedModel.on_task_switch` overrides method defined in `BaseModel` inconsistently. Could not find parameter `training` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 115,
    "warning_line": "    def on_task_switch(self, task_id: int) -> None:"
  },
  {
    "project": "lebrice/Sequoia",
    "commit": "e98dbb8003432c13e09a2cd7212381ada8ae047e",
    "filename": "sequoia/methods/models/output_heads/rl/actor_critic_head.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/lebrice-Sequoia/sequoia/methods/models/output_heads/rl/actor_critic_head.py",
    "file_hunks_size": 6,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "sequoia/methods/models/output_heads/rl/actor_critic_head.py:52:12 Incompatible parameter type [6]: Expected `sequoia.methods.models.output_heads.output_head.OutputHead.HParams` for 4th parameter `hparams` to call `ClassificationHead.__init__` but got `ActorCriticHead.HParams`.",
    "message": " Expected `sequoia.methods.models.output_heads.output_head.OutputHead.HParams` for 4th parameter `hparams` to call `ClassificationHead.__init__` but got `ActorCriticHead.HParams`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 52,
    "warning_line": "            hparams=hparams,"
  },
  {
    "project": "lebrice/Sequoia",
    "commit": "e98dbb8003432c13e09a2cd7212381ada8ae047e",
    "filename": "sequoia/methods/models/output_heads/rl/policy_head.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/lebrice-Sequoia/sequoia/methods/models/output_heads/rl/policy_head.py",
    "file_hunks_size": 31,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "sequoia/methods/models/output_heads/rl/policy_head.py:179:12 Incompatible parameter type [6]: Expected `OutputHead.HParams` for 4th parameter `hparams` to call `ClassificationHead.__init__` but got `PolicyHead.HParams`.",
    "message": " Expected `OutputHead.HParams` for 4th parameter `hparams` to call `ClassificationHead.__init__` but got `PolicyHead.HParams`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 179,
    "warning_line": "            hparams=hparams,"
  },
  {
    "project": "lebrice/Sequoia",
    "commit": "e98dbb8003432c13e09a2cd7212381ada8ae047e",
    "filename": "sequoia/methods/models/output_heads/rl/policy_head.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/lebrice-Sequoia/sequoia/methods/models/output_heads/rl/policy_head.py",
    "file_hunks_size": 31,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "sequoia/methods/models/output_heads/rl/policy_head.py:307:19 Unsupported operand [58]: `>=` is not supported for operand types `Sequence[int]` and `int`.",
    "message": " `>=` is not supported for operand types `Sequence[int]` and `int`.",
    "rule_id": "Unsupported operand [58]",
    "warning_line_no": 307,
    "warning_line": "            if all(self.num_episodes_since_update >= self.hparams.min_episodes_before_update):"
  },
  {
    "project": "lebrice/Sequoia",
    "commit": "e98dbb8003432c13e09a2cd7212381ada8ae047e",
    "filename": "sequoia/methods/models/output_heads/rl/policy_head.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/lebrice-Sequoia/sequoia/methods/models/output_heads/rl/policy_head.py",
    "file_hunks_size": 31,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "sequoia/methods/models/output_heads/rl/policy_head.py:548:27 Incompatible variable type [9]: elements is declared to have type `Sequence[typing.Any]` but is used as type `None`.",
    "message": " elements is declared to have type `Sequence[typing.Any]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 548,
    "warning_line": "    def _make_buffer(self, elements: Sequence[Any] = None) -> deque:"
  },
  {
    "project": "lebrice/Sequoia",
    "commit": "e98dbb8003432c13e09a2cd7212381ada8ae047e",
    "filename": "sequoia/settings/active/continual/continual_rl_setting.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/lebrice-Sequoia/sequoia/settings/active/continual/continual_rl_setting.py",
    "file_hunks_size": 11,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "sequoia/settings/active/continual/continual_rl_setting.py:660:16 Incompatible parameter type [6]: Expected `int` for 3rd parameter `num_workers` to call `make_batched_env` but got `Optional[int]`.",
    "message": " Expected `int` for 3rd parameter `num_workers` to call `make_batched_env` but got `Optional[int]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 660,
    "warning_line": "                num_workers=num_workers,"
  },
  {
    "project": "lebrice/Sequoia",
    "commit": "e98dbb8003432c13e09a2cd7212381ada8ae047e",
    "filename": "sequoia/settings/active/continual/gym_dataloader.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/lebrice-Sequoia/sequoia/settings/active/continual/gym_dataloader.py",
    "file_hunks_size": 1,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "sequoia/settings/active/continual/gym_dataloader.py:229:19 Incompatible variable type [9]: num_workers is declared to have type `int` but is used as type `None`.",
    "message": " num_workers is declared to have type `int` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 229,
    "warning_line": "                   num_workers: int = None,"
  },
  {
    "project": "lebrice/Sequoia",
    "commit": "e98dbb8003432c13e09a2cd7212381ada8ae047e",
    "filename": "sequoia/settings/active/continual/gym_dataloader.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/lebrice-Sequoia/sequoia/settings/active/continual/gym_dataloader.py",
    "file_hunks_size": 1,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "sequoia/settings/active/continual/gym_dataloader.py:230:19 Incompatible variable type [9]: max_epochs is declared to have type `int` but is used as type `None`.",
    "message": " max_epochs is declared to have type `int` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 230,
    "warning_line": "                   max_epochs: int = None,"
  },
  {
    "project": "lebrice/Sequoia",
    "commit": "e98dbb8003432c13e09a2cd7212381ada8ae047e",
    "filename": "sequoia/settings/active/continual/make_env.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/lebrice-Sequoia/sequoia/settings/active/continual/make_env.py",
    "file_hunks_size": 8,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "sequoia/settings/active/continual/make_env.py:39:21 Incompatible variable type [9]: num_workers is declared to have type `int` but is used as type `None`.",
    "message": " num_workers is declared to have type `int` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 39,
    "warning_line": "                     num_workers: int = None,"
  },
  {
    "project": "lebrice/Sequoia",
    "commit": "e98dbb8003432c13e09a2cd7212381ada8ae047e",
    "filename": "sequoia/settings/passive/cl/class_incremental_setting.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/lebrice-Sequoia/sequoia/settings/passive/cl/class_incremental_setting.py",
    "file_hunks_size": 3,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "sequoia/settings/passive/cl/class_incremental_setting.py:288:54 Incompatible parameter type [6]: Expected `Union[List[str], str]` for 1st positional only parameter to call `sequoia.utils.parseable.Parseable.from_args` but got `Optional[List[str]]`.",
    "message": " Expected `Union[List[str], str]` for 1st positional only parameter to call `sequoia.utils.parseable.Parseable.from_args` but got `Optional[List[str]]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 288,
    "warning_line": "        self.config = self.config or Config.from_args(self._argv, strict=False)"
  }
]