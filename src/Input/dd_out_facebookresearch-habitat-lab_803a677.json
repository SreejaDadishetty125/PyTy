[
  {
    "project": "facebookresearch/habitat-lab",
    "commit": "803a677f5c9e0c4673ba03f54a26dccbac31caea",
    "filename": "habitat_baselines/common/utils.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/facebookresearch-habitat-lab/habitat_baselines/common/utils.py",
    "file_hunks_size": 13,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "habitat_baselines/common/utils.py:70:24 Incompatible parameter type [6]: Expected `Union[_SupportsTrunc, bytes, str, typing.SupportsInt, typing_extensions.SupportsIndex]` for 1st positional only parameter to call `int.__new__` but got `numbers.Number`.",
    "message": " Expected `Union[_SupportsTrunc, bytes, str, typing.SupportsInt, typing_extensions.SupportsIndex]` for 1st positional only parameter to call `int.__new__` but got `numbers.Number`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 70,
    "warning_line": "            size = (int(size), int(size))",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\nclass ResizeCenterCropper(nn.Module):\n    def __init__(self, size, channels_last: bool = False):\n        r\"\"\"An nn module the resizes and center crops your input.\n        Args:\n            size: A sequence (w, h) or int of the size you wish to\n                resize/center_crop. If int, assumes square crop\n            channels_list: indicates if channels is the last dimension\n        \"\"\"\n        super().__init__()\n        if isinstance(size, numbers.Number):\n            size = (int(size), int(size))\n        assert len(size) == 2, \"forced input size must be len of 2 (w, h)\"\n        self._size = size\n        self.channels_last = channels_last\n\n    def transform_observation_space(\n        self, observation_space, trans_keys=(\"rgb\", \"depth\", \"semantic\")\n    ):\n        size = self._size\n        observation_space = copy.deepcopy(observation_space)\n        if size:\n            for key in observation_space.spaces:\n                if (\n                    key in trans_keys\n                    and observation_space.spaces[key].shape != size\n                ):\n                    logger.info(\n                        \"Overwriting CNN input size of %s: %s\" % (key, size)\n                    )\n                    observation_space.spaces[key] = overwrite_gym_box_shape(\n                        observation_space.spaces[key], size\n                    )\n        self.observation_space = observation_space\n        return observation_space\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        if self._size is None:\n            return input\n\n        return center_crop(\n            image_resize_shortest_edge(\n                input, max(self._size), channels_last=self.channels_last\n            ),\n            self._size,\n            channels_last=self.channels_last,\n        )\n\n\ndef linear_decay(epoch: int, total_num_updates: int) -> float:\n",
        "source_code_len": 1861,
        "target_code": "\ndef linear_decay(epoch: int, total_num_updates: int) -> float:\n",
        "target_code_len": 64,
        "diff_format": "@@ -59,50 +57,2 @@\n \n-class ResizeCenterCropper(nn.Module):\n-    def __init__(self, size, channels_last: bool = False):\n-        r\"\"\"An nn module the resizes and center crops your input.\n-        Args:\n-            size: A sequence (w, h) or int of the size you wish to\n-                resize/center_crop. If int, assumes square crop\n-            channels_list: indicates if channels is the last dimension\n-        \"\"\"\n-        super().__init__()\n-        if isinstance(size, numbers.Number):\n-            size = (int(size), int(size))\n-        assert len(size) == 2, \"forced input size must be len of 2 (w, h)\"\n-        self._size = size\n-        self.channels_last = channels_last\n-\n-    def transform_observation_space(\n-        self, observation_space, trans_keys=(\"rgb\", \"depth\", \"semantic\")\n-    ):\n-        size = self._size\n-        observation_space = copy.deepcopy(observation_space)\n-        if size:\n-            for key in observation_space.spaces:\n-                if (\n-                    key in trans_keys\n-                    and observation_space.spaces[key].shape != size\n-                ):\n-                    logger.info(\n-                        \"Overwriting CNN input size of %s: %s\" % (key, size)\n-                    )\n-                    observation_space.spaces[key] = overwrite_gym_box_shape(\n-                        observation_space.spaces[key], size\n-                    )\n-        self.observation_space = observation_space\n-        return observation_space\n-\n-    def forward(self, input: torch.Tensor) -> torch.Tensor:\n-        if self._size is None:\n-            return input\n-\n-        return center_crop(\n-            image_resize_shortest_edge(\n-                input, max(self._size), channels_last=self.channels_last\n-            ),\n-            self._size,\n-            channels_last=self.channels_last,\n-        )\n-\n-\n def linear_decay(epoch: int, total_num_updates: int) -> float:\n",
        "source_code_with_indent": "\n<DED><DED>class ResizeCenterCropper(nn.Module):\n    <IND>def __init__(self, size, channels_last: bool = False):\n        <IND>r\"\"\"An nn module the resizes and center crops your input.\n        Args:\n            size: A sequence (w, h) or int of the size you wish to\n                resize/center_crop. If int, assumes square crop\n            channels_list: indicates if channels is the last dimension\n        \"\"\"\n        super().__init__()\n        if isinstance(size, numbers.Number):\n            <IND>size = (int(size), int(size))\n        <DED>assert len(size) == 2, \"forced input size must be len of 2 (w, h)\"\n        self._size = size\n        self.channels_last = channels_last\n\n    <DED>def transform_observation_space(\n        self, observation_space, trans_keys=(\"rgb\", \"depth\", \"semantic\")\n    ):\n        <IND>size = self._size\n        observation_space = copy.deepcopy(observation_space)\n        if size:\n            <IND>for key in observation_space.spaces:\n                <IND>if (\n                    key in trans_keys\n                    and observation_space.spaces[key].shape != size\n                ):\n                    <IND>logger.info(\n                        \"Overwriting CNN input size of %s: %s\" % (key, size)\n                    )\n                    observation_space.spaces[key] = overwrite_gym_box_shape(\n                        observation_space.spaces[key], size\n                    )\n        <DED><DED><DED>self.observation_space = observation_space\n        return observation_space\n\n    <DED>def forward(self, input: torch.Tensor) -> torch.Tensor:\n        <IND>if self._size is None:\n            <IND>return input\n\n        <DED>return center_crop(\n            image_resize_shortest_edge(\n                input, max(self._size), channels_last=self.channels_last\n            ),\n            self._size,\n            channels_last=self.channels_last,\n        )\n\n\n<DED><DED>def linear_decay(epoch: int, total_num_updates: int) -> float:\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n<DED><DED>def linear_decay(epoch: int, total_num_updates: int) -> float:\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "facebookresearch/habitat-lab",
    "commit": "803a677f5c9e0c4673ba03f54a26dccbac31caea",
    "filename": "habitat_baselines/common/utils.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/facebookresearch-habitat-lab/habitat_baselines/common/utils.py",
    "file_hunks_size": 13,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "habitat_baselines/common/utils.py:70:35 Incompatible parameter type [6]: Expected `Union[_SupportsTrunc, bytes, str, typing.SupportsInt, typing_extensions.SupportsIndex]` for 1st positional only parameter to call `int.__new__` but got `numbers.Number`.",
    "message": " Expected `Union[_SupportsTrunc, bytes, str, typing.SupportsInt, typing_extensions.SupportsIndex]` for 1st positional only parameter to call `int.__new__` but got `numbers.Number`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 70,
    "warning_line": "            size = (int(size), int(size))",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\nclass ResizeCenterCropper(nn.Module):\n    def __init__(self, size, channels_last: bool = False):\n        r\"\"\"An nn module the resizes and center crops your input.\n        Args:\n            size: A sequence (w, h) or int of the size you wish to\n                resize/center_crop. If int, assumes square crop\n            channels_list: indicates if channels is the last dimension\n        \"\"\"\n        super().__init__()\n        if isinstance(size, numbers.Number):\n            size = (int(size), int(size))\n        assert len(size) == 2, \"forced input size must be len of 2 (w, h)\"\n        self._size = size\n        self.channels_last = channels_last\n\n    def transform_observation_space(\n        self, observation_space, trans_keys=(\"rgb\", \"depth\", \"semantic\")\n    ):\n        size = self._size\n        observation_space = copy.deepcopy(observation_space)\n        if size:\n            for key in observation_space.spaces:\n                if (\n                    key in trans_keys\n                    and observation_space.spaces[key].shape != size\n                ):\n                    logger.info(\n                        \"Overwriting CNN input size of %s: %s\" % (key, size)\n                    )\n                    observation_space.spaces[key] = overwrite_gym_box_shape(\n                        observation_space.spaces[key], size\n                    )\n        self.observation_space = observation_space\n        return observation_space\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        if self._size is None:\n            return input\n\n        return center_crop(\n            image_resize_shortest_edge(\n                input, max(self._size), channels_last=self.channels_last\n            ),\n            self._size,\n            channels_last=self.channels_last,\n        )\n\n\ndef linear_decay(epoch: int, total_num_updates: int) -> float:\n",
        "source_code_len": 1861,
        "target_code": "\ndef linear_decay(epoch: int, total_num_updates: int) -> float:\n",
        "target_code_len": 64,
        "diff_format": "@@ -59,50 +57,2 @@\n \n-class ResizeCenterCropper(nn.Module):\n-    def __init__(self, size, channels_last: bool = False):\n-        r\"\"\"An nn module the resizes and center crops your input.\n-        Args:\n-            size: A sequence (w, h) or int of the size you wish to\n-                resize/center_crop. If int, assumes square crop\n-            channels_list: indicates if channels is the last dimension\n-        \"\"\"\n-        super().__init__()\n-        if isinstance(size, numbers.Number):\n-            size = (int(size), int(size))\n-        assert len(size) == 2, \"forced input size must be len of 2 (w, h)\"\n-        self._size = size\n-        self.channels_last = channels_last\n-\n-    def transform_observation_space(\n-        self, observation_space, trans_keys=(\"rgb\", \"depth\", \"semantic\")\n-    ):\n-        size = self._size\n-        observation_space = copy.deepcopy(observation_space)\n-        if size:\n-            for key in observation_space.spaces:\n-                if (\n-                    key in trans_keys\n-                    and observation_space.spaces[key].shape != size\n-                ):\n-                    logger.info(\n-                        \"Overwriting CNN input size of %s: %s\" % (key, size)\n-                    )\n-                    observation_space.spaces[key] = overwrite_gym_box_shape(\n-                        observation_space.spaces[key], size\n-                    )\n-        self.observation_space = observation_space\n-        return observation_space\n-\n-    def forward(self, input: torch.Tensor) -> torch.Tensor:\n-        if self._size is None:\n-            return input\n-\n-        return center_crop(\n-            image_resize_shortest_edge(\n-                input, max(self._size), channels_last=self.channels_last\n-            ),\n-            self._size,\n-            channels_last=self.channels_last,\n-        )\n-\n-\n def linear_decay(epoch: int, total_num_updates: int) -> float:\n",
        "source_code_with_indent": "\n<DED><DED>class ResizeCenterCropper(nn.Module):\n    <IND>def __init__(self, size, channels_last: bool = False):\n        <IND>r\"\"\"An nn module the resizes and center crops your input.\n        Args:\n            size: A sequence (w, h) or int of the size you wish to\n                resize/center_crop. If int, assumes square crop\n            channels_list: indicates if channels is the last dimension\n        \"\"\"\n        super().__init__()\n        if isinstance(size, numbers.Number):\n            <IND>size = (int(size), int(size))\n        <DED>assert len(size) == 2, \"forced input size must be len of 2 (w, h)\"\n        self._size = size\n        self.channels_last = channels_last\n\n    <DED>def transform_observation_space(\n        self, observation_space, trans_keys=(\"rgb\", \"depth\", \"semantic\")\n    ):\n        <IND>size = self._size\n        observation_space = copy.deepcopy(observation_space)\n        if size:\n            <IND>for key in observation_space.spaces:\n                <IND>if (\n                    key in trans_keys\n                    and observation_space.spaces[key].shape != size\n                ):\n                    <IND>logger.info(\n                        \"Overwriting CNN input size of %s: %s\" % (key, size)\n                    )\n                    observation_space.spaces[key] = overwrite_gym_box_shape(\n                        observation_space.spaces[key], size\n                    )\n        <DED><DED><DED>self.observation_space = observation_space\n        return observation_space\n\n    <DED>def forward(self, input: torch.Tensor) -> torch.Tensor:\n        <IND>if self._size is None:\n            <IND>return input\n\n        <DED>return center_crop(\n            image_resize_shortest_edge(\n                input, max(self._size), channels_last=self.channels_last\n            ),\n            self._size,\n            channels_last=self.channels_last,\n        )\n\n\n<DED><DED>def linear_decay(epoch: int, total_num_updates: int) -> float:\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n<DED><DED>def linear_decay(epoch: int, total_num_updates: int) -> float:\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "facebookresearch/habitat-lab",
    "commit": "803a677f5c9e0c4673ba03f54a26dccbac31caea",
    "filename": "habitat_baselines/common/utils.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/facebookresearch-habitat-lab/habitat_baselines/common/utils.py",
    "file_hunks_size": 13,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "habitat_baselines/common/utils.py:336:20 Incompatible parameter type [6]: Expected `Union[_SupportsTrunc, bytes, str, typing.SupportsInt, typing_extensions.SupportsIndex]` for 1st positional only parameter to call `int.__new__` but got `numbers.Number`.",
    "message": " Expected `Union[_SupportsTrunc, bytes, str, typing.SupportsInt, typing_extensions.SupportsIndex]` for 1st positional only parameter to call `int.__new__` but got `numbers.Number`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 336,
    "warning_line": "        size = (int(size), int(size))",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        h, w = img.shape[-2:]\n\n    if isinstance(size, numbers.Number):\n        size = (int(size), int(size))\n    assert len(size) == 2, \"size should be (h,w) you wish to resize to\"\n    cropx, cropy = size\n\n    startx = w // 2 - (cropx // 2)\n    starty = h // 2 - (cropy // 2)\n    if channels_last:\n        return img[..., starty : starty + cropy, startx : startx + cropx, :]\n    else:\n        return img[..., starty : starty + cropy, startx : startx + cropx]\n\n",
        "source_code_len": 461,
        "target_code": "        h, w = img.shape[-2:]\n    return h, w\n\n",
        "target_code_len": 47,
        "diff_format": "@@ -333,14 +299,3 @@\n         h, w = img.shape[-2:]\n-\n-    if isinstance(size, numbers.Number):\n-        size = (int(size), int(size))\n-    assert len(size) == 2, \"size should be (h,w) you wish to resize to\"\n-    cropx, cropy = size\n-\n-    startx = w // 2 - (cropx // 2)\n-    starty = h // 2 - (cropy // 2)\n-    if channels_last:\n-        return img[..., starty : starty + cropy, startx : startx + cropx, :]\n-    else:\n-        return img[..., starty : starty + cropy, startx : startx + cropx]\n+    return h, w\n \n",
        "source_code_with_indent": "        <IND>h, w = img.shape[-2:]\n\n    <DED>if isinstance(size, numbers.Number):\n        <IND>size = (int(size), int(size))\n    <DED>assert len(size) == 2, \"size should be (h,w) you wish to resize to\"\n    cropx, cropy = size\n\n    startx = w // 2 - (cropx // 2)\n    starty = h // 2 - (cropy // 2)\n    if channels_last:\n        <IND>return img[..., starty : starty + cropy, startx : startx + cropx, :]\n    <DED>else:\n        <IND>return img[..., starty : starty + cropy, startx : startx + cropx]\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <IND>h, w = img.shape[-2:]\n    <DED>return h, w\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "facebookresearch/habitat-lab",
    "commit": "803a677f5c9e0c4673ba03f54a26dccbac31caea",
    "filename": "habitat_baselines/common/utils.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/facebookresearch-habitat-lab/habitat_baselines/common/utils.py",
    "file_hunks_size": 13,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "habitat_baselines/common/utils.py:336:31 Incompatible parameter type [6]: Expected `Union[_SupportsTrunc, bytes, str, typing.SupportsInt, typing_extensions.SupportsIndex]` for 1st positional only parameter to call `int.__new__` but got `numbers.Number`.",
    "message": " Expected `Union[_SupportsTrunc, bytes, str, typing.SupportsInt, typing_extensions.SupportsIndex]` for 1st positional only parameter to call `int.__new__` but got `numbers.Number`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 336,
    "warning_line": "        size = (int(size), int(size))",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        h, w = img.shape[-2:]\n\n    if isinstance(size, numbers.Number):\n        size = (int(size), int(size))\n    assert len(size) == 2, \"size should be (h,w) you wish to resize to\"\n    cropx, cropy = size\n\n    startx = w // 2 - (cropx // 2)\n    starty = h // 2 - (cropy // 2)\n    if channels_last:\n        return img[..., starty : starty + cropy, startx : startx + cropx, :]\n    else:\n        return img[..., starty : starty + cropy, startx : startx + cropx]\n\n",
        "source_code_len": 461,
        "target_code": "        h, w = img.shape[-2:]\n    return h, w\n\n",
        "target_code_len": 47,
        "diff_format": "@@ -333,14 +299,3 @@\n         h, w = img.shape[-2:]\n-\n-    if isinstance(size, numbers.Number):\n-        size = (int(size), int(size))\n-    assert len(size) == 2, \"size should be (h,w) you wish to resize to\"\n-    cropx, cropy = size\n-\n-    startx = w // 2 - (cropx // 2)\n-    starty = h // 2 - (cropy // 2)\n-    if channels_last:\n-        return img[..., starty : starty + cropy, startx : startx + cropx, :]\n-    else:\n-        return img[..., starty : starty + cropy, startx : startx + cropx]\n+    return h, w\n \n",
        "source_code_with_indent": "        <IND>h, w = img.shape[-2:]\n\n    <DED>if isinstance(size, numbers.Number):\n        <IND>size = (int(size), int(size))\n    <DED>assert len(size) == 2, \"size should be (h,w) you wish to resize to\"\n    cropx, cropy = size\n\n    startx = w // 2 - (cropx // 2)\n    starty = h // 2 - (cropy // 2)\n    if channels_last:\n        <IND>return img[..., starty : starty + cropy, startx : startx + cropx, :]\n    <DED>else:\n        <IND>return img[..., starty : starty + cropy, startx : startx + cropx]\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        <IND>h, w = img.shape[-2:]\n    <DED>return h, w\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  }
]