[
  {
    "project": "apache/airflow",
    "commit": "c6dbb3f8856be75ff2619476ab3ca587a52e033a",
    "filename": "airflow/providers/google/cloud/hooks/bigquery.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/apache-airflow/airflow/providers/google/cloud/hooks/bigquery.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "airflow/providers/google/cloud/hooks/bigquery.py:1422:12 Incompatible variable type [9]: schema_fields_updates is declared to have type `List[Dict[str, typing.Any]]` but is used as type `Dict[typing.Any, Dict[str, typing.Any]]`.",
    "message": " schema_fields_updates is declared to have type `List[Dict[str, typing.Any]]` but is used as type `Dict[typing.Any, Dict[str, typing.Any]]`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 1422,
    "warning_line": "            schema_fields_updates = {field[\"name\"]: field for field in deepcopy(schema_fields_updates)}",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "            # Turn schema_field_updates into a dict keyed on field names\n            schema_fields_updates = {field[\"name\"]: field for field in deepcopy(schema_fields_updates)}\n\n",
        "source_code_len": 178,
        "target_code": "            # Turn schema_field_updates into a dict keyed on field names\n            schema_fields_updates_dict = {field[\"name\"]: field for field in deepcopy(schema_fields_updates)}\n\n",
        "target_code_len": 183,
        "diff_format": "@@ -1421,3 +1421,3 @@\n             # Turn schema_field_updates into a dict keyed on field names\n-            schema_fields_updates = {field[\"name\"]: field for field in deepcopy(schema_fields_updates)}\n+            schema_fields_updates_dict = {field[\"name\"]: field for field in deepcopy(schema_fields_updates)}\n \n",
        "source_code_with_indent": "            # Turn schema_field_updates into a dict keyed on field names\n            <IND>schema_fields_updates = {field[\"name\"]: field for field in deepcopy(schema_fields_updates)}\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "            # Turn schema_field_updates into a dict keyed on field names\n            <IND>schema_fields_updates_dict = {field[\"name\"]: field for field in deepcopy(schema_fields_updates)}\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "apache/airflow",
    "commit": "c6dbb3f8856be75ff2619476ab3ca587a52e033a",
    "filename": "airflow/providers/google/cloud/hooks/bigquery.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/apache-airflow/airflow/providers/google/cloud/hooks/bigquery.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "airflow/providers/google/cloud/hooks/bigquery.py:1846:41 Incompatible parameter type [6]: Expected `Mapping[str, Union[Dict[str, str], List[typing.Any], bool, str]]` for 1st positional only parameter to call `dict.update` but got `Mapping[str, Dict[str, List[typing.Any]]]`.",
    "message": " Expected `Mapping[str, Union[Dict[str, str], List[typing.Any], bool, str]]` for 1st positional only parameter to call `dict.update` but got `Mapping[str, Dict[str, List[typing.Any]]]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 1846,
    "warning_line": "            configuration['load'].update({'clustering': {'fields': cluster_fields}})",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        configuration = {\n            'load': {\n",
        "source_code_len": 49,
        "target_code": "\n        configuration: Dict[str, Any] = {\n            'load': {\n",
        "target_code_len": 65,
        "diff_format": "@@ -1824,3 +1824,3 @@\n \n-        configuration = {\n+        configuration: Dict[str, Any] = {\n             'load': {\n",
        "source_code_with_indent": "\n        configuration = {\n            'load': {\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        configuration: Dict[str, Any] = {\n            'load': {\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "apache/airflow",
    "commit": "c6dbb3f8856be75ff2619476ab3ca587a52e033a",
    "filename": "airflow/providers/google/cloud/hooks/bigquery.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/apache-airflow/airflow/providers/google/cloud/hooks/bigquery.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "airflow/providers/google/cloud/hooks/bigquery.py:1849:46 Incompatible parameter type [6]: Expected `Union[Dict[str, str], List[typing.Any], bool, str]` for 2nd positional only parameter to call `dict.__setitem__` but got `Dict[str, List[typing.Any]]`.",
    "message": " Expected `Union[Dict[str, str], List[typing.Any], bool, str]` for 2nd positional only parameter to call `dict.__setitem__` but got `Dict[str, List[typing.Any]]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 1849,
    "warning_line": "            configuration['load']['schema'] = {'fields': schema_fields}",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        configuration = {\n            'load': {\n",
        "source_code_len": 49,
        "target_code": "\n        configuration: Dict[str, Any] = {\n            'load': {\n",
        "target_code_len": 65,
        "diff_format": "@@ -1824,3 +1824,3 @@\n \n-        configuration = {\n+        configuration: Dict[str, Any] = {\n             'load': {\n",
        "source_code_with_indent": "\n        configuration = {\n            'load': {\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        configuration: Dict[str, Any] = {\n            'load': {\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "apache/airflow",
    "commit": "c6dbb3f8856be75ff2619476ab3ca587a52e033a",
    "filename": "airflow/providers/google/cloud/hooks/bigquery.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/apache-airflow/airflow/providers/google/cloud/hooks/bigquery.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "airflow/providers/google/cloud/hooks/bigquery.py:1863:53 Incompatible parameter type [6]: Expected `Union[Dict[str, str], List[typing.Any], bool, str]` for 2nd positional only parameter to call `dict.__setitem__` but got `int`.",
    "message": " Expected `Union[Dict[str, str], List[typing.Any], bool, str]` for 2nd positional only parameter to call `dict.__setitem__` but got `int`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 1863,
    "warning_line": "            configuration['load']['maxBadRecords'] = max_bad_records",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        configuration = {\n            'load': {\n",
        "source_code_len": 49,
        "target_code": "\n        configuration: Dict[str, Any] = {\n            'load': {\n",
        "target_code_len": 65,
        "diff_format": "@@ -1824,3 +1824,3 @@\n \n-        configuration = {\n+        configuration: Dict[str, Any] = {\n             'load': {\n",
        "source_code_with_indent": "\n        configuration = {\n            'load': {\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        configuration: Dict[str, Any] = {\n            'load': {\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "apache/airflow",
    "commit": "c6dbb3f8856be75ff2619476ab3ca587a52e033a",
    "filename": "airflow/providers/google/cloud/operators/bigquery.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/apache-airflow/airflow/providers/google/cloud/operators/bigquery.py",
    "file_hunks_size": 9,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "airflow/providers/google/cloud/operators/bigquery.py:1094:8 Incompatible variable type [9]: destination_project_dataset_table is declared to have type `str` but is used as type `None`.",
    "message": " destination_project_dataset_table is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 1094,
    "warning_line": "        destination_project_dataset_table: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        bucket: Optional[str] = None,\n        source_objects: Optional[List] = None,\n        destination_project_dataset_table: str = None,\n        table_resource: Optional[Dict[str, Any]] = None,\n",
        "source_code_len": 197,
        "target_code": "        bucket: Optional[str] = None,\n        source_objects: Optional[List[str]] = None,\n        destination_project_dataset_table: Optional[str] = None,\n        table_resource: Optional[Dict[str, Any]] = None,\n",
        "target_code_len": 212,
        "diff_format": "@@ -1092,4 +1093,4 @@\n         bucket: Optional[str] = None,\n-        source_objects: Optional[List] = None,\n-        destination_project_dataset_table: str = None,\n+        source_objects: Optional[List[str]] = None,\n+        destination_project_dataset_table: Optional[str] = None,\n         table_resource: Optional[Dict[str, Any]] = None,\n",
        "source_code_with_indent": "        bucket: Optional[str] = None,\n        source_objects: Optional[List] = None,\n        destination_project_dataset_table: str = None,\n        table_resource: Optional[Dict[str, Any]] = None,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        bucket: Optional[str] = None,\n        source_objects: Optional[List[str]] = None,\n        destination_project_dataset_table: Optional[str] = None,\n        table_resource: Optional[Dict[str, Any]] = None,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "apache/airflow",
    "commit": "c6dbb3f8856be75ff2619476ab3ca587a52e033a",
    "filename": "airflow/providers/google/cloud/operators/bigquery.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/apache-airflow/airflow/providers/google/cloud/operators/bigquery.py",
    "file_hunks_size": 9,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "airflow/providers/google/cloud/operators/bigquery.py:1206:57 Incompatible parameter type [6]: Expected `str` for 1st positional only parameter to call `GCSHook.download` but got `Optional[str]`.",
    "message": " Expected `str` for 1st positional only parameter to call `GCSHook.download` but got `Optional[str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 1206,
    "warning_line": "            schema_fields = json.loads(gcs_hook.download(self.bucket, self.schema_object).decode(\"utf-8\"))",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\n        # GCS config\n        self.bucket = bucket\n        self.source_objects = source_objects\n        self.schema_object = schema_object\n\n        # BQ config\n",
        "source_code_len": 160,
        "target_code": "\n        # BQ config\n",
        "target_code_len": 21,
        "diff_format": "@@ -1117,7 +1118,2 @@\n \n-        # GCS config\n-        self.bucket = bucket\n-        self.source_objects = source_objects\n-        self.schema_object = schema_object\n-\n         # BQ config\n",
        "source_code_with_indent": "\n        # GCS config\n        self.bucket = bucket\n        self.source_objects = source_objects\n        self.schema_object = schema_object\n\n        # BQ config\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        # BQ config\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "                field_delimiter = \",\"\n\n",
        "source_code_len": 39,
        "target_code": "                field_delimiter = \",\"\n            if not destination_project_dataset_table:\n                raise ValueError(\n                    \"`destination_project_dataset_table` is required when not using `table_resource`.\"\n                )\n            self.bucket = bucket\n            self.source_objects = source_objects\n            self.schema_object = schema_object\n            self.destination_project_dataset_table = destination_project_dataset_table\n            self.schema_fields = schema_fields\n            self.source_format = source_format\n            self.compression = compression\n            self.skip_leading_rows = skip_leading_rows\n            self.field_delimiter = field_delimiter\n            self.table_resource = None\n        else:\n            self.table_resource = table_resource\n\n",
        "target_code_len": 809,
        "diff_format": "@@ -1160,2 +1156,18 @@\n                 field_delimiter = \",\"\n+            if not destination_project_dataset_table:\n+                raise ValueError(\n+                    \"`destination_project_dataset_table` is required when not using `table_resource`.\"\n+                )\n+            self.bucket = bucket\n+            self.source_objects = source_objects\n+            self.schema_object = schema_object\n+            self.destination_project_dataset_table = destination_project_dataset_table\n+            self.schema_fields = schema_fields\n+            self.source_format = source_format\n+            self.compression = compression\n+            self.skip_leading_rows = skip_leading_rows\n+            self.field_delimiter = field_delimiter\n+            self.table_resource = None\n+        else:\n+            self.table_resource = table_resource\n \n",
        "source_code_with_indent": "                <IND>field_delimiter = \",\"\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                <IND>field_delimiter = \",\"\n            <DED>if not destination_project_dataset_table:\n                <IND>raise ValueError(\n                    \"`destination_project_dataset_table` is required when not using `table_resource`.\"\n                )\n            <DED>self.bucket = bucket\n            self.source_objects = source_objects\n            self.schema_object = schema_object\n            self.destination_project_dataset_table = destination_project_dataset_table\n            self.schema_fields = schema_fields\n            self.source_format = source_format\n            self.compression = compression\n            self.skip_leading_rows = skip_leading_rows\n            self.field_delimiter = field_delimiter\n            self.table_resource = None\n        <DED>else:\n            <IND>self.table_resource = table_resource\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "apache/airflow",
    "commit": "c6dbb3f8856be75ff2619476ab3ca587a52e033a",
    "filename": "airflow/providers/google/cloud/operators/bigquery.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/apache-airflow/airflow/providers/google/cloud/operators/bigquery.py",
    "file_hunks_size": 9,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "airflow/providers/google/cloud/operators/bigquery.py:1206:70 Incompatible parameter type [6]: Expected `str` for 2nd positional only parameter to call `GCSHook.download` but got `Optional[str]`.",
    "message": " Expected `str` for 2nd positional only parameter to call `GCSHook.download` but got `Optional[str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 1206,
    "warning_line": "            schema_fields = json.loads(gcs_hook.download(self.bucket, self.schema_object).decode(\"utf-8\"))",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\n        # GCS config\n        self.bucket = bucket\n        self.source_objects = source_objects\n        self.schema_object = schema_object\n\n        # BQ config\n",
        "source_code_len": 160,
        "target_code": "\n        # BQ config\n",
        "target_code_len": 21,
        "diff_format": "@@ -1117,7 +1118,2 @@\n \n-        # GCS config\n-        self.bucket = bucket\n-        self.source_objects = source_objects\n-        self.schema_object = schema_object\n-\n         # BQ config\n",
        "source_code_with_indent": "\n        # GCS config\n        self.bucket = bucket\n        self.source_objects = source_objects\n        self.schema_object = schema_object\n\n        # BQ config\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        # BQ config\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "                field_delimiter = \",\"\n\n",
        "source_code_len": 39,
        "target_code": "                field_delimiter = \",\"\n            if not destination_project_dataset_table:\n                raise ValueError(\n                    \"`destination_project_dataset_table` is required when not using `table_resource`.\"\n                )\n            self.bucket = bucket\n            self.source_objects = source_objects\n            self.schema_object = schema_object\n            self.destination_project_dataset_table = destination_project_dataset_table\n            self.schema_fields = schema_fields\n            self.source_format = source_format\n            self.compression = compression\n            self.skip_leading_rows = skip_leading_rows\n            self.field_delimiter = field_delimiter\n            self.table_resource = None\n        else:\n            self.table_resource = table_resource\n\n",
        "target_code_len": 809,
        "diff_format": "@@ -1160,2 +1156,18 @@\n                 field_delimiter = \",\"\n+            if not destination_project_dataset_table:\n+                raise ValueError(\n+                    \"`destination_project_dataset_table` is required when not using `table_resource`.\"\n+                )\n+            self.bucket = bucket\n+            self.source_objects = source_objects\n+            self.schema_object = schema_object\n+            self.destination_project_dataset_table = destination_project_dataset_table\n+            self.schema_fields = schema_fields\n+            self.source_format = source_format\n+            self.compression = compression\n+            self.skip_leading_rows = skip_leading_rows\n+            self.field_delimiter = field_delimiter\n+            self.table_resource = None\n+        else:\n+            self.table_resource = table_resource\n \n",
        "source_code_with_indent": "                <IND>field_delimiter = \",\"\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                <IND>field_delimiter = \",\"\n            <DED>if not destination_project_dataset_table:\n                <IND>raise ValueError(\n                    \"`destination_project_dataset_table` is required when not using `table_resource`.\"\n                )\n            <DED>self.bucket = bucket\n            self.source_objects = source_objects\n            self.schema_object = schema_object\n            self.destination_project_dataset_table = destination_project_dataset_table\n            self.schema_fields = schema_fields\n            self.source_format = source_format\n            self.compression = compression\n            self.skip_leading_rows = skip_leading_rows\n            self.field_delimiter = field_delimiter\n            self.table_resource = None\n        <DED>else:\n            <IND>self.table_resource = table_resource\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  }
]