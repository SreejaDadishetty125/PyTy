[
  {
    "project": "unit8co/darts",
    "commit": "c2f5a498aa5873177dd39010b5f678c44055fc14",
    "filename": "darts/dataprocessing/transformers/boxcox.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/unit8co-darts/darts/dataprocessing/transformers/boxcox.py",
    "file_hunks_size": 4,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "darts/dataprocessing/transformers/boxcox.py:128:4 Inconsistent override [14]: `darts.dataprocessing.transformers.boxcox.BoxCox.fit` overrides method defined in `darts.dataprocessing.transformers.fittable_data_transformer.FittableDataTransformer` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "message": " `darts.dataprocessing.transformers.boxcox.BoxCox.fit` overrides method defined in `darts.dataprocessing.transformers.fittable_data_transformer.FittableDataTransformer` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 128,
    "warning_line": "    def fit(self, series: Union[TimeSeries, Sequence[TimeSeries]]) -> 'FittableDataTransformer':"
  },
  {
    "project": "unit8co/darts",
    "commit": "c2f5a498aa5873177dd39010b5f678c44055fc14",
    "filename": "darts/models/forecasting/tft_model.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/unit8co-darts/darts/models/forecasting/tft_model.py",
    "file_hunks_size": 16,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "darts/models/forecasting/tft_model.py:819:42 Incompatible parameter type [6]: Expected `str` for 2nd parameter `attribute` to call `datetime_attribute_timeseries` but got `Optional[str]`.",
    "message": " Expected `str` for 2nd parameter `attribute` to call `datetime_attribute_timeseries` but got `Optional[str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 819,
    "warning_line": "                                          attribute=self.add_cyclic_encoder, ",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n        raise_if(future_covariates is None and self.add_cyclic_encoder is None and not self.add_relative_index,\n                 'TFTModel requires future covariates. The model applies multi-head attention queries on future '\n                 'inputs. Consider specifying `add_cyclic_encoder` or setting `add_relative_index` to `True` '\n                 'at model creation (read TFT model docs for more information). These will automatically generate '\n                 '`future_covariates` from indexes.',\n                 logger)\n\n        if self.add_cyclic_encoder is not None:\n            future_covariates = self._add_cyclic_encoder(target, future_covariates=future_covariates, n=None)\n\n",
        "source_code_len": 693,
        "target_code": "\n        raise_if(future_covariates is None and not self.add_relative_index,\n                 'TFTModel requires future covariates. The model applies multi-head attention queries on future '\n                 'inputs. Consider specifying a future encoder with `add_encoders` or setting `add_relative_index` '\n                 'to `True` at model creation (read TFT model docs for more information). '\n                 'These will automatically generate `future_covariates` from indexes.',\n                 logger)\n\n",
        "target_code_len": 514,
        "diff_format": "@@ -760,11 +769,8 @@\n \n-        raise_if(future_covariates is None and self.add_cyclic_encoder is None and not self.add_relative_index,\n+        raise_if(future_covariates is None and not self.add_relative_index,\n                  'TFTModel requires future covariates. The model applies multi-head attention queries on future '\n-                 'inputs. Consider specifying `add_cyclic_encoder` or setting `add_relative_index` to `True` '\n-                 'at model creation (read TFT model docs for more information). These will automatically generate '\n-                 '`future_covariates` from indexes.',\n+                 'inputs. Consider specifying a future encoder with `add_encoders` or setting `add_relative_index` '\n+                 'to `True` at model creation (read TFT model docs for more information). '\n+                 'These will automatically generate `future_covariates` from indexes.',\n                  logger)\n-\n-        if self.add_cyclic_encoder is not None:\n-            future_covariates = self._add_cyclic_encoder(target, future_covariates=future_covariates, n=None)\n \n",
        "source_code_with_indent": "\n        <IND>raise_if(future_covariates is None and self.add_cyclic_encoder is None and not self.add_relative_index,\n                 'TFTModel requires future covariates. The model applies multi-head attention queries on future '\n                 'inputs. Consider specifying `add_cyclic_encoder` or setting `add_relative_index` to `True` '\n                 'at model creation (read TFT model docs for more information). These will automatically generate '\n                 '`future_covariates` from indexes.',\n                 logger)\n\n        if self.add_cyclic_encoder is not None:\n            <IND>future_covariates = self._add_cyclic_encoder(target, future_covariates=future_covariates, n=None)\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n        <IND>raise_if(future_covariates is None and not self.add_relative_index,\n                 'TFTModel requires future covariates. The model applies multi-head attention queries on future '\n                 'inputs. Consider specifying a future encoder with `add_encoders` or setting `add_relative_index` '\n                 'to `True` at model creation (read TFT model docs for more information). '\n                 'These will automatically generate `future_covariates` from indexes.',\n                 logger)\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\n    def _add_cyclic_encoder(self,\n                            target: Sequence[TimeSeries],\n                            future_covariates: Optional[Sequence[TimeSeries]] = None,\n                            n: Optional[int] = None) -> Sequence[TimeSeries]:\n        \"\"\"adds cyclic encoding of time index to future covariates.\n        For training (when `n` is `None`) we can simply use the future covariates (if available) or target as\n        reference to extract the time index.\n        For prediction (`n` is given) we have to distinguish between two cases:\n            1)\n                if future covariates are given, we can use them as reference\n            2)\n                if future covariates are missing, we need to generate a time index that starts `input_chunk_length`\n                before the end of `target` and ends `max(n, output_chunk_length)` after the end of `target`\n\n        Parameters\n        ----------\n        target\n            past target TimeSeries\n        future_covariates\n            future covariates TimeSeries\n        n\n            prediciton length (only given for predictions)\n\n        Returns\n        -------\n        Sequence[TimeSeries]\n            future covariates including cyclic encoded time index\n        \"\"\"\n\n        if n is None:  # training\n            encode_ts = future_covariates if future_covariates is not None else target\n        else:  # prediction\n            if future_covariates is not None:\n                encode_ts = future_covariates\n            else:\n                encode_ts = [_generate_index(start=ts.end_time() - ts.freq * (self.input_chunk_length - 1),\n                                             length=self.input_chunk_length + max(n, self.output_chunk_length),\n                                             freq=ts.freq) for ts in target]\n\n        encoded_times = [\n            datetime_attribute_timeseries(ts, \n                                          attribute=self.add_cyclic_encoder, \n                                          cyclic=True, \n                                          dtype=target[0].dtype) \n            for ts in encode_ts\n        ]\n\n        if future_covariates is None:\n            future_covariates = encoded_times\n        else:\n            future_covariates = [fc.stack(et) for fc, et in zip(future_covariates, encoded_times)]\n\n        return future_covariates\n\n    def _verify_train_dataset_type(self, train_dataset: TrainingDataset):\n",
        "source_code_len": 2436,
        "target_code": "\n    def _verify_train_dataset_type(self, train_dataset: TrainingDataset):\n",
        "target_code_len": 75,
        "diff_format": "@@ -777,56 +783,2 @@\n \n-    def _add_cyclic_encoder(self,\n-                            target: Sequence[TimeSeries],\n-                            future_covariates: Optional[Sequence[TimeSeries]] = None,\n-                            n: Optional[int] = None) -> Sequence[TimeSeries]:\n-        \"\"\"adds cyclic encoding of time index to future covariates.\n-        For training (when `n` is `None`) we can simply use the future covariates (if available) or target as\n-        reference to extract the time index.\n-        For prediction (`n` is given) we have to distinguish between two cases:\n-            1)\n-                if future covariates are given, we can use them as reference\n-            2)\n-                if future covariates are missing, we need to generate a time index that starts `input_chunk_length`\n-                before the end of `target` and ends `max(n, output_chunk_length)` after the end of `target`\n-\n-        Parameters\n-        ----------\n-        target\n-            past target TimeSeries\n-        future_covariates\n-            future covariates TimeSeries\n-        n\n-            prediciton length (only given for predictions)\n-\n-        Returns\n-        -------\n-        Sequence[TimeSeries]\n-            future covariates including cyclic encoded time index\n-        \"\"\"\n-\n-        if n is None:  # training\n-            encode_ts = future_covariates if future_covariates is not None else target\n-        else:  # prediction\n-            if future_covariates is not None:\n-                encode_ts = future_covariates\n-            else:\n-                encode_ts = [_generate_index(start=ts.end_time() - ts.freq * (self.input_chunk_length - 1),\n-                                             length=self.input_chunk_length + max(n, self.output_chunk_length),\n-                                             freq=ts.freq) for ts in target]\n-\n-        encoded_times = [\n-            datetime_attribute_timeseries(ts, \n-                                          attribute=self.add_cyclic_encoder, \n-                                          cyclic=True, \n-                                          dtype=target[0].dtype) \n-            for ts in encode_ts\n-        ]\n-\n-        if future_covariates is None:\n-            future_covariates = encoded_times\n-        else:\n-            future_covariates = [fc.stack(et) for fc, et in zip(future_covariates, encoded_times)]\n-\n-        return future_covariates\n-\n     def _verify_train_dataset_type(self, train_dataset: TrainingDataset):\n",
        "source_code_with_indent": "\n    <DED>def _add_cyclic_encoder(self,\n                            target: Sequence[TimeSeries],\n                            future_covariates: Optional[Sequence[TimeSeries]] = None,\n                            n: Optional[int] = None) -> Sequence[TimeSeries]:\n        <IND>\"\"\"adds cyclic encoding of time index to future covariates.\n        For training (when `n` is `None`) we can simply use the future covariates (if available) or target as\n        reference to extract the time index.\n        For prediction (`n` is given) we have to distinguish between two cases:\n            1)\n                if future covariates are given, we can use them as reference\n            2)\n                if future covariates are missing, we need to generate a time index that starts `input_chunk_length`\n                before the end of `target` and ends `max(n, output_chunk_length)` after the end of `target`\n\n        Parameters\n        ----------\n        target\n            past target TimeSeries\n        future_covariates\n            future covariates TimeSeries\n        n\n            prediciton length (only given for predictions)\n\n        Returns\n        -------\n        Sequence[TimeSeries]\n            future covariates including cyclic encoded time index\n        \"\"\"\n\n        if n is None:  # training\n            <IND>encode_ts = future_covariates if future_covariates is not None else target\n        <DED>else:  # prediction\n            <IND>if future_covariates is not None:\n                <IND>encode_ts = future_covariates\n            <DED>else:\n                <IND>encode_ts = [_generate_index(start=ts.end_time() - ts.freq * (self.input_chunk_length - 1),\n                                             length=self.input_chunk_length + max(n, self.output_chunk_length),\n                                             freq=ts.freq) for ts in target]\n\n        <DED><DED>encoded_times = [\n            datetime_attribute_timeseries(ts, \n                                          attribute=self.add_cyclic_encoder, \n                                          cyclic=True, \n                                          dtype=target[0].dtype) \n            for ts in encode_ts\n        ]\n\n        if future_covariates is None:\n            <IND>future_covariates = encoded_times\n        <DED>else:\n            <IND>future_covariates = [fc.stack(et) for fc, et in zip(future_covariates, encoded_times)]\n\n        <DED>return future_covariates\n\n    <DED>def _verify_train_dataset_type(self, train_dataset: TrainingDataset):\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def _verify_train_dataset_type(self, train_dataset: TrainingDataset):\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "                                 future_covariates: Optional[Sequence[TimeSeries]]) -> MixedCovariatesInferenceDataset:\n\n        if self.add_cyclic_encoder is not None:\n            future_covariates = self._add_cyclic_encoder(target, future_covariates=future_covariates, n=n)\n\n",
        "source_code_len": 277,
        "target_code": "                                 future_covariates: Optional[Sequence[TimeSeries]]) -> MixedCovariatesInferenceDataset:\n\n",
        "target_code_len": 121,
        "diff_format": "@@ -840,5 +792,2 @@\n                                  future_covariates: Optional[Sequence[TimeSeries]]) -> MixedCovariatesInferenceDataset:\n-\n-        if self.add_cyclic_encoder is not None:\n-            future_covariates = self._add_cyclic_encoder(target, future_covariates=future_covariates, n=n)\n \n",
        "source_code_with_indent": "                                 future_covariates: Optional[Sequence[TimeSeries]]) -> MixedCovariatesInferenceDataset:\n\n        <IND>if self.add_cyclic_encoder is not None:\n            <IND>future_covariates = self._add_cyclic_encoder(target, future_covariates=future_covariates, n=n)\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                                 future_covariates: Optional[Sequence[TimeSeries]]) -> MixedCovariatesInferenceDataset:\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "unit8co/darts",
    "commit": "c2f5a498aa5873177dd39010b5f678c44055fc14",
    "filename": "darts/utils/data/inference_dataset.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/unit8co-darts/darts/utils/data/inference_dataset.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "darts/utils/data/inference_dataset.py:160:75 Incompatible parameter type [6]: Expected `typing.Sized` for 1st positional only parameter to call `len` but got `Optional[Sequence[TimeSeries]]`.",
    "message": " Expected `typing.Sized` for 1st positional only parameter to call `len` but got `Optional[Sequence[TimeSeries]]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 160,
    "warning_line": "        raise_if_not((covariates is None or len(self.target_series) == len(self.covariates)),"
  }
]