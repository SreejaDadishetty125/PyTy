[
  {
    "project": "allenai/allennlp",
    "commit": "8cb07b261c4825e56ec0925f05412fca70d19a57",
    "filename": "allennlp/commands/fine_tune.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/commands/fine_tune.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/commands/fine_tune.py:174:4 Incompatible variable type [9]: cache_directory is declared to have type `str` but is used as type `None`.",
    "message": " cache_directory is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 174,
    "warning_line": "    cache_directory: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "    force: bool = False,\n    cache_directory: str = None,\n    cache_prefix: str = None,\n    batch_weight_key: str = \"\",\n",
        "source_code_len": 120,
        "target_code": "    force: bool = False,\n    batch_weight_key: str = \"\",\n",
        "target_code_len": 57,
        "diff_format": "@@ -173,4 +173,2 @@\n     force: bool = False,\n-    cache_directory: str = None,\n-    cache_prefix: str = None,\n     batch_weight_key: str = \"\",\n",
        "source_code_with_indent": "    force: bool = False,\n    cache_directory: str = None,\n    cache_prefix: str = None,\n    batch_weight_key: str = \"\",\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    force: bool = False,\n    batch_weight_key: str = \"\",\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        force=force,\n        cache_directory=cache_directory,\n        cache_prefix=cache_prefix,\n        batch_weight_key=batch_weight_key,\n",
        "source_code_len": 140,
        "target_code": "        force=force,\n        batch_weight_key=batch_weight_key,\n",
        "target_code_len": 64,
        "diff_format": "@@ -226,4 +220,2 @@\n         force=force,\n-        cache_directory=cache_directory,\n-        cache_prefix=cache_prefix,\n         batch_weight_key=batch_weight_key,\n",
        "source_code_with_indent": "        force=force,\n        cache_directory=cache_directory,\n        cache_prefix=cache_prefix,\n        batch_weight_key=batch_weight_key,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        force=force,\n        batch_weight_key=batch_weight_key,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "8cb07b261c4825e56ec0925f05412fca70d19a57",
    "filename": "allennlp/commands/fine_tune.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/commands/fine_tune.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/commands/fine_tune.py:175:4 Incompatible variable type [9]: cache_prefix is declared to have type `str` but is used as type `None`.",
    "message": " cache_prefix is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 175,
    "warning_line": "    cache_prefix: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "    force: bool = False,\n    cache_directory: str = None,\n    cache_prefix: str = None,\n    batch_weight_key: str = \"\",\n",
        "source_code_len": 120,
        "target_code": "    force: bool = False,\n    batch_weight_key: str = \"\",\n",
        "target_code_len": 57,
        "diff_format": "@@ -173,4 +173,2 @@\n     force: bool = False,\n-    cache_directory: str = None,\n-    cache_prefix: str = None,\n     batch_weight_key: str = \"\",\n",
        "source_code_with_indent": "    force: bool = False,\n    cache_directory: str = None,\n    cache_prefix: str = None,\n    batch_weight_key: str = \"\",\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    force: bool = False,\n    batch_weight_key: str = \"\",\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        force=force,\n        cache_directory=cache_directory,\n        cache_prefix=cache_prefix,\n        batch_weight_key=batch_weight_key,\n",
        "source_code_len": 140,
        "target_code": "        force=force,\n        batch_weight_key=batch_weight_key,\n",
        "target_code_len": 64,
        "diff_format": "@@ -226,4 +220,2 @@\n         force=force,\n-        cache_directory=cache_directory,\n-        cache_prefix=cache_prefix,\n         batch_weight_key=batch_weight_key,\n",
        "source_code_with_indent": "        force=force,\n        cache_directory=cache_directory,\n        cache_prefix=cache_prefix,\n        batch_weight_key=batch_weight_key,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        force=force,\n        batch_weight_key=batch_weight_key,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "8cb07b261c4825e56ec0925f05412fca70d19a57",
    "filename": "allennlp/commands/train.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/commands/train.py",
    "file_hunks_size": 15,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/commands/train.py:174:4 Incompatible variable type [9]: cache_directory is declared to have type `str` but is used as type `None`.",
    "message": " cache_directory is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 174,
    "warning_line": "    cache_directory: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        force=args.force,\n        cache_directory=args.cache_directory,\n        cache_prefix=args.cache_prefix,\n        node_rank=args.node_rank,\n",
        "source_code_len": 146,
        "target_code": "        force=args.force,\n        node_rank=args.node_rank,\n",
        "target_code_len": 60,
        "diff_format": "@@ -159,4 +136,2 @@\n         force=args.force,\n-        cache_directory=args.cache_directory,\n-        cache_prefix=args.cache_prefix,\n         node_rank=args.node_rank,\n",
        "source_code_with_indent": "        force=args.force,\n        cache_directory=args.cache_directory,\n        cache_prefix=args.cache_prefix,\n        node_rank=args.node_rank,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        force=args.force,\n        node_rank=args.node_rank,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "    force: bool = False,\n    cache_directory: str = None,\n    cache_prefix: str = None,\n    node_rank: int = 0,\n",
        "source_code_len": 112,
        "target_code": "    force: bool = False,\n    node_rank: int = 0,\n",
        "target_code_len": 49,
        "diff_format": "@@ -173,4 +148,2 @@\n     force: bool = False,\n-    cache_directory: str = None,\n-    cache_prefix: str = None,\n     node_rank: int = 0,\n",
        "source_code_with_indent": "    force: bool = False,\n    cache_directory: str = None,\n    cache_prefix: str = None,\n    node_rank: int = 0,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    force: bool = False,\n    node_rank: int = 0,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        force=force,\n        cache_directory=cache_directory,\n        cache_prefix=cache_prefix,\n        node_rank=node_rank,\n",
        "source_code_len": 126,
        "target_code": "        force=force,\n        node_rank=node_rank,\n",
        "target_code_len": 50,
        "diff_format": "@@ -216,4 +185,2 @@\n         force=force,\n-        cache_directory=cache_directory,\n-        cache_prefix=cache_prefix,\n         node_rank=node_rank,\n",
        "source_code_with_indent": "        force=force,\n        cache_directory=cache_directory,\n        cache_prefix=cache_prefix,\n        node_rank=node_rank,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        force=force,\n        node_rank=node_rank,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "8cb07b261c4825e56ec0925f05412fca70d19a57",
    "filename": "allennlp/commands/train.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/commands/train.py",
    "file_hunks_size": 15,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/commands/train.py:175:4 Incompatible variable type [9]: cache_prefix is declared to have type `str` but is used as type `None`.",
    "message": " cache_prefix is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 175,
    "warning_line": "    cache_prefix: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        force=args.force,\n        cache_directory=args.cache_directory,\n        cache_prefix=args.cache_prefix,\n        node_rank=args.node_rank,\n",
        "source_code_len": 146,
        "target_code": "        force=args.force,\n        node_rank=args.node_rank,\n",
        "target_code_len": 60,
        "diff_format": "@@ -159,4 +136,2 @@\n         force=args.force,\n-        cache_directory=args.cache_directory,\n-        cache_prefix=args.cache_prefix,\n         node_rank=args.node_rank,\n",
        "source_code_with_indent": "        force=args.force,\n        cache_directory=args.cache_directory,\n        cache_prefix=args.cache_prefix,\n        node_rank=args.node_rank,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        force=args.force,\n        node_rank=args.node_rank,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "    force: bool = False,\n    cache_directory: str = None,\n    cache_prefix: str = None,\n    node_rank: int = 0,\n",
        "source_code_len": 112,
        "target_code": "    force: bool = False,\n    node_rank: int = 0,\n",
        "target_code_len": 49,
        "diff_format": "@@ -173,4 +148,2 @@\n     force: bool = False,\n-    cache_directory: str = None,\n-    cache_prefix: str = None,\n     node_rank: int = 0,\n",
        "source_code_with_indent": "    force: bool = False,\n    cache_directory: str = None,\n    cache_prefix: str = None,\n    node_rank: int = 0,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    force: bool = False,\n    node_rank: int = 0,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        force=force,\n        cache_directory=cache_directory,\n        cache_prefix=cache_prefix,\n        node_rank=node_rank,\n",
        "source_code_len": 126,
        "target_code": "        force=force,\n        node_rank=node_rank,\n",
        "target_code_len": 50,
        "diff_format": "@@ -216,4 +185,2 @@\n         force=force,\n-        cache_directory=cache_directory,\n-        cache_prefix=cache_prefix,\n         node_rank=node_rank,\n",
        "source_code_with_indent": "        force=force,\n        cache_directory=cache_directory,\n        cache_prefix=cache_prefix,\n        node_rank=node_rank,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        force=force,\n        node_rank=node_rank,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "8cb07b261c4825e56ec0925f05412fca70d19a57",
    "filename": "allennlp/commands/train.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/commands/train.py",
    "file_hunks_size": 15,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/commands/train.py:230:4 Incompatible variable type [9]: cache_directory is declared to have type `str` but is used as type `None`.",
    "message": " cache_directory is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 230,
    "warning_line": "    cache_directory: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        force=force,\n        cache_directory=cache_directory,\n        cache_prefix=cache_prefix,\n        node_rank=node_rank,\n",
        "source_code_len": 126,
        "target_code": "        force=force,\n        node_rank=node_rank,\n",
        "target_code_len": 50,
        "diff_format": "@@ -216,4 +185,2 @@\n         force=force,\n-        cache_directory=cache_directory,\n-        cache_prefix=cache_prefix,\n         node_rank=node_rank,\n",
        "source_code_with_indent": "        force=force,\n        cache_directory=cache_directory,\n        cache_prefix=cache_prefix,\n        node_rank=node_rank,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        force=force,\n        node_rank=node_rank,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "    force: bool = False,\n    cache_directory: str = None,\n    cache_prefix: str = None,\n    node_rank: int = 0,\n",
        "source_code_len": 112,
        "target_code": "    force: bool = False,\n    node_rank: int = 0,\n",
        "target_code_len": 49,
        "diff_format": "@@ -229,4 +196,2 @@\n     force: bool = False,\n-    cache_directory: str = None,\n-    cache_prefix: str = None,\n     node_rank: int = 0,\n",
        "source_code_with_indent": "    force: bool = False,\n    cache_directory: str = None,\n    cache_prefix: str = None,\n    node_rank: int = 0,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    force: bool = False,\n    node_rank: int = 0,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "            recover=recover,\n            cache_directory=cache_directory,\n            cache_prefix=cache_prefix,\n            include_package=include_package,\n",
        "source_code_len": 158,
        "target_code": "            recover=recover,\n            include_package=include_package,\n",
        "target_code_len": 74,
        "diff_format": "@@ -295,4 +256,2 @@\n             recover=recover,\n-            cache_directory=cache_directory,\n-            cache_prefix=cache_prefix,\n             include_package=include_package,\n",
        "source_code_with_indent": "            recover=recover,\n            cache_directory=cache_directory,\n            cache_prefix=cache_prefix,\n            include_package=include_package,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "            recover=recover,\n            include_package=include_package,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "                recover,\n                cache_directory,\n                cache_prefix,\n                include_package,\n",
        "source_code_len": 121,
        "target_code": "                recover,\n                include_package,\n",
        "target_code_len": 58,
        "diff_format": "@@ -349,4 +308,2 @@\n                 recover,\n-                cache_directory,\n-                cache_prefix,\n                 include_package,\n",
        "source_code_with_indent": "                recover,\n                cache_directory,\n                cache_prefix,\n                include_package,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                recover,\n                include_package,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "8cb07b261c4825e56ec0925f05412fca70d19a57",
    "filename": "allennlp/commands/train.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/commands/train.py",
    "file_hunks_size": 15,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/commands/train.py:231:4 Incompatible variable type [9]: cache_prefix is declared to have type `str` but is used as type `None`.",
    "message": " cache_prefix is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 231,
    "warning_line": "    cache_prefix: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        force=force,\n        cache_directory=cache_directory,\n        cache_prefix=cache_prefix,\n        node_rank=node_rank,\n",
        "source_code_len": 126,
        "target_code": "        force=force,\n        node_rank=node_rank,\n",
        "target_code_len": 50,
        "diff_format": "@@ -216,4 +185,2 @@\n         force=force,\n-        cache_directory=cache_directory,\n-        cache_prefix=cache_prefix,\n         node_rank=node_rank,\n",
        "source_code_with_indent": "        force=force,\n        cache_directory=cache_directory,\n        cache_prefix=cache_prefix,\n        node_rank=node_rank,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        force=force,\n        node_rank=node_rank,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "    force: bool = False,\n    cache_directory: str = None,\n    cache_prefix: str = None,\n    node_rank: int = 0,\n",
        "source_code_len": 112,
        "target_code": "    force: bool = False,\n    node_rank: int = 0,\n",
        "target_code_len": 49,
        "diff_format": "@@ -229,4 +196,2 @@\n     force: bool = False,\n-    cache_directory: str = None,\n-    cache_prefix: str = None,\n     node_rank: int = 0,\n",
        "source_code_with_indent": "    force: bool = False,\n    cache_directory: str = None,\n    cache_prefix: str = None,\n    node_rank: int = 0,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    force: bool = False,\n    node_rank: int = 0,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "            recover=recover,\n            cache_directory=cache_directory,\n            cache_prefix=cache_prefix,\n            include_package=include_package,\n",
        "source_code_len": 158,
        "target_code": "            recover=recover,\n            include_package=include_package,\n",
        "target_code_len": 74,
        "diff_format": "@@ -295,4 +256,2 @@\n             recover=recover,\n-            cache_directory=cache_directory,\n-            cache_prefix=cache_prefix,\n             include_package=include_package,\n",
        "source_code_with_indent": "            recover=recover,\n            cache_directory=cache_directory,\n            cache_prefix=cache_prefix,\n            include_package=include_package,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "            recover=recover,\n            include_package=include_package,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "                recover,\n                cache_directory,\n                cache_prefix,\n                include_package,\n",
        "source_code_len": 121,
        "target_code": "                recover,\n                include_package,\n",
        "target_code_len": 58,
        "diff_format": "@@ -349,4 +308,2 @@\n                 recover,\n-                cache_directory,\n-                cache_prefix,\n                 include_package,\n",
        "source_code_with_indent": "                recover,\n                cache_directory,\n                cache_prefix,\n                include_package,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                recover,\n                include_package,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "8cb07b261c4825e56ec0925f05412fca70d19a57",
    "filename": "allennlp/commands/train.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/commands/train.py",
    "file_hunks_size": 15,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/commands/train.py:376:4 Incompatible variable type [9]: cache_directory is declared to have type `str` but is used as type `None`.",
    "message": " cache_directory is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 376,
    "warning_line": "    cache_directory: str = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "8cb07b261c4825e56ec0925f05412fca70d19a57",
    "filename": "allennlp/commands/train.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/commands/train.py",
    "file_hunks_size": 15,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/commands/train.py:377:4 Incompatible variable type [9]: cache_prefix is declared to have type `str` but is used as type `None`.",
    "message": " cache_prefix is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 377,
    "warning_line": "    cache_prefix: str = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "8cb07b261c4825e56ec0925f05412fca70d19a57",
    "filename": "allennlp/training/callback_trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/callback_trainer.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/callback_trainer.py:303:8 Incompatible variable type [9]: cache_directory is declared to have type `str` but is used as type `None`.",
    "message": " cache_directory is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 303,
    "warning_line": "        cache_directory: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    def from_params(  # type: ignore\n        cls,\n        params: Params,\n        serialization_dir: str,\n        recover: bool = False,\n        cache_directory: str = None,\n        cache_prefix: str = None,\n    ) -> \"CallbackTrainer\":\n        pieces = TrainerPieces.from_params(\n            params, serialization_dir, recover, cache_directory, cache_prefix\n        )\n        model = pieces.model\n",
        "source_code_len": 397,
        "target_code": "    def from_params(  # type: ignore\n        cls, params: Params, serialization_dir: str, recover: bool = False,\n    ) -> \"CallbackTrainer\":\n        pieces = TrainerPieces.from_params(params, serialization_dir, recover)\n        model = pieces.model\n",
        "target_code_len": 249,
        "diff_format": "@@ -298,12 +298,5 @@\n     def from_params(  # type: ignore\n-        cls,\n-        params: Params,\n-        serialization_dir: str,\n-        recover: bool = False,\n-        cache_directory: str = None,\n-        cache_prefix: str = None,\n+        cls, params: Params, serialization_dir: str, recover: bool = False,\n     ) -> \"CallbackTrainer\":\n-        pieces = TrainerPieces.from_params(\n-            params, serialization_dir, recover, cache_directory, cache_prefix\n-        )\n+        pieces = TrainerPieces.from_params(params, serialization_dir, recover)\n         model = pieces.model\n",
        "source_code_with_indent": "    def from_params(  # type: ignore\n        cls,\n        params: Params,\n        serialization_dir: str,\n        recover: bool = False,\n        cache_directory: str = None,\n        cache_prefix: str = None,\n    ) -> \"CallbackTrainer\":\n        <IND>pieces = TrainerPieces.from_params(\n            params, serialization_dir, recover, cache_directory, cache_prefix\n        )\n        model = pieces.model\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    def from_params(  # type: ignore\n        cls, params: Params, serialization_dir: str, recover: bool = False,\n    ) -> \"CallbackTrainer\":\n        <IND>pieces = TrainerPieces.from_params(params, serialization_dir, recover)\n        model = pieces.model\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "8cb07b261c4825e56ec0925f05412fca70d19a57",
    "filename": "allennlp/training/callback_trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/callback_trainer.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/callback_trainer.py:304:8 Incompatible variable type [9]: cache_prefix is declared to have type `str` but is used as type `None`.",
    "message": " cache_prefix is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 304,
    "warning_line": "        cache_prefix: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    def from_params(  # type: ignore\n        cls,\n        params: Params,\n        serialization_dir: str,\n        recover: bool = False,\n        cache_directory: str = None,\n        cache_prefix: str = None,\n    ) -> \"CallbackTrainer\":\n        pieces = TrainerPieces.from_params(\n            params, serialization_dir, recover, cache_directory, cache_prefix\n        )\n        model = pieces.model\n",
        "source_code_len": 397,
        "target_code": "    def from_params(  # type: ignore\n        cls, params: Params, serialization_dir: str, recover: bool = False,\n    ) -> \"CallbackTrainer\":\n        pieces = TrainerPieces.from_params(params, serialization_dir, recover)\n        model = pieces.model\n",
        "target_code_len": 249,
        "diff_format": "@@ -298,12 +298,5 @@\n     def from_params(  # type: ignore\n-        cls,\n-        params: Params,\n-        serialization_dir: str,\n-        recover: bool = False,\n-        cache_directory: str = None,\n-        cache_prefix: str = None,\n+        cls, params: Params, serialization_dir: str, recover: bool = False,\n     ) -> \"CallbackTrainer\":\n-        pieces = TrainerPieces.from_params(\n-            params, serialization_dir, recover, cache_directory, cache_prefix\n-        )\n+        pieces = TrainerPieces.from_params(params, serialization_dir, recover)\n         model = pieces.model\n",
        "source_code_with_indent": "    def from_params(  # type: ignore\n        cls,\n        params: Params,\n        serialization_dir: str,\n        recover: bool = False,\n        cache_directory: str = None,\n        cache_prefix: str = None,\n    ) -> \"CallbackTrainer\":\n        <IND>pieces = TrainerPieces.from_params(\n            params, serialization_dir, recover, cache_directory, cache_prefix\n        )\n        model = pieces.model\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    def from_params(  # type: ignore\n        cls, params: Params, serialization_dir: str, recover: bool = False,\n    ) -> \"CallbackTrainer\":\n        <IND>pieces = TrainerPieces.from_params(params, serialization_dir, recover)\n        model = pieces.model\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "8cb07b261c4825e56ec0925f05412fca70d19a57",
    "filename": "allennlp/training/trainer_base.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer_base.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer_base.py:83:8 Incompatible variable type [9]: cache_directory is declared to have type `str` but is used as type `None`.",
    "message": " cache_directory is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 83,
    "warning_line": "        cache_directory: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    def from_params(  # type: ignore\n        cls,\n        params: Params,\n        serialization_dir: str,\n        recover: bool = False,\n        cache_directory: str = None,\n        cache_prefix: str = None,\n    ):\n",
        "source_code_len": 215,
        "target_code": "    def from_params(  # type: ignore\n        cls, params: Params, serialization_dir: str, recover: bool = False,\n    ):\n",
        "target_code_len": 120,
        "diff_format": "@@ -78,8 +78,3 @@\n     def from_params(  # type: ignore\n-        cls,\n-        params: Params,\n-        serialization_dir: str,\n-        recover: bool = False,\n-        cache_directory: str = None,\n-        cache_prefix: str = None,\n+        cls, params: Params, serialization_dir: str, recover: bool = False,\n     ):\n",
        "source_code_with_indent": "    def from_params(  # type: ignore\n        cls,\n        params: Params,\n        serialization_dir: str,\n        recover: bool = False,\n        cache_directory: str = None,\n        cache_prefix: str = None,\n    ):\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    def from_params(  # type: ignore\n        cls, params: Params, serialization_dir: str, recover: bool = False,\n    ):\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n            pieces = TrainerPieces.from_params(\n                params, serialization_dir, recover, cache_directory, cache_prefix\n            )\n            return Trainer.from_params(\n",
        "source_code_len": 185,
        "target_code": "\n            pieces = TrainerPieces.from_params(params, serialization_dir, recover)\n            return Trainer.from_params(\n",
        "target_code_len": 124,
        "diff_format": "@@ -93,5 +88,3 @@\n \n-            pieces = TrainerPieces.from_params(\n-                params, serialization_dir, recover, cache_directory, cache_prefix\n-            )\n+            pieces = TrainerPieces.from_params(params, serialization_dir, recover)\n             return Trainer.from_params(\n",
        "source_code_with_indent": "\n            pieces = TrainerPieces.from_params(\n                params, serialization_dir, recover, cache_directory, cache_prefix\n            )\n            return Trainer.from_params(\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n            pieces = TrainerPieces.from_params(params, serialization_dir, recover)\n            return Trainer.from_params(\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "            assert is_overriden, f\"Class {klass.__name__} must override `from_params`.\"\n            return klass.from_params(\n                params, serialization_dir, recover, cache_directory, cache_prefix\n            )\n",
        "source_code_len": 222,
        "target_code": "            assert is_overriden, f\"Class {klass.__name__} must override `from_params`.\"\n            return klass.from_params(params, serialization_dir, recover)\n",
        "target_code_len": 161,
        "diff_format": "@@ -112,4 +105,2 @@\n             assert is_overriden, f\"Class {klass.__name__} must override `from_params`.\"\n-            return klass.from_params(\n-                params, serialization_dir, recover, cache_directory, cache_prefix\n-            )\n+            return klass.from_params(params, serialization_dir, recover)\n",
        "source_code_with_indent": "            assert is_overriden, f\"Class {klass.__name__} must override `from_params`.\"\n            return klass.from_params(\n                params, serialization_dir, recover, cache_directory, cache_prefix\n            )\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "            assert is_overriden, f\"Class {klass.__name__} must override `from_params`.\"\n            return klass.from_params(params, serialization_dir, recover)\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "8cb07b261c4825e56ec0925f05412fca70d19a57",
    "filename": "allennlp/training/trainer_base.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer_base.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer_base.py:84:8 Incompatible variable type [9]: cache_prefix is declared to have type `str` but is used as type `None`.",
    "message": " cache_prefix is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 84,
    "warning_line": "        cache_prefix: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    def from_params(  # type: ignore\n        cls,\n        params: Params,\n        serialization_dir: str,\n        recover: bool = False,\n        cache_directory: str = None,\n        cache_prefix: str = None,\n    ):\n",
        "source_code_len": 215,
        "target_code": "    def from_params(  # type: ignore\n        cls, params: Params, serialization_dir: str, recover: bool = False,\n    ):\n",
        "target_code_len": 120,
        "diff_format": "@@ -78,8 +78,3 @@\n     def from_params(  # type: ignore\n-        cls,\n-        params: Params,\n-        serialization_dir: str,\n-        recover: bool = False,\n-        cache_directory: str = None,\n-        cache_prefix: str = None,\n+        cls, params: Params, serialization_dir: str, recover: bool = False,\n     ):\n",
        "source_code_with_indent": "    def from_params(  # type: ignore\n        cls,\n        params: Params,\n        serialization_dir: str,\n        recover: bool = False,\n        cache_directory: str = None,\n        cache_prefix: str = None,\n    ):\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    def from_params(  # type: ignore\n        cls, params: Params, serialization_dir: str, recover: bool = False,\n    ):\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n            pieces = TrainerPieces.from_params(\n                params, serialization_dir, recover, cache_directory, cache_prefix\n            )\n            return Trainer.from_params(\n",
        "source_code_len": 185,
        "target_code": "\n            pieces = TrainerPieces.from_params(params, serialization_dir, recover)\n            return Trainer.from_params(\n",
        "target_code_len": 124,
        "diff_format": "@@ -93,5 +88,3 @@\n \n-            pieces = TrainerPieces.from_params(\n-                params, serialization_dir, recover, cache_directory, cache_prefix\n-            )\n+            pieces = TrainerPieces.from_params(params, serialization_dir, recover)\n             return Trainer.from_params(\n",
        "source_code_with_indent": "\n            pieces = TrainerPieces.from_params(\n                params, serialization_dir, recover, cache_directory, cache_prefix\n            )\n            return Trainer.from_params(\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n            pieces = TrainerPieces.from_params(params, serialization_dir, recover)\n            return Trainer.from_params(\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "            assert is_overriden, f\"Class {klass.__name__} must override `from_params`.\"\n            return klass.from_params(\n                params, serialization_dir, recover, cache_directory, cache_prefix\n            )\n",
        "source_code_len": 222,
        "target_code": "            assert is_overriden, f\"Class {klass.__name__} must override `from_params`.\"\n            return klass.from_params(params, serialization_dir, recover)\n",
        "target_code_len": 161,
        "diff_format": "@@ -112,4 +105,2 @@\n             assert is_overriden, f\"Class {klass.__name__} must override `from_params`.\"\n-            return klass.from_params(\n-                params, serialization_dir, recover, cache_directory, cache_prefix\n-            )\n+            return klass.from_params(params, serialization_dir, recover)\n",
        "source_code_with_indent": "            assert is_overriden, f\"Class {klass.__name__} must override `from_params`.\"\n            return klass.from_params(\n                params, serialization_dir, recover, cache_directory, cache_prefix\n            )\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "            assert is_overriden, f\"Class {klass.__name__} must override `from_params`.\"\n            return klass.from_params(params, serialization_dir, recover)\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "8cb07b261c4825e56ec0925f05412fca70d19a57",
    "filename": "allennlp/training/trainer_pieces.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer_pieces.py",
    "file_hunks_size": 2,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer_pieces.py:42:8 Incompatible variable type [9]: cache_directory is declared to have type `str` but is used as type `None`.",
    "message": " cache_directory is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 42,
    "warning_line": "        cache_directory: str = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "8cb07b261c4825e56ec0925f05412fca70d19a57",
    "filename": "allennlp/training/trainer_pieces.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer_pieces.py",
    "file_hunks_size": 2,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer_pieces.py:43:8 Incompatible variable type [9]: cache_prefix is declared to have type `str` but is used as type `None`.",
    "message": " cache_prefix is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 43,
    "warning_line": "        cache_prefix: str = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "8cb07b261c4825e56ec0925f05412fca70d19a57",
    "filename": "allennlp/training/util.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/util.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/util.py:132:20 Incompatible variable type [9]: cache_directory is declared to have type `str` but is used as type `None`.",
    "message": " cache_directory is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 132,
    "warning_line": "    params: Params, cache_directory: str = None, cache_prefix: str = None",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\ndef datasets_from_params(\n    params: Params, cache_directory: str = None, cache_prefix: str = None\n) -> Dict[str, Iterable[Instance]]:\n    \"\"\"\n",
        "source_code_len": 145,
        "target_code": "\ndef datasets_from_params(params: Params) -> Dict[str, Iterable[Instance]]:\n    \"\"\"\n",
        "target_code_len": 84,
        "diff_format": "@@ -130,5 +128,3 @@\n \n-def datasets_from_params(\n-    params: Params, cache_directory: str = None, cache_prefix: str = None\n-) -> Dict[str, Iterable[Instance]]:\n+def datasets_from_params(params: Params) -> Dict[str, Iterable[Instance]]:\n     \"\"\"\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n<DED>def datasets_from_params(\n    params: Params, cache_directory: str = None, cache_prefix: str = None\n) -> Dict[str, Iterable[Instance]]:\n    <IND>",
        "target_code_with_indent": "\n<DED>def datasets_from_params(params: Params) -> Dict[str, Iterable[Instance]]:\n    <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n    train_cache_dir, validation_cache_dir = _set_up_cache_files(\n        dataset_reader_params, validation_dataset_reader_params, cache_directory, cache_prefix\n    )\n\n",
        "source_code_len": 252,
        "target_code": "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n\n",
        "target_code_len": 86,
        "diff_format": "@@ -162,5 +158,2 @@\n     validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n-    train_cache_dir, validation_cache_dir = _set_up_cache_files(\n-        dataset_reader_params, validation_dataset_reader_params, cache_directory, cache_prefix\n-    )\n \n",
        "source_code_with_indent": "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n    train_cache_dir, validation_cache_dir = _set_up_cache_files(\n        dataset_reader_params, validation_dataset_reader_params, cache_directory, cache_prefix\n    )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\n    if train_cache_dir:\n        dataset_reader.cache_data(train_cache_dir)\n        validation_and_test_dataset_reader.cache_data(validation_cache_dir)\n\n    train_data_path = params.pop(\"train_data_path\")\n",
        "source_code_len": 205,
        "target_code": "\n    train_data_path = params.pop(\"train_data_path\")\n",
        "target_code_len": 53,
        "diff_format": "@@ -175,6 +168,2 @@\n \n-    if train_cache_dir:\n-        dataset_reader.cache_data(train_cache_dir)\n-        validation_and_test_dataset_reader.cache_data(validation_cache_dir)\n-\n     train_data_path = params.pop(\"train_data_path\")\n",
        "source_code_with_indent": "\n    <DED>if train_cache_dir:\n        <IND>dataset_reader.cache_data(train_cache_dir)\n        validation_and_test_dataset_reader.cache_data(validation_cache_dir)\n\n    <DED>train_data_path = params.pop(\"train_data_path\")\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>train_data_path = params.pop(\"train_data_path\")\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "8cb07b261c4825e56ec0925f05412fca70d19a57",
    "filename": "allennlp/training/util.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/util.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/util.py:132:49 Incompatible variable type [9]: cache_prefix is declared to have type `str` but is used as type `None`.",
    "message": " cache_prefix is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 132,
    "warning_line": "    params: Params, cache_directory: str = None, cache_prefix: str = None",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\ndef datasets_from_params(\n    params: Params, cache_directory: str = None, cache_prefix: str = None\n) -> Dict[str, Iterable[Instance]]:\n    \"\"\"\n",
        "source_code_len": 145,
        "target_code": "\ndef datasets_from_params(params: Params) -> Dict[str, Iterable[Instance]]:\n    \"\"\"\n",
        "target_code_len": 84,
        "diff_format": "@@ -130,5 +128,3 @@\n \n-def datasets_from_params(\n-    params: Params, cache_directory: str = None, cache_prefix: str = None\n-) -> Dict[str, Iterable[Instance]]:\n+def datasets_from_params(params: Params) -> Dict[str, Iterable[Instance]]:\n     \"\"\"\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n<DED>def datasets_from_params(\n    params: Params, cache_directory: str = None, cache_prefix: str = None\n) -> Dict[str, Iterable[Instance]]:\n    <IND>",
        "target_code_with_indent": "\n<DED>def datasets_from_params(params: Params) -> Dict[str, Iterable[Instance]]:\n    <IND>"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n    train_cache_dir, validation_cache_dir = _set_up_cache_files(\n        dataset_reader_params, validation_dataset_reader_params, cache_directory, cache_prefix\n    )\n\n",
        "source_code_len": 252,
        "target_code": "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n\n",
        "target_code_len": 86,
        "diff_format": "@@ -162,5 +158,2 @@\n     validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n-    train_cache_dir, validation_cache_dir = _set_up_cache_files(\n-        dataset_reader_params, validation_dataset_reader_params, cache_directory, cache_prefix\n-    )\n \n",
        "source_code_with_indent": "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n    train_cache_dir, validation_cache_dir = _set_up_cache_files(\n        dataset_reader_params, validation_dataset_reader_params, cache_directory, cache_prefix\n    )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\n    if train_cache_dir:\n        dataset_reader.cache_data(train_cache_dir)\n        validation_and_test_dataset_reader.cache_data(validation_cache_dir)\n\n    train_data_path = params.pop(\"train_data_path\")\n",
        "source_code_len": 205,
        "target_code": "\n    train_data_path = params.pop(\"train_data_path\")\n",
        "target_code_len": 53,
        "diff_format": "@@ -175,6 +168,2 @@\n \n-    if train_cache_dir:\n-        dataset_reader.cache_data(train_cache_dir)\n-        validation_and_test_dataset_reader.cache_data(validation_cache_dir)\n-\n     train_data_path = params.pop(\"train_data_path\")\n",
        "source_code_with_indent": "\n    <DED>if train_cache_dir:\n        <IND>dataset_reader.cache_data(train_cache_dir)\n        validation_and_test_dataset_reader.cache_data(validation_cache_dir)\n\n    <DED>train_data_path = params.pop(\"train_data_path\")\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>train_data_path = params.pop(\"train_data_path\")\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "8cb07b261c4825e56ec0925f05412fca70d19a57",
    "filename": "allennlp/training/util.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/util.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/util.py:203:4 Incompatible variable type [9]: validation_params is declared to have type `Params` but is used as type `None`.",
    "message": " validation_params is declared to have type `Params` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 203,
    "warning_line": "    validation_params: Params = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n    train_cache_dir, validation_cache_dir = _set_up_cache_files(\n        dataset_reader_params, validation_dataset_reader_params, cache_directory, cache_prefix\n    )\n\n",
        "source_code_len": 252,
        "target_code": "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n\n",
        "target_code_len": 86,
        "diff_format": "@@ -162,5 +158,2 @@\n     validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n-    train_cache_dir, validation_cache_dir = _set_up_cache_files(\n-        dataset_reader_params, validation_dataset_reader_params, cache_directory, cache_prefix\n-    )\n \n",
        "source_code_with_indent": "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n    train_cache_dir, validation_cache_dir = _set_up_cache_files(\n        dataset_reader_params, validation_dataset_reader_params, cache_directory, cache_prefix\n    )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\n    if train_cache_dir:\n        dataset_reader.cache_data(train_cache_dir)\n        validation_and_test_dataset_reader.cache_data(validation_cache_dir)\n\n    train_data_path = params.pop(\"train_data_path\")\n",
        "source_code_len": 205,
        "target_code": "\n    train_data_path = params.pop(\"train_data_path\")\n",
        "target_code_len": 53,
        "diff_format": "@@ -175,6 +168,2 @@\n \n-    if train_cache_dir:\n-        dataset_reader.cache_data(train_cache_dir)\n-        validation_and_test_dataset_reader.cache_data(validation_cache_dir)\n-\n     train_data_path = params.pop(\"train_data_path\")\n",
        "source_code_with_indent": "\n    <DED>if train_cache_dir:\n        <IND>dataset_reader.cache_data(train_cache_dir)\n        validation_and_test_dataset_reader.cache_data(validation_cache_dir)\n\n    <DED>train_data_path = params.pop(\"train_data_path\")\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>train_data_path = params.pop(\"train_data_path\")\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "    return datasets\n\n\ndef _set_up_cache_files(\n    train_params: Params,\n    validation_params: Params = None,\n    cache_directory: str = None,\n    cache_prefix: str = None,\n) -> Tuple[str, str]:\n    if not cache_directory:\n        return None, None\n\n    # We need to compute the parameter hash before the parameters get destroyed when they're\n    # passed to `DatasetReader.from_params`.\n    if not cache_prefix:\n        cache_prefix = _dataset_reader_param_hash(train_params)\n        if validation_params:\n            validation_cache_prefix = _dataset_reader_param_hash(validation_params)\n        else:\n            validation_cache_prefix = cache_prefix\n    else:\n        validation_cache_prefix = cache_prefix\n\n    train_cache_dir = pathlib.Path(cache_directory) / cache_prefix\n    validation_cache_dir = pathlib.Path(cache_directory) / validation_cache_prefix\n\n    # For easy human inspection of what parameters were used to create the cache.  This will\n    # overwrite old files, but they should be identical.  This could bite someone who gave\n    # their own prefix instead of letting us compute it, and then _re-used_ that name with\n    # different parameters, without clearing the cache first.  But correctly handling that case\n    # is more work than it's worth.\n    os.makedirs(train_cache_dir, exist_ok=True)\n    with open(train_cache_dir / \"params.json\", \"w\") as param_file:\n        json.dump(train_params.as_dict(quiet=True), param_file)\n    os.makedirs(validation_cache_dir, exist_ok=True)\n    with open(validation_cache_dir / \"params.json\", \"w\") as param_file:\n        if validation_params:\n            json.dump(validation_params.as_dict(quiet=True), param_file)\n        else:\n            json.dump(train_params.as_dict(quiet=True), param_file)\n    return str(train_cache_dir), str(validation_cache_dir)\n\n\ndef _dataset_reader_param_hash(params: Params) -> str:\n    copied_params = params.duplicate()\n    # Laziness doesn't affect how the data is computed, so it shouldn't affect the hash.\n    copied_params.pop(\"lazy\", default=None)\n    return copied_params.get_hash()\n\n",
        "source_code_len": 2087,
        "target_code": "    return datasets\n\n",
        "target_code_len": 21,
        "diff_format": "@@ -198,49 +187,2 @@\n     return datasets\n-\n-\n-def _set_up_cache_files(\n-    train_params: Params,\n-    validation_params: Params = None,\n-    cache_directory: str = None,\n-    cache_prefix: str = None,\n-) -> Tuple[str, str]:\n-    if not cache_directory:\n-        return None, None\n-\n-    # We need to compute the parameter hash before the parameters get destroyed when they're\n-    # passed to `DatasetReader.from_params`.\n-    if not cache_prefix:\n-        cache_prefix = _dataset_reader_param_hash(train_params)\n-        if validation_params:\n-            validation_cache_prefix = _dataset_reader_param_hash(validation_params)\n-        else:\n-            validation_cache_prefix = cache_prefix\n-    else:\n-        validation_cache_prefix = cache_prefix\n-\n-    train_cache_dir = pathlib.Path(cache_directory) / cache_prefix\n-    validation_cache_dir = pathlib.Path(cache_directory) / validation_cache_prefix\n-\n-    # For easy human inspection of what parameters were used to create the cache.  This will\n-    # overwrite old files, but they should be identical.  This could bite someone who gave\n-    # their own prefix instead of letting us compute it, and then _re-used_ that name with\n-    # different parameters, without clearing the cache first.  But correctly handling that case\n-    # is more work than it's worth.\n-    os.makedirs(train_cache_dir, exist_ok=True)\n-    with open(train_cache_dir / \"params.json\", \"w\") as param_file:\n-        json.dump(train_params.as_dict(quiet=True), param_file)\n-    os.makedirs(validation_cache_dir, exist_ok=True)\n-    with open(validation_cache_dir / \"params.json\", \"w\") as param_file:\n-        if validation_params:\n-            json.dump(validation_params.as_dict(quiet=True), param_file)\n-        else:\n-            json.dump(train_params.as_dict(quiet=True), param_file)\n-    return str(train_cache_dir), str(validation_cache_dir)\n-\n-\n-def _dataset_reader_param_hash(params: Params) -> str:\n-    copied_params = params.duplicate()\n-    # Laziness doesn't affect how the data is computed, so it shouldn't affect the hash.\n-    copied_params.pop(\"lazy\", default=None)\n-    return copied_params.get_hash()\n \n",
        "source_code_with_indent": "    <DED>return datasets\n\n\n<DED>def _set_up_cache_files(\n    train_params: Params,\n    validation_params: Params = None,\n    cache_directory: str = None,\n    cache_prefix: str = None,\n) -> Tuple[str, str]:\n    <IND>if not cache_directory:\n        <IND>return None, None\n\n    # We need to compute the parameter hash before the parameters get destroyed when they're\n    # passed to `DatasetReader.from_params`.\n    <DED>if not cache_prefix:\n        <IND>cache_prefix = _dataset_reader_param_hash(train_params)\n        if validation_params:\n            <IND>validation_cache_prefix = _dataset_reader_param_hash(validation_params)\n        <DED>else:\n            <IND>validation_cache_prefix = cache_prefix\n    <DED><DED>else:\n        <IND>validation_cache_prefix = cache_prefix\n\n    <DED>train_cache_dir = pathlib.Path(cache_directory) / cache_prefix\n    validation_cache_dir = pathlib.Path(cache_directory) / validation_cache_prefix\n\n    # For easy human inspection of what parameters were used to create the cache.  This will\n    # overwrite old files, but they should be identical.  This could bite someone who gave\n    # their own prefix instead of letting us compute it, and then _re-used_ that name with\n    # different parameters, without clearing the cache first.  But correctly handling that case\n    # is more work than it's worth.\n    os.makedirs(train_cache_dir, exist_ok=True)\n    with open(train_cache_dir / \"params.json\", \"w\") as param_file:\n        <IND>json.dump(train_params.as_dict(quiet=True), param_file)\n    <DED>os.makedirs(validation_cache_dir, exist_ok=True)\n    with open(validation_cache_dir / \"params.json\", \"w\") as param_file:\n        <IND>if validation_params:\n            <IND>json.dump(validation_params.as_dict(quiet=True), param_file)\n        <DED>else:\n            <IND>json.dump(train_params.as_dict(quiet=True), param_file)\n    <DED><DED>return str(train_cache_dir), str(validation_cache_dir)\n\n\n<DED>def _dataset_reader_param_hash(params: Params) -> str:\n    <IND>copied_params = params.duplicate()\n    # Laziness doesn't affect how the data is computed, so it shouldn't affect the hash.\n    copied_params.pop(\"lazy\", default=None)\n    return copied_params.get_hash()\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    <DED>return datasets\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "8cb07b261c4825e56ec0925f05412fca70d19a57",
    "filename": "allennlp/training/util.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/util.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/util.py:204:4 Incompatible variable type [9]: cache_directory is declared to have type `str` but is used as type `None`.",
    "message": " cache_directory is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 204,
    "warning_line": "    cache_directory: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n    train_cache_dir, validation_cache_dir = _set_up_cache_files(\n        dataset_reader_params, validation_dataset_reader_params, cache_directory, cache_prefix\n    )\n\n",
        "source_code_len": 252,
        "target_code": "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n\n",
        "target_code_len": 86,
        "diff_format": "@@ -162,5 +158,2 @@\n     validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n-    train_cache_dir, validation_cache_dir = _set_up_cache_files(\n-        dataset_reader_params, validation_dataset_reader_params, cache_directory, cache_prefix\n-    )\n \n",
        "source_code_with_indent": "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n    train_cache_dir, validation_cache_dir = _set_up_cache_files(\n        dataset_reader_params, validation_dataset_reader_params, cache_directory, cache_prefix\n    )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\n    if train_cache_dir:\n        dataset_reader.cache_data(train_cache_dir)\n        validation_and_test_dataset_reader.cache_data(validation_cache_dir)\n\n    train_data_path = params.pop(\"train_data_path\")\n",
        "source_code_len": 205,
        "target_code": "\n    train_data_path = params.pop(\"train_data_path\")\n",
        "target_code_len": 53,
        "diff_format": "@@ -175,6 +168,2 @@\n \n-    if train_cache_dir:\n-        dataset_reader.cache_data(train_cache_dir)\n-        validation_and_test_dataset_reader.cache_data(validation_cache_dir)\n-\n     train_data_path = params.pop(\"train_data_path\")\n",
        "source_code_with_indent": "\n    <DED>if train_cache_dir:\n        <IND>dataset_reader.cache_data(train_cache_dir)\n        validation_and_test_dataset_reader.cache_data(validation_cache_dir)\n\n    <DED>train_data_path = params.pop(\"train_data_path\")\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>train_data_path = params.pop(\"train_data_path\")\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "    return datasets\n\n\ndef _set_up_cache_files(\n    train_params: Params,\n    validation_params: Params = None,\n    cache_directory: str = None,\n    cache_prefix: str = None,\n) -> Tuple[str, str]:\n    if not cache_directory:\n        return None, None\n\n    # We need to compute the parameter hash before the parameters get destroyed when they're\n    # passed to `DatasetReader.from_params`.\n    if not cache_prefix:\n        cache_prefix = _dataset_reader_param_hash(train_params)\n        if validation_params:\n            validation_cache_prefix = _dataset_reader_param_hash(validation_params)\n        else:\n            validation_cache_prefix = cache_prefix\n    else:\n        validation_cache_prefix = cache_prefix\n\n    train_cache_dir = pathlib.Path(cache_directory) / cache_prefix\n    validation_cache_dir = pathlib.Path(cache_directory) / validation_cache_prefix\n\n    # For easy human inspection of what parameters were used to create the cache.  This will\n    # overwrite old files, but they should be identical.  This could bite someone who gave\n    # their own prefix instead of letting us compute it, and then _re-used_ that name with\n    # different parameters, without clearing the cache first.  But correctly handling that case\n    # is more work than it's worth.\n    os.makedirs(train_cache_dir, exist_ok=True)\n    with open(train_cache_dir / \"params.json\", \"w\") as param_file:\n        json.dump(train_params.as_dict(quiet=True), param_file)\n    os.makedirs(validation_cache_dir, exist_ok=True)\n    with open(validation_cache_dir / \"params.json\", \"w\") as param_file:\n        if validation_params:\n            json.dump(validation_params.as_dict(quiet=True), param_file)\n        else:\n            json.dump(train_params.as_dict(quiet=True), param_file)\n    return str(train_cache_dir), str(validation_cache_dir)\n\n\ndef _dataset_reader_param_hash(params: Params) -> str:\n    copied_params = params.duplicate()\n    # Laziness doesn't affect how the data is computed, so it shouldn't affect the hash.\n    copied_params.pop(\"lazy\", default=None)\n    return copied_params.get_hash()\n\n",
        "source_code_len": 2087,
        "target_code": "    return datasets\n\n",
        "target_code_len": 21,
        "diff_format": "@@ -198,49 +187,2 @@\n     return datasets\n-\n-\n-def _set_up_cache_files(\n-    train_params: Params,\n-    validation_params: Params = None,\n-    cache_directory: str = None,\n-    cache_prefix: str = None,\n-) -> Tuple[str, str]:\n-    if not cache_directory:\n-        return None, None\n-\n-    # We need to compute the parameter hash before the parameters get destroyed when they're\n-    # passed to `DatasetReader.from_params`.\n-    if not cache_prefix:\n-        cache_prefix = _dataset_reader_param_hash(train_params)\n-        if validation_params:\n-            validation_cache_prefix = _dataset_reader_param_hash(validation_params)\n-        else:\n-            validation_cache_prefix = cache_prefix\n-    else:\n-        validation_cache_prefix = cache_prefix\n-\n-    train_cache_dir = pathlib.Path(cache_directory) / cache_prefix\n-    validation_cache_dir = pathlib.Path(cache_directory) / validation_cache_prefix\n-\n-    # For easy human inspection of what parameters were used to create the cache.  This will\n-    # overwrite old files, but they should be identical.  This could bite someone who gave\n-    # their own prefix instead of letting us compute it, and then _re-used_ that name with\n-    # different parameters, without clearing the cache first.  But correctly handling that case\n-    # is more work than it's worth.\n-    os.makedirs(train_cache_dir, exist_ok=True)\n-    with open(train_cache_dir / \"params.json\", \"w\") as param_file:\n-        json.dump(train_params.as_dict(quiet=True), param_file)\n-    os.makedirs(validation_cache_dir, exist_ok=True)\n-    with open(validation_cache_dir / \"params.json\", \"w\") as param_file:\n-        if validation_params:\n-            json.dump(validation_params.as_dict(quiet=True), param_file)\n-        else:\n-            json.dump(train_params.as_dict(quiet=True), param_file)\n-    return str(train_cache_dir), str(validation_cache_dir)\n-\n-\n-def _dataset_reader_param_hash(params: Params) -> str:\n-    copied_params = params.duplicate()\n-    # Laziness doesn't affect how the data is computed, so it shouldn't affect the hash.\n-    copied_params.pop(\"lazy\", default=None)\n-    return copied_params.get_hash()\n \n",
        "source_code_with_indent": "    <DED>return datasets\n\n\n<DED>def _set_up_cache_files(\n    train_params: Params,\n    validation_params: Params = None,\n    cache_directory: str = None,\n    cache_prefix: str = None,\n) -> Tuple[str, str]:\n    <IND>if not cache_directory:\n        <IND>return None, None\n\n    # We need to compute the parameter hash before the parameters get destroyed when they're\n    # passed to `DatasetReader.from_params`.\n    <DED>if not cache_prefix:\n        <IND>cache_prefix = _dataset_reader_param_hash(train_params)\n        if validation_params:\n            <IND>validation_cache_prefix = _dataset_reader_param_hash(validation_params)\n        <DED>else:\n            <IND>validation_cache_prefix = cache_prefix\n    <DED><DED>else:\n        <IND>validation_cache_prefix = cache_prefix\n\n    <DED>train_cache_dir = pathlib.Path(cache_directory) / cache_prefix\n    validation_cache_dir = pathlib.Path(cache_directory) / validation_cache_prefix\n\n    # For easy human inspection of what parameters were used to create the cache.  This will\n    # overwrite old files, but they should be identical.  This could bite someone who gave\n    # their own prefix instead of letting us compute it, and then _re-used_ that name with\n    # different parameters, without clearing the cache first.  But correctly handling that case\n    # is more work than it's worth.\n    os.makedirs(train_cache_dir, exist_ok=True)\n    with open(train_cache_dir / \"params.json\", \"w\") as param_file:\n        <IND>json.dump(train_params.as_dict(quiet=True), param_file)\n    <DED>os.makedirs(validation_cache_dir, exist_ok=True)\n    with open(validation_cache_dir / \"params.json\", \"w\") as param_file:\n        <IND>if validation_params:\n            <IND>json.dump(validation_params.as_dict(quiet=True), param_file)\n        <DED>else:\n            <IND>json.dump(train_params.as_dict(quiet=True), param_file)\n    <DED><DED>return str(train_cache_dir), str(validation_cache_dir)\n\n\n<DED>def _dataset_reader_param_hash(params: Params) -> str:\n    <IND>copied_params = params.duplicate()\n    # Laziness doesn't affect how the data is computed, so it shouldn't affect the hash.\n    copied_params.pop(\"lazy\", default=None)\n    return copied_params.get_hash()\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    <DED>return datasets\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "8cb07b261c4825e56ec0925f05412fca70d19a57",
    "filename": "allennlp/training/util.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/util.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/util.py:205:4 Incompatible variable type [9]: cache_prefix is declared to have type `str` but is used as type `None`.",
    "message": " cache_prefix is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 205,
    "warning_line": "    cache_prefix: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n    train_cache_dir, validation_cache_dir = _set_up_cache_files(\n        dataset_reader_params, validation_dataset_reader_params, cache_directory, cache_prefix\n    )\n\n",
        "source_code_len": 252,
        "target_code": "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n\n",
        "target_code_len": 86,
        "diff_format": "@@ -162,5 +158,2 @@\n     validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n-    train_cache_dir, validation_cache_dir = _set_up_cache_files(\n-        dataset_reader_params, validation_dataset_reader_params, cache_directory, cache_prefix\n-    )\n \n",
        "source_code_with_indent": "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n    train_cache_dir, validation_cache_dir = _set_up_cache_files(\n        dataset_reader_params, validation_dataset_reader_params, cache_directory, cache_prefix\n    )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\n    if train_cache_dir:\n        dataset_reader.cache_data(train_cache_dir)\n        validation_and_test_dataset_reader.cache_data(validation_cache_dir)\n\n    train_data_path = params.pop(\"train_data_path\")\n",
        "source_code_len": 205,
        "target_code": "\n    train_data_path = params.pop(\"train_data_path\")\n",
        "target_code_len": 53,
        "diff_format": "@@ -175,6 +168,2 @@\n \n-    if train_cache_dir:\n-        dataset_reader.cache_data(train_cache_dir)\n-        validation_and_test_dataset_reader.cache_data(validation_cache_dir)\n-\n     train_data_path = params.pop(\"train_data_path\")\n",
        "source_code_with_indent": "\n    <DED>if train_cache_dir:\n        <IND>dataset_reader.cache_data(train_cache_dir)\n        validation_and_test_dataset_reader.cache_data(validation_cache_dir)\n\n    <DED>train_data_path = params.pop(\"train_data_path\")\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>train_data_path = params.pop(\"train_data_path\")\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "    return datasets\n\n\ndef _set_up_cache_files(\n    train_params: Params,\n    validation_params: Params = None,\n    cache_directory: str = None,\n    cache_prefix: str = None,\n) -> Tuple[str, str]:\n    if not cache_directory:\n        return None, None\n\n    # We need to compute the parameter hash before the parameters get destroyed when they're\n    # passed to `DatasetReader.from_params`.\n    if not cache_prefix:\n        cache_prefix = _dataset_reader_param_hash(train_params)\n        if validation_params:\n            validation_cache_prefix = _dataset_reader_param_hash(validation_params)\n        else:\n            validation_cache_prefix = cache_prefix\n    else:\n        validation_cache_prefix = cache_prefix\n\n    train_cache_dir = pathlib.Path(cache_directory) / cache_prefix\n    validation_cache_dir = pathlib.Path(cache_directory) / validation_cache_prefix\n\n    # For easy human inspection of what parameters were used to create the cache.  This will\n    # overwrite old files, but they should be identical.  This could bite someone who gave\n    # their own prefix instead of letting us compute it, and then _re-used_ that name with\n    # different parameters, without clearing the cache first.  But correctly handling that case\n    # is more work than it's worth.\n    os.makedirs(train_cache_dir, exist_ok=True)\n    with open(train_cache_dir / \"params.json\", \"w\") as param_file:\n        json.dump(train_params.as_dict(quiet=True), param_file)\n    os.makedirs(validation_cache_dir, exist_ok=True)\n    with open(validation_cache_dir / \"params.json\", \"w\") as param_file:\n        if validation_params:\n            json.dump(validation_params.as_dict(quiet=True), param_file)\n        else:\n            json.dump(train_params.as_dict(quiet=True), param_file)\n    return str(train_cache_dir), str(validation_cache_dir)\n\n\ndef _dataset_reader_param_hash(params: Params) -> str:\n    copied_params = params.duplicate()\n    # Laziness doesn't affect how the data is computed, so it shouldn't affect the hash.\n    copied_params.pop(\"lazy\", default=None)\n    return copied_params.get_hash()\n\n",
        "source_code_len": 2087,
        "target_code": "    return datasets\n\n",
        "target_code_len": 21,
        "diff_format": "@@ -198,49 +187,2 @@\n     return datasets\n-\n-\n-def _set_up_cache_files(\n-    train_params: Params,\n-    validation_params: Params = None,\n-    cache_directory: str = None,\n-    cache_prefix: str = None,\n-) -> Tuple[str, str]:\n-    if not cache_directory:\n-        return None, None\n-\n-    # We need to compute the parameter hash before the parameters get destroyed when they're\n-    # passed to `DatasetReader.from_params`.\n-    if not cache_prefix:\n-        cache_prefix = _dataset_reader_param_hash(train_params)\n-        if validation_params:\n-            validation_cache_prefix = _dataset_reader_param_hash(validation_params)\n-        else:\n-            validation_cache_prefix = cache_prefix\n-    else:\n-        validation_cache_prefix = cache_prefix\n-\n-    train_cache_dir = pathlib.Path(cache_directory) / cache_prefix\n-    validation_cache_dir = pathlib.Path(cache_directory) / validation_cache_prefix\n-\n-    # For easy human inspection of what parameters were used to create the cache.  This will\n-    # overwrite old files, but they should be identical.  This could bite someone who gave\n-    # their own prefix instead of letting us compute it, and then _re-used_ that name with\n-    # different parameters, without clearing the cache first.  But correctly handling that case\n-    # is more work than it's worth.\n-    os.makedirs(train_cache_dir, exist_ok=True)\n-    with open(train_cache_dir / \"params.json\", \"w\") as param_file:\n-        json.dump(train_params.as_dict(quiet=True), param_file)\n-    os.makedirs(validation_cache_dir, exist_ok=True)\n-    with open(validation_cache_dir / \"params.json\", \"w\") as param_file:\n-        if validation_params:\n-            json.dump(validation_params.as_dict(quiet=True), param_file)\n-        else:\n-            json.dump(train_params.as_dict(quiet=True), param_file)\n-    return str(train_cache_dir), str(validation_cache_dir)\n-\n-\n-def _dataset_reader_param_hash(params: Params) -> str:\n-    copied_params = params.duplicate()\n-    # Laziness doesn't affect how the data is computed, so it shouldn't affect the hash.\n-    copied_params.pop(\"lazy\", default=None)\n-    return copied_params.get_hash()\n \n",
        "source_code_with_indent": "    <DED>return datasets\n\n\n<DED>def _set_up_cache_files(\n    train_params: Params,\n    validation_params: Params = None,\n    cache_directory: str = None,\n    cache_prefix: str = None,\n) -> Tuple[str, str]:\n    <IND>if not cache_directory:\n        <IND>return None, None\n\n    # We need to compute the parameter hash before the parameters get destroyed when they're\n    # passed to `DatasetReader.from_params`.\n    <DED>if not cache_prefix:\n        <IND>cache_prefix = _dataset_reader_param_hash(train_params)\n        if validation_params:\n            <IND>validation_cache_prefix = _dataset_reader_param_hash(validation_params)\n        <DED>else:\n            <IND>validation_cache_prefix = cache_prefix\n    <DED><DED>else:\n        <IND>validation_cache_prefix = cache_prefix\n\n    <DED>train_cache_dir = pathlib.Path(cache_directory) / cache_prefix\n    validation_cache_dir = pathlib.Path(cache_directory) / validation_cache_prefix\n\n    # For easy human inspection of what parameters were used to create the cache.  This will\n    # overwrite old files, but they should be identical.  This could bite someone who gave\n    # their own prefix instead of letting us compute it, and then _re-used_ that name with\n    # different parameters, without clearing the cache first.  But correctly handling that case\n    # is more work than it's worth.\n    os.makedirs(train_cache_dir, exist_ok=True)\n    with open(train_cache_dir / \"params.json\", \"w\") as param_file:\n        <IND>json.dump(train_params.as_dict(quiet=True), param_file)\n    <DED>os.makedirs(validation_cache_dir, exist_ok=True)\n    with open(validation_cache_dir / \"params.json\", \"w\") as param_file:\n        <IND>if validation_params:\n            <IND>json.dump(validation_params.as_dict(quiet=True), param_file)\n        <DED>else:\n            <IND>json.dump(train_params.as_dict(quiet=True), param_file)\n    <DED><DED>return str(train_cache_dir), str(validation_cache_dir)\n\n\n<DED>def _dataset_reader_param_hash(params: Params) -> str:\n    <IND>copied_params = params.duplicate()\n    # Laziness doesn't affect how the data is computed, so it shouldn't affect the hash.\n    copied_params.pop(\"lazy\", default=None)\n    return copied_params.get_hash()\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    <DED>return datasets\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "8cb07b261c4825e56ec0925f05412fca70d19a57",
    "filename": "allennlp/training/util.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/util.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/util.py:208:8 Incompatible return type [7]: Expected `Tuple[str, str]` but got `Tuple[None, None]`.",
    "message": " Expected `Tuple[str, str]` but got `Tuple[None, None]`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 208,
    "warning_line": "        return None, None",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n    train_cache_dir, validation_cache_dir = _set_up_cache_files(\n        dataset_reader_params, validation_dataset_reader_params, cache_directory, cache_prefix\n    )\n\n",
        "source_code_len": 252,
        "target_code": "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n\n",
        "target_code_len": 86,
        "diff_format": "@@ -162,5 +158,2 @@\n     validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n-    train_cache_dir, validation_cache_dir = _set_up_cache_files(\n-        dataset_reader_params, validation_dataset_reader_params, cache_directory, cache_prefix\n-    )\n \n",
        "source_code_with_indent": "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n    train_cache_dir, validation_cache_dir = _set_up_cache_files(\n        dataset_reader_params, validation_dataset_reader_params, cache_directory, cache_prefix\n    )\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\n    if train_cache_dir:\n        dataset_reader.cache_data(train_cache_dir)\n        validation_and_test_dataset_reader.cache_data(validation_cache_dir)\n\n    train_data_path = params.pop(\"train_data_path\")\n",
        "source_code_len": 205,
        "target_code": "\n    train_data_path = params.pop(\"train_data_path\")\n",
        "target_code_len": 53,
        "diff_format": "@@ -175,6 +168,2 @@\n \n-    if train_cache_dir:\n-        dataset_reader.cache_data(train_cache_dir)\n-        validation_and_test_dataset_reader.cache_data(validation_cache_dir)\n-\n     train_data_path = params.pop(\"train_data_path\")\n",
        "source_code_with_indent": "\n    <DED>if train_cache_dir:\n        <IND>dataset_reader.cache_data(train_cache_dir)\n        validation_and_test_dataset_reader.cache_data(validation_cache_dir)\n\n    <DED>train_data_path = params.pop(\"train_data_path\")\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>train_data_path = params.pop(\"train_data_path\")\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "    return datasets\n\n\ndef _set_up_cache_files(\n    train_params: Params,\n    validation_params: Params = None,\n    cache_directory: str = None,\n    cache_prefix: str = None,\n) -> Tuple[str, str]:\n    if not cache_directory:\n        return None, None\n\n    # We need to compute the parameter hash before the parameters get destroyed when they're\n    # passed to `DatasetReader.from_params`.\n    if not cache_prefix:\n        cache_prefix = _dataset_reader_param_hash(train_params)\n        if validation_params:\n            validation_cache_prefix = _dataset_reader_param_hash(validation_params)\n        else:\n            validation_cache_prefix = cache_prefix\n    else:\n        validation_cache_prefix = cache_prefix\n\n    train_cache_dir = pathlib.Path(cache_directory) / cache_prefix\n    validation_cache_dir = pathlib.Path(cache_directory) / validation_cache_prefix\n\n    # For easy human inspection of what parameters were used to create the cache.  This will\n    # overwrite old files, but they should be identical.  This could bite someone who gave\n    # their own prefix instead of letting us compute it, and then _re-used_ that name with\n    # different parameters, without clearing the cache first.  But correctly handling that case\n    # is more work than it's worth.\n    os.makedirs(train_cache_dir, exist_ok=True)\n    with open(train_cache_dir / \"params.json\", \"w\") as param_file:\n        json.dump(train_params.as_dict(quiet=True), param_file)\n    os.makedirs(validation_cache_dir, exist_ok=True)\n    with open(validation_cache_dir / \"params.json\", \"w\") as param_file:\n        if validation_params:\n            json.dump(validation_params.as_dict(quiet=True), param_file)\n        else:\n            json.dump(train_params.as_dict(quiet=True), param_file)\n    return str(train_cache_dir), str(validation_cache_dir)\n\n\ndef _dataset_reader_param_hash(params: Params) -> str:\n    copied_params = params.duplicate()\n    # Laziness doesn't affect how the data is computed, so it shouldn't affect the hash.\n    copied_params.pop(\"lazy\", default=None)\n    return copied_params.get_hash()\n\n",
        "source_code_len": 2087,
        "target_code": "    return datasets\n\n",
        "target_code_len": 21,
        "diff_format": "@@ -198,49 +187,2 @@\n     return datasets\n-\n-\n-def _set_up_cache_files(\n-    train_params: Params,\n-    validation_params: Params = None,\n-    cache_directory: str = None,\n-    cache_prefix: str = None,\n-) -> Tuple[str, str]:\n-    if not cache_directory:\n-        return None, None\n-\n-    # We need to compute the parameter hash before the parameters get destroyed when they're\n-    # passed to `DatasetReader.from_params`.\n-    if not cache_prefix:\n-        cache_prefix = _dataset_reader_param_hash(train_params)\n-        if validation_params:\n-            validation_cache_prefix = _dataset_reader_param_hash(validation_params)\n-        else:\n-            validation_cache_prefix = cache_prefix\n-    else:\n-        validation_cache_prefix = cache_prefix\n-\n-    train_cache_dir = pathlib.Path(cache_directory) / cache_prefix\n-    validation_cache_dir = pathlib.Path(cache_directory) / validation_cache_prefix\n-\n-    # For easy human inspection of what parameters were used to create the cache.  This will\n-    # overwrite old files, but they should be identical.  This could bite someone who gave\n-    # their own prefix instead of letting us compute it, and then _re-used_ that name with\n-    # different parameters, without clearing the cache first.  But correctly handling that case\n-    # is more work than it's worth.\n-    os.makedirs(train_cache_dir, exist_ok=True)\n-    with open(train_cache_dir / \"params.json\", \"w\") as param_file:\n-        json.dump(train_params.as_dict(quiet=True), param_file)\n-    os.makedirs(validation_cache_dir, exist_ok=True)\n-    with open(validation_cache_dir / \"params.json\", \"w\") as param_file:\n-        if validation_params:\n-            json.dump(validation_params.as_dict(quiet=True), param_file)\n-        else:\n-            json.dump(train_params.as_dict(quiet=True), param_file)\n-    return str(train_cache_dir), str(validation_cache_dir)\n-\n-\n-def _dataset_reader_param_hash(params: Params) -> str:\n-    copied_params = params.duplicate()\n-    # Laziness doesn't affect how the data is computed, so it shouldn't affect the hash.\n-    copied_params.pop(\"lazy\", default=None)\n-    return copied_params.get_hash()\n \n",
        "source_code_with_indent": "    <DED>return datasets\n\n\n<DED>def _set_up_cache_files(\n    train_params: Params,\n    validation_params: Params = None,\n    cache_directory: str = None,\n    cache_prefix: str = None,\n) -> Tuple[str, str]:\n    <IND>if not cache_directory:\n        <IND>return None, None\n\n    # We need to compute the parameter hash before the parameters get destroyed when they're\n    # passed to `DatasetReader.from_params`.\n    <DED>if not cache_prefix:\n        <IND>cache_prefix = _dataset_reader_param_hash(train_params)\n        if validation_params:\n            <IND>validation_cache_prefix = _dataset_reader_param_hash(validation_params)\n        <DED>else:\n            <IND>validation_cache_prefix = cache_prefix\n    <DED><DED>else:\n        <IND>validation_cache_prefix = cache_prefix\n\n    <DED>train_cache_dir = pathlib.Path(cache_directory) / cache_prefix\n    validation_cache_dir = pathlib.Path(cache_directory) / validation_cache_prefix\n\n    # For easy human inspection of what parameters were used to create the cache.  This will\n    # overwrite old files, but they should be identical.  This could bite someone who gave\n    # their own prefix instead of letting us compute it, and then _re-used_ that name with\n    # different parameters, without clearing the cache first.  But correctly handling that case\n    # is more work than it's worth.\n    os.makedirs(train_cache_dir, exist_ok=True)\n    with open(train_cache_dir / \"params.json\", \"w\") as param_file:\n        <IND>json.dump(train_params.as_dict(quiet=True), param_file)\n    <DED>os.makedirs(validation_cache_dir, exist_ok=True)\n    with open(validation_cache_dir / \"params.json\", \"w\") as param_file:\n        <IND>if validation_params:\n            <IND>json.dump(validation_params.as_dict(quiet=True), param_file)\n        <DED>else:\n            <IND>json.dump(train_params.as_dict(quiet=True), param_file)\n    <DED><DED>return str(train_cache_dir), str(validation_cache_dir)\n\n\n<DED>def _dataset_reader_param_hash(params: Params) -> str:\n    <IND>copied_params = params.duplicate()\n    # Laziness doesn't affect how the data is computed, so it shouldn't affect the hash.\n    copied_params.pop(\"lazy\", default=None)\n    return copied_params.get_hash()\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    <DED>return datasets\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  }
]