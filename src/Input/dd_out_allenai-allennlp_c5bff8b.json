[
  {
    "project": "allenai/allennlp",
    "commit": "c5bff8ba0d835eb03931f10f4f427ffe936cf796",
    "filename": "allennlp/training/checkpointer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/checkpointer.py",
    "file_hunks_size": 10,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/checkpointer.py:52:8 Incompatible variable type [9]: keep_serialized_model_every_num_seconds is declared to have type `int` but is used as type `None`.",
    "message": " keep_serialized_model_every_num_seconds is declared to have type `int` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 52,
    "warning_line": "        keep_serialized_model_every_num_seconds: int = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "c5bff8ba0d835eb03931f10f4f427ffe936cf796",
    "filename": "allennlp/training/checkpointer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/checkpointer.py",
    "file_hunks_size": 10,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/checkpointer.py:54:8 Incompatible variable type [9]: model_save_interval is declared to have type `float` but is used as type `None`.",
    "message": " model_save_interval is declared to have type `float` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 54,
    "warning_line": "        model_save_interval: float = None,"
  },
  {
    "project": "allenai/allennlp",
    "commit": "c5bff8ba0d835eb03931f10f4f427ffe936cf796",
    "filename": "allennlp/training/checkpointer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/checkpointer.py",
    "file_hunks_size": 10,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/checkpointer.py:175:38 Incompatible parameter type [6]: Expected `Union[os.PathLike[bytes], os.PathLike[str], bytes, str]` for 1st positional only parameter to call `os.remove` but got `Union[float, str]`.",
    "message": " Expected `Union[os.PathLike[bytes], os.PathLike[str], bytes, str]` for 1st positional only parameter to call `os.remove` but got `Union[float, str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 175,
    "warning_line": "                            os.remove(fname)"
  },
  {
    "project": "allenai/allennlp",
    "commit": "c5bff8ba0d835eb03931f10f4f427ffe936cf796",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:288:8 Incompatible variable type [9]: validation_data_loader is declared to have type `allennlp.data.data_loaders.data_loader.DataLoader` but is used as type `None`.",
    "message": " validation_data_loader is declared to have type `allennlp.data.data_loaders.data_loader.DataLoader` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 288,
    "warning_line": "        validation_data_loader: DataLoader = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n\n@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    \"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            self._validation_data_loader.set_target_device(self.cuda_device)\n        self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            if validation_data_loader is not None:\n                logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        elif (not isinstance(patience, int)) or patience <= 0:\n            raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            self._checkpointer = Checkpointer(serialization_dir)\n\n        self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            default_callbacks.append(ConfidenceChecksCallback)\n        for callback_cls in default_callbacks:\n            for callback in self._callbacks:\n                if callback.__class__ == callback_cls:\n                    break\n            else:\n                self._callbacks.append(callback_cls(self._serialization_dir))\n\n        self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            if self.cuda_device == torch.device(\"cpu\"):\n                raise ValueError(\"Using AMP requires a cuda device\")\n            self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        if self._distributed:\n            self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        else:\n            self._pytorch_model = self.model\n\n    def rescale_gradients(self) -> float:\n        \"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                self._scaler.unscale_(self.optimizer)\n            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        else:\n            return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            try:\n                assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            except AssertionError:\n                if for_training:\n                    raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        return output_dict\n\n    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        \"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        except TypeError:\n            num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        else:\n            batch_group_generator_tqdm = batch_group_generator\n\n        self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            self._batch_num_total = 0\n\n        done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            if done_early:\n                break\n\n            batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                for p in param_group[\"params\"]:\n                    p.grad = None\n\n            batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                with amp.autocast(self._use_amp):\n                    batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        raise ValueError(\"nan loss encountered\")\n                    loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                if self._scaler is not None:\n                    self._scaler.scale(loss).backward()\n                else:\n                    loss.backward()\n            if len(batch_group_outputs) <= 0:\n                continue\n\n            train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step_batch(batch_num_total)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step_batch(batch_num_total)\n\n            if self._scaler is not None:\n                self._scaler.step(self.optimizer)\n                self._scaler.update()\n            else:\n                self.optimizer.step()\n\n            # Update moving averages\n            if self._moving_average is not None:\n                self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        if self._distributed:\n            dist.barrier()\n\n        metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        for (gpu_num, memory) in gpu_memory_usage:\n            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        return metrics\n\n    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        \"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            self._moving_average.assign_average_value()\n\n        if self._validation_data_loader is not None:\n            validation_data_loader = self._validation_data_loader\n        else:\n            raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        else:\n            val_generator_tqdm = validation_data_loader\n\n        batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            with amp.autocast(self._use_amp):\n                batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                val_generator_tqdm.set_description(description, refresh=False)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        if self._moving_average is not None:\n            self._moving_average.restore()\n\n        return val_loss, val_reg_loss, batches_this_epoch\n\n    def train(self) -> Dict[str, Any]:\n        \"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        epoch = None\n        metrics = None\n\n        try:\n            metrics, epoch = self._try_train()\n            return metrics\n        finally:\n            for callback in self._callbacks:\n                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        try:\n            epoch_counter = self._restore_checkpoint()\n        except RuntimeError:\n            traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            metrics[\"best_validation_\" + key] = value\n\n        for epoch in range(epoch_counter, self._num_epochs):\n            epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            # get peak of memory usage\n            for key, value in train_metrics.items():\n                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        dist.barrier()\n\n                    val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                metrics[\"training_\" + key] = value\n            for key, value in val_metrics.items():\n                metrics[\"validation_\" + key] = value\n\n            if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    metrics[\"best_validation_\" + key] = value\n\n                self._metric_tracker.best_epoch_metrics = val_metrics\n\n            if self._serialization_dir and self._primary:\n                common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step(this_epoch_val_metric)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            for callback in self._callbacks:\n                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        else:\n            epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            self.model.load_state_dict(best_model_state)\n\n        return metrics, epoch\n\n    @contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            self._moving_average.assign_average_value()\n\n        model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        if self._momentum_scheduler is not None:\n            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        try:\n            yield model_state, training_states\n        finally:\n            if self._moving_average is not None:\n                self._moving_average.restore()\n\n    def _restore_checkpoint(self) -> int:\n        \"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            return 0\n\n        model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            return 0\n\n        self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        else:\n            self._metric_tracker.clear()\n\n        if isinstance(training_state[\"epoch\"], int):\n            epoch_to_return = training_state[\"epoch\"] + 1\n        else:\n            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            self._batch_num_total = batch_num_total\n\n        return epoch_to_return\n\n    @classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        \"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            from torch import cuda\n\n            if cuda.device_count() > 0:\n                cuda_device = 0\n            else:\n                cuda_device = -1\n\n        check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            model = model.cuda(cuda_device)\n\n        if no_grad:\n            for name, parameter in model.named_parameters():\n                if any(re.search(regex, name) for regex in no_grad):\n                    parameter.requires_grad_(False)\n\n        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        except TypeError:\n            batches_per_epoch = None\n\n        moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\nDEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_len": 45826,
        "target_code": "\n    def get_best_weights_path(self) -> Optional[str]:\n        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_len": 151,
        "diff_format": "@@ -115,1021 +89,4 @@\n \n-\n-@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\n-class GradientDescentTrainer(Trainer):\n-    \"\"\"\n-    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n-    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n-    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n-    stopping. There are many other bells and whistles as well.\n-\n-    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n-    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n-    see the arguments to that function for the exact keys that should be used, if you are using\n-    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n-    docstrings in `from_partial_objects`.\n-\n-    [0]: https://tinyurl.com/y5mv44fw\n-\n-    # Parameters\n-\n-    model : `Model`, required.\n-        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n-        their `forward` method returns a dictionary with a \"loss\" key, containing a\n-        scalar tensor representing the loss function to be optimized.\n-\n-        If you are training your model using GPUs, your model should already be\n-        on the correct device. (If you are using our `train` command this will be\n-        handled for you.)\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    optimizer : `torch.nn.Optimizer`, required.\n-        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n-        model to be optimized.\n-\n-    data_loader : `DataLoader`, required.\n-        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    patience : `Optional[int] > 0`, optional (default=`None`)\n-        Number of epochs to be patient before early stopping: the training is stopped\n-        after `patience` epochs with no improvement. If given, it must be `> 0`.\n-        If None, early stopping is disabled.\n-\n-    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n-        Validation metric to measure for whether to stop training using patience\n-        and whether to serialize an `is_best` model each epoch. The metric name\n-        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n-        is an increasing or decreasing function. If you specify more than one metric,\n-        the metrics will be summed to make the `is_best` decision.\n-\n-    validation_data_loader : `DataLoader`, optional (default=`None`)\n-        A `DataLoader` to use for the validation set.  If `None`, then\n-        use the training `DataLoader` with the validation data.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_epochs : `int`, optional (default = `20`)\n-        Number of training epochs.\n-\n-    serialization_dir : `str`, optional (default=`None`)\n-        Path to directory for saving and loading model files. Models will not be saved if\n-        this parameter is not passed.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    checkpointer : `Checkpointer`, optional (default=`None`)\n-        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n-        here, we will construct one with default parameters.\n-\n-    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n-        An integer or `torch.device` specifying the CUDA device to use for this process.\n-        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n-\n-        !!! Note\n-            If you *don't* intend to use a GPU, but you have one available, you'll need\n-            to explicitly set `cuda_device=-1`.\n-\n-        !!! Note\n-            If you intend to use a GPU, your model already needs to be on the correct device,\n-            which you can do with `model = model.cuda()`.\n-\n-        !!! Note\n-            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n-\n-    grad_norm : `float`, optional, (default = `None`).\n-        If provided, gradient norms will be rescaled to have a maximum of this value.\n-\n-    grad_clipping : `float`, optional (default = `None`).\n-        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n-        maximum of this value.  If you are getting `NaNs` in your gradients during training\n-        that are not solved by using `grad_norm`, you may need this.\n-\n-    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n-        If specified, the learning rate will be decayed with respect to\n-        this schedule at the end of each epoch (or batch, if the scheduler implements\n-        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n-        this will use the `validation_metric` provided to determine if learning has plateaued.\n-        To support updating the learning rate on every batch, this can optionally implement\n-        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n-\n-    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n-        If specified, the momentum will be updated at the end of each batch or epoch\n-        according to the schedule.\n-\n-    moving_average : `MovingAverage`, optional, (default = `None`)\n-        If provided, we will maintain moving averages for all parameters. During training, we\n-        employ a shadow variable for each parameter, which maintains the moving average. During\n-        evaluation, we backup the original parameters and assign the moving averages to corresponding\n-        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n-        parameters. This is necessary because we want the saved model to perform as well as the validated\n-        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n-\n-    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n-        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n-        and end of training, etc.\n-\n-    distributed : `bool`, optional, (default = `False`)\n-        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n-        requires `world_size` to be greater than 1.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n-        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n-\n-    local_rank : `int`, optional, (default = `0`)\n-        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n-        used as the rank.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    world_size : `int`, (default = `1`)\n-        The number of `Trainer` workers participating in the distributed training.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n-        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n-        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n-        post][0] for details on Gradient Accumulation.\n-\n-    use_amp : `bool`, optional, (default = `False`)\n-        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n-\n-    enable_default_callbacks : `bool`, optional (default = `True`)\n-        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n-        addition to any other callbacks listed in the `callbacks` parameter.\n-        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n-\n-    run_confidence_checks : `bool`, optional (default = `True`)\n-        Determines whether model confidence checks, such as\n-        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n-        are run.\n-\n-    run_sanity_checks : `bool`, optional (default = `True`)\n-        This parameter is deprecated. Please use `run_confidence_checks` instead.\n-\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        model: Model,\n-        optimizer: torch.optim.Optimizer,\n-        data_loader: DataLoader,\n-        patience: Optional[int] = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        validation_data_loader: DataLoader = None,\n-        num_epochs: int = 20,\n-        serialization_dir: Optional[str] = None,\n-        checkpointer: Checkpointer = None,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: Optional[float] = None,\n-        grad_clipping: Optional[float] = None,\n-        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n-        momentum_scheduler: Optional[MomentumScheduler] = None,\n-        moving_average: Optional[MovingAverage] = None,\n-        callbacks: List[TrainerCallback] = None,\n-        distributed: bool = False,\n-        local_rank: int = 0,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-        )\n-\n-        if \"run_sanity_checks\" in kwargs:\n-            warnings.warn(\n-                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n-                DeprecationWarning,\n-            )\n-            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n-\n-        # I am not calling move_to_gpu here, because if the model is\n-        # not already on the GPU then the optimizer is going to be wrong.\n-        self.model = model\n-\n-        self.data_loader = data_loader\n-        self.data_loader.set_target_device(self.cuda_device)\n-        self._validation_data_loader = validation_data_loader\n-        if self._validation_data_loader is not None:\n-            self._validation_data_loader.set_target_device(self.cuda_device)\n-        self.optimizer = optimizer\n-\n-        if patience is None:  # no early stopping\n-            if validation_data_loader is not None:\n-                logger.warning(\n-                    \"You provided a validation dataset but patience was set to None, \"\n-                    \"meaning that early stopping is disabled\"\n-                )\n-        elif (not isinstance(patience, int)) or patience <= 0:\n-            raise ConfigurationError(\n-                '{} is an invalid value for \"patience\": it must be a positive integer '\n-                \"or None (if you want to disable early stopping)\".format(patience)\n-            )\n-\n-        # For tracking is_best_so_far and should_stop_early\n-        self._metric_tracker = MetricTracker(validation_metric, patience)\n-\n-        self._num_epochs = num_epochs\n-\n-        self._checkpointer: Optional[Checkpointer] = checkpointer\n-        if checkpointer is None and serialization_dir is not None:\n-            self._checkpointer = Checkpointer(serialization_dir)\n-\n-        self._grad_norm = grad_norm\n-        self._grad_clipping = grad_clipping\n-\n-        self._learning_rate_scheduler = learning_rate_scheduler\n-        self._momentum_scheduler = momentum_scheduler\n-        self._moving_average = moving_average\n-\n-        self._callbacks = callbacks or []\n-        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n-\n-        if run_confidence_checks:\n-            default_callbacks.append(ConfidenceChecksCallback)\n-        for callback_cls in default_callbacks:\n-            for callback in self._callbacks:\n-                if callback.__class__ == callback_cls:\n-                    break\n-            else:\n-                self._callbacks.append(callback_cls(self._serialization_dir))\n-\n-        self._batch_num_total = 0\n-        self._last_log = 0.0  # time of last logging\n-        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n-\n-        # Enable automatic mixed precision training.\n-        self._scaler: Optional[amp.GradScaler] = None\n-        self._use_amp = use_amp\n-        if self._use_amp:\n-            if self.cuda_device == torch.device(\"cpu\"):\n-                raise ValueError(\"Using AMP requires a cuda device\")\n-            self._scaler = amp.GradScaler()\n-\n-        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n-        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n-        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n-        #\n-        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n-        # normal case, reference to `Model` is retained. This reference is only used in\n-        # these places: `model.__call__`, `model.train` and `model.eval`.\n-        if self._distributed:\n-            self._pytorch_model = DistributedDataParallel(\n-                self.model,\n-                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n-                find_unused_parameters=True,\n-            )\n-        else:\n-            self._pytorch_model = self.model\n-\n-    def rescale_gradients(self) -> float:\n-        \"\"\"\n-        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n-\n-        Returns the norm of the gradients.\n-        \"\"\"\n-        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n-        if self._grad_norm:\n-            if self._scaler is not None:\n-                # Need to first unscale gradients in order to clip as usual.\n-                self._scaler.unscale_(self.optimizer)\n-            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n-        else:\n-            return torch.norm(\n-                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n-            )\n-\n-    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n-        \"\"\"\n-        Does a forward pass on the given batch and returns the output dictionary that the model\n-        returns, after adding any specified regularization penalty to the loss (if training).\n-        \"\"\"\n-        output_dict = self._pytorch_model(**batch)\n-\n-        if for_training:\n-            try:\n-                assert \"loss\" in output_dict\n-                regularization_penalty = self.model.get_regularization_penalty()\n-\n-                if regularization_penalty is not None:\n-                    output_dict[\"reg_loss\"] = regularization_penalty\n-                    output_dict[\"loss\"] += regularization_penalty\n-\n-            except AssertionError:\n-                if for_training:\n-                    raise RuntimeError(\n-                        \"The model you are trying to optimize does not contain a\"\n-                        \" 'loss' key in the output of model.forward(inputs).\"\n-                    )\n-\n-        return output_dict\n-\n-    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n-        \"\"\"\n-        Trains one epoch and returns metrics.\n-        \"\"\"\n-        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n-        cpu_memory_usage = []\n-        for worker, memory in common_util.peak_cpu_memory().items():\n-            cpu_memory_usage.append((worker, memory))\n-            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n-        gpu_memory_usage = []\n-        for gpu, memory in common_util.peak_gpu_memory().items():\n-            gpu_memory_usage.append((gpu, memory))\n-            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        train_loss = 0.0\n-        batch_loss = 0.0\n-        train_reg_loss = None if regularization_penalty is None else 0.0\n-        batch_reg_loss = None if regularization_penalty is None else 0.0\n-\n-        # Set the model to \"train\" mode.\n-        self._pytorch_model.train()\n-\n-        # Get tqdm for the training batches\n-        batch_generator = iter(self.data_loader)\n-        batch_group_generator = common_util.lazy_groups_of(\n-            batch_generator, self._num_gradient_accumulation_steps\n-        )\n-\n-        logger.info(\"Training\")\n-\n-        num_training_batches: Union[int, float]\n-        try:\n-            len_data_loader = len(self.data_loader)\n-            num_training_batches = math.ceil(\n-                len_data_loader / self._num_gradient_accumulation_steps\n-            )\n-        except TypeError:\n-            num_training_batches = float(\"inf\")\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            batch_group_generator_tqdm = Tqdm.tqdm(\n-                batch_group_generator, total=num_training_batches\n-            )\n-        else:\n-            batch_group_generator_tqdm = batch_group_generator\n-\n-        self._last_log = time.time()\n-\n-        batches_this_epoch = 0\n-        if self._batch_num_total is None:\n-            self._batch_num_total = 0\n-\n-        done_early = False\n-        for batch_group in batch_group_generator_tqdm:\n-            if done_early:\n-                break\n-\n-            batches_this_epoch += 1\n-            self._batch_num_total += 1\n-            batch_num_total = self._batch_num_total\n-\n-            # Zero gradients.\n-            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n-            # because it avoids a read op when the gradients are first updated below.\n-            for param_group in self.optimizer.param_groups:\n-                for p in param_group[\"params\"]:\n-                    p.grad = None\n-\n-            batch_loss = 0.0\n-            batch_group_outputs = []\n-            for batch in batch_group:\n-                if self._distributed:\n-                    # Check whether the other workers have stopped already (due to differing amounts of\n-                    # data in each). If so, we can't proceed because we would hang when we hit the\n-                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                    # here because NCCL process groups apparently don't support BoolTensor.\n-                    done = torch.tensor(0, device=self.cuda_device)\n-                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                    if done.item() > 0:\n-                        done_early = True\n-                        logger.warning(\n-                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n-                            \"This implies that there is an imbalance in your training \"\n-                            \"data across the workers and that some amount of it will be \"\n-                            \"ignored. A small amount of this is fine, but a major imbalance \"\n-                            \"should be avoided. Note: This warning will appear unless your \"\n-                            \"data is perfectly balanced.\"\n-                        )\n-                        break\n-\n-                with amp.autocast(self._use_amp):\n-                    batch_outputs = self.batch_outputs(batch, for_training=True)\n-                    batch_group_outputs.append(batch_outputs)\n-                    loss = batch_outputs[\"loss\"]\n-                    reg_loss = batch_outputs.get(\"reg_loss\")\n-                    if torch.isnan(loss):\n-                        raise ValueError(\"nan loss encountered\")\n-                    loss = loss / len(batch_group)\n-\n-                    batch_loss += loss.item()\n-                    if reg_loss is not None:\n-                        reg_loss = reg_loss / len(batch_group)\n-                        batch_reg_loss = reg_loss.item()\n-                        train_reg_loss += batch_reg_loss  # type: ignore\n-\n-                if self._scaler is not None:\n-                    self._scaler.scale(loss).backward()\n-                else:\n-                    loss.backward()\n-            if len(batch_group_outputs) <= 0:\n-                continue\n-\n-            train_loss += batch_loss\n-\n-            batch_grad_norm = self.rescale_gradients()\n-\n-            # This does nothing if batch_num_total is None or you are using a\n-            # scheduler which doesn't update per batch.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step_batch(batch_num_total)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step_batch(batch_num_total)\n-\n-            if self._scaler is not None:\n-                self._scaler.step(self.optimizer)\n-                self._scaler.update()\n-            else:\n-                self.optimizer.step()\n-\n-            # Update moving averages\n-            if self._moving_average is not None:\n-                self._moving_average.apply(batch_num_total)\n-\n-            # Update the description with the latest metrics\n-            metrics = training_util.get_metrics(\n-                self.model,\n-                train_loss,\n-                train_reg_loss,\n-                batch_loss,\n-                batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            if self._primary:\n-                # Updating tqdm only for the primary as the trainers wouldn't have one\n-                description = training_util.description_from_metrics(metrics)\n-                batch_group_generator_tqdm.set_description(description, refresh=False)\n-\n-                if self._checkpointer is not None:\n-                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    batch_group,\n-                    batch_group_outputs,\n-                    metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=True,\n-                    is_primary=self._primary,\n-                    batch_grad_norm=batch_grad_norm,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Let all workers finish their epoch before computing\n-        # the final statistics for the epoch.\n-        if self._distributed:\n-            dist.barrier()\n-\n-        metrics = training_util.get_metrics(\n-            self.model,\n-            train_loss,\n-            train_reg_loss,\n-            batch_loss=None,\n-            batch_reg_loss=None,\n-            num_batches=batches_this_epoch,\n-            reset=True,\n-            world_size=self._world_size,\n-            cuda_device=self.cuda_device,\n-        )\n-\n-        for (worker, memory) in cpu_memory_usage:\n-            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        for (gpu_num, memory) in gpu_memory_usage:\n-            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        return metrics\n-\n-    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n-        \"\"\"\n-        Computes the validation loss. Returns it and the number of batches.\n-        \"\"\"\n-        logger.info(\"Validating\")\n-\n-        self._pytorch_model.eval()\n-\n-        # Replace parameter values with the shadow values from the moving averages.\n-        if self._moving_average is not None:\n-            self._moving_average.assign_average_value()\n-\n-        if self._validation_data_loader is not None:\n-            validation_data_loader = self._validation_data_loader\n-        else:\n-            raise ConfigurationError(\n-                \"Validation results cannot be calculated without a validation_data_loader\"\n-            )\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n-        else:\n-            val_generator_tqdm = validation_data_loader\n-\n-        batches_this_epoch = 0\n-        val_loss = 0.0\n-        val_batch_loss = 0.0\n-        val_reg_loss = None if regularization_penalty is None else 0.0\n-        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n-        done_early = False\n-        for batch in val_generator_tqdm:\n-            if self._distributed:\n-                # Check whether the other workers have stopped already (due to differing amounts of\n-                # data in each). If so, we can't proceed because we would hang when we hit the\n-                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                # here because NCCL process groups apparently don't support BoolTensor.\n-                done = torch.tensor(0, device=self.cuda_device)\n-                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                if done.item() > 0:\n-                    done_early = True\n-                    logger.warning(\n-                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n-                        \"This implies that there is an imbalance in your validation \"\n-                        \"data across the workers and that some amount of it will be \"\n-                        \"ignored. A small amount of this is fine, but a major imbalance \"\n-                        \"should be avoided. Note: This warning will appear unless your \"\n-                        \"data is perfectly balanced.\"\n-                    )\n-                    break\n-\n-            with amp.autocast(self._use_amp):\n-                batch_outputs = self.batch_outputs(batch, for_training=False)\n-                loss = batch_outputs.get(\"loss\")\n-                reg_loss = batch_outputs.get(\"reg_loss\")\n-                if loss is not None:\n-                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n-                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n-                    # currently only used as the divisor for the loss function, so we can safely only\n-                    # count those batches for which we actually have a loss.  If this variable ever\n-                    # gets used for something else, we might need to change things around a bit.\n-                    batches_this_epoch += 1\n-                    val_batch_loss = loss.item()\n-                    val_loss += val_batch_loss\n-                    if reg_loss is not None:\n-                        val_batch_reg_loss = reg_loss.item()\n-                        val_reg_loss += val_batch_reg_loss  # type: ignore\n-\n-            # Update the description with the latest metrics\n-            val_metrics = training_util.get_metrics(\n-                self.model,\n-                val_loss,\n-                val_reg_loss,\n-                val_batch_loss,\n-                val_batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            description = training_util.description_from_metrics(val_metrics)\n-            if self._primary:\n-                val_generator_tqdm.set_description(description, refresh=False)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    [batch],\n-                    [batch_outputs],\n-                    val_metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=False,\n-                    is_primary=self._primary,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop validation early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Now restore the original parameter values.\n-        if self._moving_average is not None:\n-            self._moving_average.restore()\n-\n-        return val_loss, val_reg_loss, batches_this_epoch\n-\n-    def train(self) -> Dict[str, Any]:\n-        \"\"\"\n-        Trains the supplied model with the supplied parameters.\n-        \"\"\"\n-\n-        for callback in self._callbacks:\n-            callback.on_start(self, is_primary=self._primary)\n-\n-        # Set default values in case of failure\n-        epoch = None\n-        metrics = None\n-\n-        try:\n-            metrics, epoch = self._try_train()\n-            return metrics\n-        finally:\n-            for callback in self._callbacks:\n-                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n-        try:\n-            epoch_counter = self._restore_checkpoint()\n-        except RuntimeError:\n-            traceback.print_exc()\n-            raise ConfigurationError(\n-                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n-                \"a different serialization directory or delete the existing serialization \"\n-                \"directory?\"\n-            )\n-\n-        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n-\n-        logger.info(\"Beginning training.\")\n-\n-        val_metrics: Dict[str, float] = {}\n-        metrics: Dict[str, Any] = {}\n-        epochs_trained = 0\n-        training_start_time = time.time()\n-\n-        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n-        for key, value in self._metric_tracker.best_epoch_metrics.items():\n-            metrics[\"best_validation_\" + key] = value\n-\n-        for epoch in range(epoch_counter, self._num_epochs):\n-            epoch_start_time = time.time()\n-            train_metrics = self._train_epoch(epoch)\n-\n-            # Back up the model now, in case something goes wrong later with the evaluation\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.shelve_model(epoch, self)\n-            # Wait for the primary process to finish saving the model checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            # get peak of memory usage\n-            for key, value in train_metrics.items():\n-                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-\n-            this_epoch_val_metric: float = 0.0\n-            if self._validation_data_loader is not None:\n-                with torch.no_grad():\n-                    # We have a validation set, so compute all the metrics on it.\n-                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n-\n-                    # It is safe again to wait till the validation is done. This is\n-                    # important to get the metrics right.\n-                    if self._distributed:\n-                        dist.barrier()\n-\n-                    val_metrics = training_util.get_metrics(\n-                        self.model,\n-                        val_loss,\n-                        val_reg_loss,\n-                        batch_loss=None,\n-                        batch_reg_loss=None,\n-                        num_batches=num_batches,\n-                        reset=True,\n-                        world_size=self._world_size,\n-                        cuda_device=self.cuda_device,\n-                    )\n-\n-                    # Check validation metric for early stopping\n-                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n-                    self._metric_tracker.add_metrics(val_metrics)\n-\n-            # Create overall metrics dict\n-            training_elapsed_time = time.time() - training_start_time\n-            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n-            metrics[\"training_start_epoch\"] = epoch_counter\n-            metrics[\"training_epochs\"] = epochs_trained\n-            metrics[\"epoch\"] = epoch\n-\n-            for key, value in train_metrics.items():\n-                metrics[\"training_\" + key] = value\n-            for key, value in val_metrics.items():\n-                metrics[\"validation_\" + key] = value\n-\n-            if self._metric_tracker.is_best_so_far():\n-                # Update all the best_ metrics.\n-                # (Otherwise they just stay the same as they were.)\n-                metrics[\"best_epoch\"] = epoch\n-                for key, value in val_metrics.items():\n-                    metrics[\"best_validation_\" + key] = value\n-\n-                self._metric_tracker.best_epoch_metrics = val_metrics\n-\n-            if self._serialization_dir and self._primary:\n-                common_util.dump_metrics(\n-                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n-                    metrics,\n-                )\n-\n-            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n-            # if it doesn't, the validation metric passed here is ignored.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step(this_epoch_val_metric)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step(this_epoch_val_metric)\n-\n-            # The checkpointer saves state from the learning rate scheduler and the momentum\n-            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.save_checkpoint(\n-                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n-                )\n-            # Wait for the primary process to finish saving the checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            for callback in self._callbacks:\n-                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-            epoch_elapsed_time = time.time() - epoch_start_time\n-            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n-\n-            if epoch < self._num_epochs - 1:\n-                training_elapsed_time = time.time() - training_start_time\n-                estimated_time_remaining = training_elapsed_time * (\n-                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n-                )\n-                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n-                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n-\n-            epochs_trained += 1\n-\n-            if self._metric_tracker.should_stop_early():\n-                logger.info(\"Ran out of patience. Stopping training.\")\n-                break\n-        else:\n-            epoch = self._num_epochs - 1\n-\n-        # Load the best model state before returning\n-        best_model_state = (\n-            None if self._checkpointer is None else self._checkpointer.best_model_state()\n-        )\n-        if best_model_state:\n-            self.model.load_state_dict(best_model_state)\n-\n-        return metrics, epoch\n-\n-    @contextmanager\n-    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n-        if self._moving_average is not None:\n-            # Assigning average value to model parameters.  The checkpointer will call\n-            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n-            self._moving_average.assign_average_value()\n-\n-        model_state = self.model.state_dict()\n-\n-        # These are the training states we need to persist.\n-        training_states = {\n-            \"metric_tracker\": self._metric_tracker.state_dict(),\n-            \"optimizer\": self.optimizer.state_dict(),\n-            \"batch_num_total\": self._batch_num_total,\n-        }\n-\n-        # If we have a learning rate or momentum scheduler, we should persist them too.\n-        if self._learning_rate_scheduler is not None:\n-            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n-        if self._momentum_scheduler is not None:\n-            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n-\n-        try:\n-            yield model_state, training_states\n-        finally:\n-            if self._moving_average is not None:\n-                self._moving_average.restore()\n-\n-    def _restore_checkpoint(self) -> int:\n-        \"\"\"\n-        Restores the model and training state from the last saved checkpoint.\n-        This includes an epoch count and optimizer state, which is serialized separately\n-        from model parameters. This function should only be used to continue training -\n-        if you wish to load a model for inference/load parts of a model into a new\n-        computation graph, you should use the native Pytorch functions:\n-        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n-\n-        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n-        this function will do nothing and return 0.\n-\n-        # Returns\n-\n-        epoch: `int`\n-            The epoch at which to resume training, which should be one after the epoch\n-            in the saved training state.\n-        \"\"\"\n-        if self._checkpointer is None:\n-            return 0\n-\n-        model_state, training_state = self._checkpointer.restore_checkpoint()\n-\n-        if not training_state:\n-            # No checkpoint to restore, start at 0\n-            return 0\n-\n-        self.model.load_state_dict(model_state)\n-        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n-        if (\n-            self._learning_rate_scheduler is not None\n-            and \"learning_rate_scheduler\" in training_state\n-        ):\n-            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n-        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n-            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n-        training_util.move_optimizer_to_cuda(self.optimizer)\n-\n-        # Currently the `training_state` contains a serialized `MetricTracker`.\n-        if \"metric_tracker\" in training_state:\n-            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n-        else:\n-            self._metric_tracker.clear()\n-\n-        if isinstance(training_state[\"epoch\"], int):\n-            epoch_to_return = training_state[\"epoch\"] + 1\n-        else:\n-            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n-\n-        # For older checkpoints with batch_num_total missing, default to old behavior where\n-        # it is unchanged.\n-        batch_num_total = training_state.get(\"batch_num_total\")\n-        if batch_num_total is not None:\n-            self._batch_num_total = batch_num_total\n-\n-        return epoch_to_return\n-\n-    @classmethod\n-    def from_partial_objects(\n-        cls,\n-        model: Model,\n-        serialization_dir: str,\n-        data_loader: DataLoader,\n-        validation_data_loader: DataLoader = None,\n-        local_rank: int = 0,\n-        patience: int = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        num_epochs: int = 20,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: float = None,\n-        grad_clipping: float = None,\n-        distributed: bool = False,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        no_grad: List[str] = None,\n-        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n-        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n-        momentum_scheduler: Lazy[MomentumScheduler] = None,\n-        moving_average: Lazy[MovingAverage] = None,\n-        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n-        callbacks: List[Lazy[TrainerCallback]] = None,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> \"Trainer\":\n-        \"\"\"\n-        This method exists so that we can have a documented method to construct this class using\n-        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n-        method.\n-\n-        The reason we can't just use `__init__` with `FromParams` here is because there are\n-        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n-        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n-        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n-        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n-        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n-\n-        If you're not using `FromParams`, you can just construct these arguments in the right order\n-        yourself in your code and call the constructor directly.\n-        \"\"\"\n-        if cuda_device is None:\n-            from torch import cuda\n-\n-            if cuda.device_count() > 0:\n-                cuda_device = 0\n-            else:\n-                cuda_device = -1\n-\n-        check_for_gpu(cuda_device)\n-        if cuda_device >= 0:\n-            # Moving model to GPU here so that the optimizer state gets constructed on\n-            # the right device.\n-            model = model.cuda(cuda_device)\n-\n-        if no_grad:\n-            for name, parameter in model.named_parameters():\n-                if any(re.search(regex, name) for regex in no_grad):\n-                    parameter.requires_grad_(False)\n-\n-        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n-        optimizer_ = optimizer.construct(model_parameters=parameters)\n-\n-        common_util.log_frozen_and_tunable_parameter_names(model)\n-\n-        batches_per_epoch: Optional[int]\n-        try:\n-            batches_per_epoch = len(data_loader)\n-            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n-        except TypeError:\n-            batches_per_epoch = None\n-\n-        moving_average_ = (\n-            None if moving_average is None else moving_average.construct(parameters=parameters)\n-        )\n-        learning_rate_scheduler_ = (\n-            None\n-            if learning_rate_scheduler is None\n-            else learning_rate_scheduler.construct(\n-                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n-            )\n-        )\n-        momentum_scheduler_ = (\n-            None\n-            if momentum_scheduler is None\n-            else momentum_scheduler.construct(optimizer=optimizer_)\n-        )\n-        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n-\n-        callbacks_: List[TrainerCallback] = []\n-        for callback_ in callbacks or []:\n-            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n-\n-        return cls(\n-            model,\n-            optimizer_,\n-            data_loader,\n-            patience=patience,\n-            validation_metric=validation_metric,\n-            validation_data_loader=validation_data_loader,\n-            num_epochs=num_epochs,\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            grad_norm=grad_norm,\n-            grad_clipping=grad_clipping,\n-            learning_rate_scheduler=learning_rate_scheduler_,\n-            momentum_scheduler=momentum_scheduler_,\n-            checkpointer=checkpointer_,\n-            moving_average=moving_average_,\n-            callbacks=callbacks_,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n-            use_amp=use_amp,\n-            enable_default_callbacks=enable_default_callbacks,\n-            run_confidence_checks=run_confidence_checks,\n-            **kwargs,\n-        )\n-\n-\n-DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n-\"\"\"\n-The default callbacks used by `GradientDescentTrainer`.\n-\"\"\"\n+    def get_best_weights_path(self) -> Optional[str]:\n+        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n+        return None\n",
        "source_code_with_indent": "\n\n<DED><DED>@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    <IND>\"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        <IND>super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            <IND>warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        <DED>self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            <IND>self._validation_data_loader.set_target_device(self.cuda_device)\n        <DED>self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            <IND>if validation_data_loader is not None:\n                <IND>logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        <DED><DED>elif (not isinstance(patience, int)) or patience <= 0:\n            <IND>raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        <DED>self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            <IND>self._checkpointer = Checkpointer(serialization_dir)\n\n        <DED>self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            <IND>default_callbacks.append(ConfidenceChecksCallback)\n        <DED>for callback_cls in default_callbacks:\n            <IND>for callback in self._callbacks:\n                <IND>if callback.__class__ == callback_cls:\n                    <IND>break\n            <DED><DED>else:\n                <IND>self._callbacks.append(callback_cls(self._serialization_dir))\n\n        <DED><DED>self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            <IND>if self.cuda_device == torch.device(\"cpu\"):\n                <IND>raise ValueError(\"Using AMP requires a cuda device\")\n            <DED>self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        <DED>if self._distributed:\n            <IND>self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        <DED>else:\n            <IND>self._pytorch_model = self.model\n\n    <DED><DED>def rescale_gradients(self) -> float:\n        <IND>\"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            <IND>if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                <IND>self._scaler.unscale_(self.optimizer)\n            <DED>return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        <DED>else:\n            <IND>return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    <DED><DED>def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        <IND>\"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            <IND>try:\n                <IND>assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    <IND>output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            <DED><DED>except AssertionError:\n                <IND>if for_training:\n                    <IND>raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        <DED><DED><DED>return output_dict\n\n    <DED>def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        <IND>\"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            <IND>cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        <DED>gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            <IND>gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            <IND>len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        <DED>except TypeError:\n            <IND>num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        <DED>if self._primary:\n            <IND>batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        <DED>else:\n            <IND>batch_group_generator_tqdm = batch_group_generator\n\n        <DED>self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            <IND>self._batch_num_total = 0\n\n        <DED>done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            <IND>if done_early:\n                <IND>break\n\n            <DED>batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                <IND>for p in param_group[\"params\"]:\n                    <IND>p.grad = None\n\n            <DED><DED>batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                <IND>if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    <IND>done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        <IND>done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                <DED><DED>with amp.autocast(self._use_amp):\n                    <IND>batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        <IND>raise ValueError(\"nan loss encountered\")\n                    <DED>loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        <IND>reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                <DED><DED>if self._scaler is not None:\n                    <IND>self._scaler.scale(loss).backward()\n                <DED>else:\n                    <IND>loss.backward()\n            <DED><DED>if len(batch_group_outputs) <= 0:\n                <IND>continue\n\n            <DED>train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step_batch(batch_num_total)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step_batch(batch_num_total)\n\n            <DED>if self._scaler is not None:\n                <IND>self._scaler.step(self.optimizer)\n                self._scaler.update()\n            <DED>else:\n                <IND>self.optimizer.step()\n\n            # Update moving averages\n            <DED>if self._moving_average is not None:\n                <IND>self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            <DED>metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                <IND>description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    <IND>self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            <DED><DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        <DED>if self._distributed:\n            <IND>dist.barrier()\n\n        <DED>metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            <IND>metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>for (gpu_num, memory) in gpu_memory_usage:\n            <IND>metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>return metrics\n\n    <DED>def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        <IND>\"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>if self._validation_data_loader is not None:\n            <IND>validation_data_loader = self._validation_data_loader\n        <DED>else:\n            <IND>raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            <IND>val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        <DED>else:\n            <IND>val_generator_tqdm = validation_data_loader\n\n        <DED>batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            <IND>if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                <IND>done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    <IND>done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            <DED><DED>with amp.autocast(self._use_amp):\n                <IND>batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    <IND>batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        <IND>val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            <DED><DED><DED>val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                <IND>val_generator_tqdm.set_description(description, refresh=False)\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        <DED>if self._moving_average is not None:\n            <IND>self._moving_average.restore()\n\n        <DED>return val_loss, val_reg_loss, batches_this_epoch\n\n    <DED>def train(self) -> Dict[str, Any]:\n        <IND>\"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            <IND>callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        <DED>epoch = None\n        metrics = None\n\n        try:\n            <IND>metrics, epoch = self._try_train()\n            return metrics\n        <DED>finally:\n            <IND>for callback in self._callbacks:\n                <IND>callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    <DED><DED><DED>def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        <IND>try:\n            <IND>epoch_counter = self._restore_checkpoint()\n        <DED>except RuntimeError:\n            <IND>traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        <DED>training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            <IND>metrics[\"best_validation_\" + key] = value\n\n        <DED>for epoch in range(epoch_counter, self._num_epochs):\n            <IND>epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            # get peak of memory usage\n            <DED>for key, value in train_metrics.items():\n                <IND>if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                <DED>elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            <DED><DED>this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                <IND>with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    <IND>val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        <IND>dist.barrier()\n\n                    <DED>val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            <DED><DED>training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                <IND>metrics[\"training_\" + key] = value\n            <DED>for key, value in val_metrics.items():\n                <IND>metrics[\"validation_\" + key] = value\n\n            <DED>if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                <IND>metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    <IND>metrics[\"best_validation_\" + key] = value\n\n                <DED>self._metric_tracker.best_epoch_metrics = val_metrics\n\n            <DED>if self._serialization_dir and self._primary:\n                <IND>common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            <DED>if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step(this_epoch_val_metric)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            <DED>if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            <DED>epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                <IND>training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            <DED>epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                <IND>logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        <DED><DED>else:\n            <IND>epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        <DED>best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            <IND>self.model.load_state_dict(best_model_state)\n\n        <DED>return metrics, epoch\n\n    <DED>@contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        <IND>if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            <IND>training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        <DED>if self._momentum_scheduler is not None:\n            <IND>training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        <DED>try:\n            <IND>yield model_state, training_states\n        <DED>finally:\n            <IND>if self._moving_average is not None:\n                <IND>self._moving_average.restore()\n\n    <DED><DED><DED>def _restore_checkpoint(self) -> int:\n        <IND>\"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            <IND>return 0\n\n        <DED>model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            <IND>return 0\n\n        <DED>self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            <IND>self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        <DED>if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            <IND>self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        <DED>training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            <IND>self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        <DED>else:\n            <IND>self._metric_tracker.clear()\n\n        <DED>if isinstance(training_state[\"epoch\"], int):\n            <IND>epoch_to_return = training_state[\"epoch\"] + 1\n        <DED>else:\n            <IND>epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        <DED>batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            <IND>self._batch_num_total = batch_num_total\n\n        <DED>return epoch_to_return\n\n    <DED>@classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        <IND>\"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            <IND>from torch import cuda\n\n            if cuda.device_count() > 0:\n                <IND>cuda_device = 0\n            <DED>else:\n                <IND>cuda_device = -1\n\n        <DED><DED>check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            <IND>model = model.cuda(cuda_device)\n\n        <DED>if no_grad:\n            <IND>for name, parameter in model.named_parameters():\n                <IND>if any(re.search(regex, name) for regex in no_grad):\n                    <IND>parameter.requires_grad_(False)\n\n        <DED><DED><DED>parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            <IND>batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        <DED>except TypeError:\n            <IND>batches_per_epoch = None\n\n        <DED>moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            <IND>callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        <DED>return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\n<DED><DED>DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def get_best_weights_path(self) -> Optional[str]:\n        <IND>\"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "c5bff8ba0d835eb03931f10f4f427ffe936cf796",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:291:8 Incompatible variable type [9]: checkpointer is declared to have type `Checkpointer` but is used as type `None`.",
    "message": " checkpointer is declared to have type `Checkpointer` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 291,
    "warning_line": "        checkpointer: Checkpointer = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n\n@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    \"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            self._validation_data_loader.set_target_device(self.cuda_device)\n        self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            if validation_data_loader is not None:\n                logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        elif (not isinstance(patience, int)) or patience <= 0:\n            raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            self._checkpointer = Checkpointer(serialization_dir)\n\n        self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            default_callbacks.append(ConfidenceChecksCallback)\n        for callback_cls in default_callbacks:\n            for callback in self._callbacks:\n                if callback.__class__ == callback_cls:\n                    break\n            else:\n                self._callbacks.append(callback_cls(self._serialization_dir))\n\n        self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            if self.cuda_device == torch.device(\"cpu\"):\n                raise ValueError(\"Using AMP requires a cuda device\")\n            self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        if self._distributed:\n            self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        else:\n            self._pytorch_model = self.model\n\n    def rescale_gradients(self) -> float:\n        \"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                self._scaler.unscale_(self.optimizer)\n            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        else:\n            return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            try:\n                assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            except AssertionError:\n                if for_training:\n                    raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        return output_dict\n\n    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        \"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        except TypeError:\n            num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        else:\n            batch_group_generator_tqdm = batch_group_generator\n\n        self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            self._batch_num_total = 0\n\n        done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            if done_early:\n                break\n\n            batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                for p in param_group[\"params\"]:\n                    p.grad = None\n\n            batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                with amp.autocast(self._use_amp):\n                    batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        raise ValueError(\"nan loss encountered\")\n                    loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                if self._scaler is not None:\n                    self._scaler.scale(loss).backward()\n                else:\n                    loss.backward()\n            if len(batch_group_outputs) <= 0:\n                continue\n\n            train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step_batch(batch_num_total)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step_batch(batch_num_total)\n\n            if self._scaler is not None:\n                self._scaler.step(self.optimizer)\n                self._scaler.update()\n            else:\n                self.optimizer.step()\n\n            # Update moving averages\n            if self._moving_average is not None:\n                self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        if self._distributed:\n            dist.barrier()\n\n        metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        for (gpu_num, memory) in gpu_memory_usage:\n            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        return metrics\n\n    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        \"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            self._moving_average.assign_average_value()\n\n        if self._validation_data_loader is not None:\n            validation_data_loader = self._validation_data_loader\n        else:\n            raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        else:\n            val_generator_tqdm = validation_data_loader\n\n        batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            with amp.autocast(self._use_amp):\n                batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                val_generator_tqdm.set_description(description, refresh=False)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        if self._moving_average is not None:\n            self._moving_average.restore()\n\n        return val_loss, val_reg_loss, batches_this_epoch\n\n    def train(self) -> Dict[str, Any]:\n        \"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        epoch = None\n        metrics = None\n\n        try:\n            metrics, epoch = self._try_train()\n            return metrics\n        finally:\n            for callback in self._callbacks:\n                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        try:\n            epoch_counter = self._restore_checkpoint()\n        except RuntimeError:\n            traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            metrics[\"best_validation_\" + key] = value\n\n        for epoch in range(epoch_counter, self._num_epochs):\n            epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            # get peak of memory usage\n            for key, value in train_metrics.items():\n                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        dist.barrier()\n\n                    val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                metrics[\"training_\" + key] = value\n            for key, value in val_metrics.items():\n                metrics[\"validation_\" + key] = value\n\n            if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    metrics[\"best_validation_\" + key] = value\n\n                self._metric_tracker.best_epoch_metrics = val_metrics\n\n            if self._serialization_dir and self._primary:\n                common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step(this_epoch_val_metric)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            for callback in self._callbacks:\n                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        else:\n            epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            self.model.load_state_dict(best_model_state)\n\n        return metrics, epoch\n\n    @contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            self._moving_average.assign_average_value()\n\n        model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        if self._momentum_scheduler is not None:\n            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        try:\n            yield model_state, training_states\n        finally:\n            if self._moving_average is not None:\n                self._moving_average.restore()\n\n    def _restore_checkpoint(self) -> int:\n        \"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            return 0\n\n        model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            return 0\n\n        self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        else:\n            self._metric_tracker.clear()\n\n        if isinstance(training_state[\"epoch\"], int):\n            epoch_to_return = training_state[\"epoch\"] + 1\n        else:\n            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            self._batch_num_total = batch_num_total\n\n        return epoch_to_return\n\n    @classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        \"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            from torch import cuda\n\n            if cuda.device_count() > 0:\n                cuda_device = 0\n            else:\n                cuda_device = -1\n\n        check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            model = model.cuda(cuda_device)\n\n        if no_grad:\n            for name, parameter in model.named_parameters():\n                if any(re.search(regex, name) for regex in no_grad):\n                    parameter.requires_grad_(False)\n\n        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        except TypeError:\n            batches_per_epoch = None\n\n        moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\nDEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_len": 45826,
        "target_code": "\n    def get_best_weights_path(self) -> Optional[str]:\n        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_len": 151,
        "diff_format": "@@ -115,1021 +89,4 @@\n \n-\n-@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\n-class GradientDescentTrainer(Trainer):\n-    \"\"\"\n-    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n-    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n-    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n-    stopping. There are many other bells and whistles as well.\n-\n-    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n-    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n-    see the arguments to that function for the exact keys that should be used, if you are using\n-    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n-    docstrings in `from_partial_objects`.\n-\n-    [0]: https://tinyurl.com/y5mv44fw\n-\n-    # Parameters\n-\n-    model : `Model`, required.\n-        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n-        their `forward` method returns a dictionary with a \"loss\" key, containing a\n-        scalar tensor representing the loss function to be optimized.\n-\n-        If you are training your model using GPUs, your model should already be\n-        on the correct device. (If you are using our `train` command this will be\n-        handled for you.)\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    optimizer : `torch.nn.Optimizer`, required.\n-        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n-        model to be optimized.\n-\n-    data_loader : `DataLoader`, required.\n-        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    patience : `Optional[int] > 0`, optional (default=`None`)\n-        Number of epochs to be patient before early stopping: the training is stopped\n-        after `patience` epochs with no improvement. If given, it must be `> 0`.\n-        If None, early stopping is disabled.\n-\n-    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n-        Validation metric to measure for whether to stop training using patience\n-        and whether to serialize an `is_best` model each epoch. The metric name\n-        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n-        is an increasing or decreasing function. If you specify more than one metric,\n-        the metrics will be summed to make the `is_best` decision.\n-\n-    validation_data_loader : `DataLoader`, optional (default=`None`)\n-        A `DataLoader` to use for the validation set.  If `None`, then\n-        use the training `DataLoader` with the validation data.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_epochs : `int`, optional (default = `20`)\n-        Number of training epochs.\n-\n-    serialization_dir : `str`, optional (default=`None`)\n-        Path to directory for saving and loading model files. Models will not be saved if\n-        this parameter is not passed.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    checkpointer : `Checkpointer`, optional (default=`None`)\n-        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n-        here, we will construct one with default parameters.\n-\n-    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n-        An integer or `torch.device` specifying the CUDA device to use for this process.\n-        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n-\n-        !!! Note\n-            If you *don't* intend to use a GPU, but you have one available, you'll need\n-            to explicitly set `cuda_device=-1`.\n-\n-        !!! Note\n-            If you intend to use a GPU, your model already needs to be on the correct device,\n-            which you can do with `model = model.cuda()`.\n-\n-        !!! Note\n-            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n-\n-    grad_norm : `float`, optional, (default = `None`).\n-        If provided, gradient norms will be rescaled to have a maximum of this value.\n-\n-    grad_clipping : `float`, optional (default = `None`).\n-        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n-        maximum of this value.  If you are getting `NaNs` in your gradients during training\n-        that are not solved by using `grad_norm`, you may need this.\n-\n-    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n-        If specified, the learning rate will be decayed with respect to\n-        this schedule at the end of each epoch (or batch, if the scheduler implements\n-        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n-        this will use the `validation_metric` provided to determine if learning has plateaued.\n-        To support updating the learning rate on every batch, this can optionally implement\n-        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n-\n-    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n-        If specified, the momentum will be updated at the end of each batch or epoch\n-        according to the schedule.\n-\n-    moving_average : `MovingAverage`, optional, (default = `None`)\n-        If provided, we will maintain moving averages for all parameters. During training, we\n-        employ a shadow variable for each parameter, which maintains the moving average. During\n-        evaluation, we backup the original parameters and assign the moving averages to corresponding\n-        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n-        parameters. This is necessary because we want the saved model to perform as well as the validated\n-        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n-\n-    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n-        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n-        and end of training, etc.\n-\n-    distributed : `bool`, optional, (default = `False`)\n-        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n-        requires `world_size` to be greater than 1.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n-        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n-\n-    local_rank : `int`, optional, (default = `0`)\n-        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n-        used as the rank.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    world_size : `int`, (default = `1`)\n-        The number of `Trainer` workers participating in the distributed training.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n-        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n-        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n-        post][0] for details on Gradient Accumulation.\n-\n-    use_amp : `bool`, optional, (default = `False`)\n-        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n-\n-    enable_default_callbacks : `bool`, optional (default = `True`)\n-        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n-        addition to any other callbacks listed in the `callbacks` parameter.\n-        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n-\n-    run_confidence_checks : `bool`, optional (default = `True`)\n-        Determines whether model confidence checks, such as\n-        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n-        are run.\n-\n-    run_sanity_checks : `bool`, optional (default = `True`)\n-        This parameter is deprecated. Please use `run_confidence_checks` instead.\n-\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        model: Model,\n-        optimizer: torch.optim.Optimizer,\n-        data_loader: DataLoader,\n-        patience: Optional[int] = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        validation_data_loader: DataLoader = None,\n-        num_epochs: int = 20,\n-        serialization_dir: Optional[str] = None,\n-        checkpointer: Checkpointer = None,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: Optional[float] = None,\n-        grad_clipping: Optional[float] = None,\n-        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n-        momentum_scheduler: Optional[MomentumScheduler] = None,\n-        moving_average: Optional[MovingAverage] = None,\n-        callbacks: List[TrainerCallback] = None,\n-        distributed: bool = False,\n-        local_rank: int = 0,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-        )\n-\n-        if \"run_sanity_checks\" in kwargs:\n-            warnings.warn(\n-                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n-                DeprecationWarning,\n-            )\n-            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n-\n-        # I am not calling move_to_gpu here, because if the model is\n-        # not already on the GPU then the optimizer is going to be wrong.\n-        self.model = model\n-\n-        self.data_loader = data_loader\n-        self.data_loader.set_target_device(self.cuda_device)\n-        self._validation_data_loader = validation_data_loader\n-        if self._validation_data_loader is not None:\n-            self._validation_data_loader.set_target_device(self.cuda_device)\n-        self.optimizer = optimizer\n-\n-        if patience is None:  # no early stopping\n-            if validation_data_loader is not None:\n-                logger.warning(\n-                    \"You provided a validation dataset but patience was set to None, \"\n-                    \"meaning that early stopping is disabled\"\n-                )\n-        elif (not isinstance(patience, int)) or patience <= 0:\n-            raise ConfigurationError(\n-                '{} is an invalid value for \"patience\": it must be a positive integer '\n-                \"or None (if you want to disable early stopping)\".format(patience)\n-            )\n-\n-        # For tracking is_best_so_far and should_stop_early\n-        self._metric_tracker = MetricTracker(validation_metric, patience)\n-\n-        self._num_epochs = num_epochs\n-\n-        self._checkpointer: Optional[Checkpointer] = checkpointer\n-        if checkpointer is None and serialization_dir is not None:\n-            self._checkpointer = Checkpointer(serialization_dir)\n-\n-        self._grad_norm = grad_norm\n-        self._grad_clipping = grad_clipping\n-\n-        self._learning_rate_scheduler = learning_rate_scheduler\n-        self._momentum_scheduler = momentum_scheduler\n-        self._moving_average = moving_average\n-\n-        self._callbacks = callbacks or []\n-        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n-\n-        if run_confidence_checks:\n-            default_callbacks.append(ConfidenceChecksCallback)\n-        for callback_cls in default_callbacks:\n-            for callback in self._callbacks:\n-                if callback.__class__ == callback_cls:\n-                    break\n-            else:\n-                self._callbacks.append(callback_cls(self._serialization_dir))\n-\n-        self._batch_num_total = 0\n-        self._last_log = 0.0  # time of last logging\n-        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n-\n-        # Enable automatic mixed precision training.\n-        self._scaler: Optional[amp.GradScaler] = None\n-        self._use_amp = use_amp\n-        if self._use_amp:\n-            if self.cuda_device == torch.device(\"cpu\"):\n-                raise ValueError(\"Using AMP requires a cuda device\")\n-            self._scaler = amp.GradScaler()\n-\n-        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n-        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n-        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n-        #\n-        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n-        # normal case, reference to `Model` is retained. This reference is only used in\n-        # these places: `model.__call__`, `model.train` and `model.eval`.\n-        if self._distributed:\n-            self._pytorch_model = DistributedDataParallel(\n-                self.model,\n-                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n-                find_unused_parameters=True,\n-            )\n-        else:\n-            self._pytorch_model = self.model\n-\n-    def rescale_gradients(self) -> float:\n-        \"\"\"\n-        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n-\n-        Returns the norm of the gradients.\n-        \"\"\"\n-        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n-        if self._grad_norm:\n-            if self._scaler is not None:\n-                # Need to first unscale gradients in order to clip as usual.\n-                self._scaler.unscale_(self.optimizer)\n-            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n-        else:\n-            return torch.norm(\n-                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n-            )\n-\n-    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n-        \"\"\"\n-        Does a forward pass on the given batch and returns the output dictionary that the model\n-        returns, after adding any specified regularization penalty to the loss (if training).\n-        \"\"\"\n-        output_dict = self._pytorch_model(**batch)\n-\n-        if for_training:\n-            try:\n-                assert \"loss\" in output_dict\n-                regularization_penalty = self.model.get_regularization_penalty()\n-\n-                if regularization_penalty is not None:\n-                    output_dict[\"reg_loss\"] = regularization_penalty\n-                    output_dict[\"loss\"] += regularization_penalty\n-\n-            except AssertionError:\n-                if for_training:\n-                    raise RuntimeError(\n-                        \"The model you are trying to optimize does not contain a\"\n-                        \" 'loss' key in the output of model.forward(inputs).\"\n-                    )\n-\n-        return output_dict\n-\n-    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n-        \"\"\"\n-        Trains one epoch and returns metrics.\n-        \"\"\"\n-        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n-        cpu_memory_usage = []\n-        for worker, memory in common_util.peak_cpu_memory().items():\n-            cpu_memory_usage.append((worker, memory))\n-            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n-        gpu_memory_usage = []\n-        for gpu, memory in common_util.peak_gpu_memory().items():\n-            gpu_memory_usage.append((gpu, memory))\n-            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        train_loss = 0.0\n-        batch_loss = 0.0\n-        train_reg_loss = None if regularization_penalty is None else 0.0\n-        batch_reg_loss = None if regularization_penalty is None else 0.0\n-\n-        # Set the model to \"train\" mode.\n-        self._pytorch_model.train()\n-\n-        # Get tqdm for the training batches\n-        batch_generator = iter(self.data_loader)\n-        batch_group_generator = common_util.lazy_groups_of(\n-            batch_generator, self._num_gradient_accumulation_steps\n-        )\n-\n-        logger.info(\"Training\")\n-\n-        num_training_batches: Union[int, float]\n-        try:\n-            len_data_loader = len(self.data_loader)\n-            num_training_batches = math.ceil(\n-                len_data_loader / self._num_gradient_accumulation_steps\n-            )\n-        except TypeError:\n-            num_training_batches = float(\"inf\")\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            batch_group_generator_tqdm = Tqdm.tqdm(\n-                batch_group_generator, total=num_training_batches\n-            )\n-        else:\n-            batch_group_generator_tqdm = batch_group_generator\n-\n-        self._last_log = time.time()\n-\n-        batches_this_epoch = 0\n-        if self._batch_num_total is None:\n-            self._batch_num_total = 0\n-\n-        done_early = False\n-        for batch_group in batch_group_generator_tqdm:\n-            if done_early:\n-                break\n-\n-            batches_this_epoch += 1\n-            self._batch_num_total += 1\n-            batch_num_total = self._batch_num_total\n-\n-            # Zero gradients.\n-            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n-            # because it avoids a read op when the gradients are first updated below.\n-            for param_group in self.optimizer.param_groups:\n-                for p in param_group[\"params\"]:\n-                    p.grad = None\n-\n-            batch_loss = 0.0\n-            batch_group_outputs = []\n-            for batch in batch_group:\n-                if self._distributed:\n-                    # Check whether the other workers have stopped already (due to differing amounts of\n-                    # data in each). If so, we can't proceed because we would hang when we hit the\n-                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                    # here because NCCL process groups apparently don't support BoolTensor.\n-                    done = torch.tensor(0, device=self.cuda_device)\n-                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                    if done.item() > 0:\n-                        done_early = True\n-                        logger.warning(\n-                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n-                            \"This implies that there is an imbalance in your training \"\n-                            \"data across the workers and that some amount of it will be \"\n-                            \"ignored. A small amount of this is fine, but a major imbalance \"\n-                            \"should be avoided. Note: This warning will appear unless your \"\n-                            \"data is perfectly balanced.\"\n-                        )\n-                        break\n-\n-                with amp.autocast(self._use_amp):\n-                    batch_outputs = self.batch_outputs(batch, for_training=True)\n-                    batch_group_outputs.append(batch_outputs)\n-                    loss = batch_outputs[\"loss\"]\n-                    reg_loss = batch_outputs.get(\"reg_loss\")\n-                    if torch.isnan(loss):\n-                        raise ValueError(\"nan loss encountered\")\n-                    loss = loss / len(batch_group)\n-\n-                    batch_loss += loss.item()\n-                    if reg_loss is not None:\n-                        reg_loss = reg_loss / len(batch_group)\n-                        batch_reg_loss = reg_loss.item()\n-                        train_reg_loss += batch_reg_loss  # type: ignore\n-\n-                if self._scaler is not None:\n-                    self._scaler.scale(loss).backward()\n-                else:\n-                    loss.backward()\n-            if len(batch_group_outputs) <= 0:\n-                continue\n-\n-            train_loss += batch_loss\n-\n-            batch_grad_norm = self.rescale_gradients()\n-\n-            # This does nothing if batch_num_total is None or you are using a\n-            # scheduler which doesn't update per batch.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step_batch(batch_num_total)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step_batch(batch_num_total)\n-\n-            if self._scaler is not None:\n-                self._scaler.step(self.optimizer)\n-                self._scaler.update()\n-            else:\n-                self.optimizer.step()\n-\n-            # Update moving averages\n-            if self._moving_average is not None:\n-                self._moving_average.apply(batch_num_total)\n-\n-            # Update the description with the latest metrics\n-            metrics = training_util.get_metrics(\n-                self.model,\n-                train_loss,\n-                train_reg_loss,\n-                batch_loss,\n-                batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            if self._primary:\n-                # Updating tqdm only for the primary as the trainers wouldn't have one\n-                description = training_util.description_from_metrics(metrics)\n-                batch_group_generator_tqdm.set_description(description, refresh=False)\n-\n-                if self._checkpointer is not None:\n-                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    batch_group,\n-                    batch_group_outputs,\n-                    metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=True,\n-                    is_primary=self._primary,\n-                    batch_grad_norm=batch_grad_norm,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Let all workers finish their epoch before computing\n-        # the final statistics for the epoch.\n-        if self._distributed:\n-            dist.barrier()\n-\n-        metrics = training_util.get_metrics(\n-            self.model,\n-            train_loss,\n-            train_reg_loss,\n-            batch_loss=None,\n-            batch_reg_loss=None,\n-            num_batches=batches_this_epoch,\n-            reset=True,\n-            world_size=self._world_size,\n-            cuda_device=self.cuda_device,\n-        )\n-\n-        for (worker, memory) in cpu_memory_usage:\n-            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        for (gpu_num, memory) in gpu_memory_usage:\n-            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        return metrics\n-\n-    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n-        \"\"\"\n-        Computes the validation loss. Returns it and the number of batches.\n-        \"\"\"\n-        logger.info(\"Validating\")\n-\n-        self._pytorch_model.eval()\n-\n-        # Replace parameter values with the shadow values from the moving averages.\n-        if self._moving_average is not None:\n-            self._moving_average.assign_average_value()\n-\n-        if self._validation_data_loader is not None:\n-            validation_data_loader = self._validation_data_loader\n-        else:\n-            raise ConfigurationError(\n-                \"Validation results cannot be calculated without a validation_data_loader\"\n-            )\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n-        else:\n-            val_generator_tqdm = validation_data_loader\n-\n-        batches_this_epoch = 0\n-        val_loss = 0.0\n-        val_batch_loss = 0.0\n-        val_reg_loss = None if regularization_penalty is None else 0.0\n-        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n-        done_early = False\n-        for batch in val_generator_tqdm:\n-            if self._distributed:\n-                # Check whether the other workers have stopped already (due to differing amounts of\n-                # data in each). If so, we can't proceed because we would hang when we hit the\n-                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                # here because NCCL process groups apparently don't support BoolTensor.\n-                done = torch.tensor(0, device=self.cuda_device)\n-                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                if done.item() > 0:\n-                    done_early = True\n-                    logger.warning(\n-                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n-                        \"This implies that there is an imbalance in your validation \"\n-                        \"data across the workers and that some amount of it will be \"\n-                        \"ignored. A small amount of this is fine, but a major imbalance \"\n-                        \"should be avoided. Note: This warning will appear unless your \"\n-                        \"data is perfectly balanced.\"\n-                    )\n-                    break\n-\n-            with amp.autocast(self._use_amp):\n-                batch_outputs = self.batch_outputs(batch, for_training=False)\n-                loss = batch_outputs.get(\"loss\")\n-                reg_loss = batch_outputs.get(\"reg_loss\")\n-                if loss is not None:\n-                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n-                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n-                    # currently only used as the divisor for the loss function, so we can safely only\n-                    # count those batches for which we actually have a loss.  If this variable ever\n-                    # gets used for something else, we might need to change things around a bit.\n-                    batches_this_epoch += 1\n-                    val_batch_loss = loss.item()\n-                    val_loss += val_batch_loss\n-                    if reg_loss is not None:\n-                        val_batch_reg_loss = reg_loss.item()\n-                        val_reg_loss += val_batch_reg_loss  # type: ignore\n-\n-            # Update the description with the latest metrics\n-            val_metrics = training_util.get_metrics(\n-                self.model,\n-                val_loss,\n-                val_reg_loss,\n-                val_batch_loss,\n-                val_batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            description = training_util.description_from_metrics(val_metrics)\n-            if self._primary:\n-                val_generator_tqdm.set_description(description, refresh=False)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    [batch],\n-                    [batch_outputs],\n-                    val_metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=False,\n-                    is_primary=self._primary,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop validation early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Now restore the original parameter values.\n-        if self._moving_average is not None:\n-            self._moving_average.restore()\n-\n-        return val_loss, val_reg_loss, batches_this_epoch\n-\n-    def train(self) -> Dict[str, Any]:\n-        \"\"\"\n-        Trains the supplied model with the supplied parameters.\n-        \"\"\"\n-\n-        for callback in self._callbacks:\n-            callback.on_start(self, is_primary=self._primary)\n-\n-        # Set default values in case of failure\n-        epoch = None\n-        metrics = None\n-\n-        try:\n-            metrics, epoch = self._try_train()\n-            return metrics\n-        finally:\n-            for callback in self._callbacks:\n-                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n-        try:\n-            epoch_counter = self._restore_checkpoint()\n-        except RuntimeError:\n-            traceback.print_exc()\n-            raise ConfigurationError(\n-                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n-                \"a different serialization directory or delete the existing serialization \"\n-                \"directory?\"\n-            )\n-\n-        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n-\n-        logger.info(\"Beginning training.\")\n-\n-        val_metrics: Dict[str, float] = {}\n-        metrics: Dict[str, Any] = {}\n-        epochs_trained = 0\n-        training_start_time = time.time()\n-\n-        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n-        for key, value in self._metric_tracker.best_epoch_metrics.items():\n-            metrics[\"best_validation_\" + key] = value\n-\n-        for epoch in range(epoch_counter, self._num_epochs):\n-            epoch_start_time = time.time()\n-            train_metrics = self._train_epoch(epoch)\n-\n-            # Back up the model now, in case something goes wrong later with the evaluation\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.shelve_model(epoch, self)\n-            # Wait for the primary process to finish saving the model checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            # get peak of memory usage\n-            for key, value in train_metrics.items():\n-                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-\n-            this_epoch_val_metric: float = 0.0\n-            if self._validation_data_loader is not None:\n-                with torch.no_grad():\n-                    # We have a validation set, so compute all the metrics on it.\n-                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n-\n-                    # It is safe again to wait till the validation is done. This is\n-                    # important to get the metrics right.\n-                    if self._distributed:\n-                        dist.barrier()\n-\n-                    val_metrics = training_util.get_metrics(\n-                        self.model,\n-                        val_loss,\n-                        val_reg_loss,\n-                        batch_loss=None,\n-                        batch_reg_loss=None,\n-                        num_batches=num_batches,\n-                        reset=True,\n-                        world_size=self._world_size,\n-                        cuda_device=self.cuda_device,\n-                    )\n-\n-                    # Check validation metric for early stopping\n-                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n-                    self._metric_tracker.add_metrics(val_metrics)\n-\n-            # Create overall metrics dict\n-            training_elapsed_time = time.time() - training_start_time\n-            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n-            metrics[\"training_start_epoch\"] = epoch_counter\n-            metrics[\"training_epochs\"] = epochs_trained\n-            metrics[\"epoch\"] = epoch\n-\n-            for key, value in train_metrics.items():\n-                metrics[\"training_\" + key] = value\n-            for key, value in val_metrics.items():\n-                metrics[\"validation_\" + key] = value\n-\n-            if self._metric_tracker.is_best_so_far():\n-                # Update all the best_ metrics.\n-                # (Otherwise they just stay the same as they were.)\n-                metrics[\"best_epoch\"] = epoch\n-                for key, value in val_metrics.items():\n-                    metrics[\"best_validation_\" + key] = value\n-\n-                self._metric_tracker.best_epoch_metrics = val_metrics\n-\n-            if self._serialization_dir and self._primary:\n-                common_util.dump_metrics(\n-                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n-                    metrics,\n-                )\n-\n-            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n-            # if it doesn't, the validation metric passed here is ignored.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step(this_epoch_val_metric)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step(this_epoch_val_metric)\n-\n-            # The checkpointer saves state from the learning rate scheduler and the momentum\n-            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.save_checkpoint(\n-                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n-                )\n-            # Wait for the primary process to finish saving the checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            for callback in self._callbacks:\n-                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-            epoch_elapsed_time = time.time() - epoch_start_time\n-            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n-\n-            if epoch < self._num_epochs - 1:\n-                training_elapsed_time = time.time() - training_start_time\n-                estimated_time_remaining = training_elapsed_time * (\n-                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n-                )\n-                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n-                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n-\n-            epochs_trained += 1\n-\n-            if self._metric_tracker.should_stop_early():\n-                logger.info(\"Ran out of patience. Stopping training.\")\n-                break\n-        else:\n-            epoch = self._num_epochs - 1\n-\n-        # Load the best model state before returning\n-        best_model_state = (\n-            None if self._checkpointer is None else self._checkpointer.best_model_state()\n-        )\n-        if best_model_state:\n-            self.model.load_state_dict(best_model_state)\n-\n-        return metrics, epoch\n-\n-    @contextmanager\n-    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n-        if self._moving_average is not None:\n-            # Assigning average value to model parameters.  The checkpointer will call\n-            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n-            self._moving_average.assign_average_value()\n-\n-        model_state = self.model.state_dict()\n-\n-        # These are the training states we need to persist.\n-        training_states = {\n-            \"metric_tracker\": self._metric_tracker.state_dict(),\n-            \"optimizer\": self.optimizer.state_dict(),\n-            \"batch_num_total\": self._batch_num_total,\n-        }\n-\n-        # If we have a learning rate or momentum scheduler, we should persist them too.\n-        if self._learning_rate_scheduler is not None:\n-            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n-        if self._momentum_scheduler is not None:\n-            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n-\n-        try:\n-            yield model_state, training_states\n-        finally:\n-            if self._moving_average is not None:\n-                self._moving_average.restore()\n-\n-    def _restore_checkpoint(self) -> int:\n-        \"\"\"\n-        Restores the model and training state from the last saved checkpoint.\n-        This includes an epoch count and optimizer state, which is serialized separately\n-        from model parameters. This function should only be used to continue training -\n-        if you wish to load a model for inference/load parts of a model into a new\n-        computation graph, you should use the native Pytorch functions:\n-        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n-\n-        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n-        this function will do nothing and return 0.\n-\n-        # Returns\n-\n-        epoch: `int`\n-            The epoch at which to resume training, which should be one after the epoch\n-            in the saved training state.\n-        \"\"\"\n-        if self._checkpointer is None:\n-            return 0\n-\n-        model_state, training_state = self._checkpointer.restore_checkpoint()\n-\n-        if not training_state:\n-            # No checkpoint to restore, start at 0\n-            return 0\n-\n-        self.model.load_state_dict(model_state)\n-        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n-        if (\n-            self._learning_rate_scheduler is not None\n-            and \"learning_rate_scheduler\" in training_state\n-        ):\n-            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n-        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n-            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n-        training_util.move_optimizer_to_cuda(self.optimizer)\n-\n-        # Currently the `training_state` contains a serialized `MetricTracker`.\n-        if \"metric_tracker\" in training_state:\n-            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n-        else:\n-            self._metric_tracker.clear()\n-\n-        if isinstance(training_state[\"epoch\"], int):\n-            epoch_to_return = training_state[\"epoch\"] + 1\n-        else:\n-            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n-\n-        # For older checkpoints with batch_num_total missing, default to old behavior where\n-        # it is unchanged.\n-        batch_num_total = training_state.get(\"batch_num_total\")\n-        if batch_num_total is not None:\n-            self._batch_num_total = batch_num_total\n-\n-        return epoch_to_return\n-\n-    @classmethod\n-    def from_partial_objects(\n-        cls,\n-        model: Model,\n-        serialization_dir: str,\n-        data_loader: DataLoader,\n-        validation_data_loader: DataLoader = None,\n-        local_rank: int = 0,\n-        patience: int = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        num_epochs: int = 20,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: float = None,\n-        grad_clipping: float = None,\n-        distributed: bool = False,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        no_grad: List[str] = None,\n-        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n-        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n-        momentum_scheduler: Lazy[MomentumScheduler] = None,\n-        moving_average: Lazy[MovingAverage] = None,\n-        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n-        callbacks: List[Lazy[TrainerCallback]] = None,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> \"Trainer\":\n-        \"\"\"\n-        This method exists so that we can have a documented method to construct this class using\n-        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n-        method.\n-\n-        The reason we can't just use `__init__` with `FromParams` here is because there are\n-        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n-        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n-        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n-        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n-        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n-\n-        If you're not using `FromParams`, you can just construct these arguments in the right order\n-        yourself in your code and call the constructor directly.\n-        \"\"\"\n-        if cuda_device is None:\n-            from torch import cuda\n-\n-            if cuda.device_count() > 0:\n-                cuda_device = 0\n-            else:\n-                cuda_device = -1\n-\n-        check_for_gpu(cuda_device)\n-        if cuda_device >= 0:\n-            # Moving model to GPU here so that the optimizer state gets constructed on\n-            # the right device.\n-            model = model.cuda(cuda_device)\n-\n-        if no_grad:\n-            for name, parameter in model.named_parameters():\n-                if any(re.search(regex, name) for regex in no_grad):\n-                    parameter.requires_grad_(False)\n-\n-        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n-        optimizer_ = optimizer.construct(model_parameters=parameters)\n-\n-        common_util.log_frozen_and_tunable_parameter_names(model)\n-\n-        batches_per_epoch: Optional[int]\n-        try:\n-            batches_per_epoch = len(data_loader)\n-            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n-        except TypeError:\n-            batches_per_epoch = None\n-\n-        moving_average_ = (\n-            None if moving_average is None else moving_average.construct(parameters=parameters)\n-        )\n-        learning_rate_scheduler_ = (\n-            None\n-            if learning_rate_scheduler is None\n-            else learning_rate_scheduler.construct(\n-                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n-            )\n-        )\n-        momentum_scheduler_ = (\n-            None\n-            if momentum_scheduler is None\n-            else momentum_scheduler.construct(optimizer=optimizer_)\n-        )\n-        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n-\n-        callbacks_: List[TrainerCallback] = []\n-        for callback_ in callbacks or []:\n-            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n-\n-        return cls(\n-            model,\n-            optimizer_,\n-            data_loader,\n-            patience=patience,\n-            validation_metric=validation_metric,\n-            validation_data_loader=validation_data_loader,\n-            num_epochs=num_epochs,\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            grad_norm=grad_norm,\n-            grad_clipping=grad_clipping,\n-            learning_rate_scheduler=learning_rate_scheduler_,\n-            momentum_scheduler=momentum_scheduler_,\n-            checkpointer=checkpointer_,\n-            moving_average=moving_average_,\n-            callbacks=callbacks_,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n-            use_amp=use_amp,\n-            enable_default_callbacks=enable_default_callbacks,\n-            run_confidence_checks=run_confidence_checks,\n-            **kwargs,\n-        )\n-\n-\n-DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n-\"\"\"\n-The default callbacks used by `GradientDescentTrainer`.\n-\"\"\"\n+    def get_best_weights_path(self) -> Optional[str]:\n+        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n+        return None\n",
        "source_code_with_indent": "\n\n<DED><DED>@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    <IND>\"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        <IND>super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            <IND>warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        <DED>self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            <IND>self._validation_data_loader.set_target_device(self.cuda_device)\n        <DED>self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            <IND>if validation_data_loader is not None:\n                <IND>logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        <DED><DED>elif (not isinstance(patience, int)) or patience <= 0:\n            <IND>raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        <DED>self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            <IND>self._checkpointer = Checkpointer(serialization_dir)\n\n        <DED>self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            <IND>default_callbacks.append(ConfidenceChecksCallback)\n        <DED>for callback_cls in default_callbacks:\n            <IND>for callback in self._callbacks:\n                <IND>if callback.__class__ == callback_cls:\n                    <IND>break\n            <DED><DED>else:\n                <IND>self._callbacks.append(callback_cls(self._serialization_dir))\n\n        <DED><DED>self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            <IND>if self.cuda_device == torch.device(\"cpu\"):\n                <IND>raise ValueError(\"Using AMP requires a cuda device\")\n            <DED>self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        <DED>if self._distributed:\n            <IND>self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        <DED>else:\n            <IND>self._pytorch_model = self.model\n\n    <DED><DED>def rescale_gradients(self) -> float:\n        <IND>\"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            <IND>if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                <IND>self._scaler.unscale_(self.optimizer)\n            <DED>return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        <DED>else:\n            <IND>return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    <DED><DED>def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        <IND>\"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            <IND>try:\n                <IND>assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    <IND>output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            <DED><DED>except AssertionError:\n                <IND>if for_training:\n                    <IND>raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        <DED><DED><DED>return output_dict\n\n    <DED>def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        <IND>\"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            <IND>cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        <DED>gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            <IND>gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            <IND>len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        <DED>except TypeError:\n            <IND>num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        <DED>if self._primary:\n            <IND>batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        <DED>else:\n            <IND>batch_group_generator_tqdm = batch_group_generator\n\n        <DED>self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            <IND>self._batch_num_total = 0\n\n        <DED>done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            <IND>if done_early:\n                <IND>break\n\n            <DED>batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                <IND>for p in param_group[\"params\"]:\n                    <IND>p.grad = None\n\n            <DED><DED>batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                <IND>if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    <IND>done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        <IND>done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                <DED><DED>with amp.autocast(self._use_amp):\n                    <IND>batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        <IND>raise ValueError(\"nan loss encountered\")\n                    <DED>loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        <IND>reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                <DED><DED>if self._scaler is not None:\n                    <IND>self._scaler.scale(loss).backward()\n                <DED>else:\n                    <IND>loss.backward()\n            <DED><DED>if len(batch_group_outputs) <= 0:\n                <IND>continue\n\n            <DED>train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step_batch(batch_num_total)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step_batch(batch_num_total)\n\n            <DED>if self._scaler is not None:\n                <IND>self._scaler.step(self.optimizer)\n                self._scaler.update()\n            <DED>else:\n                <IND>self.optimizer.step()\n\n            # Update moving averages\n            <DED>if self._moving_average is not None:\n                <IND>self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            <DED>metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                <IND>description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    <IND>self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            <DED><DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        <DED>if self._distributed:\n            <IND>dist.barrier()\n\n        <DED>metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            <IND>metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>for (gpu_num, memory) in gpu_memory_usage:\n            <IND>metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>return metrics\n\n    <DED>def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        <IND>\"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>if self._validation_data_loader is not None:\n            <IND>validation_data_loader = self._validation_data_loader\n        <DED>else:\n            <IND>raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            <IND>val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        <DED>else:\n            <IND>val_generator_tqdm = validation_data_loader\n\n        <DED>batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            <IND>if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                <IND>done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    <IND>done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            <DED><DED>with amp.autocast(self._use_amp):\n                <IND>batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    <IND>batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        <IND>val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            <DED><DED><DED>val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                <IND>val_generator_tqdm.set_description(description, refresh=False)\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        <DED>if self._moving_average is not None:\n            <IND>self._moving_average.restore()\n\n        <DED>return val_loss, val_reg_loss, batches_this_epoch\n\n    <DED>def train(self) -> Dict[str, Any]:\n        <IND>\"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            <IND>callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        <DED>epoch = None\n        metrics = None\n\n        try:\n            <IND>metrics, epoch = self._try_train()\n            return metrics\n        <DED>finally:\n            <IND>for callback in self._callbacks:\n                <IND>callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    <DED><DED><DED>def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        <IND>try:\n            <IND>epoch_counter = self._restore_checkpoint()\n        <DED>except RuntimeError:\n            <IND>traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        <DED>training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            <IND>metrics[\"best_validation_\" + key] = value\n\n        <DED>for epoch in range(epoch_counter, self._num_epochs):\n            <IND>epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            # get peak of memory usage\n            <DED>for key, value in train_metrics.items():\n                <IND>if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                <DED>elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            <DED><DED>this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                <IND>with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    <IND>val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        <IND>dist.barrier()\n\n                    <DED>val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            <DED><DED>training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                <IND>metrics[\"training_\" + key] = value\n            <DED>for key, value in val_metrics.items():\n                <IND>metrics[\"validation_\" + key] = value\n\n            <DED>if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                <IND>metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    <IND>metrics[\"best_validation_\" + key] = value\n\n                <DED>self._metric_tracker.best_epoch_metrics = val_metrics\n\n            <DED>if self._serialization_dir and self._primary:\n                <IND>common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            <DED>if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step(this_epoch_val_metric)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            <DED>if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            <DED>epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                <IND>training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            <DED>epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                <IND>logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        <DED><DED>else:\n            <IND>epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        <DED>best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            <IND>self.model.load_state_dict(best_model_state)\n\n        <DED>return metrics, epoch\n\n    <DED>@contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        <IND>if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            <IND>training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        <DED>if self._momentum_scheduler is not None:\n            <IND>training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        <DED>try:\n            <IND>yield model_state, training_states\n        <DED>finally:\n            <IND>if self._moving_average is not None:\n                <IND>self._moving_average.restore()\n\n    <DED><DED><DED>def _restore_checkpoint(self) -> int:\n        <IND>\"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            <IND>return 0\n\n        <DED>model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            <IND>return 0\n\n        <DED>self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            <IND>self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        <DED>if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            <IND>self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        <DED>training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            <IND>self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        <DED>else:\n            <IND>self._metric_tracker.clear()\n\n        <DED>if isinstance(training_state[\"epoch\"], int):\n            <IND>epoch_to_return = training_state[\"epoch\"] + 1\n        <DED>else:\n            <IND>epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        <DED>batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            <IND>self._batch_num_total = batch_num_total\n\n        <DED>return epoch_to_return\n\n    <DED>@classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        <IND>\"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            <IND>from torch import cuda\n\n            if cuda.device_count() > 0:\n                <IND>cuda_device = 0\n            <DED>else:\n                <IND>cuda_device = -1\n\n        <DED><DED>check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            <IND>model = model.cuda(cuda_device)\n\n        <DED>if no_grad:\n            <IND>for name, parameter in model.named_parameters():\n                <IND>if any(re.search(regex, name) for regex in no_grad):\n                    <IND>parameter.requires_grad_(False)\n\n        <DED><DED><DED>parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            <IND>batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        <DED>except TypeError:\n            <IND>batches_per_epoch = None\n\n        <DED>moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            <IND>callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        <DED>return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\n<DED><DED>DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def get_best_weights_path(self) -> Optional[str]:\n        <IND>\"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "c5bff8ba0d835eb03931f10f4f427ffe936cf796",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:298:8 Incompatible variable type [9]: callbacks is declared to have type `List[allennlp.training.callbacks.callback.TrainerCallback]` but is used as type `None`.",
    "message": " callbacks is declared to have type `List[allennlp.training.callbacks.callback.TrainerCallback]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 298,
    "warning_line": "        callbacks: List[TrainerCallback] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n\n@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    \"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            self._validation_data_loader.set_target_device(self.cuda_device)\n        self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            if validation_data_loader is not None:\n                logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        elif (not isinstance(patience, int)) or patience <= 0:\n            raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            self._checkpointer = Checkpointer(serialization_dir)\n\n        self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            default_callbacks.append(ConfidenceChecksCallback)\n        for callback_cls in default_callbacks:\n            for callback in self._callbacks:\n                if callback.__class__ == callback_cls:\n                    break\n            else:\n                self._callbacks.append(callback_cls(self._serialization_dir))\n\n        self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            if self.cuda_device == torch.device(\"cpu\"):\n                raise ValueError(\"Using AMP requires a cuda device\")\n            self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        if self._distributed:\n            self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        else:\n            self._pytorch_model = self.model\n\n    def rescale_gradients(self) -> float:\n        \"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                self._scaler.unscale_(self.optimizer)\n            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        else:\n            return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            try:\n                assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            except AssertionError:\n                if for_training:\n                    raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        return output_dict\n\n    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        \"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        except TypeError:\n            num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        else:\n            batch_group_generator_tqdm = batch_group_generator\n\n        self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            self._batch_num_total = 0\n\n        done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            if done_early:\n                break\n\n            batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                for p in param_group[\"params\"]:\n                    p.grad = None\n\n            batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                with amp.autocast(self._use_amp):\n                    batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        raise ValueError(\"nan loss encountered\")\n                    loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                if self._scaler is not None:\n                    self._scaler.scale(loss).backward()\n                else:\n                    loss.backward()\n            if len(batch_group_outputs) <= 0:\n                continue\n\n            train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step_batch(batch_num_total)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step_batch(batch_num_total)\n\n            if self._scaler is not None:\n                self._scaler.step(self.optimizer)\n                self._scaler.update()\n            else:\n                self.optimizer.step()\n\n            # Update moving averages\n            if self._moving_average is not None:\n                self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        if self._distributed:\n            dist.barrier()\n\n        metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        for (gpu_num, memory) in gpu_memory_usage:\n            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        return metrics\n\n    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        \"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            self._moving_average.assign_average_value()\n\n        if self._validation_data_loader is not None:\n            validation_data_loader = self._validation_data_loader\n        else:\n            raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        else:\n            val_generator_tqdm = validation_data_loader\n\n        batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            with amp.autocast(self._use_amp):\n                batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                val_generator_tqdm.set_description(description, refresh=False)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        if self._moving_average is not None:\n            self._moving_average.restore()\n\n        return val_loss, val_reg_loss, batches_this_epoch\n\n    def train(self) -> Dict[str, Any]:\n        \"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        epoch = None\n        metrics = None\n\n        try:\n            metrics, epoch = self._try_train()\n            return metrics\n        finally:\n            for callback in self._callbacks:\n                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        try:\n            epoch_counter = self._restore_checkpoint()\n        except RuntimeError:\n            traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            metrics[\"best_validation_\" + key] = value\n\n        for epoch in range(epoch_counter, self._num_epochs):\n            epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            # get peak of memory usage\n            for key, value in train_metrics.items():\n                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        dist.barrier()\n\n                    val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                metrics[\"training_\" + key] = value\n            for key, value in val_metrics.items():\n                metrics[\"validation_\" + key] = value\n\n            if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    metrics[\"best_validation_\" + key] = value\n\n                self._metric_tracker.best_epoch_metrics = val_metrics\n\n            if self._serialization_dir and self._primary:\n                common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step(this_epoch_val_metric)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            for callback in self._callbacks:\n                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        else:\n            epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            self.model.load_state_dict(best_model_state)\n\n        return metrics, epoch\n\n    @contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            self._moving_average.assign_average_value()\n\n        model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        if self._momentum_scheduler is not None:\n            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        try:\n            yield model_state, training_states\n        finally:\n            if self._moving_average is not None:\n                self._moving_average.restore()\n\n    def _restore_checkpoint(self) -> int:\n        \"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            return 0\n\n        model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            return 0\n\n        self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        else:\n            self._metric_tracker.clear()\n\n        if isinstance(training_state[\"epoch\"], int):\n            epoch_to_return = training_state[\"epoch\"] + 1\n        else:\n            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            self._batch_num_total = batch_num_total\n\n        return epoch_to_return\n\n    @classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        \"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            from torch import cuda\n\n            if cuda.device_count() > 0:\n                cuda_device = 0\n            else:\n                cuda_device = -1\n\n        check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            model = model.cuda(cuda_device)\n\n        if no_grad:\n            for name, parameter in model.named_parameters():\n                if any(re.search(regex, name) for regex in no_grad):\n                    parameter.requires_grad_(False)\n\n        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        except TypeError:\n            batches_per_epoch = None\n\n        moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\nDEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_len": 45826,
        "target_code": "\n    def get_best_weights_path(self) -> Optional[str]:\n        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_len": 151,
        "diff_format": "@@ -115,1021 +89,4 @@\n \n-\n-@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\n-class GradientDescentTrainer(Trainer):\n-    \"\"\"\n-    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n-    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n-    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n-    stopping. There are many other bells and whistles as well.\n-\n-    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n-    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n-    see the arguments to that function for the exact keys that should be used, if you are using\n-    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n-    docstrings in `from_partial_objects`.\n-\n-    [0]: https://tinyurl.com/y5mv44fw\n-\n-    # Parameters\n-\n-    model : `Model`, required.\n-        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n-        their `forward` method returns a dictionary with a \"loss\" key, containing a\n-        scalar tensor representing the loss function to be optimized.\n-\n-        If you are training your model using GPUs, your model should already be\n-        on the correct device. (If you are using our `train` command this will be\n-        handled for you.)\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    optimizer : `torch.nn.Optimizer`, required.\n-        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n-        model to be optimized.\n-\n-    data_loader : `DataLoader`, required.\n-        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    patience : `Optional[int] > 0`, optional (default=`None`)\n-        Number of epochs to be patient before early stopping: the training is stopped\n-        after `patience` epochs with no improvement. If given, it must be `> 0`.\n-        If None, early stopping is disabled.\n-\n-    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n-        Validation metric to measure for whether to stop training using patience\n-        and whether to serialize an `is_best` model each epoch. The metric name\n-        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n-        is an increasing or decreasing function. If you specify more than one metric,\n-        the metrics will be summed to make the `is_best` decision.\n-\n-    validation_data_loader : `DataLoader`, optional (default=`None`)\n-        A `DataLoader` to use for the validation set.  If `None`, then\n-        use the training `DataLoader` with the validation data.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_epochs : `int`, optional (default = `20`)\n-        Number of training epochs.\n-\n-    serialization_dir : `str`, optional (default=`None`)\n-        Path to directory for saving and loading model files. Models will not be saved if\n-        this parameter is not passed.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    checkpointer : `Checkpointer`, optional (default=`None`)\n-        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n-        here, we will construct one with default parameters.\n-\n-    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n-        An integer or `torch.device` specifying the CUDA device to use for this process.\n-        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n-\n-        !!! Note\n-            If you *don't* intend to use a GPU, but you have one available, you'll need\n-            to explicitly set `cuda_device=-1`.\n-\n-        !!! Note\n-            If you intend to use a GPU, your model already needs to be on the correct device,\n-            which you can do with `model = model.cuda()`.\n-\n-        !!! Note\n-            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n-\n-    grad_norm : `float`, optional, (default = `None`).\n-        If provided, gradient norms will be rescaled to have a maximum of this value.\n-\n-    grad_clipping : `float`, optional (default = `None`).\n-        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n-        maximum of this value.  If you are getting `NaNs` in your gradients during training\n-        that are not solved by using `grad_norm`, you may need this.\n-\n-    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n-        If specified, the learning rate will be decayed with respect to\n-        this schedule at the end of each epoch (or batch, if the scheduler implements\n-        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n-        this will use the `validation_metric` provided to determine if learning has plateaued.\n-        To support updating the learning rate on every batch, this can optionally implement\n-        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n-\n-    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n-        If specified, the momentum will be updated at the end of each batch or epoch\n-        according to the schedule.\n-\n-    moving_average : `MovingAverage`, optional, (default = `None`)\n-        If provided, we will maintain moving averages for all parameters. During training, we\n-        employ a shadow variable for each parameter, which maintains the moving average. During\n-        evaluation, we backup the original parameters and assign the moving averages to corresponding\n-        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n-        parameters. This is necessary because we want the saved model to perform as well as the validated\n-        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n-\n-    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n-        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n-        and end of training, etc.\n-\n-    distributed : `bool`, optional, (default = `False`)\n-        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n-        requires `world_size` to be greater than 1.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n-        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n-\n-    local_rank : `int`, optional, (default = `0`)\n-        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n-        used as the rank.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    world_size : `int`, (default = `1`)\n-        The number of `Trainer` workers participating in the distributed training.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n-        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n-        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n-        post][0] for details on Gradient Accumulation.\n-\n-    use_amp : `bool`, optional, (default = `False`)\n-        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n-\n-    enable_default_callbacks : `bool`, optional (default = `True`)\n-        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n-        addition to any other callbacks listed in the `callbacks` parameter.\n-        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n-\n-    run_confidence_checks : `bool`, optional (default = `True`)\n-        Determines whether model confidence checks, such as\n-        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n-        are run.\n-\n-    run_sanity_checks : `bool`, optional (default = `True`)\n-        This parameter is deprecated. Please use `run_confidence_checks` instead.\n-\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        model: Model,\n-        optimizer: torch.optim.Optimizer,\n-        data_loader: DataLoader,\n-        patience: Optional[int] = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        validation_data_loader: DataLoader = None,\n-        num_epochs: int = 20,\n-        serialization_dir: Optional[str] = None,\n-        checkpointer: Checkpointer = None,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: Optional[float] = None,\n-        grad_clipping: Optional[float] = None,\n-        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n-        momentum_scheduler: Optional[MomentumScheduler] = None,\n-        moving_average: Optional[MovingAverage] = None,\n-        callbacks: List[TrainerCallback] = None,\n-        distributed: bool = False,\n-        local_rank: int = 0,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-        )\n-\n-        if \"run_sanity_checks\" in kwargs:\n-            warnings.warn(\n-                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n-                DeprecationWarning,\n-            )\n-            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n-\n-        # I am not calling move_to_gpu here, because if the model is\n-        # not already on the GPU then the optimizer is going to be wrong.\n-        self.model = model\n-\n-        self.data_loader = data_loader\n-        self.data_loader.set_target_device(self.cuda_device)\n-        self._validation_data_loader = validation_data_loader\n-        if self._validation_data_loader is not None:\n-            self._validation_data_loader.set_target_device(self.cuda_device)\n-        self.optimizer = optimizer\n-\n-        if patience is None:  # no early stopping\n-            if validation_data_loader is not None:\n-                logger.warning(\n-                    \"You provided a validation dataset but patience was set to None, \"\n-                    \"meaning that early stopping is disabled\"\n-                )\n-        elif (not isinstance(patience, int)) or patience <= 0:\n-            raise ConfigurationError(\n-                '{} is an invalid value for \"patience\": it must be a positive integer '\n-                \"or None (if you want to disable early stopping)\".format(patience)\n-            )\n-\n-        # For tracking is_best_so_far and should_stop_early\n-        self._metric_tracker = MetricTracker(validation_metric, patience)\n-\n-        self._num_epochs = num_epochs\n-\n-        self._checkpointer: Optional[Checkpointer] = checkpointer\n-        if checkpointer is None and serialization_dir is not None:\n-            self._checkpointer = Checkpointer(serialization_dir)\n-\n-        self._grad_norm = grad_norm\n-        self._grad_clipping = grad_clipping\n-\n-        self._learning_rate_scheduler = learning_rate_scheduler\n-        self._momentum_scheduler = momentum_scheduler\n-        self._moving_average = moving_average\n-\n-        self._callbacks = callbacks or []\n-        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n-\n-        if run_confidence_checks:\n-            default_callbacks.append(ConfidenceChecksCallback)\n-        for callback_cls in default_callbacks:\n-            for callback in self._callbacks:\n-                if callback.__class__ == callback_cls:\n-                    break\n-            else:\n-                self._callbacks.append(callback_cls(self._serialization_dir))\n-\n-        self._batch_num_total = 0\n-        self._last_log = 0.0  # time of last logging\n-        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n-\n-        # Enable automatic mixed precision training.\n-        self._scaler: Optional[amp.GradScaler] = None\n-        self._use_amp = use_amp\n-        if self._use_amp:\n-            if self.cuda_device == torch.device(\"cpu\"):\n-                raise ValueError(\"Using AMP requires a cuda device\")\n-            self._scaler = amp.GradScaler()\n-\n-        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n-        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n-        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n-        #\n-        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n-        # normal case, reference to `Model` is retained. This reference is only used in\n-        # these places: `model.__call__`, `model.train` and `model.eval`.\n-        if self._distributed:\n-            self._pytorch_model = DistributedDataParallel(\n-                self.model,\n-                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n-                find_unused_parameters=True,\n-            )\n-        else:\n-            self._pytorch_model = self.model\n-\n-    def rescale_gradients(self) -> float:\n-        \"\"\"\n-        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n-\n-        Returns the norm of the gradients.\n-        \"\"\"\n-        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n-        if self._grad_norm:\n-            if self._scaler is not None:\n-                # Need to first unscale gradients in order to clip as usual.\n-                self._scaler.unscale_(self.optimizer)\n-            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n-        else:\n-            return torch.norm(\n-                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n-            )\n-\n-    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n-        \"\"\"\n-        Does a forward pass on the given batch and returns the output dictionary that the model\n-        returns, after adding any specified regularization penalty to the loss (if training).\n-        \"\"\"\n-        output_dict = self._pytorch_model(**batch)\n-\n-        if for_training:\n-            try:\n-                assert \"loss\" in output_dict\n-                regularization_penalty = self.model.get_regularization_penalty()\n-\n-                if regularization_penalty is not None:\n-                    output_dict[\"reg_loss\"] = regularization_penalty\n-                    output_dict[\"loss\"] += regularization_penalty\n-\n-            except AssertionError:\n-                if for_training:\n-                    raise RuntimeError(\n-                        \"The model you are trying to optimize does not contain a\"\n-                        \" 'loss' key in the output of model.forward(inputs).\"\n-                    )\n-\n-        return output_dict\n-\n-    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n-        \"\"\"\n-        Trains one epoch and returns metrics.\n-        \"\"\"\n-        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n-        cpu_memory_usage = []\n-        for worker, memory in common_util.peak_cpu_memory().items():\n-            cpu_memory_usage.append((worker, memory))\n-            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n-        gpu_memory_usage = []\n-        for gpu, memory in common_util.peak_gpu_memory().items():\n-            gpu_memory_usage.append((gpu, memory))\n-            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        train_loss = 0.0\n-        batch_loss = 0.0\n-        train_reg_loss = None if regularization_penalty is None else 0.0\n-        batch_reg_loss = None if regularization_penalty is None else 0.0\n-\n-        # Set the model to \"train\" mode.\n-        self._pytorch_model.train()\n-\n-        # Get tqdm for the training batches\n-        batch_generator = iter(self.data_loader)\n-        batch_group_generator = common_util.lazy_groups_of(\n-            batch_generator, self._num_gradient_accumulation_steps\n-        )\n-\n-        logger.info(\"Training\")\n-\n-        num_training_batches: Union[int, float]\n-        try:\n-            len_data_loader = len(self.data_loader)\n-            num_training_batches = math.ceil(\n-                len_data_loader / self._num_gradient_accumulation_steps\n-            )\n-        except TypeError:\n-            num_training_batches = float(\"inf\")\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            batch_group_generator_tqdm = Tqdm.tqdm(\n-                batch_group_generator, total=num_training_batches\n-            )\n-        else:\n-            batch_group_generator_tqdm = batch_group_generator\n-\n-        self._last_log = time.time()\n-\n-        batches_this_epoch = 0\n-        if self._batch_num_total is None:\n-            self._batch_num_total = 0\n-\n-        done_early = False\n-        for batch_group in batch_group_generator_tqdm:\n-            if done_early:\n-                break\n-\n-            batches_this_epoch += 1\n-            self._batch_num_total += 1\n-            batch_num_total = self._batch_num_total\n-\n-            # Zero gradients.\n-            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n-            # because it avoids a read op when the gradients are first updated below.\n-            for param_group in self.optimizer.param_groups:\n-                for p in param_group[\"params\"]:\n-                    p.grad = None\n-\n-            batch_loss = 0.0\n-            batch_group_outputs = []\n-            for batch in batch_group:\n-                if self._distributed:\n-                    # Check whether the other workers have stopped already (due to differing amounts of\n-                    # data in each). If so, we can't proceed because we would hang when we hit the\n-                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                    # here because NCCL process groups apparently don't support BoolTensor.\n-                    done = torch.tensor(0, device=self.cuda_device)\n-                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                    if done.item() > 0:\n-                        done_early = True\n-                        logger.warning(\n-                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n-                            \"This implies that there is an imbalance in your training \"\n-                            \"data across the workers and that some amount of it will be \"\n-                            \"ignored. A small amount of this is fine, but a major imbalance \"\n-                            \"should be avoided. Note: This warning will appear unless your \"\n-                            \"data is perfectly balanced.\"\n-                        )\n-                        break\n-\n-                with amp.autocast(self._use_amp):\n-                    batch_outputs = self.batch_outputs(batch, for_training=True)\n-                    batch_group_outputs.append(batch_outputs)\n-                    loss = batch_outputs[\"loss\"]\n-                    reg_loss = batch_outputs.get(\"reg_loss\")\n-                    if torch.isnan(loss):\n-                        raise ValueError(\"nan loss encountered\")\n-                    loss = loss / len(batch_group)\n-\n-                    batch_loss += loss.item()\n-                    if reg_loss is not None:\n-                        reg_loss = reg_loss / len(batch_group)\n-                        batch_reg_loss = reg_loss.item()\n-                        train_reg_loss += batch_reg_loss  # type: ignore\n-\n-                if self._scaler is not None:\n-                    self._scaler.scale(loss).backward()\n-                else:\n-                    loss.backward()\n-            if len(batch_group_outputs) <= 0:\n-                continue\n-\n-            train_loss += batch_loss\n-\n-            batch_grad_norm = self.rescale_gradients()\n-\n-            # This does nothing if batch_num_total is None or you are using a\n-            # scheduler which doesn't update per batch.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step_batch(batch_num_total)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step_batch(batch_num_total)\n-\n-            if self._scaler is not None:\n-                self._scaler.step(self.optimizer)\n-                self._scaler.update()\n-            else:\n-                self.optimizer.step()\n-\n-            # Update moving averages\n-            if self._moving_average is not None:\n-                self._moving_average.apply(batch_num_total)\n-\n-            # Update the description with the latest metrics\n-            metrics = training_util.get_metrics(\n-                self.model,\n-                train_loss,\n-                train_reg_loss,\n-                batch_loss,\n-                batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            if self._primary:\n-                # Updating tqdm only for the primary as the trainers wouldn't have one\n-                description = training_util.description_from_metrics(metrics)\n-                batch_group_generator_tqdm.set_description(description, refresh=False)\n-\n-                if self._checkpointer is not None:\n-                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    batch_group,\n-                    batch_group_outputs,\n-                    metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=True,\n-                    is_primary=self._primary,\n-                    batch_grad_norm=batch_grad_norm,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Let all workers finish their epoch before computing\n-        # the final statistics for the epoch.\n-        if self._distributed:\n-            dist.barrier()\n-\n-        metrics = training_util.get_metrics(\n-            self.model,\n-            train_loss,\n-            train_reg_loss,\n-            batch_loss=None,\n-            batch_reg_loss=None,\n-            num_batches=batches_this_epoch,\n-            reset=True,\n-            world_size=self._world_size,\n-            cuda_device=self.cuda_device,\n-        )\n-\n-        for (worker, memory) in cpu_memory_usage:\n-            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        for (gpu_num, memory) in gpu_memory_usage:\n-            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        return metrics\n-\n-    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n-        \"\"\"\n-        Computes the validation loss. Returns it and the number of batches.\n-        \"\"\"\n-        logger.info(\"Validating\")\n-\n-        self._pytorch_model.eval()\n-\n-        # Replace parameter values with the shadow values from the moving averages.\n-        if self._moving_average is not None:\n-            self._moving_average.assign_average_value()\n-\n-        if self._validation_data_loader is not None:\n-            validation_data_loader = self._validation_data_loader\n-        else:\n-            raise ConfigurationError(\n-                \"Validation results cannot be calculated without a validation_data_loader\"\n-            )\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n-        else:\n-            val_generator_tqdm = validation_data_loader\n-\n-        batches_this_epoch = 0\n-        val_loss = 0.0\n-        val_batch_loss = 0.0\n-        val_reg_loss = None if regularization_penalty is None else 0.0\n-        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n-        done_early = False\n-        for batch in val_generator_tqdm:\n-            if self._distributed:\n-                # Check whether the other workers have stopped already (due to differing amounts of\n-                # data in each). If so, we can't proceed because we would hang when we hit the\n-                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                # here because NCCL process groups apparently don't support BoolTensor.\n-                done = torch.tensor(0, device=self.cuda_device)\n-                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                if done.item() > 0:\n-                    done_early = True\n-                    logger.warning(\n-                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n-                        \"This implies that there is an imbalance in your validation \"\n-                        \"data across the workers and that some amount of it will be \"\n-                        \"ignored. A small amount of this is fine, but a major imbalance \"\n-                        \"should be avoided. Note: This warning will appear unless your \"\n-                        \"data is perfectly balanced.\"\n-                    )\n-                    break\n-\n-            with amp.autocast(self._use_amp):\n-                batch_outputs = self.batch_outputs(batch, for_training=False)\n-                loss = batch_outputs.get(\"loss\")\n-                reg_loss = batch_outputs.get(\"reg_loss\")\n-                if loss is not None:\n-                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n-                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n-                    # currently only used as the divisor for the loss function, so we can safely only\n-                    # count those batches for which we actually have a loss.  If this variable ever\n-                    # gets used for something else, we might need to change things around a bit.\n-                    batches_this_epoch += 1\n-                    val_batch_loss = loss.item()\n-                    val_loss += val_batch_loss\n-                    if reg_loss is not None:\n-                        val_batch_reg_loss = reg_loss.item()\n-                        val_reg_loss += val_batch_reg_loss  # type: ignore\n-\n-            # Update the description with the latest metrics\n-            val_metrics = training_util.get_metrics(\n-                self.model,\n-                val_loss,\n-                val_reg_loss,\n-                val_batch_loss,\n-                val_batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            description = training_util.description_from_metrics(val_metrics)\n-            if self._primary:\n-                val_generator_tqdm.set_description(description, refresh=False)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    [batch],\n-                    [batch_outputs],\n-                    val_metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=False,\n-                    is_primary=self._primary,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop validation early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Now restore the original parameter values.\n-        if self._moving_average is not None:\n-            self._moving_average.restore()\n-\n-        return val_loss, val_reg_loss, batches_this_epoch\n-\n-    def train(self) -> Dict[str, Any]:\n-        \"\"\"\n-        Trains the supplied model with the supplied parameters.\n-        \"\"\"\n-\n-        for callback in self._callbacks:\n-            callback.on_start(self, is_primary=self._primary)\n-\n-        # Set default values in case of failure\n-        epoch = None\n-        metrics = None\n-\n-        try:\n-            metrics, epoch = self._try_train()\n-            return metrics\n-        finally:\n-            for callback in self._callbacks:\n-                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n-        try:\n-            epoch_counter = self._restore_checkpoint()\n-        except RuntimeError:\n-            traceback.print_exc()\n-            raise ConfigurationError(\n-                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n-                \"a different serialization directory or delete the existing serialization \"\n-                \"directory?\"\n-            )\n-\n-        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n-\n-        logger.info(\"Beginning training.\")\n-\n-        val_metrics: Dict[str, float] = {}\n-        metrics: Dict[str, Any] = {}\n-        epochs_trained = 0\n-        training_start_time = time.time()\n-\n-        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n-        for key, value in self._metric_tracker.best_epoch_metrics.items():\n-            metrics[\"best_validation_\" + key] = value\n-\n-        for epoch in range(epoch_counter, self._num_epochs):\n-            epoch_start_time = time.time()\n-            train_metrics = self._train_epoch(epoch)\n-\n-            # Back up the model now, in case something goes wrong later with the evaluation\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.shelve_model(epoch, self)\n-            # Wait for the primary process to finish saving the model checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            # get peak of memory usage\n-            for key, value in train_metrics.items():\n-                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-\n-            this_epoch_val_metric: float = 0.0\n-            if self._validation_data_loader is not None:\n-                with torch.no_grad():\n-                    # We have a validation set, so compute all the metrics on it.\n-                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n-\n-                    # It is safe again to wait till the validation is done. This is\n-                    # important to get the metrics right.\n-                    if self._distributed:\n-                        dist.barrier()\n-\n-                    val_metrics = training_util.get_metrics(\n-                        self.model,\n-                        val_loss,\n-                        val_reg_loss,\n-                        batch_loss=None,\n-                        batch_reg_loss=None,\n-                        num_batches=num_batches,\n-                        reset=True,\n-                        world_size=self._world_size,\n-                        cuda_device=self.cuda_device,\n-                    )\n-\n-                    # Check validation metric for early stopping\n-                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n-                    self._metric_tracker.add_metrics(val_metrics)\n-\n-            # Create overall metrics dict\n-            training_elapsed_time = time.time() - training_start_time\n-            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n-            metrics[\"training_start_epoch\"] = epoch_counter\n-            metrics[\"training_epochs\"] = epochs_trained\n-            metrics[\"epoch\"] = epoch\n-\n-            for key, value in train_metrics.items():\n-                metrics[\"training_\" + key] = value\n-            for key, value in val_metrics.items():\n-                metrics[\"validation_\" + key] = value\n-\n-            if self._metric_tracker.is_best_so_far():\n-                # Update all the best_ metrics.\n-                # (Otherwise they just stay the same as they were.)\n-                metrics[\"best_epoch\"] = epoch\n-                for key, value in val_metrics.items():\n-                    metrics[\"best_validation_\" + key] = value\n-\n-                self._metric_tracker.best_epoch_metrics = val_metrics\n-\n-            if self._serialization_dir and self._primary:\n-                common_util.dump_metrics(\n-                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n-                    metrics,\n-                )\n-\n-            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n-            # if it doesn't, the validation metric passed here is ignored.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step(this_epoch_val_metric)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step(this_epoch_val_metric)\n-\n-            # The checkpointer saves state from the learning rate scheduler and the momentum\n-            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.save_checkpoint(\n-                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n-                )\n-            # Wait for the primary process to finish saving the checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            for callback in self._callbacks:\n-                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-            epoch_elapsed_time = time.time() - epoch_start_time\n-            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n-\n-            if epoch < self._num_epochs - 1:\n-                training_elapsed_time = time.time() - training_start_time\n-                estimated_time_remaining = training_elapsed_time * (\n-                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n-                )\n-                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n-                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n-\n-            epochs_trained += 1\n-\n-            if self._metric_tracker.should_stop_early():\n-                logger.info(\"Ran out of patience. Stopping training.\")\n-                break\n-        else:\n-            epoch = self._num_epochs - 1\n-\n-        # Load the best model state before returning\n-        best_model_state = (\n-            None if self._checkpointer is None else self._checkpointer.best_model_state()\n-        )\n-        if best_model_state:\n-            self.model.load_state_dict(best_model_state)\n-\n-        return metrics, epoch\n-\n-    @contextmanager\n-    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n-        if self._moving_average is not None:\n-            # Assigning average value to model parameters.  The checkpointer will call\n-            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n-            self._moving_average.assign_average_value()\n-\n-        model_state = self.model.state_dict()\n-\n-        # These are the training states we need to persist.\n-        training_states = {\n-            \"metric_tracker\": self._metric_tracker.state_dict(),\n-            \"optimizer\": self.optimizer.state_dict(),\n-            \"batch_num_total\": self._batch_num_total,\n-        }\n-\n-        # If we have a learning rate or momentum scheduler, we should persist them too.\n-        if self._learning_rate_scheduler is not None:\n-            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n-        if self._momentum_scheduler is not None:\n-            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n-\n-        try:\n-            yield model_state, training_states\n-        finally:\n-            if self._moving_average is not None:\n-                self._moving_average.restore()\n-\n-    def _restore_checkpoint(self) -> int:\n-        \"\"\"\n-        Restores the model and training state from the last saved checkpoint.\n-        This includes an epoch count and optimizer state, which is serialized separately\n-        from model parameters. This function should only be used to continue training -\n-        if you wish to load a model for inference/load parts of a model into a new\n-        computation graph, you should use the native Pytorch functions:\n-        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n-\n-        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n-        this function will do nothing and return 0.\n-\n-        # Returns\n-\n-        epoch: `int`\n-            The epoch at which to resume training, which should be one after the epoch\n-            in the saved training state.\n-        \"\"\"\n-        if self._checkpointer is None:\n-            return 0\n-\n-        model_state, training_state = self._checkpointer.restore_checkpoint()\n-\n-        if not training_state:\n-            # No checkpoint to restore, start at 0\n-            return 0\n-\n-        self.model.load_state_dict(model_state)\n-        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n-        if (\n-            self._learning_rate_scheduler is not None\n-            and \"learning_rate_scheduler\" in training_state\n-        ):\n-            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n-        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n-            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n-        training_util.move_optimizer_to_cuda(self.optimizer)\n-\n-        # Currently the `training_state` contains a serialized `MetricTracker`.\n-        if \"metric_tracker\" in training_state:\n-            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n-        else:\n-            self._metric_tracker.clear()\n-\n-        if isinstance(training_state[\"epoch\"], int):\n-            epoch_to_return = training_state[\"epoch\"] + 1\n-        else:\n-            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n-\n-        # For older checkpoints with batch_num_total missing, default to old behavior where\n-        # it is unchanged.\n-        batch_num_total = training_state.get(\"batch_num_total\")\n-        if batch_num_total is not None:\n-            self._batch_num_total = batch_num_total\n-\n-        return epoch_to_return\n-\n-    @classmethod\n-    def from_partial_objects(\n-        cls,\n-        model: Model,\n-        serialization_dir: str,\n-        data_loader: DataLoader,\n-        validation_data_loader: DataLoader = None,\n-        local_rank: int = 0,\n-        patience: int = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        num_epochs: int = 20,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: float = None,\n-        grad_clipping: float = None,\n-        distributed: bool = False,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        no_grad: List[str] = None,\n-        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n-        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n-        momentum_scheduler: Lazy[MomentumScheduler] = None,\n-        moving_average: Lazy[MovingAverage] = None,\n-        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n-        callbacks: List[Lazy[TrainerCallback]] = None,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> \"Trainer\":\n-        \"\"\"\n-        This method exists so that we can have a documented method to construct this class using\n-        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n-        method.\n-\n-        The reason we can't just use `__init__` with `FromParams` here is because there are\n-        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n-        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n-        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n-        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n-        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n-\n-        If you're not using `FromParams`, you can just construct these arguments in the right order\n-        yourself in your code and call the constructor directly.\n-        \"\"\"\n-        if cuda_device is None:\n-            from torch import cuda\n-\n-            if cuda.device_count() > 0:\n-                cuda_device = 0\n-            else:\n-                cuda_device = -1\n-\n-        check_for_gpu(cuda_device)\n-        if cuda_device >= 0:\n-            # Moving model to GPU here so that the optimizer state gets constructed on\n-            # the right device.\n-            model = model.cuda(cuda_device)\n-\n-        if no_grad:\n-            for name, parameter in model.named_parameters():\n-                if any(re.search(regex, name) for regex in no_grad):\n-                    parameter.requires_grad_(False)\n-\n-        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n-        optimizer_ = optimizer.construct(model_parameters=parameters)\n-\n-        common_util.log_frozen_and_tunable_parameter_names(model)\n-\n-        batches_per_epoch: Optional[int]\n-        try:\n-            batches_per_epoch = len(data_loader)\n-            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n-        except TypeError:\n-            batches_per_epoch = None\n-\n-        moving_average_ = (\n-            None if moving_average is None else moving_average.construct(parameters=parameters)\n-        )\n-        learning_rate_scheduler_ = (\n-            None\n-            if learning_rate_scheduler is None\n-            else learning_rate_scheduler.construct(\n-                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n-            )\n-        )\n-        momentum_scheduler_ = (\n-            None\n-            if momentum_scheduler is None\n-            else momentum_scheduler.construct(optimizer=optimizer_)\n-        )\n-        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n-\n-        callbacks_: List[TrainerCallback] = []\n-        for callback_ in callbacks or []:\n-            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n-\n-        return cls(\n-            model,\n-            optimizer_,\n-            data_loader,\n-            patience=patience,\n-            validation_metric=validation_metric,\n-            validation_data_loader=validation_data_loader,\n-            num_epochs=num_epochs,\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            grad_norm=grad_norm,\n-            grad_clipping=grad_clipping,\n-            learning_rate_scheduler=learning_rate_scheduler_,\n-            momentum_scheduler=momentum_scheduler_,\n-            checkpointer=checkpointer_,\n-            moving_average=moving_average_,\n-            callbacks=callbacks_,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n-            use_amp=use_amp,\n-            enable_default_callbacks=enable_default_callbacks,\n-            run_confidence_checks=run_confidence_checks,\n-            **kwargs,\n-        )\n-\n-\n-DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n-\"\"\"\n-The default callbacks used by `GradientDescentTrainer`.\n-\"\"\"\n+    def get_best_weights_path(self) -> Optional[str]:\n+        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n+        return None\n",
        "source_code_with_indent": "\n\n<DED><DED>@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    <IND>\"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        <IND>super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            <IND>warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        <DED>self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            <IND>self._validation_data_loader.set_target_device(self.cuda_device)\n        <DED>self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            <IND>if validation_data_loader is not None:\n                <IND>logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        <DED><DED>elif (not isinstance(patience, int)) or patience <= 0:\n            <IND>raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        <DED>self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            <IND>self._checkpointer = Checkpointer(serialization_dir)\n\n        <DED>self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            <IND>default_callbacks.append(ConfidenceChecksCallback)\n        <DED>for callback_cls in default_callbacks:\n            <IND>for callback in self._callbacks:\n                <IND>if callback.__class__ == callback_cls:\n                    <IND>break\n            <DED><DED>else:\n                <IND>self._callbacks.append(callback_cls(self._serialization_dir))\n\n        <DED><DED>self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            <IND>if self.cuda_device == torch.device(\"cpu\"):\n                <IND>raise ValueError(\"Using AMP requires a cuda device\")\n            <DED>self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        <DED>if self._distributed:\n            <IND>self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        <DED>else:\n            <IND>self._pytorch_model = self.model\n\n    <DED><DED>def rescale_gradients(self) -> float:\n        <IND>\"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            <IND>if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                <IND>self._scaler.unscale_(self.optimizer)\n            <DED>return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        <DED>else:\n            <IND>return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    <DED><DED>def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        <IND>\"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            <IND>try:\n                <IND>assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    <IND>output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            <DED><DED>except AssertionError:\n                <IND>if for_training:\n                    <IND>raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        <DED><DED><DED>return output_dict\n\n    <DED>def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        <IND>\"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            <IND>cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        <DED>gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            <IND>gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            <IND>len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        <DED>except TypeError:\n            <IND>num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        <DED>if self._primary:\n            <IND>batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        <DED>else:\n            <IND>batch_group_generator_tqdm = batch_group_generator\n\n        <DED>self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            <IND>self._batch_num_total = 0\n\n        <DED>done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            <IND>if done_early:\n                <IND>break\n\n            <DED>batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                <IND>for p in param_group[\"params\"]:\n                    <IND>p.grad = None\n\n            <DED><DED>batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                <IND>if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    <IND>done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        <IND>done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                <DED><DED>with amp.autocast(self._use_amp):\n                    <IND>batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        <IND>raise ValueError(\"nan loss encountered\")\n                    <DED>loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        <IND>reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                <DED><DED>if self._scaler is not None:\n                    <IND>self._scaler.scale(loss).backward()\n                <DED>else:\n                    <IND>loss.backward()\n            <DED><DED>if len(batch_group_outputs) <= 0:\n                <IND>continue\n\n            <DED>train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step_batch(batch_num_total)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step_batch(batch_num_total)\n\n            <DED>if self._scaler is not None:\n                <IND>self._scaler.step(self.optimizer)\n                self._scaler.update()\n            <DED>else:\n                <IND>self.optimizer.step()\n\n            # Update moving averages\n            <DED>if self._moving_average is not None:\n                <IND>self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            <DED>metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                <IND>description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    <IND>self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            <DED><DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        <DED>if self._distributed:\n            <IND>dist.barrier()\n\n        <DED>metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            <IND>metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>for (gpu_num, memory) in gpu_memory_usage:\n            <IND>metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>return metrics\n\n    <DED>def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        <IND>\"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>if self._validation_data_loader is not None:\n            <IND>validation_data_loader = self._validation_data_loader\n        <DED>else:\n            <IND>raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            <IND>val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        <DED>else:\n            <IND>val_generator_tqdm = validation_data_loader\n\n        <DED>batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            <IND>if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                <IND>done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    <IND>done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            <DED><DED>with amp.autocast(self._use_amp):\n                <IND>batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    <IND>batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        <IND>val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            <DED><DED><DED>val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                <IND>val_generator_tqdm.set_description(description, refresh=False)\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        <DED>if self._moving_average is not None:\n            <IND>self._moving_average.restore()\n\n        <DED>return val_loss, val_reg_loss, batches_this_epoch\n\n    <DED>def train(self) -> Dict[str, Any]:\n        <IND>\"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            <IND>callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        <DED>epoch = None\n        metrics = None\n\n        try:\n            <IND>metrics, epoch = self._try_train()\n            return metrics\n        <DED>finally:\n            <IND>for callback in self._callbacks:\n                <IND>callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    <DED><DED><DED>def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        <IND>try:\n            <IND>epoch_counter = self._restore_checkpoint()\n        <DED>except RuntimeError:\n            <IND>traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        <DED>training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            <IND>metrics[\"best_validation_\" + key] = value\n\n        <DED>for epoch in range(epoch_counter, self._num_epochs):\n            <IND>epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            # get peak of memory usage\n            <DED>for key, value in train_metrics.items():\n                <IND>if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                <DED>elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            <DED><DED>this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                <IND>with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    <IND>val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        <IND>dist.barrier()\n\n                    <DED>val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            <DED><DED>training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                <IND>metrics[\"training_\" + key] = value\n            <DED>for key, value in val_metrics.items():\n                <IND>metrics[\"validation_\" + key] = value\n\n            <DED>if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                <IND>metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    <IND>metrics[\"best_validation_\" + key] = value\n\n                <DED>self._metric_tracker.best_epoch_metrics = val_metrics\n\n            <DED>if self._serialization_dir and self._primary:\n                <IND>common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            <DED>if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step(this_epoch_val_metric)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            <DED>if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            <DED>epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                <IND>training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            <DED>epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                <IND>logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        <DED><DED>else:\n            <IND>epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        <DED>best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            <IND>self.model.load_state_dict(best_model_state)\n\n        <DED>return metrics, epoch\n\n    <DED>@contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        <IND>if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            <IND>training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        <DED>if self._momentum_scheduler is not None:\n            <IND>training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        <DED>try:\n            <IND>yield model_state, training_states\n        <DED>finally:\n            <IND>if self._moving_average is not None:\n                <IND>self._moving_average.restore()\n\n    <DED><DED><DED>def _restore_checkpoint(self) -> int:\n        <IND>\"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            <IND>return 0\n\n        <DED>model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            <IND>return 0\n\n        <DED>self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            <IND>self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        <DED>if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            <IND>self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        <DED>training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            <IND>self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        <DED>else:\n            <IND>self._metric_tracker.clear()\n\n        <DED>if isinstance(training_state[\"epoch\"], int):\n            <IND>epoch_to_return = training_state[\"epoch\"] + 1\n        <DED>else:\n            <IND>epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        <DED>batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            <IND>self._batch_num_total = batch_num_total\n\n        <DED>return epoch_to_return\n\n    <DED>@classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        <IND>\"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            <IND>from torch import cuda\n\n            if cuda.device_count() > 0:\n                <IND>cuda_device = 0\n            <DED>else:\n                <IND>cuda_device = -1\n\n        <DED><DED>check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            <IND>model = model.cuda(cuda_device)\n\n        <DED>if no_grad:\n            <IND>for name, parameter in model.named_parameters():\n                <IND>if any(re.search(regex, name) for regex in no_grad):\n                    <IND>parameter.requires_grad_(False)\n\n        <DED><DED><DED>parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            <IND>batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        <DED>except TypeError:\n            <IND>batches_per_epoch = None\n\n        <DED>moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            <IND>callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        <DED>return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\n<DED><DED>DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def get_best_weights_path(self) -> Optional[str]:\n        <IND>\"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "c5bff8ba0d835eb03931f10f4f427ffe936cf796",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:309:12 Incompatible parameter type [6]: Expected `str` for 1st parameter `serialization_dir` to call `Trainer.__init__` but got `Optional[str]`.",
    "message": " Expected `str` for 1st parameter `serialization_dir` to call `Trainer.__init__` but got `Optional[str]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 309,
    "warning_line": "            serialization_dir=serialization_dir,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n\n@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    \"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            self._validation_data_loader.set_target_device(self.cuda_device)\n        self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            if validation_data_loader is not None:\n                logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        elif (not isinstance(patience, int)) or patience <= 0:\n            raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            self._checkpointer = Checkpointer(serialization_dir)\n\n        self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            default_callbacks.append(ConfidenceChecksCallback)\n        for callback_cls in default_callbacks:\n            for callback in self._callbacks:\n                if callback.__class__ == callback_cls:\n                    break\n            else:\n                self._callbacks.append(callback_cls(self._serialization_dir))\n\n        self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            if self.cuda_device == torch.device(\"cpu\"):\n                raise ValueError(\"Using AMP requires a cuda device\")\n            self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        if self._distributed:\n            self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        else:\n            self._pytorch_model = self.model\n\n    def rescale_gradients(self) -> float:\n        \"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                self._scaler.unscale_(self.optimizer)\n            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        else:\n            return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            try:\n                assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            except AssertionError:\n                if for_training:\n                    raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        return output_dict\n\n    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        \"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        except TypeError:\n            num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        else:\n            batch_group_generator_tqdm = batch_group_generator\n\n        self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            self._batch_num_total = 0\n\n        done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            if done_early:\n                break\n\n            batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                for p in param_group[\"params\"]:\n                    p.grad = None\n\n            batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                with amp.autocast(self._use_amp):\n                    batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        raise ValueError(\"nan loss encountered\")\n                    loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                if self._scaler is not None:\n                    self._scaler.scale(loss).backward()\n                else:\n                    loss.backward()\n            if len(batch_group_outputs) <= 0:\n                continue\n\n            train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step_batch(batch_num_total)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step_batch(batch_num_total)\n\n            if self._scaler is not None:\n                self._scaler.step(self.optimizer)\n                self._scaler.update()\n            else:\n                self.optimizer.step()\n\n            # Update moving averages\n            if self._moving_average is not None:\n                self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        if self._distributed:\n            dist.barrier()\n\n        metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        for (gpu_num, memory) in gpu_memory_usage:\n            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        return metrics\n\n    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        \"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            self._moving_average.assign_average_value()\n\n        if self._validation_data_loader is not None:\n            validation_data_loader = self._validation_data_loader\n        else:\n            raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        else:\n            val_generator_tqdm = validation_data_loader\n\n        batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            with amp.autocast(self._use_amp):\n                batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                val_generator_tqdm.set_description(description, refresh=False)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        if self._moving_average is not None:\n            self._moving_average.restore()\n\n        return val_loss, val_reg_loss, batches_this_epoch\n\n    def train(self) -> Dict[str, Any]:\n        \"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        epoch = None\n        metrics = None\n\n        try:\n            metrics, epoch = self._try_train()\n            return metrics\n        finally:\n            for callback in self._callbacks:\n                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        try:\n            epoch_counter = self._restore_checkpoint()\n        except RuntimeError:\n            traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            metrics[\"best_validation_\" + key] = value\n\n        for epoch in range(epoch_counter, self._num_epochs):\n            epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            # get peak of memory usage\n            for key, value in train_metrics.items():\n                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        dist.barrier()\n\n                    val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                metrics[\"training_\" + key] = value\n            for key, value in val_metrics.items():\n                metrics[\"validation_\" + key] = value\n\n            if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    metrics[\"best_validation_\" + key] = value\n\n                self._metric_tracker.best_epoch_metrics = val_metrics\n\n            if self._serialization_dir and self._primary:\n                common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step(this_epoch_val_metric)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            for callback in self._callbacks:\n                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        else:\n            epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            self.model.load_state_dict(best_model_state)\n\n        return metrics, epoch\n\n    @contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            self._moving_average.assign_average_value()\n\n        model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        if self._momentum_scheduler is not None:\n            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        try:\n            yield model_state, training_states\n        finally:\n            if self._moving_average is not None:\n                self._moving_average.restore()\n\n    def _restore_checkpoint(self) -> int:\n        \"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            return 0\n\n        model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            return 0\n\n        self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        else:\n            self._metric_tracker.clear()\n\n        if isinstance(training_state[\"epoch\"], int):\n            epoch_to_return = training_state[\"epoch\"] + 1\n        else:\n            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            self._batch_num_total = batch_num_total\n\n        return epoch_to_return\n\n    @classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        \"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            from torch import cuda\n\n            if cuda.device_count() > 0:\n                cuda_device = 0\n            else:\n                cuda_device = -1\n\n        check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            model = model.cuda(cuda_device)\n\n        if no_grad:\n            for name, parameter in model.named_parameters():\n                if any(re.search(regex, name) for regex in no_grad):\n                    parameter.requires_grad_(False)\n\n        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        except TypeError:\n            batches_per_epoch = None\n\n        moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\nDEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_len": 45826,
        "target_code": "\n    def get_best_weights_path(self) -> Optional[str]:\n        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_len": 151,
        "diff_format": "@@ -115,1021 +89,4 @@\n \n-\n-@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\n-class GradientDescentTrainer(Trainer):\n-    \"\"\"\n-    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n-    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n-    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n-    stopping. There are many other bells and whistles as well.\n-\n-    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n-    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n-    see the arguments to that function for the exact keys that should be used, if you are using\n-    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n-    docstrings in `from_partial_objects`.\n-\n-    [0]: https://tinyurl.com/y5mv44fw\n-\n-    # Parameters\n-\n-    model : `Model`, required.\n-        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n-        their `forward` method returns a dictionary with a \"loss\" key, containing a\n-        scalar tensor representing the loss function to be optimized.\n-\n-        If you are training your model using GPUs, your model should already be\n-        on the correct device. (If you are using our `train` command this will be\n-        handled for you.)\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    optimizer : `torch.nn.Optimizer`, required.\n-        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n-        model to be optimized.\n-\n-    data_loader : `DataLoader`, required.\n-        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    patience : `Optional[int] > 0`, optional (default=`None`)\n-        Number of epochs to be patient before early stopping: the training is stopped\n-        after `patience` epochs with no improvement. If given, it must be `> 0`.\n-        If None, early stopping is disabled.\n-\n-    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n-        Validation metric to measure for whether to stop training using patience\n-        and whether to serialize an `is_best` model each epoch. The metric name\n-        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n-        is an increasing or decreasing function. If you specify more than one metric,\n-        the metrics will be summed to make the `is_best` decision.\n-\n-    validation_data_loader : `DataLoader`, optional (default=`None`)\n-        A `DataLoader` to use for the validation set.  If `None`, then\n-        use the training `DataLoader` with the validation data.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_epochs : `int`, optional (default = `20`)\n-        Number of training epochs.\n-\n-    serialization_dir : `str`, optional (default=`None`)\n-        Path to directory for saving and loading model files. Models will not be saved if\n-        this parameter is not passed.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    checkpointer : `Checkpointer`, optional (default=`None`)\n-        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n-        here, we will construct one with default parameters.\n-\n-    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n-        An integer or `torch.device` specifying the CUDA device to use for this process.\n-        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n-\n-        !!! Note\n-            If you *don't* intend to use a GPU, but you have one available, you'll need\n-            to explicitly set `cuda_device=-1`.\n-\n-        !!! Note\n-            If you intend to use a GPU, your model already needs to be on the correct device,\n-            which you can do with `model = model.cuda()`.\n-\n-        !!! Note\n-            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n-\n-    grad_norm : `float`, optional, (default = `None`).\n-        If provided, gradient norms will be rescaled to have a maximum of this value.\n-\n-    grad_clipping : `float`, optional (default = `None`).\n-        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n-        maximum of this value.  If you are getting `NaNs` in your gradients during training\n-        that are not solved by using `grad_norm`, you may need this.\n-\n-    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n-        If specified, the learning rate will be decayed with respect to\n-        this schedule at the end of each epoch (or batch, if the scheduler implements\n-        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n-        this will use the `validation_metric` provided to determine if learning has plateaued.\n-        To support updating the learning rate on every batch, this can optionally implement\n-        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n-\n-    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n-        If specified, the momentum will be updated at the end of each batch or epoch\n-        according to the schedule.\n-\n-    moving_average : `MovingAverage`, optional, (default = `None`)\n-        If provided, we will maintain moving averages for all parameters. During training, we\n-        employ a shadow variable for each parameter, which maintains the moving average. During\n-        evaluation, we backup the original parameters and assign the moving averages to corresponding\n-        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n-        parameters. This is necessary because we want the saved model to perform as well as the validated\n-        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n-\n-    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n-        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n-        and end of training, etc.\n-\n-    distributed : `bool`, optional, (default = `False`)\n-        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n-        requires `world_size` to be greater than 1.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n-        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n-\n-    local_rank : `int`, optional, (default = `0`)\n-        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n-        used as the rank.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    world_size : `int`, (default = `1`)\n-        The number of `Trainer` workers participating in the distributed training.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n-        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n-        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n-        post][0] for details on Gradient Accumulation.\n-\n-    use_amp : `bool`, optional, (default = `False`)\n-        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n-\n-    enable_default_callbacks : `bool`, optional (default = `True`)\n-        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n-        addition to any other callbacks listed in the `callbacks` parameter.\n-        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n-\n-    run_confidence_checks : `bool`, optional (default = `True`)\n-        Determines whether model confidence checks, such as\n-        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n-        are run.\n-\n-    run_sanity_checks : `bool`, optional (default = `True`)\n-        This parameter is deprecated. Please use `run_confidence_checks` instead.\n-\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        model: Model,\n-        optimizer: torch.optim.Optimizer,\n-        data_loader: DataLoader,\n-        patience: Optional[int] = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        validation_data_loader: DataLoader = None,\n-        num_epochs: int = 20,\n-        serialization_dir: Optional[str] = None,\n-        checkpointer: Checkpointer = None,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: Optional[float] = None,\n-        grad_clipping: Optional[float] = None,\n-        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n-        momentum_scheduler: Optional[MomentumScheduler] = None,\n-        moving_average: Optional[MovingAverage] = None,\n-        callbacks: List[TrainerCallback] = None,\n-        distributed: bool = False,\n-        local_rank: int = 0,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-        )\n-\n-        if \"run_sanity_checks\" in kwargs:\n-            warnings.warn(\n-                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n-                DeprecationWarning,\n-            )\n-            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n-\n-        # I am not calling move_to_gpu here, because if the model is\n-        # not already on the GPU then the optimizer is going to be wrong.\n-        self.model = model\n-\n-        self.data_loader = data_loader\n-        self.data_loader.set_target_device(self.cuda_device)\n-        self._validation_data_loader = validation_data_loader\n-        if self._validation_data_loader is not None:\n-            self._validation_data_loader.set_target_device(self.cuda_device)\n-        self.optimizer = optimizer\n-\n-        if patience is None:  # no early stopping\n-            if validation_data_loader is not None:\n-                logger.warning(\n-                    \"You provided a validation dataset but patience was set to None, \"\n-                    \"meaning that early stopping is disabled\"\n-                )\n-        elif (not isinstance(patience, int)) or patience <= 0:\n-            raise ConfigurationError(\n-                '{} is an invalid value for \"patience\": it must be a positive integer '\n-                \"or None (if you want to disable early stopping)\".format(patience)\n-            )\n-\n-        # For tracking is_best_so_far and should_stop_early\n-        self._metric_tracker = MetricTracker(validation_metric, patience)\n-\n-        self._num_epochs = num_epochs\n-\n-        self._checkpointer: Optional[Checkpointer] = checkpointer\n-        if checkpointer is None and serialization_dir is not None:\n-            self._checkpointer = Checkpointer(serialization_dir)\n-\n-        self._grad_norm = grad_norm\n-        self._grad_clipping = grad_clipping\n-\n-        self._learning_rate_scheduler = learning_rate_scheduler\n-        self._momentum_scheduler = momentum_scheduler\n-        self._moving_average = moving_average\n-\n-        self._callbacks = callbacks or []\n-        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n-\n-        if run_confidence_checks:\n-            default_callbacks.append(ConfidenceChecksCallback)\n-        for callback_cls in default_callbacks:\n-            for callback in self._callbacks:\n-                if callback.__class__ == callback_cls:\n-                    break\n-            else:\n-                self._callbacks.append(callback_cls(self._serialization_dir))\n-\n-        self._batch_num_total = 0\n-        self._last_log = 0.0  # time of last logging\n-        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n-\n-        # Enable automatic mixed precision training.\n-        self._scaler: Optional[amp.GradScaler] = None\n-        self._use_amp = use_amp\n-        if self._use_amp:\n-            if self.cuda_device == torch.device(\"cpu\"):\n-                raise ValueError(\"Using AMP requires a cuda device\")\n-            self._scaler = amp.GradScaler()\n-\n-        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n-        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n-        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n-        #\n-        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n-        # normal case, reference to `Model` is retained. This reference is only used in\n-        # these places: `model.__call__`, `model.train` and `model.eval`.\n-        if self._distributed:\n-            self._pytorch_model = DistributedDataParallel(\n-                self.model,\n-                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n-                find_unused_parameters=True,\n-            )\n-        else:\n-            self._pytorch_model = self.model\n-\n-    def rescale_gradients(self) -> float:\n-        \"\"\"\n-        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n-\n-        Returns the norm of the gradients.\n-        \"\"\"\n-        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n-        if self._grad_norm:\n-            if self._scaler is not None:\n-                # Need to first unscale gradients in order to clip as usual.\n-                self._scaler.unscale_(self.optimizer)\n-            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n-        else:\n-            return torch.norm(\n-                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n-            )\n-\n-    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n-        \"\"\"\n-        Does a forward pass on the given batch and returns the output dictionary that the model\n-        returns, after adding any specified regularization penalty to the loss (if training).\n-        \"\"\"\n-        output_dict = self._pytorch_model(**batch)\n-\n-        if for_training:\n-            try:\n-                assert \"loss\" in output_dict\n-                regularization_penalty = self.model.get_regularization_penalty()\n-\n-                if regularization_penalty is not None:\n-                    output_dict[\"reg_loss\"] = regularization_penalty\n-                    output_dict[\"loss\"] += regularization_penalty\n-\n-            except AssertionError:\n-                if for_training:\n-                    raise RuntimeError(\n-                        \"The model you are trying to optimize does not contain a\"\n-                        \" 'loss' key in the output of model.forward(inputs).\"\n-                    )\n-\n-        return output_dict\n-\n-    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n-        \"\"\"\n-        Trains one epoch and returns metrics.\n-        \"\"\"\n-        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n-        cpu_memory_usage = []\n-        for worker, memory in common_util.peak_cpu_memory().items():\n-            cpu_memory_usage.append((worker, memory))\n-            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n-        gpu_memory_usage = []\n-        for gpu, memory in common_util.peak_gpu_memory().items():\n-            gpu_memory_usage.append((gpu, memory))\n-            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        train_loss = 0.0\n-        batch_loss = 0.0\n-        train_reg_loss = None if regularization_penalty is None else 0.0\n-        batch_reg_loss = None if regularization_penalty is None else 0.0\n-\n-        # Set the model to \"train\" mode.\n-        self._pytorch_model.train()\n-\n-        # Get tqdm for the training batches\n-        batch_generator = iter(self.data_loader)\n-        batch_group_generator = common_util.lazy_groups_of(\n-            batch_generator, self._num_gradient_accumulation_steps\n-        )\n-\n-        logger.info(\"Training\")\n-\n-        num_training_batches: Union[int, float]\n-        try:\n-            len_data_loader = len(self.data_loader)\n-            num_training_batches = math.ceil(\n-                len_data_loader / self._num_gradient_accumulation_steps\n-            )\n-        except TypeError:\n-            num_training_batches = float(\"inf\")\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            batch_group_generator_tqdm = Tqdm.tqdm(\n-                batch_group_generator, total=num_training_batches\n-            )\n-        else:\n-            batch_group_generator_tqdm = batch_group_generator\n-\n-        self._last_log = time.time()\n-\n-        batches_this_epoch = 0\n-        if self._batch_num_total is None:\n-            self._batch_num_total = 0\n-\n-        done_early = False\n-        for batch_group in batch_group_generator_tqdm:\n-            if done_early:\n-                break\n-\n-            batches_this_epoch += 1\n-            self._batch_num_total += 1\n-            batch_num_total = self._batch_num_total\n-\n-            # Zero gradients.\n-            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n-            # because it avoids a read op when the gradients are first updated below.\n-            for param_group in self.optimizer.param_groups:\n-                for p in param_group[\"params\"]:\n-                    p.grad = None\n-\n-            batch_loss = 0.0\n-            batch_group_outputs = []\n-            for batch in batch_group:\n-                if self._distributed:\n-                    # Check whether the other workers have stopped already (due to differing amounts of\n-                    # data in each). If so, we can't proceed because we would hang when we hit the\n-                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                    # here because NCCL process groups apparently don't support BoolTensor.\n-                    done = torch.tensor(0, device=self.cuda_device)\n-                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                    if done.item() > 0:\n-                        done_early = True\n-                        logger.warning(\n-                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n-                            \"This implies that there is an imbalance in your training \"\n-                            \"data across the workers and that some amount of it will be \"\n-                            \"ignored. A small amount of this is fine, but a major imbalance \"\n-                            \"should be avoided. Note: This warning will appear unless your \"\n-                            \"data is perfectly balanced.\"\n-                        )\n-                        break\n-\n-                with amp.autocast(self._use_amp):\n-                    batch_outputs = self.batch_outputs(batch, for_training=True)\n-                    batch_group_outputs.append(batch_outputs)\n-                    loss = batch_outputs[\"loss\"]\n-                    reg_loss = batch_outputs.get(\"reg_loss\")\n-                    if torch.isnan(loss):\n-                        raise ValueError(\"nan loss encountered\")\n-                    loss = loss / len(batch_group)\n-\n-                    batch_loss += loss.item()\n-                    if reg_loss is not None:\n-                        reg_loss = reg_loss / len(batch_group)\n-                        batch_reg_loss = reg_loss.item()\n-                        train_reg_loss += batch_reg_loss  # type: ignore\n-\n-                if self._scaler is not None:\n-                    self._scaler.scale(loss).backward()\n-                else:\n-                    loss.backward()\n-            if len(batch_group_outputs) <= 0:\n-                continue\n-\n-            train_loss += batch_loss\n-\n-            batch_grad_norm = self.rescale_gradients()\n-\n-            # This does nothing if batch_num_total is None or you are using a\n-            # scheduler which doesn't update per batch.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step_batch(batch_num_total)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step_batch(batch_num_total)\n-\n-            if self._scaler is not None:\n-                self._scaler.step(self.optimizer)\n-                self._scaler.update()\n-            else:\n-                self.optimizer.step()\n-\n-            # Update moving averages\n-            if self._moving_average is not None:\n-                self._moving_average.apply(batch_num_total)\n-\n-            # Update the description with the latest metrics\n-            metrics = training_util.get_metrics(\n-                self.model,\n-                train_loss,\n-                train_reg_loss,\n-                batch_loss,\n-                batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            if self._primary:\n-                # Updating tqdm only for the primary as the trainers wouldn't have one\n-                description = training_util.description_from_metrics(metrics)\n-                batch_group_generator_tqdm.set_description(description, refresh=False)\n-\n-                if self._checkpointer is not None:\n-                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    batch_group,\n-                    batch_group_outputs,\n-                    metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=True,\n-                    is_primary=self._primary,\n-                    batch_grad_norm=batch_grad_norm,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Let all workers finish their epoch before computing\n-        # the final statistics for the epoch.\n-        if self._distributed:\n-            dist.barrier()\n-\n-        metrics = training_util.get_metrics(\n-            self.model,\n-            train_loss,\n-            train_reg_loss,\n-            batch_loss=None,\n-            batch_reg_loss=None,\n-            num_batches=batches_this_epoch,\n-            reset=True,\n-            world_size=self._world_size,\n-            cuda_device=self.cuda_device,\n-        )\n-\n-        for (worker, memory) in cpu_memory_usage:\n-            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        for (gpu_num, memory) in gpu_memory_usage:\n-            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        return metrics\n-\n-    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n-        \"\"\"\n-        Computes the validation loss. Returns it and the number of batches.\n-        \"\"\"\n-        logger.info(\"Validating\")\n-\n-        self._pytorch_model.eval()\n-\n-        # Replace parameter values with the shadow values from the moving averages.\n-        if self._moving_average is not None:\n-            self._moving_average.assign_average_value()\n-\n-        if self._validation_data_loader is not None:\n-            validation_data_loader = self._validation_data_loader\n-        else:\n-            raise ConfigurationError(\n-                \"Validation results cannot be calculated without a validation_data_loader\"\n-            )\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n-        else:\n-            val_generator_tqdm = validation_data_loader\n-\n-        batches_this_epoch = 0\n-        val_loss = 0.0\n-        val_batch_loss = 0.0\n-        val_reg_loss = None if regularization_penalty is None else 0.0\n-        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n-        done_early = False\n-        for batch in val_generator_tqdm:\n-            if self._distributed:\n-                # Check whether the other workers have stopped already (due to differing amounts of\n-                # data in each). If so, we can't proceed because we would hang when we hit the\n-                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                # here because NCCL process groups apparently don't support BoolTensor.\n-                done = torch.tensor(0, device=self.cuda_device)\n-                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                if done.item() > 0:\n-                    done_early = True\n-                    logger.warning(\n-                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n-                        \"This implies that there is an imbalance in your validation \"\n-                        \"data across the workers and that some amount of it will be \"\n-                        \"ignored. A small amount of this is fine, but a major imbalance \"\n-                        \"should be avoided. Note: This warning will appear unless your \"\n-                        \"data is perfectly balanced.\"\n-                    )\n-                    break\n-\n-            with amp.autocast(self._use_amp):\n-                batch_outputs = self.batch_outputs(batch, for_training=False)\n-                loss = batch_outputs.get(\"loss\")\n-                reg_loss = batch_outputs.get(\"reg_loss\")\n-                if loss is not None:\n-                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n-                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n-                    # currently only used as the divisor for the loss function, so we can safely only\n-                    # count those batches for which we actually have a loss.  If this variable ever\n-                    # gets used for something else, we might need to change things around a bit.\n-                    batches_this_epoch += 1\n-                    val_batch_loss = loss.item()\n-                    val_loss += val_batch_loss\n-                    if reg_loss is not None:\n-                        val_batch_reg_loss = reg_loss.item()\n-                        val_reg_loss += val_batch_reg_loss  # type: ignore\n-\n-            # Update the description with the latest metrics\n-            val_metrics = training_util.get_metrics(\n-                self.model,\n-                val_loss,\n-                val_reg_loss,\n-                val_batch_loss,\n-                val_batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            description = training_util.description_from_metrics(val_metrics)\n-            if self._primary:\n-                val_generator_tqdm.set_description(description, refresh=False)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    [batch],\n-                    [batch_outputs],\n-                    val_metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=False,\n-                    is_primary=self._primary,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop validation early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Now restore the original parameter values.\n-        if self._moving_average is not None:\n-            self._moving_average.restore()\n-\n-        return val_loss, val_reg_loss, batches_this_epoch\n-\n-    def train(self) -> Dict[str, Any]:\n-        \"\"\"\n-        Trains the supplied model with the supplied parameters.\n-        \"\"\"\n-\n-        for callback in self._callbacks:\n-            callback.on_start(self, is_primary=self._primary)\n-\n-        # Set default values in case of failure\n-        epoch = None\n-        metrics = None\n-\n-        try:\n-            metrics, epoch = self._try_train()\n-            return metrics\n-        finally:\n-            for callback in self._callbacks:\n-                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n-        try:\n-            epoch_counter = self._restore_checkpoint()\n-        except RuntimeError:\n-            traceback.print_exc()\n-            raise ConfigurationError(\n-                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n-                \"a different serialization directory or delete the existing serialization \"\n-                \"directory?\"\n-            )\n-\n-        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n-\n-        logger.info(\"Beginning training.\")\n-\n-        val_metrics: Dict[str, float] = {}\n-        metrics: Dict[str, Any] = {}\n-        epochs_trained = 0\n-        training_start_time = time.time()\n-\n-        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n-        for key, value in self._metric_tracker.best_epoch_metrics.items():\n-            metrics[\"best_validation_\" + key] = value\n-\n-        for epoch in range(epoch_counter, self._num_epochs):\n-            epoch_start_time = time.time()\n-            train_metrics = self._train_epoch(epoch)\n-\n-            # Back up the model now, in case something goes wrong later with the evaluation\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.shelve_model(epoch, self)\n-            # Wait for the primary process to finish saving the model checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            # get peak of memory usage\n-            for key, value in train_metrics.items():\n-                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-\n-            this_epoch_val_metric: float = 0.0\n-            if self._validation_data_loader is not None:\n-                with torch.no_grad():\n-                    # We have a validation set, so compute all the metrics on it.\n-                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n-\n-                    # It is safe again to wait till the validation is done. This is\n-                    # important to get the metrics right.\n-                    if self._distributed:\n-                        dist.barrier()\n-\n-                    val_metrics = training_util.get_metrics(\n-                        self.model,\n-                        val_loss,\n-                        val_reg_loss,\n-                        batch_loss=None,\n-                        batch_reg_loss=None,\n-                        num_batches=num_batches,\n-                        reset=True,\n-                        world_size=self._world_size,\n-                        cuda_device=self.cuda_device,\n-                    )\n-\n-                    # Check validation metric for early stopping\n-                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n-                    self._metric_tracker.add_metrics(val_metrics)\n-\n-            # Create overall metrics dict\n-            training_elapsed_time = time.time() - training_start_time\n-            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n-            metrics[\"training_start_epoch\"] = epoch_counter\n-            metrics[\"training_epochs\"] = epochs_trained\n-            metrics[\"epoch\"] = epoch\n-\n-            for key, value in train_metrics.items():\n-                metrics[\"training_\" + key] = value\n-            for key, value in val_metrics.items():\n-                metrics[\"validation_\" + key] = value\n-\n-            if self._metric_tracker.is_best_so_far():\n-                # Update all the best_ metrics.\n-                # (Otherwise they just stay the same as they were.)\n-                metrics[\"best_epoch\"] = epoch\n-                for key, value in val_metrics.items():\n-                    metrics[\"best_validation_\" + key] = value\n-\n-                self._metric_tracker.best_epoch_metrics = val_metrics\n-\n-            if self._serialization_dir and self._primary:\n-                common_util.dump_metrics(\n-                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n-                    metrics,\n-                )\n-\n-            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n-            # if it doesn't, the validation metric passed here is ignored.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step(this_epoch_val_metric)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step(this_epoch_val_metric)\n-\n-            # The checkpointer saves state from the learning rate scheduler and the momentum\n-            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.save_checkpoint(\n-                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n-                )\n-            # Wait for the primary process to finish saving the checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            for callback in self._callbacks:\n-                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-            epoch_elapsed_time = time.time() - epoch_start_time\n-            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n-\n-            if epoch < self._num_epochs - 1:\n-                training_elapsed_time = time.time() - training_start_time\n-                estimated_time_remaining = training_elapsed_time * (\n-                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n-                )\n-                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n-                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n-\n-            epochs_trained += 1\n-\n-            if self._metric_tracker.should_stop_early():\n-                logger.info(\"Ran out of patience. Stopping training.\")\n-                break\n-        else:\n-            epoch = self._num_epochs - 1\n-\n-        # Load the best model state before returning\n-        best_model_state = (\n-            None if self._checkpointer is None else self._checkpointer.best_model_state()\n-        )\n-        if best_model_state:\n-            self.model.load_state_dict(best_model_state)\n-\n-        return metrics, epoch\n-\n-    @contextmanager\n-    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n-        if self._moving_average is not None:\n-            # Assigning average value to model parameters.  The checkpointer will call\n-            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n-            self._moving_average.assign_average_value()\n-\n-        model_state = self.model.state_dict()\n-\n-        # These are the training states we need to persist.\n-        training_states = {\n-            \"metric_tracker\": self._metric_tracker.state_dict(),\n-            \"optimizer\": self.optimizer.state_dict(),\n-            \"batch_num_total\": self._batch_num_total,\n-        }\n-\n-        # If we have a learning rate or momentum scheduler, we should persist them too.\n-        if self._learning_rate_scheduler is not None:\n-            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n-        if self._momentum_scheduler is not None:\n-            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n-\n-        try:\n-            yield model_state, training_states\n-        finally:\n-            if self._moving_average is not None:\n-                self._moving_average.restore()\n-\n-    def _restore_checkpoint(self) -> int:\n-        \"\"\"\n-        Restores the model and training state from the last saved checkpoint.\n-        This includes an epoch count and optimizer state, which is serialized separately\n-        from model parameters. This function should only be used to continue training -\n-        if you wish to load a model for inference/load parts of a model into a new\n-        computation graph, you should use the native Pytorch functions:\n-        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n-\n-        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n-        this function will do nothing and return 0.\n-\n-        # Returns\n-\n-        epoch: `int`\n-            The epoch at which to resume training, which should be one after the epoch\n-            in the saved training state.\n-        \"\"\"\n-        if self._checkpointer is None:\n-            return 0\n-\n-        model_state, training_state = self._checkpointer.restore_checkpoint()\n-\n-        if not training_state:\n-            # No checkpoint to restore, start at 0\n-            return 0\n-\n-        self.model.load_state_dict(model_state)\n-        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n-        if (\n-            self._learning_rate_scheduler is not None\n-            and \"learning_rate_scheduler\" in training_state\n-        ):\n-            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n-        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n-            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n-        training_util.move_optimizer_to_cuda(self.optimizer)\n-\n-        # Currently the `training_state` contains a serialized `MetricTracker`.\n-        if \"metric_tracker\" in training_state:\n-            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n-        else:\n-            self._metric_tracker.clear()\n-\n-        if isinstance(training_state[\"epoch\"], int):\n-            epoch_to_return = training_state[\"epoch\"] + 1\n-        else:\n-            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n-\n-        # For older checkpoints with batch_num_total missing, default to old behavior where\n-        # it is unchanged.\n-        batch_num_total = training_state.get(\"batch_num_total\")\n-        if batch_num_total is not None:\n-            self._batch_num_total = batch_num_total\n-\n-        return epoch_to_return\n-\n-    @classmethod\n-    def from_partial_objects(\n-        cls,\n-        model: Model,\n-        serialization_dir: str,\n-        data_loader: DataLoader,\n-        validation_data_loader: DataLoader = None,\n-        local_rank: int = 0,\n-        patience: int = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        num_epochs: int = 20,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: float = None,\n-        grad_clipping: float = None,\n-        distributed: bool = False,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        no_grad: List[str] = None,\n-        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n-        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n-        momentum_scheduler: Lazy[MomentumScheduler] = None,\n-        moving_average: Lazy[MovingAverage] = None,\n-        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n-        callbacks: List[Lazy[TrainerCallback]] = None,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> \"Trainer\":\n-        \"\"\"\n-        This method exists so that we can have a documented method to construct this class using\n-        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n-        method.\n-\n-        The reason we can't just use `__init__` with `FromParams` here is because there are\n-        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n-        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n-        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n-        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n-        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n-\n-        If you're not using `FromParams`, you can just construct these arguments in the right order\n-        yourself in your code and call the constructor directly.\n-        \"\"\"\n-        if cuda_device is None:\n-            from torch import cuda\n-\n-            if cuda.device_count() > 0:\n-                cuda_device = 0\n-            else:\n-                cuda_device = -1\n-\n-        check_for_gpu(cuda_device)\n-        if cuda_device >= 0:\n-            # Moving model to GPU here so that the optimizer state gets constructed on\n-            # the right device.\n-            model = model.cuda(cuda_device)\n-\n-        if no_grad:\n-            for name, parameter in model.named_parameters():\n-                if any(re.search(regex, name) for regex in no_grad):\n-                    parameter.requires_grad_(False)\n-\n-        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n-        optimizer_ = optimizer.construct(model_parameters=parameters)\n-\n-        common_util.log_frozen_and_tunable_parameter_names(model)\n-\n-        batches_per_epoch: Optional[int]\n-        try:\n-            batches_per_epoch = len(data_loader)\n-            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n-        except TypeError:\n-            batches_per_epoch = None\n-\n-        moving_average_ = (\n-            None if moving_average is None else moving_average.construct(parameters=parameters)\n-        )\n-        learning_rate_scheduler_ = (\n-            None\n-            if learning_rate_scheduler is None\n-            else learning_rate_scheduler.construct(\n-                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n-            )\n-        )\n-        momentum_scheduler_ = (\n-            None\n-            if momentum_scheduler is None\n-            else momentum_scheduler.construct(optimizer=optimizer_)\n-        )\n-        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n-\n-        callbacks_: List[TrainerCallback] = []\n-        for callback_ in callbacks or []:\n-            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n-\n-        return cls(\n-            model,\n-            optimizer_,\n-            data_loader,\n-            patience=patience,\n-            validation_metric=validation_metric,\n-            validation_data_loader=validation_data_loader,\n-            num_epochs=num_epochs,\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            grad_norm=grad_norm,\n-            grad_clipping=grad_clipping,\n-            learning_rate_scheduler=learning_rate_scheduler_,\n-            momentum_scheduler=momentum_scheduler_,\n-            checkpointer=checkpointer_,\n-            moving_average=moving_average_,\n-            callbacks=callbacks_,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n-            use_amp=use_amp,\n-            enable_default_callbacks=enable_default_callbacks,\n-            run_confidence_checks=run_confidence_checks,\n-            **kwargs,\n-        )\n-\n-\n-DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n-\"\"\"\n-The default callbacks used by `GradientDescentTrainer`.\n-\"\"\"\n+    def get_best_weights_path(self) -> Optional[str]:\n+        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n+        return None\n",
        "source_code_with_indent": "\n\n<DED><DED>@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    <IND>\"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        <IND>super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            <IND>warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        <DED>self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            <IND>self._validation_data_loader.set_target_device(self.cuda_device)\n        <DED>self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            <IND>if validation_data_loader is not None:\n                <IND>logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        <DED><DED>elif (not isinstance(patience, int)) or patience <= 0:\n            <IND>raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        <DED>self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            <IND>self._checkpointer = Checkpointer(serialization_dir)\n\n        <DED>self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            <IND>default_callbacks.append(ConfidenceChecksCallback)\n        <DED>for callback_cls in default_callbacks:\n            <IND>for callback in self._callbacks:\n                <IND>if callback.__class__ == callback_cls:\n                    <IND>break\n            <DED><DED>else:\n                <IND>self._callbacks.append(callback_cls(self._serialization_dir))\n\n        <DED><DED>self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            <IND>if self.cuda_device == torch.device(\"cpu\"):\n                <IND>raise ValueError(\"Using AMP requires a cuda device\")\n            <DED>self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        <DED>if self._distributed:\n            <IND>self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        <DED>else:\n            <IND>self._pytorch_model = self.model\n\n    <DED><DED>def rescale_gradients(self) -> float:\n        <IND>\"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            <IND>if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                <IND>self._scaler.unscale_(self.optimizer)\n            <DED>return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        <DED>else:\n            <IND>return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    <DED><DED>def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        <IND>\"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            <IND>try:\n                <IND>assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    <IND>output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            <DED><DED>except AssertionError:\n                <IND>if for_training:\n                    <IND>raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        <DED><DED><DED>return output_dict\n\n    <DED>def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        <IND>\"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            <IND>cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        <DED>gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            <IND>gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            <IND>len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        <DED>except TypeError:\n            <IND>num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        <DED>if self._primary:\n            <IND>batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        <DED>else:\n            <IND>batch_group_generator_tqdm = batch_group_generator\n\n        <DED>self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            <IND>self._batch_num_total = 0\n\n        <DED>done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            <IND>if done_early:\n                <IND>break\n\n            <DED>batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                <IND>for p in param_group[\"params\"]:\n                    <IND>p.grad = None\n\n            <DED><DED>batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                <IND>if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    <IND>done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        <IND>done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                <DED><DED>with amp.autocast(self._use_amp):\n                    <IND>batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        <IND>raise ValueError(\"nan loss encountered\")\n                    <DED>loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        <IND>reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                <DED><DED>if self._scaler is not None:\n                    <IND>self._scaler.scale(loss).backward()\n                <DED>else:\n                    <IND>loss.backward()\n            <DED><DED>if len(batch_group_outputs) <= 0:\n                <IND>continue\n\n            <DED>train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step_batch(batch_num_total)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step_batch(batch_num_total)\n\n            <DED>if self._scaler is not None:\n                <IND>self._scaler.step(self.optimizer)\n                self._scaler.update()\n            <DED>else:\n                <IND>self.optimizer.step()\n\n            # Update moving averages\n            <DED>if self._moving_average is not None:\n                <IND>self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            <DED>metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                <IND>description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    <IND>self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            <DED><DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        <DED>if self._distributed:\n            <IND>dist.barrier()\n\n        <DED>metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            <IND>metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>for (gpu_num, memory) in gpu_memory_usage:\n            <IND>metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>return metrics\n\n    <DED>def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        <IND>\"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>if self._validation_data_loader is not None:\n            <IND>validation_data_loader = self._validation_data_loader\n        <DED>else:\n            <IND>raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            <IND>val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        <DED>else:\n            <IND>val_generator_tqdm = validation_data_loader\n\n        <DED>batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            <IND>if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                <IND>done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    <IND>done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            <DED><DED>with amp.autocast(self._use_amp):\n                <IND>batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    <IND>batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        <IND>val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            <DED><DED><DED>val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                <IND>val_generator_tqdm.set_description(description, refresh=False)\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        <DED>if self._moving_average is not None:\n            <IND>self._moving_average.restore()\n\n        <DED>return val_loss, val_reg_loss, batches_this_epoch\n\n    <DED>def train(self) -> Dict[str, Any]:\n        <IND>\"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            <IND>callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        <DED>epoch = None\n        metrics = None\n\n        try:\n            <IND>metrics, epoch = self._try_train()\n            return metrics\n        <DED>finally:\n            <IND>for callback in self._callbacks:\n                <IND>callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    <DED><DED><DED>def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        <IND>try:\n            <IND>epoch_counter = self._restore_checkpoint()\n        <DED>except RuntimeError:\n            <IND>traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        <DED>training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            <IND>metrics[\"best_validation_\" + key] = value\n\n        <DED>for epoch in range(epoch_counter, self._num_epochs):\n            <IND>epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            # get peak of memory usage\n            <DED>for key, value in train_metrics.items():\n                <IND>if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                <DED>elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            <DED><DED>this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                <IND>with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    <IND>val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        <IND>dist.barrier()\n\n                    <DED>val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            <DED><DED>training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                <IND>metrics[\"training_\" + key] = value\n            <DED>for key, value in val_metrics.items():\n                <IND>metrics[\"validation_\" + key] = value\n\n            <DED>if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                <IND>metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    <IND>metrics[\"best_validation_\" + key] = value\n\n                <DED>self._metric_tracker.best_epoch_metrics = val_metrics\n\n            <DED>if self._serialization_dir and self._primary:\n                <IND>common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            <DED>if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step(this_epoch_val_metric)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            <DED>if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            <DED>epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                <IND>training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            <DED>epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                <IND>logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        <DED><DED>else:\n            <IND>epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        <DED>best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            <IND>self.model.load_state_dict(best_model_state)\n\n        <DED>return metrics, epoch\n\n    <DED>@contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        <IND>if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            <IND>training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        <DED>if self._momentum_scheduler is not None:\n            <IND>training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        <DED>try:\n            <IND>yield model_state, training_states\n        <DED>finally:\n            <IND>if self._moving_average is not None:\n                <IND>self._moving_average.restore()\n\n    <DED><DED><DED>def _restore_checkpoint(self) -> int:\n        <IND>\"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            <IND>return 0\n\n        <DED>model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            <IND>return 0\n\n        <DED>self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            <IND>self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        <DED>if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            <IND>self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        <DED>training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            <IND>self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        <DED>else:\n            <IND>self._metric_tracker.clear()\n\n        <DED>if isinstance(training_state[\"epoch\"], int):\n            <IND>epoch_to_return = training_state[\"epoch\"] + 1\n        <DED>else:\n            <IND>epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        <DED>batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            <IND>self._batch_num_total = batch_num_total\n\n        <DED>return epoch_to_return\n\n    <DED>@classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        <IND>\"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            <IND>from torch import cuda\n\n            if cuda.device_count() > 0:\n                <IND>cuda_device = 0\n            <DED>else:\n                <IND>cuda_device = -1\n\n        <DED><DED>check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            <IND>model = model.cuda(cuda_device)\n\n        <DED>if no_grad:\n            <IND>for name, parameter in model.named_parameters():\n                <IND>if any(re.search(regex, name) for regex in no_grad):\n                    <IND>parameter.requires_grad_(False)\n\n        <DED><DED><DED>parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            <IND>batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        <DED>except TypeError:\n            <IND>batches_per_epoch = None\n\n        <DED>moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            <IND>callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        <DED>return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\n<DED><DED>DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def get_best_weights_path(self) -> Optional[str]:\n        <IND>\"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "c5bff8ba0d835eb03931f10f4f427ffe936cf796",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:469:31 Incompatible parameter type [6]: Expected `typing.Iterable[Variable[_T]]` for 1st positional only parameter to call `iter` but got `allennlp.data.data_loaders.data_loader.DataLoader`.",
    "message": " Expected `typing.Iterable[Variable[_T]]` for 1st positional only parameter to call `iter` but got `allennlp.data.data_loaders.data_loader.DataLoader`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 469,
    "warning_line": "        batch_generator = iter(self.data_loader)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n\n@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    \"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            self._validation_data_loader.set_target_device(self.cuda_device)\n        self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            if validation_data_loader is not None:\n                logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        elif (not isinstance(patience, int)) or patience <= 0:\n            raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            self._checkpointer = Checkpointer(serialization_dir)\n\n        self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            default_callbacks.append(ConfidenceChecksCallback)\n        for callback_cls in default_callbacks:\n            for callback in self._callbacks:\n                if callback.__class__ == callback_cls:\n                    break\n            else:\n                self._callbacks.append(callback_cls(self._serialization_dir))\n\n        self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            if self.cuda_device == torch.device(\"cpu\"):\n                raise ValueError(\"Using AMP requires a cuda device\")\n            self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        if self._distributed:\n            self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        else:\n            self._pytorch_model = self.model\n\n    def rescale_gradients(self) -> float:\n        \"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                self._scaler.unscale_(self.optimizer)\n            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        else:\n            return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            try:\n                assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            except AssertionError:\n                if for_training:\n                    raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        return output_dict\n\n    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        \"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        except TypeError:\n            num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        else:\n            batch_group_generator_tqdm = batch_group_generator\n\n        self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            self._batch_num_total = 0\n\n        done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            if done_early:\n                break\n\n            batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                for p in param_group[\"params\"]:\n                    p.grad = None\n\n            batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                with amp.autocast(self._use_amp):\n                    batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        raise ValueError(\"nan loss encountered\")\n                    loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                if self._scaler is not None:\n                    self._scaler.scale(loss).backward()\n                else:\n                    loss.backward()\n            if len(batch_group_outputs) <= 0:\n                continue\n\n            train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step_batch(batch_num_total)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step_batch(batch_num_total)\n\n            if self._scaler is not None:\n                self._scaler.step(self.optimizer)\n                self._scaler.update()\n            else:\n                self.optimizer.step()\n\n            # Update moving averages\n            if self._moving_average is not None:\n                self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        if self._distributed:\n            dist.barrier()\n\n        metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        for (gpu_num, memory) in gpu_memory_usage:\n            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        return metrics\n\n    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        \"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            self._moving_average.assign_average_value()\n\n        if self._validation_data_loader is not None:\n            validation_data_loader = self._validation_data_loader\n        else:\n            raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        else:\n            val_generator_tqdm = validation_data_loader\n\n        batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            with amp.autocast(self._use_amp):\n                batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                val_generator_tqdm.set_description(description, refresh=False)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        if self._moving_average is not None:\n            self._moving_average.restore()\n\n        return val_loss, val_reg_loss, batches_this_epoch\n\n    def train(self) -> Dict[str, Any]:\n        \"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        epoch = None\n        metrics = None\n\n        try:\n            metrics, epoch = self._try_train()\n            return metrics\n        finally:\n            for callback in self._callbacks:\n                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        try:\n            epoch_counter = self._restore_checkpoint()\n        except RuntimeError:\n            traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            metrics[\"best_validation_\" + key] = value\n\n        for epoch in range(epoch_counter, self._num_epochs):\n            epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            # get peak of memory usage\n            for key, value in train_metrics.items():\n                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        dist.barrier()\n\n                    val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                metrics[\"training_\" + key] = value\n            for key, value in val_metrics.items():\n                metrics[\"validation_\" + key] = value\n\n            if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    metrics[\"best_validation_\" + key] = value\n\n                self._metric_tracker.best_epoch_metrics = val_metrics\n\n            if self._serialization_dir and self._primary:\n                common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step(this_epoch_val_metric)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            for callback in self._callbacks:\n                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        else:\n            epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            self.model.load_state_dict(best_model_state)\n\n        return metrics, epoch\n\n    @contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            self._moving_average.assign_average_value()\n\n        model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        if self._momentum_scheduler is not None:\n            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        try:\n            yield model_state, training_states\n        finally:\n            if self._moving_average is not None:\n                self._moving_average.restore()\n\n    def _restore_checkpoint(self) -> int:\n        \"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            return 0\n\n        model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            return 0\n\n        self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        else:\n            self._metric_tracker.clear()\n\n        if isinstance(training_state[\"epoch\"], int):\n            epoch_to_return = training_state[\"epoch\"] + 1\n        else:\n            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            self._batch_num_total = batch_num_total\n\n        return epoch_to_return\n\n    @classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        \"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            from torch import cuda\n\n            if cuda.device_count() > 0:\n                cuda_device = 0\n            else:\n                cuda_device = -1\n\n        check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            model = model.cuda(cuda_device)\n\n        if no_grad:\n            for name, parameter in model.named_parameters():\n                if any(re.search(regex, name) for regex in no_grad):\n                    parameter.requires_grad_(False)\n\n        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        except TypeError:\n            batches_per_epoch = None\n\n        moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\nDEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_len": 45826,
        "target_code": "\n    def get_best_weights_path(self) -> Optional[str]:\n        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_len": 151,
        "diff_format": "@@ -115,1021 +89,4 @@\n \n-\n-@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\n-class GradientDescentTrainer(Trainer):\n-    \"\"\"\n-    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n-    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n-    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n-    stopping. There are many other bells and whistles as well.\n-\n-    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n-    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n-    see the arguments to that function for the exact keys that should be used, if you are using\n-    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n-    docstrings in `from_partial_objects`.\n-\n-    [0]: https://tinyurl.com/y5mv44fw\n-\n-    # Parameters\n-\n-    model : `Model`, required.\n-        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n-        their `forward` method returns a dictionary with a \"loss\" key, containing a\n-        scalar tensor representing the loss function to be optimized.\n-\n-        If you are training your model using GPUs, your model should already be\n-        on the correct device. (If you are using our `train` command this will be\n-        handled for you.)\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    optimizer : `torch.nn.Optimizer`, required.\n-        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n-        model to be optimized.\n-\n-    data_loader : `DataLoader`, required.\n-        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    patience : `Optional[int] > 0`, optional (default=`None`)\n-        Number of epochs to be patient before early stopping: the training is stopped\n-        after `patience` epochs with no improvement. If given, it must be `> 0`.\n-        If None, early stopping is disabled.\n-\n-    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n-        Validation metric to measure for whether to stop training using patience\n-        and whether to serialize an `is_best` model each epoch. The metric name\n-        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n-        is an increasing or decreasing function. If you specify more than one metric,\n-        the metrics will be summed to make the `is_best` decision.\n-\n-    validation_data_loader : `DataLoader`, optional (default=`None`)\n-        A `DataLoader` to use for the validation set.  If `None`, then\n-        use the training `DataLoader` with the validation data.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_epochs : `int`, optional (default = `20`)\n-        Number of training epochs.\n-\n-    serialization_dir : `str`, optional (default=`None`)\n-        Path to directory for saving and loading model files. Models will not be saved if\n-        this parameter is not passed.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    checkpointer : `Checkpointer`, optional (default=`None`)\n-        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n-        here, we will construct one with default parameters.\n-\n-    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n-        An integer or `torch.device` specifying the CUDA device to use for this process.\n-        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n-\n-        !!! Note\n-            If you *don't* intend to use a GPU, but you have one available, you'll need\n-            to explicitly set `cuda_device=-1`.\n-\n-        !!! Note\n-            If you intend to use a GPU, your model already needs to be on the correct device,\n-            which you can do with `model = model.cuda()`.\n-\n-        !!! Note\n-            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n-\n-    grad_norm : `float`, optional, (default = `None`).\n-        If provided, gradient norms will be rescaled to have a maximum of this value.\n-\n-    grad_clipping : `float`, optional (default = `None`).\n-        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n-        maximum of this value.  If you are getting `NaNs` in your gradients during training\n-        that are not solved by using `grad_norm`, you may need this.\n-\n-    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n-        If specified, the learning rate will be decayed with respect to\n-        this schedule at the end of each epoch (or batch, if the scheduler implements\n-        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n-        this will use the `validation_metric` provided to determine if learning has plateaued.\n-        To support updating the learning rate on every batch, this can optionally implement\n-        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n-\n-    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n-        If specified, the momentum will be updated at the end of each batch or epoch\n-        according to the schedule.\n-\n-    moving_average : `MovingAverage`, optional, (default = `None`)\n-        If provided, we will maintain moving averages for all parameters. During training, we\n-        employ a shadow variable for each parameter, which maintains the moving average. During\n-        evaluation, we backup the original parameters and assign the moving averages to corresponding\n-        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n-        parameters. This is necessary because we want the saved model to perform as well as the validated\n-        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n-\n-    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n-        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n-        and end of training, etc.\n-\n-    distributed : `bool`, optional, (default = `False`)\n-        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n-        requires `world_size` to be greater than 1.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n-        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n-\n-    local_rank : `int`, optional, (default = `0`)\n-        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n-        used as the rank.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    world_size : `int`, (default = `1`)\n-        The number of `Trainer` workers participating in the distributed training.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n-        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n-        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n-        post][0] for details on Gradient Accumulation.\n-\n-    use_amp : `bool`, optional, (default = `False`)\n-        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n-\n-    enable_default_callbacks : `bool`, optional (default = `True`)\n-        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n-        addition to any other callbacks listed in the `callbacks` parameter.\n-        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n-\n-    run_confidence_checks : `bool`, optional (default = `True`)\n-        Determines whether model confidence checks, such as\n-        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n-        are run.\n-\n-    run_sanity_checks : `bool`, optional (default = `True`)\n-        This parameter is deprecated. Please use `run_confidence_checks` instead.\n-\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        model: Model,\n-        optimizer: torch.optim.Optimizer,\n-        data_loader: DataLoader,\n-        patience: Optional[int] = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        validation_data_loader: DataLoader = None,\n-        num_epochs: int = 20,\n-        serialization_dir: Optional[str] = None,\n-        checkpointer: Checkpointer = None,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: Optional[float] = None,\n-        grad_clipping: Optional[float] = None,\n-        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n-        momentum_scheduler: Optional[MomentumScheduler] = None,\n-        moving_average: Optional[MovingAverage] = None,\n-        callbacks: List[TrainerCallback] = None,\n-        distributed: bool = False,\n-        local_rank: int = 0,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-        )\n-\n-        if \"run_sanity_checks\" in kwargs:\n-            warnings.warn(\n-                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n-                DeprecationWarning,\n-            )\n-            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n-\n-        # I am not calling move_to_gpu here, because if the model is\n-        # not already on the GPU then the optimizer is going to be wrong.\n-        self.model = model\n-\n-        self.data_loader = data_loader\n-        self.data_loader.set_target_device(self.cuda_device)\n-        self._validation_data_loader = validation_data_loader\n-        if self._validation_data_loader is not None:\n-            self._validation_data_loader.set_target_device(self.cuda_device)\n-        self.optimizer = optimizer\n-\n-        if patience is None:  # no early stopping\n-            if validation_data_loader is not None:\n-                logger.warning(\n-                    \"You provided a validation dataset but patience was set to None, \"\n-                    \"meaning that early stopping is disabled\"\n-                )\n-        elif (not isinstance(patience, int)) or patience <= 0:\n-            raise ConfigurationError(\n-                '{} is an invalid value for \"patience\": it must be a positive integer '\n-                \"or None (if you want to disable early stopping)\".format(patience)\n-            )\n-\n-        # For tracking is_best_so_far and should_stop_early\n-        self._metric_tracker = MetricTracker(validation_metric, patience)\n-\n-        self._num_epochs = num_epochs\n-\n-        self._checkpointer: Optional[Checkpointer] = checkpointer\n-        if checkpointer is None and serialization_dir is not None:\n-            self._checkpointer = Checkpointer(serialization_dir)\n-\n-        self._grad_norm = grad_norm\n-        self._grad_clipping = grad_clipping\n-\n-        self._learning_rate_scheduler = learning_rate_scheduler\n-        self._momentum_scheduler = momentum_scheduler\n-        self._moving_average = moving_average\n-\n-        self._callbacks = callbacks or []\n-        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n-\n-        if run_confidence_checks:\n-            default_callbacks.append(ConfidenceChecksCallback)\n-        for callback_cls in default_callbacks:\n-            for callback in self._callbacks:\n-                if callback.__class__ == callback_cls:\n-                    break\n-            else:\n-                self._callbacks.append(callback_cls(self._serialization_dir))\n-\n-        self._batch_num_total = 0\n-        self._last_log = 0.0  # time of last logging\n-        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n-\n-        # Enable automatic mixed precision training.\n-        self._scaler: Optional[amp.GradScaler] = None\n-        self._use_amp = use_amp\n-        if self._use_amp:\n-            if self.cuda_device == torch.device(\"cpu\"):\n-                raise ValueError(\"Using AMP requires a cuda device\")\n-            self._scaler = amp.GradScaler()\n-\n-        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n-        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n-        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n-        #\n-        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n-        # normal case, reference to `Model` is retained. This reference is only used in\n-        # these places: `model.__call__`, `model.train` and `model.eval`.\n-        if self._distributed:\n-            self._pytorch_model = DistributedDataParallel(\n-                self.model,\n-                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n-                find_unused_parameters=True,\n-            )\n-        else:\n-            self._pytorch_model = self.model\n-\n-    def rescale_gradients(self) -> float:\n-        \"\"\"\n-        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n-\n-        Returns the norm of the gradients.\n-        \"\"\"\n-        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n-        if self._grad_norm:\n-            if self._scaler is not None:\n-                # Need to first unscale gradients in order to clip as usual.\n-                self._scaler.unscale_(self.optimizer)\n-            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n-        else:\n-            return torch.norm(\n-                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n-            )\n-\n-    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n-        \"\"\"\n-        Does a forward pass on the given batch and returns the output dictionary that the model\n-        returns, after adding any specified regularization penalty to the loss (if training).\n-        \"\"\"\n-        output_dict = self._pytorch_model(**batch)\n-\n-        if for_training:\n-            try:\n-                assert \"loss\" in output_dict\n-                regularization_penalty = self.model.get_regularization_penalty()\n-\n-                if regularization_penalty is not None:\n-                    output_dict[\"reg_loss\"] = regularization_penalty\n-                    output_dict[\"loss\"] += regularization_penalty\n-\n-            except AssertionError:\n-                if for_training:\n-                    raise RuntimeError(\n-                        \"The model you are trying to optimize does not contain a\"\n-                        \" 'loss' key in the output of model.forward(inputs).\"\n-                    )\n-\n-        return output_dict\n-\n-    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n-        \"\"\"\n-        Trains one epoch and returns metrics.\n-        \"\"\"\n-        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n-        cpu_memory_usage = []\n-        for worker, memory in common_util.peak_cpu_memory().items():\n-            cpu_memory_usage.append((worker, memory))\n-            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n-        gpu_memory_usage = []\n-        for gpu, memory in common_util.peak_gpu_memory().items():\n-            gpu_memory_usage.append((gpu, memory))\n-            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        train_loss = 0.0\n-        batch_loss = 0.0\n-        train_reg_loss = None if regularization_penalty is None else 0.0\n-        batch_reg_loss = None if regularization_penalty is None else 0.0\n-\n-        # Set the model to \"train\" mode.\n-        self._pytorch_model.train()\n-\n-        # Get tqdm for the training batches\n-        batch_generator = iter(self.data_loader)\n-        batch_group_generator = common_util.lazy_groups_of(\n-            batch_generator, self._num_gradient_accumulation_steps\n-        )\n-\n-        logger.info(\"Training\")\n-\n-        num_training_batches: Union[int, float]\n-        try:\n-            len_data_loader = len(self.data_loader)\n-            num_training_batches = math.ceil(\n-                len_data_loader / self._num_gradient_accumulation_steps\n-            )\n-        except TypeError:\n-            num_training_batches = float(\"inf\")\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            batch_group_generator_tqdm = Tqdm.tqdm(\n-                batch_group_generator, total=num_training_batches\n-            )\n-        else:\n-            batch_group_generator_tqdm = batch_group_generator\n-\n-        self._last_log = time.time()\n-\n-        batches_this_epoch = 0\n-        if self._batch_num_total is None:\n-            self._batch_num_total = 0\n-\n-        done_early = False\n-        for batch_group in batch_group_generator_tqdm:\n-            if done_early:\n-                break\n-\n-            batches_this_epoch += 1\n-            self._batch_num_total += 1\n-            batch_num_total = self._batch_num_total\n-\n-            # Zero gradients.\n-            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n-            # because it avoids a read op when the gradients are first updated below.\n-            for param_group in self.optimizer.param_groups:\n-                for p in param_group[\"params\"]:\n-                    p.grad = None\n-\n-            batch_loss = 0.0\n-            batch_group_outputs = []\n-            for batch in batch_group:\n-                if self._distributed:\n-                    # Check whether the other workers have stopped already (due to differing amounts of\n-                    # data in each). If so, we can't proceed because we would hang when we hit the\n-                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                    # here because NCCL process groups apparently don't support BoolTensor.\n-                    done = torch.tensor(0, device=self.cuda_device)\n-                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                    if done.item() > 0:\n-                        done_early = True\n-                        logger.warning(\n-                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n-                            \"This implies that there is an imbalance in your training \"\n-                            \"data across the workers and that some amount of it will be \"\n-                            \"ignored. A small amount of this is fine, but a major imbalance \"\n-                            \"should be avoided. Note: This warning will appear unless your \"\n-                            \"data is perfectly balanced.\"\n-                        )\n-                        break\n-\n-                with amp.autocast(self._use_amp):\n-                    batch_outputs = self.batch_outputs(batch, for_training=True)\n-                    batch_group_outputs.append(batch_outputs)\n-                    loss = batch_outputs[\"loss\"]\n-                    reg_loss = batch_outputs.get(\"reg_loss\")\n-                    if torch.isnan(loss):\n-                        raise ValueError(\"nan loss encountered\")\n-                    loss = loss / len(batch_group)\n-\n-                    batch_loss += loss.item()\n-                    if reg_loss is not None:\n-                        reg_loss = reg_loss / len(batch_group)\n-                        batch_reg_loss = reg_loss.item()\n-                        train_reg_loss += batch_reg_loss  # type: ignore\n-\n-                if self._scaler is not None:\n-                    self._scaler.scale(loss).backward()\n-                else:\n-                    loss.backward()\n-            if len(batch_group_outputs) <= 0:\n-                continue\n-\n-            train_loss += batch_loss\n-\n-            batch_grad_norm = self.rescale_gradients()\n-\n-            # This does nothing if batch_num_total is None or you are using a\n-            # scheduler which doesn't update per batch.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step_batch(batch_num_total)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step_batch(batch_num_total)\n-\n-            if self._scaler is not None:\n-                self._scaler.step(self.optimizer)\n-                self._scaler.update()\n-            else:\n-                self.optimizer.step()\n-\n-            # Update moving averages\n-            if self._moving_average is not None:\n-                self._moving_average.apply(batch_num_total)\n-\n-            # Update the description with the latest metrics\n-            metrics = training_util.get_metrics(\n-                self.model,\n-                train_loss,\n-                train_reg_loss,\n-                batch_loss,\n-                batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            if self._primary:\n-                # Updating tqdm only for the primary as the trainers wouldn't have one\n-                description = training_util.description_from_metrics(metrics)\n-                batch_group_generator_tqdm.set_description(description, refresh=False)\n-\n-                if self._checkpointer is not None:\n-                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    batch_group,\n-                    batch_group_outputs,\n-                    metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=True,\n-                    is_primary=self._primary,\n-                    batch_grad_norm=batch_grad_norm,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Let all workers finish their epoch before computing\n-        # the final statistics for the epoch.\n-        if self._distributed:\n-            dist.barrier()\n-\n-        metrics = training_util.get_metrics(\n-            self.model,\n-            train_loss,\n-            train_reg_loss,\n-            batch_loss=None,\n-            batch_reg_loss=None,\n-            num_batches=batches_this_epoch,\n-            reset=True,\n-            world_size=self._world_size,\n-            cuda_device=self.cuda_device,\n-        )\n-\n-        for (worker, memory) in cpu_memory_usage:\n-            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        for (gpu_num, memory) in gpu_memory_usage:\n-            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        return metrics\n-\n-    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n-        \"\"\"\n-        Computes the validation loss. Returns it and the number of batches.\n-        \"\"\"\n-        logger.info(\"Validating\")\n-\n-        self._pytorch_model.eval()\n-\n-        # Replace parameter values with the shadow values from the moving averages.\n-        if self._moving_average is not None:\n-            self._moving_average.assign_average_value()\n-\n-        if self._validation_data_loader is not None:\n-            validation_data_loader = self._validation_data_loader\n-        else:\n-            raise ConfigurationError(\n-                \"Validation results cannot be calculated without a validation_data_loader\"\n-            )\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n-        else:\n-            val_generator_tqdm = validation_data_loader\n-\n-        batches_this_epoch = 0\n-        val_loss = 0.0\n-        val_batch_loss = 0.0\n-        val_reg_loss = None if regularization_penalty is None else 0.0\n-        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n-        done_early = False\n-        for batch in val_generator_tqdm:\n-            if self._distributed:\n-                # Check whether the other workers have stopped already (due to differing amounts of\n-                # data in each). If so, we can't proceed because we would hang when we hit the\n-                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                # here because NCCL process groups apparently don't support BoolTensor.\n-                done = torch.tensor(0, device=self.cuda_device)\n-                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                if done.item() > 0:\n-                    done_early = True\n-                    logger.warning(\n-                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n-                        \"This implies that there is an imbalance in your validation \"\n-                        \"data across the workers and that some amount of it will be \"\n-                        \"ignored. A small amount of this is fine, but a major imbalance \"\n-                        \"should be avoided. Note: This warning will appear unless your \"\n-                        \"data is perfectly balanced.\"\n-                    )\n-                    break\n-\n-            with amp.autocast(self._use_amp):\n-                batch_outputs = self.batch_outputs(batch, for_training=False)\n-                loss = batch_outputs.get(\"loss\")\n-                reg_loss = batch_outputs.get(\"reg_loss\")\n-                if loss is not None:\n-                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n-                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n-                    # currently only used as the divisor for the loss function, so we can safely only\n-                    # count those batches for which we actually have a loss.  If this variable ever\n-                    # gets used for something else, we might need to change things around a bit.\n-                    batches_this_epoch += 1\n-                    val_batch_loss = loss.item()\n-                    val_loss += val_batch_loss\n-                    if reg_loss is not None:\n-                        val_batch_reg_loss = reg_loss.item()\n-                        val_reg_loss += val_batch_reg_loss  # type: ignore\n-\n-            # Update the description with the latest metrics\n-            val_metrics = training_util.get_metrics(\n-                self.model,\n-                val_loss,\n-                val_reg_loss,\n-                val_batch_loss,\n-                val_batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            description = training_util.description_from_metrics(val_metrics)\n-            if self._primary:\n-                val_generator_tqdm.set_description(description, refresh=False)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    [batch],\n-                    [batch_outputs],\n-                    val_metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=False,\n-                    is_primary=self._primary,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop validation early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Now restore the original parameter values.\n-        if self._moving_average is not None:\n-            self._moving_average.restore()\n-\n-        return val_loss, val_reg_loss, batches_this_epoch\n-\n-    def train(self) -> Dict[str, Any]:\n-        \"\"\"\n-        Trains the supplied model with the supplied parameters.\n-        \"\"\"\n-\n-        for callback in self._callbacks:\n-            callback.on_start(self, is_primary=self._primary)\n-\n-        # Set default values in case of failure\n-        epoch = None\n-        metrics = None\n-\n-        try:\n-            metrics, epoch = self._try_train()\n-            return metrics\n-        finally:\n-            for callback in self._callbacks:\n-                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n-        try:\n-            epoch_counter = self._restore_checkpoint()\n-        except RuntimeError:\n-            traceback.print_exc()\n-            raise ConfigurationError(\n-                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n-                \"a different serialization directory or delete the existing serialization \"\n-                \"directory?\"\n-            )\n-\n-        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n-\n-        logger.info(\"Beginning training.\")\n-\n-        val_metrics: Dict[str, float] = {}\n-        metrics: Dict[str, Any] = {}\n-        epochs_trained = 0\n-        training_start_time = time.time()\n-\n-        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n-        for key, value in self._metric_tracker.best_epoch_metrics.items():\n-            metrics[\"best_validation_\" + key] = value\n-\n-        for epoch in range(epoch_counter, self._num_epochs):\n-            epoch_start_time = time.time()\n-            train_metrics = self._train_epoch(epoch)\n-\n-            # Back up the model now, in case something goes wrong later with the evaluation\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.shelve_model(epoch, self)\n-            # Wait for the primary process to finish saving the model checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            # get peak of memory usage\n-            for key, value in train_metrics.items():\n-                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-\n-            this_epoch_val_metric: float = 0.0\n-            if self._validation_data_loader is not None:\n-                with torch.no_grad():\n-                    # We have a validation set, so compute all the metrics on it.\n-                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n-\n-                    # It is safe again to wait till the validation is done. This is\n-                    # important to get the metrics right.\n-                    if self._distributed:\n-                        dist.barrier()\n-\n-                    val_metrics = training_util.get_metrics(\n-                        self.model,\n-                        val_loss,\n-                        val_reg_loss,\n-                        batch_loss=None,\n-                        batch_reg_loss=None,\n-                        num_batches=num_batches,\n-                        reset=True,\n-                        world_size=self._world_size,\n-                        cuda_device=self.cuda_device,\n-                    )\n-\n-                    # Check validation metric for early stopping\n-                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n-                    self._metric_tracker.add_metrics(val_metrics)\n-\n-            # Create overall metrics dict\n-            training_elapsed_time = time.time() - training_start_time\n-            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n-            metrics[\"training_start_epoch\"] = epoch_counter\n-            metrics[\"training_epochs\"] = epochs_trained\n-            metrics[\"epoch\"] = epoch\n-\n-            for key, value in train_metrics.items():\n-                metrics[\"training_\" + key] = value\n-            for key, value in val_metrics.items():\n-                metrics[\"validation_\" + key] = value\n-\n-            if self._metric_tracker.is_best_so_far():\n-                # Update all the best_ metrics.\n-                # (Otherwise they just stay the same as they were.)\n-                metrics[\"best_epoch\"] = epoch\n-                for key, value in val_metrics.items():\n-                    metrics[\"best_validation_\" + key] = value\n-\n-                self._metric_tracker.best_epoch_metrics = val_metrics\n-\n-            if self._serialization_dir and self._primary:\n-                common_util.dump_metrics(\n-                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n-                    metrics,\n-                )\n-\n-            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n-            # if it doesn't, the validation metric passed here is ignored.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step(this_epoch_val_metric)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step(this_epoch_val_metric)\n-\n-            # The checkpointer saves state from the learning rate scheduler and the momentum\n-            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.save_checkpoint(\n-                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n-                )\n-            # Wait for the primary process to finish saving the checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            for callback in self._callbacks:\n-                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-            epoch_elapsed_time = time.time() - epoch_start_time\n-            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n-\n-            if epoch < self._num_epochs - 1:\n-                training_elapsed_time = time.time() - training_start_time\n-                estimated_time_remaining = training_elapsed_time * (\n-                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n-                )\n-                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n-                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n-\n-            epochs_trained += 1\n-\n-            if self._metric_tracker.should_stop_early():\n-                logger.info(\"Ran out of patience. Stopping training.\")\n-                break\n-        else:\n-            epoch = self._num_epochs - 1\n-\n-        # Load the best model state before returning\n-        best_model_state = (\n-            None if self._checkpointer is None else self._checkpointer.best_model_state()\n-        )\n-        if best_model_state:\n-            self.model.load_state_dict(best_model_state)\n-\n-        return metrics, epoch\n-\n-    @contextmanager\n-    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n-        if self._moving_average is not None:\n-            # Assigning average value to model parameters.  The checkpointer will call\n-            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n-            self._moving_average.assign_average_value()\n-\n-        model_state = self.model.state_dict()\n-\n-        # These are the training states we need to persist.\n-        training_states = {\n-            \"metric_tracker\": self._metric_tracker.state_dict(),\n-            \"optimizer\": self.optimizer.state_dict(),\n-            \"batch_num_total\": self._batch_num_total,\n-        }\n-\n-        # If we have a learning rate or momentum scheduler, we should persist them too.\n-        if self._learning_rate_scheduler is not None:\n-            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n-        if self._momentum_scheduler is not None:\n-            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n-\n-        try:\n-            yield model_state, training_states\n-        finally:\n-            if self._moving_average is not None:\n-                self._moving_average.restore()\n-\n-    def _restore_checkpoint(self) -> int:\n-        \"\"\"\n-        Restores the model and training state from the last saved checkpoint.\n-        This includes an epoch count and optimizer state, which is serialized separately\n-        from model parameters. This function should only be used to continue training -\n-        if you wish to load a model for inference/load parts of a model into a new\n-        computation graph, you should use the native Pytorch functions:\n-        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n-\n-        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n-        this function will do nothing and return 0.\n-\n-        # Returns\n-\n-        epoch: `int`\n-            The epoch at which to resume training, which should be one after the epoch\n-            in the saved training state.\n-        \"\"\"\n-        if self._checkpointer is None:\n-            return 0\n-\n-        model_state, training_state = self._checkpointer.restore_checkpoint()\n-\n-        if not training_state:\n-            # No checkpoint to restore, start at 0\n-            return 0\n-\n-        self.model.load_state_dict(model_state)\n-        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n-        if (\n-            self._learning_rate_scheduler is not None\n-            and \"learning_rate_scheduler\" in training_state\n-        ):\n-            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n-        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n-            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n-        training_util.move_optimizer_to_cuda(self.optimizer)\n-\n-        # Currently the `training_state` contains a serialized `MetricTracker`.\n-        if \"metric_tracker\" in training_state:\n-            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n-        else:\n-            self._metric_tracker.clear()\n-\n-        if isinstance(training_state[\"epoch\"], int):\n-            epoch_to_return = training_state[\"epoch\"] + 1\n-        else:\n-            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n-\n-        # For older checkpoints with batch_num_total missing, default to old behavior where\n-        # it is unchanged.\n-        batch_num_total = training_state.get(\"batch_num_total\")\n-        if batch_num_total is not None:\n-            self._batch_num_total = batch_num_total\n-\n-        return epoch_to_return\n-\n-    @classmethod\n-    def from_partial_objects(\n-        cls,\n-        model: Model,\n-        serialization_dir: str,\n-        data_loader: DataLoader,\n-        validation_data_loader: DataLoader = None,\n-        local_rank: int = 0,\n-        patience: int = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        num_epochs: int = 20,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: float = None,\n-        grad_clipping: float = None,\n-        distributed: bool = False,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        no_grad: List[str] = None,\n-        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n-        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n-        momentum_scheduler: Lazy[MomentumScheduler] = None,\n-        moving_average: Lazy[MovingAverage] = None,\n-        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n-        callbacks: List[Lazy[TrainerCallback]] = None,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> \"Trainer\":\n-        \"\"\"\n-        This method exists so that we can have a documented method to construct this class using\n-        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n-        method.\n-\n-        The reason we can't just use `__init__` with `FromParams` here is because there are\n-        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n-        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n-        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n-        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n-        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n-\n-        If you're not using `FromParams`, you can just construct these arguments in the right order\n-        yourself in your code and call the constructor directly.\n-        \"\"\"\n-        if cuda_device is None:\n-            from torch import cuda\n-\n-            if cuda.device_count() > 0:\n-                cuda_device = 0\n-            else:\n-                cuda_device = -1\n-\n-        check_for_gpu(cuda_device)\n-        if cuda_device >= 0:\n-            # Moving model to GPU here so that the optimizer state gets constructed on\n-            # the right device.\n-            model = model.cuda(cuda_device)\n-\n-        if no_grad:\n-            for name, parameter in model.named_parameters():\n-                if any(re.search(regex, name) for regex in no_grad):\n-                    parameter.requires_grad_(False)\n-\n-        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n-        optimizer_ = optimizer.construct(model_parameters=parameters)\n-\n-        common_util.log_frozen_and_tunable_parameter_names(model)\n-\n-        batches_per_epoch: Optional[int]\n-        try:\n-            batches_per_epoch = len(data_loader)\n-            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n-        except TypeError:\n-            batches_per_epoch = None\n-\n-        moving_average_ = (\n-            None if moving_average is None else moving_average.construct(parameters=parameters)\n-        )\n-        learning_rate_scheduler_ = (\n-            None\n-            if learning_rate_scheduler is None\n-            else learning_rate_scheduler.construct(\n-                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n-            )\n-        )\n-        momentum_scheduler_ = (\n-            None\n-            if momentum_scheduler is None\n-            else momentum_scheduler.construct(optimizer=optimizer_)\n-        )\n-        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n-\n-        callbacks_: List[TrainerCallback] = []\n-        for callback_ in callbacks or []:\n-            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n-\n-        return cls(\n-            model,\n-            optimizer_,\n-            data_loader,\n-            patience=patience,\n-            validation_metric=validation_metric,\n-            validation_data_loader=validation_data_loader,\n-            num_epochs=num_epochs,\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            grad_norm=grad_norm,\n-            grad_clipping=grad_clipping,\n-            learning_rate_scheduler=learning_rate_scheduler_,\n-            momentum_scheduler=momentum_scheduler_,\n-            checkpointer=checkpointer_,\n-            moving_average=moving_average_,\n-            callbacks=callbacks_,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n-            use_amp=use_amp,\n-            enable_default_callbacks=enable_default_callbacks,\n-            run_confidence_checks=run_confidence_checks,\n-            **kwargs,\n-        )\n-\n-\n-DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n-\"\"\"\n-The default callbacks used by `GradientDescentTrainer`.\n-\"\"\"\n+    def get_best_weights_path(self) -> Optional[str]:\n+        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n+        return None\n",
        "source_code_with_indent": "\n\n<DED><DED>@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    <IND>\"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        <IND>super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            <IND>warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        <DED>self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            <IND>self._validation_data_loader.set_target_device(self.cuda_device)\n        <DED>self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            <IND>if validation_data_loader is not None:\n                <IND>logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        <DED><DED>elif (not isinstance(patience, int)) or patience <= 0:\n            <IND>raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        <DED>self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            <IND>self._checkpointer = Checkpointer(serialization_dir)\n\n        <DED>self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            <IND>default_callbacks.append(ConfidenceChecksCallback)\n        <DED>for callback_cls in default_callbacks:\n            <IND>for callback in self._callbacks:\n                <IND>if callback.__class__ == callback_cls:\n                    <IND>break\n            <DED><DED>else:\n                <IND>self._callbacks.append(callback_cls(self._serialization_dir))\n\n        <DED><DED>self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            <IND>if self.cuda_device == torch.device(\"cpu\"):\n                <IND>raise ValueError(\"Using AMP requires a cuda device\")\n            <DED>self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        <DED>if self._distributed:\n            <IND>self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        <DED>else:\n            <IND>self._pytorch_model = self.model\n\n    <DED><DED>def rescale_gradients(self) -> float:\n        <IND>\"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            <IND>if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                <IND>self._scaler.unscale_(self.optimizer)\n            <DED>return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        <DED>else:\n            <IND>return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    <DED><DED>def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        <IND>\"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            <IND>try:\n                <IND>assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    <IND>output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            <DED><DED>except AssertionError:\n                <IND>if for_training:\n                    <IND>raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        <DED><DED><DED>return output_dict\n\n    <DED>def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        <IND>\"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            <IND>cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        <DED>gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            <IND>gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            <IND>len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        <DED>except TypeError:\n            <IND>num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        <DED>if self._primary:\n            <IND>batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        <DED>else:\n            <IND>batch_group_generator_tqdm = batch_group_generator\n\n        <DED>self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            <IND>self._batch_num_total = 0\n\n        <DED>done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            <IND>if done_early:\n                <IND>break\n\n            <DED>batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                <IND>for p in param_group[\"params\"]:\n                    <IND>p.grad = None\n\n            <DED><DED>batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                <IND>if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    <IND>done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        <IND>done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                <DED><DED>with amp.autocast(self._use_amp):\n                    <IND>batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        <IND>raise ValueError(\"nan loss encountered\")\n                    <DED>loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        <IND>reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                <DED><DED>if self._scaler is not None:\n                    <IND>self._scaler.scale(loss).backward()\n                <DED>else:\n                    <IND>loss.backward()\n            <DED><DED>if len(batch_group_outputs) <= 0:\n                <IND>continue\n\n            <DED>train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step_batch(batch_num_total)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step_batch(batch_num_total)\n\n            <DED>if self._scaler is not None:\n                <IND>self._scaler.step(self.optimizer)\n                self._scaler.update()\n            <DED>else:\n                <IND>self.optimizer.step()\n\n            # Update moving averages\n            <DED>if self._moving_average is not None:\n                <IND>self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            <DED>metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                <IND>description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    <IND>self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            <DED><DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        <DED>if self._distributed:\n            <IND>dist.barrier()\n\n        <DED>metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            <IND>metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>for (gpu_num, memory) in gpu_memory_usage:\n            <IND>metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>return metrics\n\n    <DED>def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        <IND>\"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>if self._validation_data_loader is not None:\n            <IND>validation_data_loader = self._validation_data_loader\n        <DED>else:\n            <IND>raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            <IND>val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        <DED>else:\n            <IND>val_generator_tqdm = validation_data_loader\n\n        <DED>batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            <IND>if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                <IND>done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    <IND>done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            <DED><DED>with amp.autocast(self._use_amp):\n                <IND>batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    <IND>batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        <IND>val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            <DED><DED><DED>val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                <IND>val_generator_tqdm.set_description(description, refresh=False)\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        <DED>if self._moving_average is not None:\n            <IND>self._moving_average.restore()\n\n        <DED>return val_loss, val_reg_loss, batches_this_epoch\n\n    <DED>def train(self) -> Dict[str, Any]:\n        <IND>\"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            <IND>callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        <DED>epoch = None\n        metrics = None\n\n        try:\n            <IND>metrics, epoch = self._try_train()\n            return metrics\n        <DED>finally:\n            <IND>for callback in self._callbacks:\n                <IND>callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    <DED><DED><DED>def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        <IND>try:\n            <IND>epoch_counter = self._restore_checkpoint()\n        <DED>except RuntimeError:\n            <IND>traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        <DED>training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            <IND>metrics[\"best_validation_\" + key] = value\n\n        <DED>for epoch in range(epoch_counter, self._num_epochs):\n            <IND>epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            # get peak of memory usage\n            <DED>for key, value in train_metrics.items():\n                <IND>if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                <DED>elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            <DED><DED>this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                <IND>with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    <IND>val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        <IND>dist.barrier()\n\n                    <DED>val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            <DED><DED>training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                <IND>metrics[\"training_\" + key] = value\n            <DED>for key, value in val_metrics.items():\n                <IND>metrics[\"validation_\" + key] = value\n\n            <DED>if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                <IND>metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    <IND>metrics[\"best_validation_\" + key] = value\n\n                <DED>self._metric_tracker.best_epoch_metrics = val_metrics\n\n            <DED>if self._serialization_dir and self._primary:\n                <IND>common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            <DED>if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step(this_epoch_val_metric)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            <DED>if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            <DED>epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                <IND>training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            <DED>epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                <IND>logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        <DED><DED>else:\n            <IND>epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        <DED>best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            <IND>self.model.load_state_dict(best_model_state)\n\n        <DED>return metrics, epoch\n\n    <DED>@contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        <IND>if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            <IND>training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        <DED>if self._momentum_scheduler is not None:\n            <IND>training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        <DED>try:\n            <IND>yield model_state, training_states\n        <DED>finally:\n            <IND>if self._moving_average is not None:\n                <IND>self._moving_average.restore()\n\n    <DED><DED><DED>def _restore_checkpoint(self) -> int:\n        <IND>\"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            <IND>return 0\n\n        <DED>model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            <IND>return 0\n\n        <DED>self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            <IND>self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        <DED>if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            <IND>self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        <DED>training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            <IND>self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        <DED>else:\n            <IND>self._metric_tracker.clear()\n\n        <DED>if isinstance(training_state[\"epoch\"], int):\n            <IND>epoch_to_return = training_state[\"epoch\"] + 1\n        <DED>else:\n            <IND>epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        <DED>batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            <IND>self._batch_num_total = batch_num_total\n\n        <DED>return epoch_to_return\n\n    <DED>@classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        <IND>\"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            <IND>from torch import cuda\n\n            if cuda.device_count() > 0:\n                <IND>cuda_device = 0\n            <DED>else:\n                <IND>cuda_device = -1\n\n        <DED><DED>check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            <IND>model = model.cuda(cuda_device)\n\n        <DED>if no_grad:\n            <IND>for name, parameter in model.named_parameters():\n                <IND>if any(re.search(regex, name) for regex in no_grad):\n                    <IND>parameter.requires_grad_(False)\n\n        <DED><DED><DED>parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            <IND>batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        <DED>except TypeError:\n            <IND>batches_per_epoch = None\n\n        <DED>moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            <IND>callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        <DED>return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\n<DED><DED>DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def get_best_weights_path(self) -> Optional[str]:\n        <IND>\"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "c5bff8ba0d835eb03931f10f4f427ffe936cf796",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:1014:8 Incompatible variable type [9]: validation_data_loader is declared to have type `allennlp.data.data_loaders.data_loader.DataLoader` but is used as type `None`.",
    "message": " validation_data_loader is declared to have type `allennlp.data.data_loaders.data_loader.DataLoader` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 1014,
    "warning_line": "        validation_data_loader: DataLoader = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n\n@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    \"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            self._validation_data_loader.set_target_device(self.cuda_device)\n        self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            if validation_data_loader is not None:\n                logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        elif (not isinstance(patience, int)) or patience <= 0:\n            raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            self._checkpointer = Checkpointer(serialization_dir)\n\n        self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            default_callbacks.append(ConfidenceChecksCallback)\n        for callback_cls in default_callbacks:\n            for callback in self._callbacks:\n                if callback.__class__ == callback_cls:\n                    break\n            else:\n                self._callbacks.append(callback_cls(self._serialization_dir))\n\n        self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            if self.cuda_device == torch.device(\"cpu\"):\n                raise ValueError(\"Using AMP requires a cuda device\")\n            self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        if self._distributed:\n            self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        else:\n            self._pytorch_model = self.model\n\n    def rescale_gradients(self) -> float:\n        \"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                self._scaler.unscale_(self.optimizer)\n            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        else:\n            return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            try:\n                assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            except AssertionError:\n                if for_training:\n                    raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        return output_dict\n\n    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        \"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        except TypeError:\n            num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        else:\n            batch_group_generator_tqdm = batch_group_generator\n\n        self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            self._batch_num_total = 0\n\n        done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            if done_early:\n                break\n\n            batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                for p in param_group[\"params\"]:\n                    p.grad = None\n\n            batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                with amp.autocast(self._use_amp):\n                    batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        raise ValueError(\"nan loss encountered\")\n                    loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                if self._scaler is not None:\n                    self._scaler.scale(loss).backward()\n                else:\n                    loss.backward()\n            if len(batch_group_outputs) <= 0:\n                continue\n\n            train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step_batch(batch_num_total)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step_batch(batch_num_total)\n\n            if self._scaler is not None:\n                self._scaler.step(self.optimizer)\n                self._scaler.update()\n            else:\n                self.optimizer.step()\n\n            # Update moving averages\n            if self._moving_average is not None:\n                self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        if self._distributed:\n            dist.barrier()\n\n        metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        for (gpu_num, memory) in gpu_memory_usage:\n            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        return metrics\n\n    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        \"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            self._moving_average.assign_average_value()\n\n        if self._validation_data_loader is not None:\n            validation_data_loader = self._validation_data_loader\n        else:\n            raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        else:\n            val_generator_tqdm = validation_data_loader\n\n        batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            with amp.autocast(self._use_amp):\n                batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                val_generator_tqdm.set_description(description, refresh=False)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        if self._moving_average is not None:\n            self._moving_average.restore()\n\n        return val_loss, val_reg_loss, batches_this_epoch\n\n    def train(self) -> Dict[str, Any]:\n        \"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        epoch = None\n        metrics = None\n\n        try:\n            metrics, epoch = self._try_train()\n            return metrics\n        finally:\n            for callback in self._callbacks:\n                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        try:\n            epoch_counter = self._restore_checkpoint()\n        except RuntimeError:\n            traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            metrics[\"best_validation_\" + key] = value\n\n        for epoch in range(epoch_counter, self._num_epochs):\n            epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            # get peak of memory usage\n            for key, value in train_metrics.items():\n                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        dist.barrier()\n\n                    val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                metrics[\"training_\" + key] = value\n            for key, value in val_metrics.items():\n                metrics[\"validation_\" + key] = value\n\n            if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    metrics[\"best_validation_\" + key] = value\n\n                self._metric_tracker.best_epoch_metrics = val_metrics\n\n            if self._serialization_dir and self._primary:\n                common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step(this_epoch_val_metric)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            for callback in self._callbacks:\n                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        else:\n            epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            self.model.load_state_dict(best_model_state)\n\n        return metrics, epoch\n\n    @contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            self._moving_average.assign_average_value()\n\n        model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        if self._momentum_scheduler is not None:\n            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        try:\n            yield model_state, training_states\n        finally:\n            if self._moving_average is not None:\n                self._moving_average.restore()\n\n    def _restore_checkpoint(self) -> int:\n        \"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            return 0\n\n        model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            return 0\n\n        self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        else:\n            self._metric_tracker.clear()\n\n        if isinstance(training_state[\"epoch\"], int):\n            epoch_to_return = training_state[\"epoch\"] + 1\n        else:\n            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            self._batch_num_total = batch_num_total\n\n        return epoch_to_return\n\n    @classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        \"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            from torch import cuda\n\n            if cuda.device_count() > 0:\n                cuda_device = 0\n            else:\n                cuda_device = -1\n\n        check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            model = model.cuda(cuda_device)\n\n        if no_grad:\n            for name, parameter in model.named_parameters():\n                if any(re.search(regex, name) for regex in no_grad):\n                    parameter.requires_grad_(False)\n\n        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        except TypeError:\n            batches_per_epoch = None\n\n        moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\nDEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_len": 45826,
        "target_code": "\n    def get_best_weights_path(self) -> Optional[str]:\n        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_len": 151,
        "diff_format": "@@ -115,1021 +89,4 @@\n \n-\n-@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\n-class GradientDescentTrainer(Trainer):\n-    \"\"\"\n-    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n-    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n-    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n-    stopping. There are many other bells and whistles as well.\n-\n-    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n-    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n-    see the arguments to that function for the exact keys that should be used, if you are using\n-    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n-    docstrings in `from_partial_objects`.\n-\n-    [0]: https://tinyurl.com/y5mv44fw\n-\n-    # Parameters\n-\n-    model : `Model`, required.\n-        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n-        their `forward` method returns a dictionary with a \"loss\" key, containing a\n-        scalar tensor representing the loss function to be optimized.\n-\n-        If you are training your model using GPUs, your model should already be\n-        on the correct device. (If you are using our `train` command this will be\n-        handled for you.)\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    optimizer : `torch.nn.Optimizer`, required.\n-        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n-        model to be optimized.\n-\n-    data_loader : `DataLoader`, required.\n-        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    patience : `Optional[int] > 0`, optional (default=`None`)\n-        Number of epochs to be patient before early stopping: the training is stopped\n-        after `patience` epochs with no improvement. If given, it must be `> 0`.\n-        If None, early stopping is disabled.\n-\n-    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n-        Validation metric to measure for whether to stop training using patience\n-        and whether to serialize an `is_best` model each epoch. The metric name\n-        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n-        is an increasing or decreasing function. If you specify more than one metric,\n-        the metrics will be summed to make the `is_best` decision.\n-\n-    validation_data_loader : `DataLoader`, optional (default=`None`)\n-        A `DataLoader` to use for the validation set.  If `None`, then\n-        use the training `DataLoader` with the validation data.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_epochs : `int`, optional (default = `20`)\n-        Number of training epochs.\n-\n-    serialization_dir : `str`, optional (default=`None`)\n-        Path to directory for saving and loading model files. Models will not be saved if\n-        this parameter is not passed.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    checkpointer : `Checkpointer`, optional (default=`None`)\n-        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n-        here, we will construct one with default parameters.\n-\n-    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n-        An integer or `torch.device` specifying the CUDA device to use for this process.\n-        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n-\n-        !!! Note\n-            If you *don't* intend to use a GPU, but you have one available, you'll need\n-            to explicitly set `cuda_device=-1`.\n-\n-        !!! Note\n-            If you intend to use a GPU, your model already needs to be on the correct device,\n-            which you can do with `model = model.cuda()`.\n-\n-        !!! Note\n-            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n-\n-    grad_norm : `float`, optional, (default = `None`).\n-        If provided, gradient norms will be rescaled to have a maximum of this value.\n-\n-    grad_clipping : `float`, optional (default = `None`).\n-        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n-        maximum of this value.  If you are getting `NaNs` in your gradients during training\n-        that are not solved by using `grad_norm`, you may need this.\n-\n-    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n-        If specified, the learning rate will be decayed with respect to\n-        this schedule at the end of each epoch (or batch, if the scheduler implements\n-        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n-        this will use the `validation_metric` provided to determine if learning has plateaued.\n-        To support updating the learning rate on every batch, this can optionally implement\n-        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n-\n-    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n-        If specified, the momentum will be updated at the end of each batch or epoch\n-        according to the schedule.\n-\n-    moving_average : `MovingAverage`, optional, (default = `None`)\n-        If provided, we will maintain moving averages for all parameters. During training, we\n-        employ a shadow variable for each parameter, which maintains the moving average. During\n-        evaluation, we backup the original parameters and assign the moving averages to corresponding\n-        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n-        parameters. This is necessary because we want the saved model to perform as well as the validated\n-        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n-\n-    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n-        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n-        and end of training, etc.\n-\n-    distributed : `bool`, optional, (default = `False`)\n-        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n-        requires `world_size` to be greater than 1.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n-        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n-\n-    local_rank : `int`, optional, (default = `0`)\n-        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n-        used as the rank.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    world_size : `int`, (default = `1`)\n-        The number of `Trainer` workers participating in the distributed training.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n-        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n-        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n-        post][0] for details on Gradient Accumulation.\n-\n-    use_amp : `bool`, optional, (default = `False`)\n-        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n-\n-    enable_default_callbacks : `bool`, optional (default = `True`)\n-        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n-        addition to any other callbacks listed in the `callbacks` parameter.\n-        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n-\n-    run_confidence_checks : `bool`, optional (default = `True`)\n-        Determines whether model confidence checks, such as\n-        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n-        are run.\n-\n-    run_sanity_checks : `bool`, optional (default = `True`)\n-        This parameter is deprecated. Please use `run_confidence_checks` instead.\n-\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        model: Model,\n-        optimizer: torch.optim.Optimizer,\n-        data_loader: DataLoader,\n-        patience: Optional[int] = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        validation_data_loader: DataLoader = None,\n-        num_epochs: int = 20,\n-        serialization_dir: Optional[str] = None,\n-        checkpointer: Checkpointer = None,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: Optional[float] = None,\n-        grad_clipping: Optional[float] = None,\n-        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n-        momentum_scheduler: Optional[MomentumScheduler] = None,\n-        moving_average: Optional[MovingAverage] = None,\n-        callbacks: List[TrainerCallback] = None,\n-        distributed: bool = False,\n-        local_rank: int = 0,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-        )\n-\n-        if \"run_sanity_checks\" in kwargs:\n-            warnings.warn(\n-                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n-                DeprecationWarning,\n-            )\n-            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n-\n-        # I am not calling move_to_gpu here, because if the model is\n-        # not already on the GPU then the optimizer is going to be wrong.\n-        self.model = model\n-\n-        self.data_loader = data_loader\n-        self.data_loader.set_target_device(self.cuda_device)\n-        self._validation_data_loader = validation_data_loader\n-        if self._validation_data_loader is not None:\n-            self._validation_data_loader.set_target_device(self.cuda_device)\n-        self.optimizer = optimizer\n-\n-        if patience is None:  # no early stopping\n-            if validation_data_loader is not None:\n-                logger.warning(\n-                    \"You provided a validation dataset but patience was set to None, \"\n-                    \"meaning that early stopping is disabled\"\n-                )\n-        elif (not isinstance(patience, int)) or patience <= 0:\n-            raise ConfigurationError(\n-                '{} is an invalid value for \"patience\": it must be a positive integer '\n-                \"or None (if you want to disable early stopping)\".format(patience)\n-            )\n-\n-        # For tracking is_best_so_far and should_stop_early\n-        self._metric_tracker = MetricTracker(validation_metric, patience)\n-\n-        self._num_epochs = num_epochs\n-\n-        self._checkpointer: Optional[Checkpointer] = checkpointer\n-        if checkpointer is None and serialization_dir is not None:\n-            self._checkpointer = Checkpointer(serialization_dir)\n-\n-        self._grad_norm = grad_norm\n-        self._grad_clipping = grad_clipping\n-\n-        self._learning_rate_scheduler = learning_rate_scheduler\n-        self._momentum_scheduler = momentum_scheduler\n-        self._moving_average = moving_average\n-\n-        self._callbacks = callbacks or []\n-        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n-\n-        if run_confidence_checks:\n-            default_callbacks.append(ConfidenceChecksCallback)\n-        for callback_cls in default_callbacks:\n-            for callback in self._callbacks:\n-                if callback.__class__ == callback_cls:\n-                    break\n-            else:\n-                self._callbacks.append(callback_cls(self._serialization_dir))\n-\n-        self._batch_num_total = 0\n-        self._last_log = 0.0  # time of last logging\n-        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n-\n-        # Enable automatic mixed precision training.\n-        self._scaler: Optional[amp.GradScaler] = None\n-        self._use_amp = use_amp\n-        if self._use_amp:\n-            if self.cuda_device == torch.device(\"cpu\"):\n-                raise ValueError(\"Using AMP requires a cuda device\")\n-            self._scaler = amp.GradScaler()\n-\n-        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n-        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n-        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n-        #\n-        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n-        # normal case, reference to `Model` is retained. This reference is only used in\n-        # these places: `model.__call__`, `model.train` and `model.eval`.\n-        if self._distributed:\n-            self._pytorch_model = DistributedDataParallel(\n-                self.model,\n-                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n-                find_unused_parameters=True,\n-            )\n-        else:\n-            self._pytorch_model = self.model\n-\n-    def rescale_gradients(self) -> float:\n-        \"\"\"\n-        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n-\n-        Returns the norm of the gradients.\n-        \"\"\"\n-        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n-        if self._grad_norm:\n-            if self._scaler is not None:\n-                # Need to first unscale gradients in order to clip as usual.\n-                self._scaler.unscale_(self.optimizer)\n-            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n-        else:\n-            return torch.norm(\n-                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n-            )\n-\n-    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n-        \"\"\"\n-        Does a forward pass on the given batch and returns the output dictionary that the model\n-        returns, after adding any specified regularization penalty to the loss (if training).\n-        \"\"\"\n-        output_dict = self._pytorch_model(**batch)\n-\n-        if for_training:\n-            try:\n-                assert \"loss\" in output_dict\n-                regularization_penalty = self.model.get_regularization_penalty()\n-\n-                if regularization_penalty is not None:\n-                    output_dict[\"reg_loss\"] = regularization_penalty\n-                    output_dict[\"loss\"] += regularization_penalty\n-\n-            except AssertionError:\n-                if for_training:\n-                    raise RuntimeError(\n-                        \"The model you are trying to optimize does not contain a\"\n-                        \" 'loss' key in the output of model.forward(inputs).\"\n-                    )\n-\n-        return output_dict\n-\n-    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n-        \"\"\"\n-        Trains one epoch and returns metrics.\n-        \"\"\"\n-        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n-        cpu_memory_usage = []\n-        for worker, memory in common_util.peak_cpu_memory().items():\n-            cpu_memory_usage.append((worker, memory))\n-            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n-        gpu_memory_usage = []\n-        for gpu, memory in common_util.peak_gpu_memory().items():\n-            gpu_memory_usage.append((gpu, memory))\n-            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        train_loss = 0.0\n-        batch_loss = 0.0\n-        train_reg_loss = None if regularization_penalty is None else 0.0\n-        batch_reg_loss = None if regularization_penalty is None else 0.0\n-\n-        # Set the model to \"train\" mode.\n-        self._pytorch_model.train()\n-\n-        # Get tqdm for the training batches\n-        batch_generator = iter(self.data_loader)\n-        batch_group_generator = common_util.lazy_groups_of(\n-            batch_generator, self._num_gradient_accumulation_steps\n-        )\n-\n-        logger.info(\"Training\")\n-\n-        num_training_batches: Union[int, float]\n-        try:\n-            len_data_loader = len(self.data_loader)\n-            num_training_batches = math.ceil(\n-                len_data_loader / self._num_gradient_accumulation_steps\n-            )\n-        except TypeError:\n-            num_training_batches = float(\"inf\")\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            batch_group_generator_tqdm = Tqdm.tqdm(\n-                batch_group_generator, total=num_training_batches\n-            )\n-        else:\n-            batch_group_generator_tqdm = batch_group_generator\n-\n-        self._last_log = time.time()\n-\n-        batches_this_epoch = 0\n-        if self._batch_num_total is None:\n-            self._batch_num_total = 0\n-\n-        done_early = False\n-        for batch_group in batch_group_generator_tqdm:\n-            if done_early:\n-                break\n-\n-            batches_this_epoch += 1\n-            self._batch_num_total += 1\n-            batch_num_total = self._batch_num_total\n-\n-            # Zero gradients.\n-            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n-            # because it avoids a read op when the gradients are first updated below.\n-            for param_group in self.optimizer.param_groups:\n-                for p in param_group[\"params\"]:\n-                    p.grad = None\n-\n-            batch_loss = 0.0\n-            batch_group_outputs = []\n-            for batch in batch_group:\n-                if self._distributed:\n-                    # Check whether the other workers have stopped already (due to differing amounts of\n-                    # data in each). If so, we can't proceed because we would hang when we hit the\n-                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                    # here because NCCL process groups apparently don't support BoolTensor.\n-                    done = torch.tensor(0, device=self.cuda_device)\n-                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                    if done.item() > 0:\n-                        done_early = True\n-                        logger.warning(\n-                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n-                            \"This implies that there is an imbalance in your training \"\n-                            \"data across the workers and that some amount of it will be \"\n-                            \"ignored. A small amount of this is fine, but a major imbalance \"\n-                            \"should be avoided. Note: This warning will appear unless your \"\n-                            \"data is perfectly balanced.\"\n-                        )\n-                        break\n-\n-                with amp.autocast(self._use_amp):\n-                    batch_outputs = self.batch_outputs(batch, for_training=True)\n-                    batch_group_outputs.append(batch_outputs)\n-                    loss = batch_outputs[\"loss\"]\n-                    reg_loss = batch_outputs.get(\"reg_loss\")\n-                    if torch.isnan(loss):\n-                        raise ValueError(\"nan loss encountered\")\n-                    loss = loss / len(batch_group)\n-\n-                    batch_loss += loss.item()\n-                    if reg_loss is not None:\n-                        reg_loss = reg_loss / len(batch_group)\n-                        batch_reg_loss = reg_loss.item()\n-                        train_reg_loss += batch_reg_loss  # type: ignore\n-\n-                if self._scaler is not None:\n-                    self._scaler.scale(loss).backward()\n-                else:\n-                    loss.backward()\n-            if len(batch_group_outputs) <= 0:\n-                continue\n-\n-            train_loss += batch_loss\n-\n-            batch_grad_norm = self.rescale_gradients()\n-\n-            # This does nothing if batch_num_total is None or you are using a\n-            # scheduler which doesn't update per batch.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step_batch(batch_num_total)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step_batch(batch_num_total)\n-\n-            if self._scaler is not None:\n-                self._scaler.step(self.optimizer)\n-                self._scaler.update()\n-            else:\n-                self.optimizer.step()\n-\n-            # Update moving averages\n-            if self._moving_average is not None:\n-                self._moving_average.apply(batch_num_total)\n-\n-            # Update the description with the latest metrics\n-            metrics = training_util.get_metrics(\n-                self.model,\n-                train_loss,\n-                train_reg_loss,\n-                batch_loss,\n-                batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            if self._primary:\n-                # Updating tqdm only for the primary as the trainers wouldn't have one\n-                description = training_util.description_from_metrics(metrics)\n-                batch_group_generator_tqdm.set_description(description, refresh=False)\n-\n-                if self._checkpointer is not None:\n-                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    batch_group,\n-                    batch_group_outputs,\n-                    metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=True,\n-                    is_primary=self._primary,\n-                    batch_grad_norm=batch_grad_norm,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Let all workers finish their epoch before computing\n-        # the final statistics for the epoch.\n-        if self._distributed:\n-            dist.barrier()\n-\n-        metrics = training_util.get_metrics(\n-            self.model,\n-            train_loss,\n-            train_reg_loss,\n-            batch_loss=None,\n-            batch_reg_loss=None,\n-            num_batches=batches_this_epoch,\n-            reset=True,\n-            world_size=self._world_size,\n-            cuda_device=self.cuda_device,\n-        )\n-\n-        for (worker, memory) in cpu_memory_usage:\n-            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        for (gpu_num, memory) in gpu_memory_usage:\n-            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        return metrics\n-\n-    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n-        \"\"\"\n-        Computes the validation loss. Returns it and the number of batches.\n-        \"\"\"\n-        logger.info(\"Validating\")\n-\n-        self._pytorch_model.eval()\n-\n-        # Replace parameter values with the shadow values from the moving averages.\n-        if self._moving_average is not None:\n-            self._moving_average.assign_average_value()\n-\n-        if self._validation_data_loader is not None:\n-            validation_data_loader = self._validation_data_loader\n-        else:\n-            raise ConfigurationError(\n-                \"Validation results cannot be calculated without a validation_data_loader\"\n-            )\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n-        else:\n-            val_generator_tqdm = validation_data_loader\n-\n-        batches_this_epoch = 0\n-        val_loss = 0.0\n-        val_batch_loss = 0.0\n-        val_reg_loss = None if regularization_penalty is None else 0.0\n-        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n-        done_early = False\n-        for batch in val_generator_tqdm:\n-            if self._distributed:\n-                # Check whether the other workers have stopped already (due to differing amounts of\n-                # data in each). If so, we can't proceed because we would hang when we hit the\n-                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                # here because NCCL process groups apparently don't support BoolTensor.\n-                done = torch.tensor(0, device=self.cuda_device)\n-                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                if done.item() > 0:\n-                    done_early = True\n-                    logger.warning(\n-                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n-                        \"This implies that there is an imbalance in your validation \"\n-                        \"data across the workers and that some amount of it will be \"\n-                        \"ignored. A small amount of this is fine, but a major imbalance \"\n-                        \"should be avoided. Note: This warning will appear unless your \"\n-                        \"data is perfectly balanced.\"\n-                    )\n-                    break\n-\n-            with amp.autocast(self._use_amp):\n-                batch_outputs = self.batch_outputs(batch, for_training=False)\n-                loss = batch_outputs.get(\"loss\")\n-                reg_loss = batch_outputs.get(\"reg_loss\")\n-                if loss is not None:\n-                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n-                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n-                    # currently only used as the divisor for the loss function, so we can safely only\n-                    # count those batches for which we actually have a loss.  If this variable ever\n-                    # gets used for something else, we might need to change things around a bit.\n-                    batches_this_epoch += 1\n-                    val_batch_loss = loss.item()\n-                    val_loss += val_batch_loss\n-                    if reg_loss is not None:\n-                        val_batch_reg_loss = reg_loss.item()\n-                        val_reg_loss += val_batch_reg_loss  # type: ignore\n-\n-            # Update the description with the latest metrics\n-            val_metrics = training_util.get_metrics(\n-                self.model,\n-                val_loss,\n-                val_reg_loss,\n-                val_batch_loss,\n-                val_batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            description = training_util.description_from_metrics(val_metrics)\n-            if self._primary:\n-                val_generator_tqdm.set_description(description, refresh=False)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    [batch],\n-                    [batch_outputs],\n-                    val_metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=False,\n-                    is_primary=self._primary,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop validation early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Now restore the original parameter values.\n-        if self._moving_average is not None:\n-            self._moving_average.restore()\n-\n-        return val_loss, val_reg_loss, batches_this_epoch\n-\n-    def train(self) -> Dict[str, Any]:\n-        \"\"\"\n-        Trains the supplied model with the supplied parameters.\n-        \"\"\"\n-\n-        for callback in self._callbacks:\n-            callback.on_start(self, is_primary=self._primary)\n-\n-        # Set default values in case of failure\n-        epoch = None\n-        metrics = None\n-\n-        try:\n-            metrics, epoch = self._try_train()\n-            return metrics\n-        finally:\n-            for callback in self._callbacks:\n-                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n-        try:\n-            epoch_counter = self._restore_checkpoint()\n-        except RuntimeError:\n-            traceback.print_exc()\n-            raise ConfigurationError(\n-                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n-                \"a different serialization directory or delete the existing serialization \"\n-                \"directory?\"\n-            )\n-\n-        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n-\n-        logger.info(\"Beginning training.\")\n-\n-        val_metrics: Dict[str, float] = {}\n-        metrics: Dict[str, Any] = {}\n-        epochs_trained = 0\n-        training_start_time = time.time()\n-\n-        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n-        for key, value in self._metric_tracker.best_epoch_metrics.items():\n-            metrics[\"best_validation_\" + key] = value\n-\n-        for epoch in range(epoch_counter, self._num_epochs):\n-            epoch_start_time = time.time()\n-            train_metrics = self._train_epoch(epoch)\n-\n-            # Back up the model now, in case something goes wrong later with the evaluation\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.shelve_model(epoch, self)\n-            # Wait for the primary process to finish saving the model checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            # get peak of memory usage\n-            for key, value in train_metrics.items():\n-                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-\n-            this_epoch_val_metric: float = 0.0\n-            if self._validation_data_loader is not None:\n-                with torch.no_grad():\n-                    # We have a validation set, so compute all the metrics on it.\n-                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n-\n-                    # It is safe again to wait till the validation is done. This is\n-                    # important to get the metrics right.\n-                    if self._distributed:\n-                        dist.barrier()\n-\n-                    val_metrics = training_util.get_metrics(\n-                        self.model,\n-                        val_loss,\n-                        val_reg_loss,\n-                        batch_loss=None,\n-                        batch_reg_loss=None,\n-                        num_batches=num_batches,\n-                        reset=True,\n-                        world_size=self._world_size,\n-                        cuda_device=self.cuda_device,\n-                    )\n-\n-                    # Check validation metric for early stopping\n-                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n-                    self._metric_tracker.add_metrics(val_metrics)\n-\n-            # Create overall metrics dict\n-            training_elapsed_time = time.time() - training_start_time\n-            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n-            metrics[\"training_start_epoch\"] = epoch_counter\n-            metrics[\"training_epochs\"] = epochs_trained\n-            metrics[\"epoch\"] = epoch\n-\n-            for key, value in train_metrics.items():\n-                metrics[\"training_\" + key] = value\n-            for key, value in val_metrics.items():\n-                metrics[\"validation_\" + key] = value\n-\n-            if self._metric_tracker.is_best_so_far():\n-                # Update all the best_ metrics.\n-                # (Otherwise they just stay the same as they were.)\n-                metrics[\"best_epoch\"] = epoch\n-                for key, value in val_metrics.items():\n-                    metrics[\"best_validation_\" + key] = value\n-\n-                self._metric_tracker.best_epoch_metrics = val_metrics\n-\n-            if self._serialization_dir and self._primary:\n-                common_util.dump_metrics(\n-                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n-                    metrics,\n-                )\n-\n-            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n-            # if it doesn't, the validation metric passed here is ignored.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step(this_epoch_val_metric)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step(this_epoch_val_metric)\n-\n-            # The checkpointer saves state from the learning rate scheduler and the momentum\n-            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.save_checkpoint(\n-                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n-                )\n-            # Wait for the primary process to finish saving the checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            for callback in self._callbacks:\n-                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-            epoch_elapsed_time = time.time() - epoch_start_time\n-            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n-\n-            if epoch < self._num_epochs - 1:\n-                training_elapsed_time = time.time() - training_start_time\n-                estimated_time_remaining = training_elapsed_time * (\n-                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n-                )\n-                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n-                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n-\n-            epochs_trained += 1\n-\n-            if self._metric_tracker.should_stop_early():\n-                logger.info(\"Ran out of patience. Stopping training.\")\n-                break\n-        else:\n-            epoch = self._num_epochs - 1\n-\n-        # Load the best model state before returning\n-        best_model_state = (\n-            None if self._checkpointer is None else self._checkpointer.best_model_state()\n-        )\n-        if best_model_state:\n-            self.model.load_state_dict(best_model_state)\n-\n-        return metrics, epoch\n-\n-    @contextmanager\n-    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n-        if self._moving_average is not None:\n-            # Assigning average value to model parameters.  The checkpointer will call\n-            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n-            self._moving_average.assign_average_value()\n-\n-        model_state = self.model.state_dict()\n-\n-        # These are the training states we need to persist.\n-        training_states = {\n-            \"metric_tracker\": self._metric_tracker.state_dict(),\n-            \"optimizer\": self.optimizer.state_dict(),\n-            \"batch_num_total\": self._batch_num_total,\n-        }\n-\n-        # If we have a learning rate or momentum scheduler, we should persist them too.\n-        if self._learning_rate_scheduler is not None:\n-            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n-        if self._momentum_scheduler is not None:\n-            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n-\n-        try:\n-            yield model_state, training_states\n-        finally:\n-            if self._moving_average is not None:\n-                self._moving_average.restore()\n-\n-    def _restore_checkpoint(self) -> int:\n-        \"\"\"\n-        Restores the model and training state from the last saved checkpoint.\n-        This includes an epoch count and optimizer state, which is serialized separately\n-        from model parameters. This function should only be used to continue training -\n-        if you wish to load a model for inference/load parts of a model into a new\n-        computation graph, you should use the native Pytorch functions:\n-        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n-\n-        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n-        this function will do nothing and return 0.\n-\n-        # Returns\n-\n-        epoch: `int`\n-            The epoch at which to resume training, which should be one after the epoch\n-            in the saved training state.\n-        \"\"\"\n-        if self._checkpointer is None:\n-            return 0\n-\n-        model_state, training_state = self._checkpointer.restore_checkpoint()\n-\n-        if not training_state:\n-            # No checkpoint to restore, start at 0\n-            return 0\n-\n-        self.model.load_state_dict(model_state)\n-        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n-        if (\n-            self._learning_rate_scheduler is not None\n-            and \"learning_rate_scheduler\" in training_state\n-        ):\n-            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n-        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n-            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n-        training_util.move_optimizer_to_cuda(self.optimizer)\n-\n-        # Currently the `training_state` contains a serialized `MetricTracker`.\n-        if \"metric_tracker\" in training_state:\n-            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n-        else:\n-            self._metric_tracker.clear()\n-\n-        if isinstance(training_state[\"epoch\"], int):\n-            epoch_to_return = training_state[\"epoch\"] + 1\n-        else:\n-            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n-\n-        # For older checkpoints with batch_num_total missing, default to old behavior where\n-        # it is unchanged.\n-        batch_num_total = training_state.get(\"batch_num_total\")\n-        if batch_num_total is not None:\n-            self._batch_num_total = batch_num_total\n-\n-        return epoch_to_return\n-\n-    @classmethod\n-    def from_partial_objects(\n-        cls,\n-        model: Model,\n-        serialization_dir: str,\n-        data_loader: DataLoader,\n-        validation_data_loader: DataLoader = None,\n-        local_rank: int = 0,\n-        patience: int = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        num_epochs: int = 20,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: float = None,\n-        grad_clipping: float = None,\n-        distributed: bool = False,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        no_grad: List[str] = None,\n-        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n-        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n-        momentum_scheduler: Lazy[MomentumScheduler] = None,\n-        moving_average: Lazy[MovingAverage] = None,\n-        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n-        callbacks: List[Lazy[TrainerCallback]] = None,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> \"Trainer\":\n-        \"\"\"\n-        This method exists so that we can have a documented method to construct this class using\n-        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n-        method.\n-\n-        The reason we can't just use `__init__` with `FromParams` here is because there are\n-        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n-        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n-        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n-        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n-        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n-\n-        If you're not using `FromParams`, you can just construct these arguments in the right order\n-        yourself in your code and call the constructor directly.\n-        \"\"\"\n-        if cuda_device is None:\n-            from torch import cuda\n-\n-            if cuda.device_count() > 0:\n-                cuda_device = 0\n-            else:\n-                cuda_device = -1\n-\n-        check_for_gpu(cuda_device)\n-        if cuda_device >= 0:\n-            # Moving model to GPU here so that the optimizer state gets constructed on\n-            # the right device.\n-            model = model.cuda(cuda_device)\n-\n-        if no_grad:\n-            for name, parameter in model.named_parameters():\n-                if any(re.search(regex, name) for regex in no_grad):\n-                    parameter.requires_grad_(False)\n-\n-        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n-        optimizer_ = optimizer.construct(model_parameters=parameters)\n-\n-        common_util.log_frozen_and_tunable_parameter_names(model)\n-\n-        batches_per_epoch: Optional[int]\n-        try:\n-            batches_per_epoch = len(data_loader)\n-            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n-        except TypeError:\n-            batches_per_epoch = None\n-\n-        moving_average_ = (\n-            None if moving_average is None else moving_average.construct(parameters=parameters)\n-        )\n-        learning_rate_scheduler_ = (\n-            None\n-            if learning_rate_scheduler is None\n-            else learning_rate_scheduler.construct(\n-                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n-            )\n-        )\n-        momentum_scheduler_ = (\n-            None\n-            if momentum_scheduler is None\n-            else momentum_scheduler.construct(optimizer=optimizer_)\n-        )\n-        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n-\n-        callbacks_: List[TrainerCallback] = []\n-        for callback_ in callbacks or []:\n-            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n-\n-        return cls(\n-            model,\n-            optimizer_,\n-            data_loader,\n-            patience=patience,\n-            validation_metric=validation_metric,\n-            validation_data_loader=validation_data_loader,\n-            num_epochs=num_epochs,\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            grad_norm=grad_norm,\n-            grad_clipping=grad_clipping,\n-            learning_rate_scheduler=learning_rate_scheduler_,\n-            momentum_scheduler=momentum_scheduler_,\n-            checkpointer=checkpointer_,\n-            moving_average=moving_average_,\n-            callbacks=callbacks_,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n-            use_amp=use_amp,\n-            enable_default_callbacks=enable_default_callbacks,\n-            run_confidence_checks=run_confidence_checks,\n-            **kwargs,\n-        )\n-\n-\n-DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n-\"\"\"\n-The default callbacks used by `GradientDescentTrainer`.\n-\"\"\"\n+    def get_best_weights_path(self) -> Optional[str]:\n+        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n+        return None\n",
        "source_code_with_indent": "\n\n<DED><DED>@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    <IND>\"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        <IND>super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            <IND>warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        <DED>self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            <IND>self._validation_data_loader.set_target_device(self.cuda_device)\n        <DED>self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            <IND>if validation_data_loader is not None:\n                <IND>logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        <DED><DED>elif (not isinstance(patience, int)) or patience <= 0:\n            <IND>raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        <DED>self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            <IND>self._checkpointer = Checkpointer(serialization_dir)\n\n        <DED>self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            <IND>default_callbacks.append(ConfidenceChecksCallback)\n        <DED>for callback_cls in default_callbacks:\n            <IND>for callback in self._callbacks:\n                <IND>if callback.__class__ == callback_cls:\n                    <IND>break\n            <DED><DED>else:\n                <IND>self._callbacks.append(callback_cls(self._serialization_dir))\n\n        <DED><DED>self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            <IND>if self.cuda_device == torch.device(\"cpu\"):\n                <IND>raise ValueError(\"Using AMP requires a cuda device\")\n            <DED>self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        <DED>if self._distributed:\n            <IND>self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        <DED>else:\n            <IND>self._pytorch_model = self.model\n\n    <DED><DED>def rescale_gradients(self) -> float:\n        <IND>\"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            <IND>if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                <IND>self._scaler.unscale_(self.optimizer)\n            <DED>return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        <DED>else:\n            <IND>return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    <DED><DED>def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        <IND>\"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            <IND>try:\n                <IND>assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    <IND>output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            <DED><DED>except AssertionError:\n                <IND>if for_training:\n                    <IND>raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        <DED><DED><DED>return output_dict\n\n    <DED>def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        <IND>\"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            <IND>cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        <DED>gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            <IND>gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            <IND>len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        <DED>except TypeError:\n            <IND>num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        <DED>if self._primary:\n            <IND>batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        <DED>else:\n            <IND>batch_group_generator_tqdm = batch_group_generator\n\n        <DED>self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            <IND>self._batch_num_total = 0\n\n        <DED>done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            <IND>if done_early:\n                <IND>break\n\n            <DED>batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                <IND>for p in param_group[\"params\"]:\n                    <IND>p.grad = None\n\n            <DED><DED>batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                <IND>if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    <IND>done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        <IND>done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                <DED><DED>with amp.autocast(self._use_amp):\n                    <IND>batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        <IND>raise ValueError(\"nan loss encountered\")\n                    <DED>loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        <IND>reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                <DED><DED>if self._scaler is not None:\n                    <IND>self._scaler.scale(loss).backward()\n                <DED>else:\n                    <IND>loss.backward()\n            <DED><DED>if len(batch_group_outputs) <= 0:\n                <IND>continue\n\n            <DED>train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step_batch(batch_num_total)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step_batch(batch_num_total)\n\n            <DED>if self._scaler is not None:\n                <IND>self._scaler.step(self.optimizer)\n                self._scaler.update()\n            <DED>else:\n                <IND>self.optimizer.step()\n\n            # Update moving averages\n            <DED>if self._moving_average is not None:\n                <IND>self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            <DED>metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                <IND>description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    <IND>self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            <DED><DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        <DED>if self._distributed:\n            <IND>dist.barrier()\n\n        <DED>metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            <IND>metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>for (gpu_num, memory) in gpu_memory_usage:\n            <IND>metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>return metrics\n\n    <DED>def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        <IND>\"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>if self._validation_data_loader is not None:\n            <IND>validation_data_loader = self._validation_data_loader\n        <DED>else:\n            <IND>raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            <IND>val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        <DED>else:\n            <IND>val_generator_tqdm = validation_data_loader\n\n        <DED>batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            <IND>if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                <IND>done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    <IND>done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            <DED><DED>with amp.autocast(self._use_amp):\n                <IND>batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    <IND>batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        <IND>val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            <DED><DED><DED>val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                <IND>val_generator_tqdm.set_description(description, refresh=False)\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        <DED>if self._moving_average is not None:\n            <IND>self._moving_average.restore()\n\n        <DED>return val_loss, val_reg_loss, batches_this_epoch\n\n    <DED>def train(self) -> Dict[str, Any]:\n        <IND>\"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            <IND>callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        <DED>epoch = None\n        metrics = None\n\n        try:\n            <IND>metrics, epoch = self._try_train()\n            return metrics\n        <DED>finally:\n            <IND>for callback in self._callbacks:\n                <IND>callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    <DED><DED><DED>def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        <IND>try:\n            <IND>epoch_counter = self._restore_checkpoint()\n        <DED>except RuntimeError:\n            <IND>traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        <DED>training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            <IND>metrics[\"best_validation_\" + key] = value\n\n        <DED>for epoch in range(epoch_counter, self._num_epochs):\n            <IND>epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            # get peak of memory usage\n            <DED>for key, value in train_metrics.items():\n                <IND>if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                <DED>elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            <DED><DED>this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                <IND>with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    <IND>val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        <IND>dist.barrier()\n\n                    <DED>val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            <DED><DED>training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                <IND>metrics[\"training_\" + key] = value\n            <DED>for key, value in val_metrics.items():\n                <IND>metrics[\"validation_\" + key] = value\n\n            <DED>if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                <IND>metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    <IND>metrics[\"best_validation_\" + key] = value\n\n                <DED>self._metric_tracker.best_epoch_metrics = val_metrics\n\n            <DED>if self._serialization_dir and self._primary:\n                <IND>common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            <DED>if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step(this_epoch_val_metric)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            <DED>if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            <DED>epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                <IND>training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            <DED>epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                <IND>logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        <DED><DED>else:\n            <IND>epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        <DED>best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            <IND>self.model.load_state_dict(best_model_state)\n\n        <DED>return metrics, epoch\n\n    <DED>@contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        <IND>if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            <IND>training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        <DED>if self._momentum_scheduler is not None:\n            <IND>training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        <DED>try:\n            <IND>yield model_state, training_states\n        <DED>finally:\n            <IND>if self._moving_average is not None:\n                <IND>self._moving_average.restore()\n\n    <DED><DED><DED>def _restore_checkpoint(self) -> int:\n        <IND>\"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            <IND>return 0\n\n        <DED>model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            <IND>return 0\n\n        <DED>self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            <IND>self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        <DED>if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            <IND>self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        <DED>training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            <IND>self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        <DED>else:\n            <IND>self._metric_tracker.clear()\n\n        <DED>if isinstance(training_state[\"epoch\"], int):\n            <IND>epoch_to_return = training_state[\"epoch\"] + 1\n        <DED>else:\n            <IND>epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        <DED>batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            <IND>self._batch_num_total = batch_num_total\n\n        <DED>return epoch_to_return\n\n    <DED>@classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        <IND>\"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            <IND>from torch import cuda\n\n            if cuda.device_count() > 0:\n                <IND>cuda_device = 0\n            <DED>else:\n                <IND>cuda_device = -1\n\n        <DED><DED>check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            <IND>model = model.cuda(cuda_device)\n\n        <DED>if no_grad:\n            <IND>for name, parameter in model.named_parameters():\n                <IND>if any(re.search(regex, name) for regex in no_grad):\n                    <IND>parameter.requires_grad_(False)\n\n        <DED><DED><DED>parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            <IND>batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        <DED>except TypeError:\n            <IND>batches_per_epoch = None\n\n        <DED>moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            <IND>callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        <DED>return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\n<DED><DED>DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def get_best_weights_path(self) -> Optional[str]:\n        <IND>\"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "c5bff8ba0d835eb03931f10f4f427ffe936cf796",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:1016:8 Incompatible variable type [9]: patience is declared to have type `int` but is used as type `None`.",
    "message": " patience is declared to have type `int` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 1016,
    "warning_line": "        patience: int = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n\n@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    \"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            self._validation_data_loader.set_target_device(self.cuda_device)\n        self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            if validation_data_loader is not None:\n                logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        elif (not isinstance(patience, int)) or patience <= 0:\n            raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            self._checkpointer = Checkpointer(serialization_dir)\n\n        self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            default_callbacks.append(ConfidenceChecksCallback)\n        for callback_cls in default_callbacks:\n            for callback in self._callbacks:\n                if callback.__class__ == callback_cls:\n                    break\n            else:\n                self._callbacks.append(callback_cls(self._serialization_dir))\n\n        self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            if self.cuda_device == torch.device(\"cpu\"):\n                raise ValueError(\"Using AMP requires a cuda device\")\n            self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        if self._distributed:\n            self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        else:\n            self._pytorch_model = self.model\n\n    def rescale_gradients(self) -> float:\n        \"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                self._scaler.unscale_(self.optimizer)\n            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        else:\n            return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            try:\n                assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            except AssertionError:\n                if for_training:\n                    raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        return output_dict\n\n    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        \"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        except TypeError:\n            num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        else:\n            batch_group_generator_tqdm = batch_group_generator\n\n        self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            self._batch_num_total = 0\n\n        done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            if done_early:\n                break\n\n            batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                for p in param_group[\"params\"]:\n                    p.grad = None\n\n            batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                with amp.autocast(self._use_amp):\n                    batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        raise ValueError(\"nan loss encountered\")\n                    loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                if self._scaler is not None:\n                    self._scaler.scale(loss).backward()\n                else:\n                    loss.backward()\n            if len(batch_group_outputs) <= 0:\n                continue\n\n            train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step_batch(batch_num_total)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step_batch(batch_num_total)\n\n            if self._scaler is not None:\n                self._scaler.step(self.optimizer)\n                self._scaler.update()\n            else:\n                self.optimizer.step()\n\n            # Update moving averages\n            if self._moving_average is not None:\n                self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        if self._distributed:\n            dist.barrier()\n\n        metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        for (gpu_num, memory) in gpu_memory_usage:\n            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        return metrics\n\n    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        \"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            self._moving_average.assign_average_value()\n\n        if self._validation_data_loader is not None:\n            validation_data_loader = self._validation_data_loader\n        else:\n            raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        else:\n            val_generator_tqdm = validation_data_loader\n\n        batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            with amp.autocast(self._use_amp):\n                batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                val_generator_tqdm.set_description(description, refresh=False)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        if self._moving_average is not None:\n            self._moving_average.restore()\n\n        return val_loss, val_reg_loss, batches_this_epoch\n\n    def train(self) -> Dict[str, Any]:\n        \"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        epoch = None\n        metrics = None\n\n        try:\n            metrics, epoch = self._try_train()\n            return metrics\n        finally:\n            for callback in self._callbacks:\n                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        try:\n            epoch_counter = self._restore_checkpoint()\n        except RuntimeError:\n            traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            metrics[\"best_validation_\" + key] = value\n\n        for epoch in range(epoch_counter, self._num_epochs):\n            epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            # get peak of memory usage\n            for key, value in train_metrics.items():\n                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        dist.barrier()\n\n                    val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                metrics[\"training_\" + key] = value\n            for key, value in val_metrics.items():\n                metrics[\"validation_\" + key] = value\n\n            if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    metrics[\"best_validation_\" + key] = value\n\n                self._metric_tracker.best_epoch_metrics = val_metrics\n\n            if self._serialization_dir and self._primary:\n                common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step(this_epoch_val_metric)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            for callback in self._callbacks:\n                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        else:\n            epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            self.model.load_state_dict(best_model_state)\n\n        return metrics, epoch\n\n    @contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            self._moving_average.assign_average_value()\n\n        model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        if self._momentum_scheduler is not None:\n            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        try:\n            yield model_state, training_states\n        finally:\n            if self._moving_average is not None:\n                self._moving_average.restore()\n\n    def _restore_checkpoint(self) -> int:\n        \"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            return 0\n\n        model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            return 0\n\n        self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        else:\n            self._metric_tracker.clear()\n\n        if isinstance(training_state[\"epoch\"], int):\n            epoch_to_return = training_state[\"epoch\"] + 1\n        else:\n            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            self._batch_num_total = batch_num_total\n\n        return epoch_to_return\n\n    @classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        \"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            from torch import cuda\n\n            if cuda.device_count() > 0:\n                cuda_device = 0\n            else:\n                cuda_device = -1\n\n        check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            model = model.cuda(cuda_device)\n\n        if no_grad:\n            for name, parameter in model.named_parameters():\n                if any(re.search(regex, name) for regex in no_grad):\n                    parameter.requires_grad_(False)\n\n        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        except TypeError:\n            batches_per_epoch = None\n\n        moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\nDEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_len": 45826,
        "target_code": "\n    def get_best_weights_path(self) -> Optional[str]:\n        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_len": 151,
        "diff_format": "@@ -115,1021 +89,4 @@\n \n-\n-@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\n-class GradientDescentTrainer(Trainer):\n-    \"\"\"\n-    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n-    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n-    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n-    stopping. There are many other bells and whistles as well.\n-\n-    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n-    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n-    see the arguments to that function for the exact keys that should be used, if you are using\n-    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n-    docstrings in `from_partial_objects`.\n-\n-    [0]: https://tinyurl.com/y5mv44fw\n-\n-    # Parameters\n-\n-    model : `Model`, required.\n-        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n-        their `forward` method returns a dictionary with a \"loss\" key, containing a\n-        scalar tensor representing the loss function to be optimized.\n-\n-        If you are training your model using GPUs, your model should already be\n-        on the correct device. (If you are using our `train` command this will be\n-        handled for you.)\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    optimizer : `torch.nn.Optimizer`, required.\n-        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n-        model to be optimized.\n-\n-    data_loader : `DataLoader`, required.\n-        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    patience : `Optional[int] > 0`, optional (default=`None`)\n-        Number of epochs to be patient before early stopping: the training is stopped\n-        after `patience` epochs with no improvement. If given, it must be `> 0`.\n-        If None, early stopping is disabled.\n-\n-    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n-        Validation metric to measure for whether to stop training using patience\n-        and whether to serialize an `is_best` model each epoch. The metric name\n-        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n-        is an increasing or decreasing function. If you specify more than one metric,\n-        the metrics will be summed to make the `is_best` decision.\n-\n-    validation_data_loader : `DataLoader`, optional (default=`None`)\n-        A `DataLoader` to use for the validation set.  If `None`, then\n-        use the training `DataLoader` with the validation data.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_epochs : `int`, optional (default = `20`)\n-        Number of training epochs.\n-\n-    serialization_dir : `str`, optional (default=`None`)\n-        Path to directory for saving and loading model files. Models will not be saved if\n-        this parameter is not passed.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    checkpointer : `Checkpointer`, optional (default=`None`)\n-        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n-        here, we will construct one with default parameters.\n-\n-    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n-        An integer or `torch.device` specifying the CUDA device to use for this process.\n-        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n-\n-        !!! Note\n-            If you *don't* intend to use a GPU, but you have one available, you'll need\n-            to explicitly set `cuda_device=-1`.\n-\n-        !!! Note\n-            If you intend to use a GPU, your model already needs to be on the correct device,\n-            which you can do with `model = model.cuda()`.\n-\n-        !!! Note\n-            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n-\n-    grad_norm : `float`, optional, (default = `None`).\n-        If provided, gradient norms will be rescaled to have a maximum of this value.\n-\n-    grad_clipping : `float`, optional (default = `None`).\n-        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n-        maximum of this value.  If you are getting `NaNs` in your gradients during training\n-        that are not solved by using `grad_norm`, you may need this.\n-\n-    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n-        If specified, the learning rate will be decayed with respect to\n-        this schedule at the end of each epoch (or batch, if the scheduler implements\n-        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n-        this will use the `validation_metric` provided to determine if learning has plateaued.\n-        To support updating the learning rate on every batch, this can optionally implement\n-        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n-\n-    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n-        If specified, the momentum will be updated at the end of each batch or epoch\n-        according to the schedule.\n-\n-    moving_average : `MovingAverage`, optional, (default = `None`)\n-        If provided, we will maintain moving averages for all parameters. During training, we\n-        employ a shadow variable for each parameter, which maintains the moving average. During\n-        evaluation, we backup the original parameters and assign the moving averages to corresponding\n-        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n-        parameters. This is necessary because we want the saved model to perform as well as the validated\n-        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n-\n-    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n-        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n-        and end of training, etc.\n-\n-    distributed : `bool`, optional, (default = `False`)\n-        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n-        requires `world_size` to be greater than 1.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n-        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n-\n-    local_rank : `int`, optional, (default = `0`)\n-        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n-        used as the rank.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    world_size : `int`, (default = `1`)\n-        The number of `Trainer` workers participating in the distributed training.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n-        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n-        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n-        post][0] for details on Gradient Accumulation.\n-\n-    use_amp : `bool`, optional, (default = `False`)\n-        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n-\n-    enable_default_callbacks : `bool`, optional (default = `True`)\n-        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n-        addition to any other callbacks listed in the `callbacks` parameter.\n-        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n-\n-    run_confidence_checks : `bool`, optional (default = `True`)\n-        Determines whether model confidence checks, such as\n-        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n-        are run.\n-\n-    run_sanity_checks : `bool`, optional (default = `True`)\n-        This parameter is deprecated. Please use `run_confidence_checks` instead.\n-\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        model: Model,\n-        optimizer: torch.optim.Optimizer,\n-        data_loader: DataLoader,\n-        patience: Optional[int] = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        validation_data_loader: DataLoader = None,\n-        num_epochs: int = 20,\n-        serialization_dir: Optional[str] = None,\n-        checkpointer: Checkpointer = None,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: Optional[float] = None,\n-        grad_clipping: Optional[float] = None,\n-        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n-        momentum_scheduler: Optional[MomentumScheduler] = None,\n-        moving_average: Optional[MovingAverage] = None,\n-        callbacks: List[TrainerCallback] = None,\n-        distributed: bool = False,\n-        local_rank: int = 0,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-        )\n-\n-        if \"run_sanity_checks\" in kwargs:\n-            warnings.warn(\n-                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n-                DeprecationWarning,\n-            )\n-            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n-\n-        # I am not calling move_to_gpu here, because if the model is\n-        # not already on the GPU then the optimizer is going to be wrong.\n-        self.model = model\n-\n-        self.data_loader = data_loader\n-        self.data_loader.set_target_device(self.cuda_device)\n-        self._validation_data_loader = validation_data_loader\n-        if self._validation_data_loader is not None:\n-            self._validation_data_loader.set_target_device(self.cuda_device)\n-        self.optimizer = optimizer\n-\n-        if patience is None:  # no early stopping\n-            if validation_data_loader is not None:\n-                logger.warning(\n-                    \"You provided a validation dataset but patience was set to None, \"\n-                    \"meaning that early stopping is disabled\"\n-                )\n-        elif (not isinstance(patience, int)) or patience <= 0:\n-            raise ConfigurationError(\n-                '{} is an invalid value for \"patience\": it must be a positive integer '\n-                \"or None (if you want to disable early stopping)\".format(patience)\n-            )\n-\n-        # For tracking is_best_so_far and should_stop_early\n-        self._metric_tracker = MetricTracker(validation_metric, patience)\n-\n-        self._num_epochs = num_epochs\n-\n-        self._checkpointer: Optional[Checkpointer] = checkpointer\n-        if checkpointer is None and serialization_dir is not None:\n-            self._checkpointer = Checkpointer(serialization_dir)\n-\n-        self._grad_norm = grad_norm\n-        self._grad_clipping = grad_clipping\n-\n-        self._learning_rate_scheduler = learning_rate_scheduler\n-        self._momentum_scheduler = momentum_scheduler\n-        self._moving_average = moving_average\n-\n-        self._callbacks = callbacks or []\n-        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n-\n-        if run_confidence_checks:\n-            default_callbacks.append(ConfidenceChecksCallback)\n-        for callback_cls in default_callbacks:\n-            for callback in self._callbacks:\n-                if callback.__class__ == callback_cls:\n-                    break\n-            else:\n-                self._callbacks.append(callback_cls(self._serialization_dir))\n-\n-        self._batch_num_total = 0\n-        self._last_log = 0.0  # time of last logging\n-        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n-\n-        # Enable automatic mixed precision training.\n-        self._scaler: Optional[amp.GradScaler] = None\n-        self._use_amp = use_amp\n-        if self._use_amp:\n-            if self.cuda_device == torch.device(\"cpu\"):\n-                raise ValueError(\"Using AMP requires a cuda device\")\n-            self._scaler = amp.GradScaler()\n-\n-        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n-        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n-        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n-        #\n-        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n-        # normal case, reference to `Model` is retained. This reference is only used in\n-        # these places: `model.__call__`, `model.train` and `model.eval`.\n-        if self._distributed:\n-            self._pytorch_model = DistributedDataParallel(\n-                self.model,\n-                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n-                find_unused_parameters=True,\n-            )\n-        else:\n-            self._pytorch_model = self.model\n-\n-    def rescale_gradients(self) -> float:\n-        \"\"\"\n-        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n-\n-        Returns the norm of the gradients.\n-        \"\"\"\n-        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n-        if self._grad_norm:\n-            if self._scaler is not None:\n-                # Need to first unscale gradients in order to clip as usual.\n-                self._scaler.unscale_(self.optimizer)\n-            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n-        else:\n-            return torch.norm(\n-                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n-            )\n-\n-    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n-        \"\"\"\n-        Does a forward pass on the given batch and returns the output dictionary that the model\n-        returns, after adding any specified regularization penalty to the loss (if training).\n-        \"\"\"\n-        output_dict = self._pytorch_model(**batch)\n-\n-        if for_training:\n-            try:\n-                assert \"loss\" in output_dict\n-                regularization_penalty = self.model.get_regularization_penalty()\n-\n-                if regularization_penalty is not None:\n-                    output_dict[\"reg_loss\"] = regularization_penalty\n-                    output_dict[\"loss\"] += regularization_penalty\n-\n-            except AssertionError:\n-                if for_training:\n-                    raise RuntimeError(\n-                        \"The model you are trying to optimize does not contain a\"\n-                        \" 'loss' key in the output of model.forward(inputs).\"\n-                    )\n-\n-        return output_dict\n-\n-    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n-        \"\"\"\n-        Trains one epoch and returns metrics.\n-        \"\"\"\n-        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n-        cpu_memory_usage = []\n-        for worker, memory in common_util.peak_cpu_memory().items():\n-            cpu_memory_usage.append((worker, memory))\n-            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n-        gpu_memory_usage = []\n-        for gpu, memory in common_util.peak_gpu_memory().items():\n-            gpu_memory_usage.append((gpu, memory))\n-            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        train_loss = 0.0\n-        batch_loss = 0.0\n-        train_reg_loss = None if regularization_penalty is None else 0.0\n-        batch_reg_loss = None if regularization_penalty is None else 0.0\n-\n-        # Set the model to \"train\" mode.\n-        self._pytorch_model.train()\n-\n-        # Get tqdm for the training batches\n-        batch_generator = iter(self.data_loader)\n-        batch_group_generator = common_util.lazy_groups_of(\n-            batch_generator, self._num_gradient_accumulation_steps\n-        )\n-\n-        logger.info(\"Training\")\n-\n-        num_training_batches: Union[int, float]\n-        try:\n-            len_data_loader = len(self.data_loader)\n-            num_training_batches = math.ceil(\n-                len_data_loader / self._num_gradient_accumulation_steps\n-            )\n-        except TypeError:\n-            num_training_batches = float(\"inf\")\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            batch_group_generator_tqdm = Tqdm.tqdm(\n-                batch_group_generator, total=num_training_batches\n-            )\n-        else:\n-            batch_group_generator_tqdm = batch_group_generator\n-\n-        self._last_log = time.time()\n-\n-        batches_this_epoch = 0\n-        if self._batch_num_total is None:\n-            self._batch_num_total = 0\n-\n-        done_early = False\n-        for batch_group in batch_group_generator_tqdm:\n-            if done_early:\n-                break\n-\n-            batches_this_epoch += 1\n-            self._batch_num_total += 1\n-            batch_num_total = self._batch_num_total\n-\n-            # Zero gradients.\n-            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n-            # because it avoids a read op when the gradients are first updated below.\n-            for param_group in self.optimizer.param_groups:\n-                for p in param_group[\"params\"]:\n-                    p.grad = None\n-\n-            batch_loss = 0.0\n-            batch_group_outputs = []\n-            for batch in batch_group:\n-                if self._distributed:\n-                    # Check whether the other workers have stopped already (due to differing amounts of\n-                    # data in each). If so, we can't proceed because we would hang when we hit the\n-                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                    # here because NCCL process groups apparently don't support BoolTensor.\n-                    done = torch.tensor(0, device=self.cuda_device)\n-                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                    if done.item() > 0:\n-                        done_early = True\n-                        logger.warning(\n-                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n-                            \"This implies that there is an imbalance in your training \"\n-                            \"data across the workers and that some amount of it will be \"\n-                            \"ignored. A small amount of this is fine, but a major imbalance \"\n-                            \"should be avoided. Note: This warning will appear unless your \"\n-                            \"data is perfectly balanced.\"\n-                        )\n-                        break\n-\n-                with amp.autocast(self._use_amp):\n-                    batch_outputs = self.batch_outputs(batch, for_training=True)\n-                    batch_group_outputs.append(batch_outputs)\n-                    loss = batch_outputs[\"loss\"]\n-                    reg_loss = batch_outputs.get(\"reg_loss\")\n-                    if torch.isnan(loss):\n-                        raise ValueError(\"nan loss encountered\")\n-                    loss = loss / len(batch_group)\n-\n-                    batch_loss += loss.item()\n-                    if reg_loss is not None:\n-                        reg_loss = reg_loss / len(batch_group)\n-                        batch_reg_loss = reg_loss.item()\n-                        train_reg_loss += batch_reg_loss  # type: ignore\n-\n-                if self._scaler is not None:\n-                    self._scaler.scale(loss).backward()\n-                else:\n-                    loss.backward()\n-            if len(batch_group_outputs) <= 0:\n-                continue\n-\n-            train_loss += batch_loss\n-\n-            batch_grad_norm = self.rescale_gradients()\n-\n-            # This does nothing if batch_num_total is None or you are using a\n-            # scheduler which doesn't update per batch.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step_batch(batch_num_total)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step_batch(batch_num_total)\n-\n-            if self._scaler is not None:\n-                self._scaler.step(self.optimizer)\n-                self._scaler.update()\n-            else:\n-                self.optimizer.step()\n-\n-            # Update moving averages\n-            if self._moving_average is not None:\n-                self._moving_average.apply(batch_num_total)\n-\n-            # Update the description with the latest metrics\n-            metrics = training_util.get_metrics(\n-                self.model,\n-                train_loss,\n-                train_reg_loss,\n-                batch_loss,\n-                batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            if self._primary:\n-                # Updating tqdm only for the primary as the trainers wouldn't have one\n-                description = training_util.description_from_metrics(metrics)\n-                batch_group_generator_tqdm.set_description(description, refresh=False)\n-\n-                if self._checkpointer is not None:\n-                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    batch_group,\n-                    batch_group_outputs,\n-                    metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=True,\n-                    is_primary=self._primary,\n-                    batch_grad_norm=batch_grad_norm,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Let all workers finish their epoch before computing\n-        # the final statistics for the epoch.\n-        if self._distributed:\n-            dist.barrier()\n-\n-        metrics = training_util.get_metrics(\n-            self.model,\n-            train_loss,\n-            train_reg_loss,\n-            batch_loss=None,\n-            batch_reg_loss=None,\n-            num_batches=batches_this_epoch,\n-            reset=True,\n-            world_size=self._world_size,\n-            cuda_device=self.cuda_device,\n-        )\n-\n-        for (worker, memory) in cpu_memory_usage:\n-            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        for (gpu_num, memory) in gpu_memory_usage:\n-            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        return metrics\n-\n-    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n-        \"\"\"\n-        Computes the validation loss. Returns it and the number of batches.\n-        \"\"\"\n-        logger.info(\"Validating\")\n-\n-        self._pytorch_model.eval()\n-\n-        # Replace parameter values with the shadow values from the moving averages.\n-        if self._moving_average is not None:\n-            self._moving_average.assign_average_value()\n-\n-        if self._validation_data_loader is not None:\n-            validation_data_loader = self._validation_data_loader\n-        else:\n-            raise ConfigurationError(\n-                \"Validation results cannot be calculated without a validation_data_loader\"\n-            )\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n-        else:\n-            val_generator_tqdm = validation_data_loader\n-\n-        batches_this_epoch = 0\n-        val_loss = 0.0\n-        val_batch_loss = 0.0\n-        val_reg_loss = None if regularization_penalty is None else 0.0\n-        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n-        done_early = False\n-        for batch in val_generator_tqdm:\n-            if self._distributed:\n-                # Check whether the other workers have stopped already (due to differing amounts of\n-                # data in each). If so, we can't proceed because we would hang when we hit the\n-                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                # here because NCCL process groups apparently don't support BoolTensor.\n-                done = torch.tensor(0, device=self.cuda_device)\n-                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                if done.item() > 0:\n-                    done_early = True\n-                    logger.warning(\n-                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n-                        \"This implies that there is an imbalance in your validation \"\n-                        \"data across the workers and that some amount of it will be \"\n-                        \"ignored. A small amount of this is fine, but a major imbalance \"\n-                        \"should be avoided. Note: This warning will appear unless your \"\n-                        \"data is perfectly balanced.\"\n-                    )\n-                    break\n-\n-            with amp.autocast(self._use_amp):\n-                batch_outputs = self.batch_outputs(batch, for_training=False)\n-                loss = batch_outputs.get(\"loss\")\n-                reg_loss = batch_outputs.get(\"reg_loss\")\n-                if loss is not None:\n-                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n-                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n-                    # currently only used as the divisor for the loss function, so we can safely only\n-                    # count those batches for which we actually have a loss.  If this variable ever\n-                    # gets used for something else, we might need to change things around a bit.\n-                    batches_this_epoch += 1\n-                    val_batch_loss = loss.item()\n-                    val_loss += val_batch_loss\n-                    if reg_loss is not None:\n-                        val_batch_reg_loss = reg_loss.item()\n-                        val_reg_loss += val_batch_reg_loss  # type: ignore\n-\n-            # Update the description with the latest metrics\n-            val_metrics = training_util.get_metrics(\n-                self.model,\n-                val_loss,\n-                val_reg_loss,\n-                val_batch_loss,\n-                val_batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            description = training_util.description_from_metrics(val_metrics)\n-            if self._primary:\n-                val_generator_tqdm.set_description(description, refresh=False)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    [batch],\n-                    [batch_outputs],\n-                    val_metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=False,\n-                    is_primary=self._primary,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop validation early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Now restore the original parameter values.\n-        if self._moving_average is not None:\n-            self._moving_average.restore()\n-\n-        return val_loss, val_reg_loss, batches_this_epoch\n-\n-    def train(self) -> Dict[str, Any]:\n-        \"\"\"\n-        Trains the supplied model with the supplied parameters.\n-        \"\"\"\n-\n-        for callback in self._callbacks:\n-            callback.on_start(self, is_primary=self._primary)\n-\n-        # Set default values in case of failure\n-        epoch = None\n-        metrics = None\n-\n-        try:\n-            metrics, epoch = self._try_train()\n-            return metrics\n-        finally:\n-            for callback in self._callbacks:\n-                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n-        try:\n-            epoch_counter = self._restore_checkpoint()\n-        except RuntimeError:\n-            traceback.print_exc()\n-            raise ConfigurationError(\n-                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n-                \"a different serialization directory or delete the existing serialization \"\n-                \"directory?\"\n-            )\n-\n-        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n-\n-        logger.info(\"Beginning training.\")\n-\n-        val_metrics: Dict[str, float] = {}\n-        metrics: Dict[str, Any] = {}\n-        epochs_trained = 0\n-        training_start_time = time.time()\n-\n-        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n-        for key, value in self._metric_tracker.best_epoch_metrics.items():\n-            metrics[\"best_validation_\" + key] = value\n-\n-        for epoch in range(epoch_counter, self._num_epochs):\n-            epoch_start_time = time.time()\n-            train_metrics = self._train_epoch(epoch)\n-\n-            # Back up the model now, in case something goes wrong later with the evaluation\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.shelve_model(epoch, self)\n-            # Wait for the primary process to finish saving the model checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            # get peak of memory usage\n-            for key, value in train_metrics.items():\n-                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-\n-            this_epoch_val_metric: float = 0.0\n-            if self._validation_data_loader is not None:\n-                with torch.no_grad():\n-                    # We have a validation set, so compute all the metrics on it.\n-                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n-\n-                    # It is safe again to wait till the validation is done. This is\n-                    # important to get the metrics right.\n-                    if self._distributed:\n-                        dist.barrier()\n-\n-                    val_metrics = training_util.get_metrics(\n-                        self.model,\n-                        val_loss,\n-                        val_reg_loss,\n-                        batch_loss=None,\n-                        batch_reg_loss=None,\n-                        num_batches=num_batches,\n-                        reset=True,\n-                        world_size=self._world_size,\n-                        cuda_device=self.cuda_device,\n-                    )\n-\n-                    # Check validation metric for early stopping\n-                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n-                    self._metric_tracker.add_metrics(val_metrics)\n-\n-            # Create overall metrics dict\n-            training_elapsed_time = time.time() - training_start_time\n-            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n-            metrics[\"training_start_epoch\"] = epoch_counter\n-            metrics[\"training_epochs\"] = epochs_trained\n-            metrics[\"epoch\"] = epoch\n-\n-            for key, value in train_metrics.items():\n-                metrics[\"training_\" + key] = value\n-            for key, value in val_metrics.items():\n-                metrics[\"validation_\" + key] = value\n-\n-            if self._metric_tracker.is_best_so_far():\n-                # Update all the best_ metrics.\n-                # (Otherwise they just stay the same as they were.)\n-                metrics[\"best_epoch\"] = epoch\n-                for key, value in val_metrics.items():\n-                    metrics[\"best_validation_\" + key] = value\n-\n-                self._metric_tracker.best_epoch_metrics = val_metrics\n-\n-            if self._serialization_dir and self._primary:\n-                common_util.dump_metrics(\n-                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n-                    metrics,\n-                )\n-\n-            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n-            # if it doesn't, the validation metric passed here is ignored.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step(this_epoch_val_metric)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step(this_epoch_val_metric)\n-\n-            # The checkpointer saves state from the learning rate scheduler and the momentum\n-            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.save_checkpoint(\n-                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n-                )\n-            # Wait for the primary process to finish saving the checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            for callback in self._callbacks:\n-                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-            epoch_elapsed_time = time.time() - epoch_start_time\n-            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n-\n-            if epoch < self._num_epochs - 1:\n-                training_elapsed_time = time.time() - training_start_time\n-                estimated_time_remaining = training_elapsed_time * (\n-                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n-                )\n-                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n-                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n-\n-            epochs_trained += 1\n-\n-            if self._metric_tracker.should_stop_early():\n-                logger.info(\"Ran out of patience. Stopping training.\")\n-                break\n-        else:\n-            epoch = self._num_epochs - 1\n-\n-        # Load the best model state before returning\n-        best_model_state = (\n-            None if self._checkpointer is None else self._checkpointer.best_model_state()\n-        )\n-        if best_model_state:\n-            self.model.load_state_dict(best_model_state)\n-\n-        return metrics, epoch\n-\n-    @contextmanager\n-    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n-        if self._moving_average is not None:\n-            # Assigning average value to model parameters.  The checkpointer will call\n-            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n-            self._moving_average.assign_average_value()\n-\n-        model_state = self.model.state_dict()\n-\n-        # These are the training states we need to persist.\n-        training_states = {\n-            \"metric_tracker\": self._metric_tracker.state_dict(),\n-            \"optimizer\": self.optimizer.state_dict(),\n-            \"batch_num_total\": self._batch_num_total,\n-        }\n-\n-        # If we have a learning rate or momentum scheduler, we should persist them too.\n-        if self._learning_rate_scheduler is not None:\n-            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n-        if self._momentum_scheduler is not None:\n-            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n-\n-        try:\n-            yield model_state, training_states\n-        finally:\n-            if self._moving_average is not None:\n-                self._moving_average.restore()\n-\n-    def _restore_checkpoint(self) -> int:\n-        \"\"\"\n-        Restores the model and training state from the last saved checkpoint.\n-        This includes an epoch count and optimizer state, which is serialized separately\n-        from model parameters. This function should only be used to continue training -\n-        if you wish to load a model for inference/load parts of a model into a new\n-        computation graph, you should use the native Pytorch functions:\n-        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n-\n-        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n-        this function will do nothing and return 0.\n-\n-        # Returns\n-\n-        epoch: `int`\n-            The epoch at which to resume training, which should be one after the epoch\n-            in the saved training state.\n-        \"\"\"\n-        if self._checkpointer is None:\n-            return 0\n-\n-        model_state, training_state = self._checkpointer.restore_checkpoint()\n-\n-        if not training_state:\n-            # No checkpoint to restore, start at 0\n-            return 0\n-\n-        self.model.load_state_dict(model_state)\n-        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n-        if (\n-            self._learning_rate_scheduler is not None\n-            and \"learning_rate_scheduler\" in training_state\n-        ):\n-            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n-        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n-            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n-        training_util.move_optimizer_to_cuda(self.optimizer)\n-\n-        # Currently the `training_state` contains a serialized `MetricTracker`.\n-        if \"metric_tracker\" in training_state:\n-            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n-        else:\n-            self._metric_tracker.clear()\n-\n-        if isinstance(training_state[\"epoch\"], int):\n-            epoch_to_return = training_state[\"epoch\"] + 1\n-        else:\n-            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n-\n-        # For older checkpoints with batch_num_total missing, default to old behavior where\n-        # it is unchanged.\n-        batch_num_total = training_state.get(\"batch_num_total\")\n-        if batch_num_total is not None:\n-            self._batch_num_total = batch_num_total\n-\n-        return epoch_to_return\n-\n-    @classmethod\n-    def from_partial_objects(\n-        cls,\n-        model: Model,\n-        serialization_dir: str,\n-        data_loader: DataLoader,\n-        validation_data_loader: DataLoader = None,\n-        local_rank: int = 0,\n-        patience: int = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        num_epochs: int = 20,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: float = None,\n-        grad_clipping: float = None,\n-        distributed: bool = False,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        no_grad: List[str] = None,\n-        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n-        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n-        momentum_scheduler: Lazy[MomentumScheduler] = None,\n-        moving_average: Lazy[MovingAverage] = None,\n-        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n-        callbacks: List[Lazy[TrainerCallback]] = None,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> \"Trainer\":\n-        \"\"\"\n-        This method exists so that we can have a documented method to construct this class using\n-        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n-        method.\n-\n-        The reason we can't just use `__init__` with `FromParams` here is because there are\n-        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n-        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n-        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n-        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n-        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n-\n-        If you're not using `FromParams`, you can just construct these arguments in the right order\n-        yourself in your code and call the constructor directly.\n-        \"\"\"\n-        if cuda_device is None:\n-            from torch import cuda\n-\n-            if cuda.device_count() > 0:\n-                cuda_device = 0\n-            else:\n-                cuda_device = -1\n-\n-        check_for_gpu(cuda_device)\n-        if cuda_device >= 0:\n-            # Moving model to GPU here so that the optimizer state gets constructed on\n-            # the right device.\n-            model = model.cuda(cuda_device)\n-\n-        if no_grad:\n-            for name, parameter in model.named_parameters():\n-                if any(re.search(regex, name) for regex in no_grad):\n-                    parameter.requires_grad_(False)\n-\n-        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n-        optimizer_ = optimizer.construct(model_parameters=parameters)\n-\n-        common_util.log_frozen_and_tunable_parameter_names(model)\n-\n-        batches_per_epoch: Optional[int]\n-        try:\n-            batches_per_epoch = len(data_loader)\n-            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n-        except TypeError:\n-            batches_per_epoch = None\n-\n-        moving_average_ = (\n-            None if moving_average is None else moving_average.construct(parameters=parameters)\n-        )\n-        learning_rate_scheduler_ = (\n-            None\n-            if learning_rate_scheduler is None\n-            else learning_rate_scheduler.construct(\n-                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n-            )\n-        )\n-        momentum_scheduler_ = (\n-            None\n-            if momentum_scheduler is None\n-            else momentum_scheduler.construct(optimizer=optimizer_)\n-        )\n-        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n-\n-        callbacks_: List[TrainerCallback] = []\n-        for callback_ in callbacks or []:\n-            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n-\n-        return cls(\n-            model,\n-            optimizer_,\n-            data_loader,\n-            patience=patience,\n-            validation_metric=validation_metric,\n-            validation_data_loader=validation_data_loader,\n-            num_epochs=num_epochs,\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            grad_norm=grad_norm,\n-            grad_clipping=grad_clipping,\n-            learning_rate_scheduler=learning_rate_scheduler_,\n-            momentum_scheduler=momentum_scheduler_,\n-            checkpointer=checkpointer_,\n-            moving_average=moving_average_,\n-            callbacks=callbacks_,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n-            use_amp=use_amp,\n-            enable_default_callbacks=enable_default_callbacks,\n-            run_confidence_checks=run_confidence_checks,\n-            **kwargs,\n-        )\n-\n-\n-DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n-\"\"\"\n-The default callbacks used by `GradientDescentTrainer`.\n-\"\"\"\n+    def get_best_weights_path(self) -> Optional[str]:\n+        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n+        return None\n",
        "source_code_with_indent": "\n\n<DED><DED>@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    <IND>\"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        <IND>super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            <IND>warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        <DED>self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            <IND>self._validation_data_loader.set_target_device(self.cuda_device)\n        <DED>self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            <IND>if validation_data_loader is not None:\n                <IND>logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        <DED><DED>elif (not isinstance(patience, int)) or patience <= 0:\n            <IND>raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        <DED>self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            <IND>self._checkpointer = Checkpointer(serialization_dir)\n\n        <DED>self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            <IND>default_callbacks.append(ConfidenceChecksCallback)\n        <DED>for callback_cls in default_callbacks:\n            <IND>for callback in self._callbacks:\n                <IND>if callback.__class__ == callback_cls:\n                    <IND>break\n            <DED><DED>else:\n                <IND>self._callbacks.append(callback_cls(self._serialization_dir))\n\n        <DED><DED>self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            <IND>if self.cuda_device == torch.device(\"cpu\"):\n                <IND>raise ValueError(\"Using AMP requires a cuda device\")\n            <DED>self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        <DED>if self._distributed:\n            <IND>self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        <DED>else:\n            <IND>self._pytorch_model = self.model\n\n    <DED><DED>def rescale_gradients(self) -> float:\n        <IND>\"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            <IND>if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                <IND>self._scaler.unscale_(self.optimizer)\n            <DED>return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        <DED>else:\n            <IND>return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    <DED><DED>def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        <IND>\"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            <IND>try:\n                <IND>assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    <IND>output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            <DED><DED>except AssertionError:\n                <IND>if for_training:\n                    <IND>raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        <DED><DED><DED>return output_dict\n\n    <DED>def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        <IND>\"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            <IND>cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        <DED>gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            <IND>gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            <IND>len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        <DED>except TypeError:\n            <IND>num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        <DED>if self._primary:\n            <IND>batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        <DED>else:\n            <IND>batch_group_generator_tqdm = batch_group_generator\n\n        <DED>self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            <IND>self._batch_num_total = 0\n\n        <DED>done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            <IND>if done_early:\n                <IND>break\n\n            <DED>batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                <IND>for p in param_group[\"params\"]:\n                    <IND>p.grad = None\n\n            <DED><DED>batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                <IND>if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    <IND>done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        <IND>done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                <DED><DED>with amp.autocast(self._use_amp):\n                    <IND>batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        <IND>raise ValueError(\"nan loss encountered\")\n                    <DED>loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        <IND>reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                <DED><DED>if self._scaler is not None:\n                    <IND>self._scaler.scale(loss).backward()\n                <DED>else:\n                    <IND>loss.backward()\n            <DED><DED>if len(batch_group_outputs) <= 0:\n                <IND>continue\n\n            <DED>train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step_batch(batch_num_total)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step_batch(batch_num_total)\n\n            <DED>if self._scaler is not None:\n                <IND>self._scaler.step(self.optimizer)\n                self._scaler.update()\n            <DED>else:\n                <IND>self.optimizer.step()\n\n            # Update moving averages\n            <DED>if self._moving_average is not None:\n                <IND>self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            <DED>metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                <IND>description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    <IND>self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            <DED><DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        <DED>if self._distributed:\n            <IND>dist.barrier()\n\n        <DED>metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            <IND>metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>for (gpu_num, memory) in gpu_memory_usage:\n            <IND>metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>return metrics\n\n    <DED>def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        <IND>\"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>if self._validation_data_loader is not None:\n            <IND>validation_data_loader = self._validation_data_loader\n        <DED>else:\n            <IND>raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            <IND>val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        <DED>else:\n            <IND>val_generator_tqdm = validation_data_loader\n\n        <DED>batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            <IND>if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                <IND>done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    <IND>done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            <DED><DED>with amp.autocast(self._use_amp):\n                <IND>batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    <IND>batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        <IND>val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            <DED><DED><DED>val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                <IND>val_generator_tqdm.set_description(description, refresh=False)\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        <DED>if self._moving_average is not None:\n            <IND>self._moving_average.restore()\n\n        <DED>return val_loss, val_reg_loss, batches_this_epoch\n\n    <DED>def train(self) -> Dict[str, Any]:\n        <IND>\"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            <IND>callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        <DED>epoch = None\n        metrics = None\n\n        try:\n            <IND>metrics, epoch = self._try_train()\n            return metrics\n        <DED>finally:\n            <IND>for callback in self._callbacks:\n                <IND>callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    <DED><DED><DED>def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        <IND>try:\n            <IND>epoch_counter = self._restore_checkpoint()\n        <DED>except RuntimeError:\n            <IND>traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        <DED>training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            <IND>metrics[\"best_validation_\" + key] = value\n\n        <DED>for epoch in range(epoch_counter, self._num_epochs):\n            <IND>epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            # get peak of memory usage\n            <DED>for key, value in train_metrics.items():\n                <IND>if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                <DED>elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            <DED><DED>this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                <IND>with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    <IND>val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        <IND>dist.barrier()\n\n                    <DED>val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            <DED><DED>training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                <IND>metrics[\"training_\" + key] = value\n            <DED>for key, value in val_metrics.items():\n                <IND>metrics[\"validation_\" + key] = value\n\n            <DED>if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                <IND>metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    <IND>metrics[\"best_validation_\" + key] = value\n\n                <DED>self._metric_tracker.best_epoch_metrics = val_metrics\n\n            <DED>if self._serialization_dir and self._primary:\n                <IND>common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            <DED>if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step(this_epoch_val_metric)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            <DED>if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            <DED>epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                <IND>training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            <DED>epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                <IND>logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        <DED><DED>else:\n            <IND>epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        <DED>best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            <IND>self.model.load_state_dict(best_model_state)\n\n        <DED>return metrics, epoch\n\n    <DED>@contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        <IND>if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            <IND>training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        <DED>if self._momentum_scheduler is not None:\n            <IND>training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        <DED>try:\n            <IND>yield model_state, training_states\n        <DED>finally:\n            <IND>if self._moving_average is not None:\n                <IND>self._moving_average.restore()\n\n    <DED><DED><DED>def _restore_checkpoint(self) -> int:\n        <IND>\"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            <IND>return 0\n\n        <DED>model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            <IND>return 0\n\n        <DED>self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            <IND>self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        <DED>if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            <IND>self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        <DED>training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            <IND>self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        <DED>else:\n            <IND>self._metric_tracker.clear()\n\n        <DED>if isinstance(training_state[\"epoch\"], int):\n            <IND>epoch_to_return = training_state[\"epoch\"] + 1\n        <DED>else:\n            <IND>epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        <DED>batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            <IND>self._batch_num_total = batch_num_total\n\n        <DED>return epoch_to_return\n\n    <DED>@classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        <IND>\"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            <IND>from torch import cuda\n\n            if cuda.device_count() > 0:\n                <IND>cuda_device = 0\n            <DED>else:\n                <IND>cuda_device = -1\n\n        <DED><DED>check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            <IND>model = model.cuda(cuda_device)\n\n        <DED>if no_grad:\n            <IND>for name, parameter in model.named_parameters():\n                <IND>if any(re.search(regex, name) for regex in no_grad):\n                    <IND>parameter.requires_grad_(False)\n\n        <DED><DED><DED>parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            <IND>batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        <DED>except TypeError:\n            <IND>batches_per_epoch = None\n\n        <DED>moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            <IND>callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        <DED>return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\n<DED><DED>DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def get_best_weights_path(self) -> Optional[str]:\n        <IND>\"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "c5bff8ba0d835eb03931f10f4f427ffe936cf796",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:1020:8 Incompatible variable type [9]: grad_norm is declared to have type `float` but is used as type `None`.",
    "message": " grad_norm is declared to have type `float` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 1020,
    "warning_line": "        grad_norm: float = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n\n@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    \"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            self._validation_data_loader.set_target_device(self.cuda_device)\n        self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            if validation_data_loader is not None:\n                logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        elif (not isinstance(patience, int)) or patience <= 0:\n            raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            self._checkpointer = Checkpointer(serialization_dir)\n\n        self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            default_callbacks.append(ConfidenceChecksCallback)\n        for callback_cls in default_callbacks:\n            for callback in self._callbacks:\n                if callback.__class__ == callback_cls:\n                    break\n            else:\n                self._callbacks.append(callback_cls(self._serialization_dir))\n\n        self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            if self.cuda_device == torch.device(\"cpu\"):\n                raise ValueError(\"Using AMP requires a cuda device\")\n            self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        if self._distributed:\n            self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        else:\n            self._pytorch_model = self.model\n\n    def rescale_gradients(self) -> float:\n        \"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                self._scaler.unscale_(self.optimizer)\n            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        else:\n            return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            try:\n                assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            except AssertionError:\n                if for_training:\n                    raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        return output_dict\n\n    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        \"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        except TypeError:\n            num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        else:\n            batch_group_generator_tqdm = batch_group_generator\n\n        self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            self._batch_num_total = 0\n\n        done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            if done_early:\n                break\n\n            batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                for p in param_group[\"params\"]:\n                    p.grad = None\n\n            batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                with amp.autocast(self._use_amp):\n                    batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        raise ValueError(\"nan loss encountered\")\n                    loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                if self._scaler is not None:\n                    self._scaler.scale(loss).backward()\n                else:\n                    loss.backward()\n            if len(batch_group_outputs) <= 0:\n                continue\n\n            train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step_batch(batch_num_total)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step_batch(batch_num_total)\n\n            if self._scaler is not None:\n                self._scaler.step(self.optimizer)\n                self._scaler.update()\n            else:\n                self.optimizer.step()\n\n            # Update moving averages\n            if self._moving_average is not None:\n                self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        if self._distributed:\n            dist.barrier()\n\n        metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        for (gpu_num, memory) in gpu_memory_usage:\n            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        return metrics\n\n    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        \"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            self._moving_average.assign_average_value()\n\n        if self._validation_data_loader is not None:\n            validation_data_loader = self._validation_data_loader\n        else:\n            raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        else:\n            val_generator_tqdm = validation_data_loader\n\n        batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            with amp.autocast(self._use_amp):\n                batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                val_generator_tqdm.set_description(description, refresh=False)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        if self._moving_average is not None:\n            self._moving_average.restore()\n\n        return val_loss, val_reg_loss, batches_this_epoch\n\n    def train(self) -> Dict[str, Any]:\n        \"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        epoch = None\n        metrics = None\n\n        try:\n            metrics, epoch = self._try_train()\n            return metrics\n        finally:\n            for callback in self._callbacks:\n                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        try:\n            epoch_counter = self._restore_checkpoint()\n        except RuntimeError:\n            traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            metrics[\"best_validation_\" + key] = value\n\n        for epoch in range(epoch_counter, self._num_epochs):\n            epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            # get peak of memory usage\n            for key, value in train_metrics.items():\n                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        dist.barrier()\n\n                    val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                metrics[\"training_\" + key] = value\n            for key, value in val_metrics.items():\n                metrics[\"validation_\" + key] = value\n\n            if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    metrics[\"best_validation_\" + key] = value\n\n                self._metric_tracker.best_epoch_metrics = val_metrics\n\n            if self._serialization_dir and self._primary:\n                common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step(this_epoch_val_metric)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            for callback in self._callbacks:\n                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        else:\n            epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            self.model.load_state_dict(best_model_state)\n\n        return metrics, epoch\n\n    @contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            self._moving_average.assign_average_value()\n\n        model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        if self._momentum_scheduler is not None:\n            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        try:\n            yield model_state, training_states\n        finally:\n            if self._moving_average is not None:\n                self._moving_average.restore()\n\n    def _restore_checkpoint(self) -> int:\n        \"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            return 0\n\n        model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            return 0\n\n        self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        else:\n            self._metric_tracker.clear()\n\n        if isinstance(training_state[\"epoch\"], int):\n            epoch_to_return = training_state[\"epoch\"] + 1\n        else:\n            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            self._batch_num_total = batch_num_total\n\n        return epoch_to_return\n\n    @classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        \"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            from torch import cuda\n\n            if cuda.device_count() > 0:\n                cuda_device = 0\n            else:\n                cuda_device = -1\n\n        check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            model = model.cuda(cuda_device)\n\n        if no_grad:\n            for name, parameter in model.named_parameters():\n                if any(re.search(regex, name) for regex in no_grad):\n                    parameter.requires_grad_(False)\n\n        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        except TypeError:\n            batches_per_epoch = None\n\n        moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\nDEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_len": 45826,
        "target_code": "\n    def get_best_weights_path(self) -> Optional[str]:\n        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_len": 151,
        "diff_format": "@@ -115,1021 +89,4 @@\n \n-\n-@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\n-class GradientDescentTrainer(Trainer):\n-    \"\"\"\n-    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n-    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n-    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n-    stopping. There are many other bells and whistles as well.\n-\n-    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n-    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n-    see the arguments to that function for the exact keys that should be used, if you are using\n-    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n-    docstrings in `from_partial_objects`.\n-\n-    [0]: https://tinyurl.com/y5mv44fw\n-\n-    # Parameters\n-\n-    model : `Model`, required.\n-        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n-        their `forward` method returns a dictionary with a \"loss\" key, containing a\n-        scalar tensor representing the loss function to be optimized.\n-\n-        If you are training your model using GPUs, your model should already be\n-        on the correct device. (If you are using our `train` command this will be\n-        handled for you.)\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    optimizer : `torch.nn.Optimizer`, required.\n-        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n-        model to be optimized.\n-\n-    data_loader : `DataLoader`, required.\n-        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    patience : `Optional[int] > 0`, optional (default=`None`)\n-        Number of epochs to be patient before early stopping: the training is stopped\n-        after `patience` epochs with no improvement. If given, it must be `> 0`.\n-        If None, early stopping is disabled.\n-\n-    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n-        Validation metric to measure for whether to stop training using patience\n-        and whether to serialize an `is_best` model each epoch. The metric name\n-        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n-        is an increasing or decreasing function. If you specify more than one metric,\n-        the metrics will be summed to make the `is_best` decision.\n-\n-    validation_data_loader : `DataLoader`, optional (default=`None`)\n-        A `DataLoader` to use for the validation set.  If `None`, then\n-        use the training `DataLoader` with the validation data.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_epochs : `int`, optional (default = `20`)\n-        Number of training epochs.\n-\n-    serialization_dir : `str`, optional (default=`None`)\n-        Path to directory for saving and loading model files. Models will not be saved if\n-        this parameter is not passed.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    checkpointer : `Checkpointer`, optional (default=`None`)\n-        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n-        here, we will construct one with default parameters.\n-\n-    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n-        An integer or `torch.device` specifying the CUDA device to use for this process.\n-        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n-\n-        !!! Note\n-            If you *don't* intend to use a GPU, but you have one available, you'll need\n-            to explicitly set `cuda_device=-1`.\n-\n-        !!! Note\n-            If you intend to use a GPU, your model already needs to be on the correct device,\n-            which you can do with `model = model.cuda()`.\n-\n-        !!! Note\n-            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n-\n-    grad_norm : `float`, optional, (default = `None`).\n-        If provided, gradient norms will be rescaled to have a maximum of this value.\n-\n-    grad_clipping : `float`, optional (default = `None`).\n-        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n-        maximum of this value.  If you are getting `NaNs` in your gradients during training\n-        that are not solved by using `grad_norm`, you may need this.\n-\n-    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n-        If specified, the learning rate will be decayed with respect to\n-        this schedule at the end of each epoch (or batch, if the scheduler implements\n-        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n-        this will use the `validation_metric` provided to determine if learning has plateaued.\n-        To support updating the learning rate on every batch, this can optionally implement\n-        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n-\n-    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n-        If specified, the momentum will be updated at the end of each batch or epoch\n-        according to the schedule.\n-\n-    moving_average : `MovingAverage`, optional, (default = `None`)\n-        If provided, we will maintain moving averages for all parameters. During training, we\n-        employ a shadow variable for each parameter, which maintains the moving average. During\n-        evaluation, we backup the original parameters and assign the moving averages to corresponding\n-        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n-        parameters. This is necessary because we want the saved model to perform as well as the validated\n-        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n-\n-    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n-        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n-        and end of training, etc.\n-\n-    distributed : `bool`, optional, (default = `False`)\n-        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n-        requires `world_size` to be greater than 1.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n-        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n-\n-    local_rank : `int`, optional, (default = `0`)\n-        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n-        used as the rank.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    world_size : `int`, (default = `1`)\n-        The number of `Trainer` workers participating in the distributed training.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n-        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n-        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n-        post][0] for details on Gradient Accumulation.\n-\n-    use_amp : `bool`, optional, (default = `False`)\n-        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n-\n-    enable_default_callbacks : `bool`, optional (default = `True`)\n-        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n-        addition to any other callbacks listed in the `callbacks` parameter.\n-        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n-\n-    run_confidence_checks : `bool`, optional (default = `True`)\n-        Determines whether model confidence checks, such as\n-        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n-        are run.\n-\n-    run_sanity_checks : `bool`, optional (default = `True`)\n-        This parameter is deprecated. Please use `run_confidence_checks` instead.\n-\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        model: Model,\n-        optimizer: torch.optim.Optimizer,\n-        data_loader: DataLoader,\n-        patience: Optional[int] = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        validation_data_loader: DataLoader = None,\n-        num_epochs: int = 20,\n-        serialization_dir: Optional[str] = None,\n-        checkpointer: Checkpointer = None,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: Optional[float] = None,\n-        grad_clipping: Optional[float] = None,\n-        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n-        momentum_scheduler: Optional[MomentumScheduler] = None,\n-        moving_average: Optional[MovingAverage] = None,\n-        callbacks: List[TrainerCallback] = None,\n-        distributed: bool = False,\n-        local_rank: int = 0,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-        )\n-\n-        if \"run_sanity_checks\" in kwargs:\n-            warnings.warn(\n-                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n-                DeprecationWarning,\n-            )\n-            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n-\n-        # I am not calling move_to_gpu here, because if the model is\n-        # not already on the GPU then the optimizer is going to be wrong.\n-        self.model = model\n-\n-        self.data_loader = data_loader\n-        self.data_loader.set_target_device(self.cuda_device)\n-        self._validation_data_loader = validation_data_loader\n-        if self._validation_data_loader is not None:\n-            self._validation_data_loader.set_target_device(self.cuda_device)\n-        self.optimizer = optimizer\n-\n-        if patience is None:  # no early stopping\n-            if validation_data_loader is not None:\n-                logger.warning(\n-                    \"You provided a validation dataset but patience was set to None, \"\n-                    \"meaning that early stopping is disabled\"\n-                )\n-        elif (not isinstance(patience, int)) or patience <= 0:\n-            raise ConfigurationError(\n-                '{} is an invalid value for \"patience\": it must be a positive integer '\n-                \"or None (if you want to disable early stopping)\".format(patience)\n-            )\n-\n-        # For tracking is_best_so_far and should_stop_early\n-        self._metric_tracker = MetricTracker(validation_metric, patience)\n-\n-        self._num_epochs = num_epochs\n-\n-        self._checkpointer: Optional[Checkpointer] = checkpointer\n-        if checkpointer is None and serialization_dir is not None:\n-            self._checkpointer = Checkpointer(serialization_dir)\n-\n-        self._grad_norm = grad_norm\n-        self._grad_clipping = grad_clipping\n-\n-        self._learning_rate_scheduler = learning_rate_scheduler\n-        self._momentum_scheduler = momentum_scheduler\n-        self._moving_average = moving_average\n-\n-        self._callbacks = callbacks or []\n-        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n-\n-        if run_confidence_checks:\n-            default_callbacks.append(ConfidenceChecksCallback)\n-        for callback_cls in default_callbacks:\n-            for callback in self._callbacks:\n-                if callback.__class__ == callback_cls:\n-                    break\n-            else:\n-                self._callbacks.append(callback_cls(self._serialization_dir))\n-\n-        self._batch_num_total = 0\n-        self._last_log = 0.0  # time of last logging\n-        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n-\n-        # Enable automatic mixed precision training.\n-        self._scaler: Optional[amp.GradScaler] = None\n-        self._use_amp = use_amp\n-        if self._use_amp:\n-            if self.cuda_device == torch.device(\"cpu\"):\n-                raise ValueError(\"Using AMP requires a cuda device\")\n-            self._scaler = amp.GradScaler()\n-\n-        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n-        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n-        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n-        #\n-        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n-        # normal case, reference to `Model` is retained. This reference is only used in\n-        # these places: `model.__call__`, `model.train` and `model.eval`.\n-        if self._distributed:\n-            self._pytorch_model = DistributedDataParallel(\n-                self.model,\n-                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n-                find_unused_parameters=True,\n-            )\n-        else:\n-            self._pytorch_model = self.model\n-\n-    def rescale_gradients(self) -> float:\n-        \"\"\"\n-        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n-\n-        Returns the norm of the gradients.\n-        \"\"\"\n-        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n-        if self._grad_norm:\n-            if self._scaler is not None:\n-                # Need to first unscale gradients in order to clip as usual.\n-                self._scaler.unscale_(self.optimizer)\n-            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n-        else:\n-            return torch.norm(\n-                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n-            )\n-\n-    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n-        \"\"\"\n-        Does a forward pass on the given batch and returns the output dictionary that the model\n-        returns, after adding any specified regularization penalty to the loss (if training).\n-        \"\"\"\n-        output_dict = self._pytorch_model(**batch)\n-\n-        if for_training:\n-            try:\n-                assert \"loss\" in output_dict\n-                regularization_penalty = self.model.get_regularization_penalty()\n-\n-                if regularization_penalty is not None:\n-                    output_dict[\"reg_loss\"] = regularization_penalty\n-                    output_dict[\"loss\"] += regularization_penalty\n-\n-            except AssertionError:\n-                if for_training:\n-                    raise RuntimeError(\n-                        \"The model you are trying to optimize does not contain a\"\n-                        \" 'loss' key in the output of model.forward(inputs).\"\n-                    )\n-\n-        return output_dict\n-\n-    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n-        \"\"\"\n-        Trains one epoch and returns metrics.\n-        \"\"\"\n-        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n-        cpu_memory_usage = []\n-        for worker, memory in common_util.peak_cpu_memory().items():\n-            cpu_memory_usage.append((worker, memory))\n-            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n-        gpu_memory_usage = []\n-        for gpu, memory in common_util.peak_gpu_memory().items():\n-            gpu_memory_usage.append((gpu, memory))\n-            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        train_loss = 0.0\n-        batch_loss = 0.0\n-        train_reg_loss = None if regularization_penalty is None else 0.0\n-        batch_reg_loss = None if regularization_penalty is None else 0.0\n-\n-        # Set the model to \"train\" mode.\n-        self._pytorch_model.train()\n-\n-        # Get tqdm for the training batches\n-        batch_generator = iter(self.data_loader)\n-        batch_group_generator = common_util.lazy_groups_of(\n-            batch_generator, self._num_gradient_accumulation_steps\n-        )\n-\n-        logger.info(\"Training\")\n-\n-        num_training_batches: Union[int, float]\n-        try:\n-            len_data_loader = len(self.data_loader)\n-            num_training_batches = math.ceil(\n-                len_data_loader / self._num_gradient_accumulation_steps\n-            )\n-        except TypeError:\n-            num_training_batches = float(\"inf\")\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            batch_group_generator_tqdm = Tqdm.tqdm(\n-                batch_group_generator, total=num_training_batches\n-            )\n-        else:\n-            batch_group_generator_tqdm = batch_group_generator\n-\n-        self._last_log = time.time()\n-\n-        batches_this_epoch = 0\n-        if self._batch_num_total is None:\n-            self._batch_num_total = 0\n-\n-        done_early = False\n-        for batch_group in batch_group_generator_tqdm:\n-            if done_early:\n-                break\n-\n-            batches_this_epoch += 1\n-            self._batch_num_total += 1\n-            batch_num_total = self._batch_num_total\n-\n-            # Zero gradients.\n-            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n-            # because it avoids a read op when the gradients are first updated below.\n-            for param_group in self.optimizer.param_groups:\n-                for p in param_group[\"params\"]:\n-                    p.grad = None\n-\n-            batch_loss = 0.0\n-            batch_group_outputs = []\n-            for batch in batch_group:\n-                if self._distributed:\n-                    # Check whether the other workers have stopped already (due to differing amounts of\n-                    # data in each). If so, we can't proceed because we would hang when we hit the\n-                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                    # here because NCCL process groups apparently don't support BoolTensor.\n-                    done = torch.tensor(0, device=self.cuda_device)\n-                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                    if done.item() > 0:\n-                        done_early = True\n-                        logger.warning(\n-                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n-                            \"This implies that there is an imbalance in your training \"\n-                            \"data across the workers and that some amount of it will be \"\n-                            \"ignored. A small amount of this is fine, but a major imbalance \"\n-                            \"should be avoided. Note: This warning will appear unless your \"\n-                            \"data is perfectly balanced.\"\n-                        )\n-                        break\n-\n-                with amp.autocast(self._use_amp):\n-                    batch_outputs = self.batch_outputs(batch, for_training=True)\n-                    batch_group_outputs.append(batch_outputs)\n-                    loss = batch_outputs[\"loss\"]\n-                    reg_loss = batch_outputs.get(\"reg_loss\")\n-                    if torch.isnan(loss):\n-                        raise ValueError(\"nan loss encountered\")\n-                    loss = loss / len(batch_group)\n-\n-                    batch_loss += loss.item()\n-                    if reg_loss is not None:\n-                        reg_loss = reg_loss / len(batch_group)\n-                        batch_reg_loss = reg_loss.item()\n-                        train_reg_loss += batch_reg_loss  # type: ignore\n-\n-                if self._scaler is not None:\n-                    self._scaler.scale(loss).backward()\n-                else:\n-                    loss.backward()\n-            if len(batch_group_outputs) <= 0:\n-                continue\n-\n-            train_loss += batch_loss\n-\n-            batch_grad_norm = self.rescale_gradients()\n-\n-            # This does nothing if batch_num_total is None or you are using a\n-            # scheduler which doesn't update per batch.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step_batch(batch_num_total)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step_batch(batch_num_total)\n-\n-            if self._scaler is not None:\n-                self._scaler.step(self.optimizer)\n-                self._scaler.update()\n-            else:\n-                self.optimizer.step()\n-\n-            # Update moving averages\n-            if self._moving_average is not None:\n-                self._moving_average.apply(batch_num_total)\n-\n-            # Update the description with the latest metrics\n-            metrics = training_util.get_metrics(\n-                self.model,\n-                train_loss,\n-                train_reg_loss,\n-                batch_loss,\n-                batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            if self._primary:\n-                # Updating tqdm only for the primary as the trainers wouldn't have one\n-                description = training_util.description_from_metrics(metrics)\n-                batch_group_generator_tqdm.set_description(description, refresh=False)\n-\n-                if self._checkpointer is not None:\n-                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    batch_group,\n-                    batch_group_outputs,\n-                    metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=True,\n-                    is_primary=self._primary,\n-                    batch_grad_norm=batch_grad_norm,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Let all workers finish their epoch before computing\n-        # the final statistics for the epoch.\n-        if self._distributed:\n-            dist.barrier()\n-\n-        metrics = training_util.get_metrics(\n-            self.model,\n-            train_loss,\n-            train_reg_loss,\n-            batch_loss=None,\n-            batch_reg_loss=None,\n-            num_batches=batches_this_epoch,\n-            reset=True,\n-            world_size=self._world_size,\n-            cuda_device=self.cuda_device,\n-        )\n-\n-        for (worker, memory) in cpu_memory_usage:\n-            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        for (gpu_num, memory) in gpu_memory_usage:\n-            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        return metrics\n-\n-    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n-        \"\"\"\n-        Computes the validation loss. Returns it and the number of batches.\n-        \"\"\"\n-        logger.info(\"Validating\")\n-\n-        self._pytorch_model.eval()\n-\n-        # Replace parameter values with the shadow values from the moving averages.\n-        if self._moving_average is not None:\n-            self._moving_average.assign_average_value()\n-\n-        if self._validation_data_loader is not None:\n-            validation_data_loader = self._validation_data_loader\n-        else:\n-            raise ConfigurationError(\n-                \"Validation results cannot be calculated without a validation_data_loader\"\n-            )\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n-        else:\n-            val_generator_tqdm = validation_data_loader\n-\n-        batches_this_epoch = 0\n-        val_loss = 0.0\n-        val_batch_loss = 0.0\n-        val_reg_loss = None if regularization_penalty is None else 0.0\n-        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n-        done_early = False\n-        for batch in val_generator_tqdm:\n-            if self._distributed:\n-                # Check whether the other workers have stopped already (due to differing amounts of\n-                # data in each). If so, we can't proceed because we would hang when we hit the\n-                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                # here because NCCL process groups apparently don't support BoolTensor.\n-                done = torch.tensor(0, device=self.cuda_device)\n-                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                if done.item() > 0:\n-                    done_early = True\n-                    logger.warning(\n-                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n-                        \"This implies that there is an imbalance in your validation \"\n-                        \"data across the workers and that some amount of it will be \"\n-                        \"ignored. A small amount of this is fine, but a major imbalance \"\n-                        \"should be avoided. Note: This warning will appear unless your \"\n-                        \"data is perfectly balanced.\"\n-                    )\n-                    break\n-\n-            with amp.autocast(self._use_amp):\n-                batch_outputs = self.batch_outputs(batch, for_training=False)\n-                loss = batch_outputs.get(\"loss\")\n-                reg_loss = batch_outputs.get(\"reg_loss\")\n-                if loss is not None:\n-                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n-                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n-                    # currently only used as the divisor for the loss function, so we can safely only\n-                    # count those batches for which we actually have a loss.  If this variable ever\n-                    # gets used for something else, we might need to change things around a bit.\n-                    batches_this_epoch += 1\n-                    val_batch_loss = loss.item()\n-                    val_loss += val_batch_loss\n-                    if reg_loss is not None:\n-                        val_batch_reg_loss = reg_loss.item()\n-                        val_reg_loss += val_batch_reg_loss  # type: ignore\n-\n-            # Update the description with the latest metrics\n-            val_metrics = training_util.get_metrics(\n-                self.model,\n-                val_loss,\n-                val_reg_loss,\n-                val_batch_loss,\n-                val_batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            description = training_util.description_from_metrics(val_metrics)\n-            if self._primary:\n-                val_generator_tqdm.set_description(description, refresh=False)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    [batch],\n-                    [batch_outputs],\n-                    val_metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=False,\n-                    is_primary=self._primary,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop validation early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Now restore the original parameter values.\n-        if self._moving_average is not None:\n-            self._moving_average.restore()\n-\n-        return val_loss, val_reg_loss, batches_this_epoch\n-\n-    def train(self) -> Dict[str, Any]:\n-        \"\"\"\n-        Trains the supplied model with the supplied parameters.\n-        \"\"\"\n-\n-        for callback in self._callbacks:\n-            callback.on_start(self, is_primary=self._primary)\n-\n-        # Set default values in case of failure\n-        epoch = None\n-        metrics = None\n-\n-        try:\n-            metrics, epoch = self._try_train()\n-            return metrics\n-        finally:\n-            for callback in self._callbacks:\n-                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n-        try:\n-            epoch_counter = self._restore_checkpoint()\n-        except RuntimeError:\n-            traceback.print_exc()\n-            raise ConfigurationError(\n-                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n-                \"a different serialization directory or delete the existing serialization \"\n-                \"directory?\"\n-            )\n-\n-        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n-\n-        logger.info(\"Beginning training.\")\n-\n-        val_metrics: Dict[str, float] = {}\n-        metrics: Dict[str, Any] = {}\n-        epochs_trained = 0\n-        training_start_time = time.time()\n-\n-        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n-        for key, value in self._metric_tracker.best_epoch_metrics.items():\n-            metrics[\"best_validation_\" + key] = value\n-\n-        for epoch in range(epoch_counter, self._num_epochs):\n-            epoch_start_time = time.time()\n-            train_metrics = self._train_epoch(epoch)\n-\n-            # Back up the model now, in case something goes wrong later with the evaluation\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.shelve_model(epoch, self)\n-            # Wait for the primary process to finish saving the model checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            # get peak of memory usage\n-            for key, value in train_metrics.items():\n-                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-\n-            this_epoch_val_metric: float = 0.0\n-            if self._validation_data_loader is not None:\n-                with torch.no_grad():\n-                    # We have a validation set, so compute all the metrics on it.\n-                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n-\n-                    # It is safe again to wait till the validation is done. This is\n-                    # important to get the metrics right.\n-                    if self._distributed:\n-                        dist.barrier()\n-\n-                    val_metrics = training_util.get_metrics(\n-                        self.model,\n-                        val_loss,\n-                        val_reg_loss,\n-                        batch_loss=None,\n-                        batch_reg_loss=None,\n-                        num_batches=num_batches,\n-                        reset=True,\n-                        world_size=self._world_size,\n-                        cuda_device=self.cuda_device,\n-                    )\n-\n-                    # Check validation metric for early stopping\n-                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n-                    self._metric_tracker.add_metrics(val_metrics)\n-\n-            # Create overall metrics dict\n-            training_elapsed_time = time.time() - training_start_time\n-            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n-            metrics[\"training_start_epoch\"] = epoch_counter\n-            metrics[\"training_epochs\"] = epochs_trained\n-            metrics[\"epoch\"] = epoch\n-\n-            for key, value in train_metrics.items():\n-                metrics[\"training_\" + key] = value\n-            for key, value in val_metrics.items():\n-                metrics[\"validation_\" + key] = value\n-\n-            if self._metric_tracker.is_best_so_far():\n-                # Update all the best_ metrics.\n-                # (Otherwise they just stay the same as they were.)\n-                metrics[\"best_epoch\"] = epoch\n-                for key, value in val_metrics.items():\n-                    metrics[\"best_validation_\" + key] = value\n-\n-                self._metric_tracker.best_epoch_metrics = val_metrics\n-\n-            if self._serialization_dir and self._primary:\n-                common_util.dump_metrics(\n-                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n-                    metrics,\n-                )\n-\n-            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n-            # if it doesn't, the validation metric passed here is ignored.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step(this_epoch_val_metric)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step(this_epoch_val_metric)\n-\n-            # The checkpointer saves state from the learning rate scheduler and the momentum\n-            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.save_checkpoint(\n-                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n-                )\n-            # Wait for the primary process to finish saving the checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            for callback in self._callbacks:\n-                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-            epoch_elapsed_time = time.time() - epoch_start_time\n-            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n-\n-            if epoch < self._num_epochs - 1:\n-                training_elapsed_time = time.time() - training_start_time\n-                estimated_time_remaining = training_elapsed_time * (\n-                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n-                )\n-                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n-                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n-\n-            epochs_trained += 1\n-\n-            if self._metric_tracker.should_stop_early():\n-                logger.info(\"Ran out of patience. Stopping training.\")\n-                break\n-        else:\n-            epoch = self._num_epochs - 1\n-\n-        # Load the best model state before returning\n-        best_model_state = (\n-            None if self._checkpointer is None else self._checkpointer.best_model_state()\n-        )\n-        if best_model_state:\n-            self.model.load_state_dict(best_model_state)\n-\n-        return metrics, epoch\n-\n-    @contextmanager\n-    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n-        if self._moving_average is not None:\n-            # Assigning average value to model parameters.  The checkpointer will call\n-            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n-            self._moving_average.assign_average_value()\n-\n-        model_state = self.model.state_dict()\n-\n-        # These are the training states we need to persist.\n-        training_states = {\n-            \"metric_tracker\": self._metric_tracker.state_dict(),\n-            \"optimizer\": self.optimizer.state_dict(),\n-            \"batch_num_total\": self._batch_num_total,\n-        }\n-\n-        # If we have a learning rate or momentum scheduler, we should persist them too.\n-        if self._learning_rate_scheduler is not None:\n-            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n-        if self._momentum_scheduler is not None:\n-            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n-\n-        try:\n-            yield model_state, training_states\n-        finally:\n-            if self._moving_average is not None:\n-                self._moving_average.restore()\n-\n-    def _restore_checkpoint(self) -> int:\n-        \"\"\"\n-        Restores the model and training state from the last saved checkpoint.\n-        This includes an epoch count and optimizer state, which is serialized separately\n-        from model parameters. This function should only be used to continue training -\n-        if you wish to load a model for inference/load parts of a model into a new\n-        computation graph, you should use the native Pytorch functions:\n-        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n-\n-        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n-        this function will do nothing and return 0.\n-\n-        # Returns\n-\n-        epoch: `int`\n-            The epoch at which to resume training, which should be one after the epoch\n-            in the saved training state.\n-        \"\"\"\n-        if self._checkpointer is None:\n-            return 0\n-\n-        model_state, training_state = self._checkpointer.restore_checkpoint()\n-\n-        if not training_state:\n-            # No checkpoint to restore, start at 0\n-            return 0\n-\n-        self.model.load_state_dict(model_state)\n-        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n-        if (\n-            self._learning_rate_scheduler is not None\n-            and \"learning_rate_scheduler\" in training_state\n-        ):\n-            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n-        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n-            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n-        training_util.move_optimizer_to_cuda(self.optimizer)\n-\n-        # Currently the `training_state` contains a serialized `MetricTracker`.\n-        if \"metric_tracker\" in training_state:\n-            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n-        else:\n-            self._metric_tracker.clear()\n-\n-        if isinstance(training_state[\"epoch\"], int):\n-            epoch_to_return = training_state[\"epoch\"] + 1\n-        else:\n-            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n-\n-        # For older checkpoints with batch_num_total missing, default to old behavior where\n-        # it is unchanged.\n-        batch_num_total = training_state.get(\"batch_num_total\")\n-        if batch_num_total is not None:\n-            self._batch_num_total = batch_num_total\n-\n-        return epoch_to_return\n-\n-    @classmethod\n-    def from_partial_objects(\n-        cls,\n-        model: Model,\n-        serialization_dir: str,\n-        data_loader: DataLoader,\n-        validation_data_loader: DataLoader = None,\n-        local_rank: int = 0,\n-        patience: int = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        num_epochs: int = 20,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: float = None,\n-        grad_clipping: float = None,\n-        distributed: bool = False,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        no_grad: List[str] = None,\n-        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n-        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n-        momentum_scheduler: Lazy[MomentumScheduler] = None,\n-        moving_average: Lazy[MovingAverage] = None,\n-        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n-        callbacks: List[Lazy[TrainerCallback]] = None,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> \"Trainer\":\n-        \"\"\"\n-        This method exists so that we can have a documented method to construct this class using\n-        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n-        method.\n-\n-        The reason we can't just use `__init__` with `FromParams` here is because there are\n-        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n-        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n-        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n-        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n-        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n-\n-        If you're not using `FromParams`, you can just construct these arguments in the right order\n-        yourself in your code and call the constructor directly.\n-        \"\"\"\n-        if cuda_device is None:\n-            from torch import cuda\n-\n-            if cuda.device_count() > 0:\n-                cuda_device = 0\n-            else:\n-                cuda_device = -1\n-\n-        check_for_gpu(cuda_device)\n-        if cuda_device >= 0:\n-            # Moving model to GPU here so that the optimizer state gets constructed on\n-            # the right device.\n-            model = model.cuda(cuda_device)\n-\n-        if no_grad:\n-            for name, parameter in model.named_parameters():\n-                if any(re.search(regex, name) for regex in no_grad):\n-                    parameter.requires_grad_(False)\n-\n-        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n-        optimizer_ = optimizer.construct(model_parameters=parameters)\n-\n-        common_util.log_frozen_and_tunable_parameter_names(model)\n-\n-        batches_per_epoch: Optional[int]\n-        try:\n-            batches_per_epoch = len(data_loader)\n-            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n-        except TypeError:\n-            batches_per_epoch = None\n-\n-        moving_average_ = (\n-            None if moving_average is None else moving_average.construct(parameters=parameters)\n-        )\n-        learning_rate_scheduler_ = (\n-            None\n-            if learning_rate_scheduler is None\n-            else learning_rate_scheduler.construct(\n-                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n-            )\n-        )\n-        momentum_scheduler_ = (\n-            None\n-            if momentum_scheduler is None\n-            else momentum_scheduler.construct(optimizer=optimizer_)\n-        )\n-        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n-\n-        callbacks_: List[TrainerCallback] = []\n-        for callback_ in callbacks or []:\n-            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n-\n-        return cls(\n-            model,\n-            optimizer_,\n-            data_loader,\n-            patience=patience,\n-            validation_metric=validation_metric,\n-            validation_data_loader=validation_data_loader,\n-            num_epochs=num_epochs,\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            grad_norm=grad_norm,\n-            grad_clipping=grad_clipping,\n-            learning_rate_scheduler=learning_rate_scheduler_,\n-            momentum_scheduler=momentum_scheduler_,\n-            checkpointer=checkpointer_,\n-            moving_average=moving_average_,\n-            callbacks=callbacks_,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n-            use_amp=use_amp,\n-            enable_default_callbacks=enable_default_callbacks,\n-            run_confidence_checks=run_confidence_checks,\n-            **kwargs,\n-        )\n-\n-\n-DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n-\"\"\"\n-The default callbacks used by `GradientDescentTrainer`.\n-\"\"\"\n+    def get_best_weights_path(self) -> Optional[str]:\n+        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n+        return None\n",
        "source_code_with_indent": "\n\n<DED><DED>@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    <IND>\"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        <IND>super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            <IND>warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        <DED>self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            <IND>self._validation_data_loader.set_target_device(self.cuda_device)\n        <DED>self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            <IND>if validation_data_loader is not None:\n                <IND>logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        <DED><DED>elif (not isinstance(patience, int)) or patience <= 0:\n            <IND>raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        <DED>self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            <IND>self._checkpointer = Checkpointer(serialization_dir)\n\n        <DED>self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            <IND>default_callbacks.append(ConfidenceChecksCallback)\n        <DED>for callback_cls in default_callbacks:\n            <IND>for callback in self._callbacks:\n                <IND>if callback.__class__ == callback_cls:\n                    <IND>break\n            <DED><DED>else:\n                <IND>self._callbacks.append(callback_cls(self._serialization_dir))\n\n        <DED><DED>self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            <IND>if self.cuda_device == torch.device(\"cpu\"):\n                <IND>raise ValueError(\"Using AMP requires a cuda device\")\n            <DED>self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        <DED>if self._distributed:\n            <IND>self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        <DED>else:\n            <IND>self._pytorch_model = self.model\n\n    <DED><DED>def rescale_gradients(self) -> float:\n        <IND>\"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            <IND>if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                <IND>self._scaler.unscale_(self.optimizer)\n            <DED>return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        <DED>else:\n            <IND>return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    <DED><DED>def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        <IND>\"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            <IND>try:\n                <IND>assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    <IND>output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            <DED><DED>except AssertionError:\n                <IND>if for_training:\n                    <IND>raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        <DED><DED><DED>return output_dict\n\n    <DED>def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        <IND>\"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            <IND>cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        <DED>gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            <IND>gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            <IND>len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        <DED>except TypeError:\n            <IND>num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        <DED>if self._primary:\n            <IND>batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        <DED>else:\n            <IND>batch_group_generator_tqdm = batch_group_generator\n\n        <DED>self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            <IND>self._batch_num_total = 0\n\n        <DED>done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            <IND>if done_early:\n                <IND>break\n\n            <DED>batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                <IND>for p in param_group[\"params\"]:\n                    <IND>p.grad = None\n\n            <DED><DED>batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                <IND>if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    <IND>done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        <IND>done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                <DED><DED>with amp.autocast(self._use_amp):\n                    <IND>batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        <IND>raise ValueError(\"nan loss encountered\")\n                    <DED>loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        <IND>reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                <DED><DED>if self._scaler is not None:\n                    <IND>self._scaler.scale(loss).backward()\n                <DED>else:\n                    <IND>loss.backward()\n            <DED><DED>if len(batch_group_outputs) <= 0:\n                <IND>continue\n\n            <DED>train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step_batch(batch_num_total)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step_batch(batch_num_total)\n\n            <DED>if self._scaler is not None:\n                <IND>self._scaler.step(self.optimizer)\n                self._scaler.update()\n            <DED>else:\n                <IND>self.optimizer.step()\n\n            # Update moving averages\n            <DED>if self._moving_average is not None:\n                <IND>self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            <DED>metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                <IND>description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    <IND>self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            <DED><DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        <DED>if self._distributed:\n            <IND>dist.barrier()\n\n        <DED>metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            <IND>metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>for (gpu_num, memory) in gpu_memory_usage:\n            <IND>metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>return metrics\n\n    <DED>def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        <IND>\"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>if self._validation_data_loader is not None:\n            <IND>validation_data_loader = self._validation_data_loader\n        <DED>else:\n            <IND>raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            <IND>val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        <DED>else:\n            <IND>val_generator_tqdm = validation_data_loader\n\n        <DED>batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            <IND>if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                <IND>done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    <IND>done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            <DED><DED>with amp.autocast(self._use_amp):\n                <IND>batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    <IND>batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        <IND>val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            <DED><DED><DED>val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                <IND>val_generator_tqdm.set_description(description, refresh=False)\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        <DED>if self._moving_average is not None:\n            <IND>self._moving_average.restore()\n\n        <DED>return val_loss, val_reg_loss, batches_this_epoch\n\n    <DED>def train(self) -> Dict[str, Any]:\n        <IND>\"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            <IND>callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        <DED>epoch = None\n        metrics = None\n\n        try:\n            <IND>metrics, epoch = self._try_train()\n            return metrics\n        <DED>finally:\n            <IND>for callback in self._callbacks:\n                <IND>callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    <DED><DED><DED>def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        <IND>try:\n            <IND>epoch_counter = self._restore_checkpoint()\n        <DED>except RuntimeError:\n            <IND>traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        <DED>training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            <IND>metrics[\"best_validation_\" + key] = value\n\n        <DED>for epoch in range(epoch_counter, self._num_epochs):\n            <IND>epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            # get peak of memory usage\n            <DED>for key, value in train_metrics.items():\n                <IND>if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                <DED>elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            <DED><DED>this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                <IND>with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    <IND>val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        <IND>dist.barrier()\n\n                    <DED>val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            <DED><DED>training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                <IND>metrics[\"training_\" + key] = value\n            <DED>for key, value in val_metrics.items():\n                <IND>metrics[\"validation_\" + key] = value\n\n            <DED>if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                <IND>metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    <IND>metrics[\"best_validation_\" + key] = value\n\n                <DED>self._metric_tracker.best_epoch_metrics = val_metrics\n\n            <DED>if self._serialization_dir and self._primary:\n                <IND>common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            <DED>if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step(this_epoch_val_metric)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            <DED>if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            <DED>epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                <IND>training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            <DED>epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                <IND>logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        <DED><DED>else:\n            <IND>epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        <DED>best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            <IND>self.model.load_state_dict(best_model_state)\n\n        <DED>return metrics, epoch\n\n    <DED>@contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        <IND>if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            <IND>training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        <DED>if self._momentum_scheduler is not None:\n            <IND>training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        <DED>try:\n            <IND>yield model_state, training_states\n        <DED>finally:\n            <IND>if self._moving_average is not None:\n                <IND>self._moving_average.restore()\n\n    <DED><DED><DED>def _restore_checkpoint(self) -> int:\n        <IND>\"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            <IND>return 0\n\n        <DED>model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            <IND>return 0\n\n        <DED>self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            <IND>self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        <DED>if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            <IND>self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        <DED>training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            <IND>self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        <DED>else:\n            <IND>self._metric_tracker.clear()\n\n        <DED>if isinstance(training_state[\"epoch\"], int):\n            <IND>epoch_to_return = training_state[\"epoch\"] + 1\n        <DED>else:\n            <IND>epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        <DED>batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            <IND>self._batch_num_total = batch_num_total\n\n        <DED>return epoch_to_return\n\n    <DED>@classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        <IND>\"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            <IND>from torch import cuda\n\n            if cuda.device_count() > 0:\n                <IND>cuda_device = 0\n            <DED>else:\n                <IND>cuda_device = -1\n\n        <DED><DED>check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            <IND>model = model.cuda(cuda_device)\n\n        <DED>if no_grad:\n            <IND>for name, parameter in model.named_parameters():\n                <IND>if any(re.search(regex, name) for regex in no_grad):\n                    <IND>parameter.requires_grad_(False)\n\n        <DED><DED><DED>parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            <IND>batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        <DED>except TypeError:\n            <IND>batches_per_epoch = None\n\n        <DED>moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            <IND>callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        <DED>return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\n<DED><DED>DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def get_best_weights_path(self) -> Optional[str]:\n        <IND>\"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "c5bff8ba0d835eb03931f10f4f427ffe936cf796",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:1021:8 Incompatible variable type [9]: grad_clipping is declared to have type `float` but is used as type `None`.",
    "message": " grad_clipping is declared to have type `float` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 1021,
    "warning_line": "        grad_clipping: float = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n\n@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    \"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            self._validation_data_loader.set_target_device(self.cuda_device)\n        self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            if validation_data_loader is not None:\n                logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        elif (not isinstance(patience, int)) or patience <= 0:\n            raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            self._checkpointer = Checkpointer(serialization_dir)\n\n        self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            default_callbacks.append(ConfidenceChecksCallback)\n        for callback_cls in default_callbacks:\n            for callback in self._callbacks:\n                if callback.__class__ == callback_cls:\n                    break\n            else:\n                self._callbacks.append(callback_cls(self._serialization_dir))\n\n        self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            if self.cuda_device == torch.device(\"cpu\"):\n                raise ValueError(\"Using AMP requires a cuda device\")\n            self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        if self._distributed:\n            self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        else:\n            self._pytorch_model = self.model\n\n    def rescale_gradients(self) -> float:\n        \"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                self._scaler.unscale_(self.optimizer)\n            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        else:\n            return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            try:\n                assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            except AssertionError:\n                if for_training:\n                    raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        return output_dict\n\n    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        \"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        except TypeError:\n            num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        else:\n            batch_group_generator_tqdm = batch_group_generator\n\n        self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            self._batch_num_total = 0\n\n        done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            if done_early:\n                break\n\n            batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                for p in param_group[\"params\"]:\n                    p.grad = None\n\n            batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                with amp.autocast(self._use_amp):\n                    batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        raise ValueError(\"nan loss encountered\")\n                    loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                if self._scaler is not None:\n                    self._scaler.scale(loss).backward()\n                else:\n                    loss.backward()\n            if len(batch_group_outputs) <= 0:\n                continue\n\n            train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step_batch(batch_num_total)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step_batch(batch_num_total)\n\n            if self._scaler is not None:\n                self._scaler.step(self.optimizer)\n                self._scaler.update()\n            else:\n                self.optimizer.step()\n\n            # Update moving averages\n            if self._moving_average is not None:\n                self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        if self._distributed:\n            dist.barrier()\n\n        metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        for (gpu_num, memory) in gpu_memory_usage:\n            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        return metrics\n\n    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        \"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            self._moving_average.assign_average_value()\n\n        if self._validation_data_loader is not None:\n            validation_data_loader = self._validation_data_loader\n        else:\n            raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        else:\n            val_generator_tqdm = validation_data_loader\n\n        batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            with amp.autocast(self._use_amp):\n                batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                val_generator_tqdm.set_description(description, refresh=False)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        if self._moving_average is not None:\n            self._moving_average.restore()\n\n        return val_loss, val_reg_loss, batches_this_epoch\n\n    def train(self) -> Dict[str, Any]:\n        \"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        epoch = None\n        metrics = None\n\n        try:\n            metrics, epoch = self._try_train()\n            return metrics\n        finally:\n            for callback in self._callbacks:\n                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        try:\n            epoch_counter = self._restore_checkpoint()\n        except RuntimeError:\n            traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            metrics[\"best_validation_\" + key] = value\n\n        for epoch in range(epoch_counter, self._num_epochs):\n            epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            # get peak of memory usage\n            for key, value in train_metrics.items():\n                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        dist.barrier()\n\n                    val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                metrics[\"training_\" + key] = value\n            for key, value in val_metrics.items():\n                metrics[\"validation_\" + key] = value\n\n            if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    metrics[\"best_validation_\" + key] = value\n\n                self._metric_tracker.best_epoch_metrics = val_metrics\n\n            if self._serialization_dir and self._primary:\n                common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step(this_epoch_val_metric)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            for callback in self._callbacks:\n                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        else:\n            epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            self.model.load_state_dict(best_model_state)\n\n        return metrics, epoch\n\n    @contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            self._moving_average.assign_average_value()\n\n        model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        if self._momentum_scheduler is not None:\n            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        try:\n            yield model_state, training_states\n        finally:\n            if self._moving_average is not None:\n                self._moving_average.restore()\n\n    def _restore_checkpoint(self) -> int:\n        \"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            return 0\n\n        model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            return 0\n\n        self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        else:\n            self._metric_tracker.clear()\n\n        if isinstance(training_state[\"epoch\"], int):\n            epoch_to_return = training_state[\"epoch\"] + 1\n        else:\n            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            self._batch_num_total = batch_num_total\n\n        return epoch_to_return\n\n    @classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        \"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            from torch import cuda\n\n            if cuda.device_count() > 0:\n                cuda_device = 0\n            else:\n                cuda_device = -1\n\n        check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            model = model.cuda(cuda_device)\n\n        if no_grad:\n            for name, parameter in model.named_parameters():\n                if any(re.search(regex, name) for regex in no_grad):\n                    parameter.requires_grad_(False)\n\n        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        except TypeError:\n            batches_per_epoch = None\n\n        moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\nDEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_len": 45826,
        "target_code": "\n    def get_best_weights_path(self) -> Optional[str]:\n        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_len": 151,
        "diff_format": "@@ -115,1021 +89,4 @@\n \n-\n-@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\n-class GradientDescentTrainer(Trainer):\n-    \"\"\"\n-    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n-    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n-    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n-    stopping. There are many other bells and whistles as well.\n-\n-    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n-    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n-    see the arguments to that function for the exact keys that should be used, if you are using\n-    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n-    docstrings in `from_partial_objects`.\n-\n-    [0]: https://tinyurl.com/y5mv44fw\n-\n-    # Parameters\n-\n-    model : `Model`, required.\n-        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n-        their `forward` method returns a dictionary with a \"loss\" key, containing a\n-        scalar tensor representing the loss function to be optimized.\n-\n-        If you are training your model using GPUs, your model should already be\n-        on the correct device. (If you are using our `train` command this will be\n-        handled for you.)\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    optimizer : `torch.nn.Optimizer`, required.\n-        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n-        model to be optimized.\n-\n-    data_loader : `DataLoader`, required.\n-        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    patience : `Optional[int] > 0`, optional (default=`None`)\n-        Number of epochs to be patient before early stopping: the training is stopped\n-        after `patience` epochs with no improvement. If given, it must be `> 0`.\n-        If None, early stopping is disabled.\n-\n-    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n-        Validation metric to measure for whether to stop training using patience\n-        and whether to serialize an `is_best` model each epoch. The metric name\n-        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n-        is an increasing or decreasing function. If you specify more than one metric,\n-        the metrics will be summed to make the `is_best` decision.\n-\n-    validation_data_loader : `DataLoader`, optional (default=`None`)\n-        A `DataLoader` to use for the validation set.  If `None`, then\n-        use the training `DataLoader` with the validation data.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_epochs : `int`, optional (default = `20`)\n-        Number of training epochs.\n-\n-    serialization_dir : `str`, optional (default=`None`)\n-        Path to directory for saving and loading model files. Models will not be saved if\n-        this parameter is not passed.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    checkpointer : `Checkpointer`, optional (default=`None`)\n-        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n-        here, we will construct one with default parameters.\n-\n-    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n-        An integer or `torch.device` specifying the CUDA device to use for this process.\n-        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n-\n-        !!! Note\n-            If you *don't* intend to use a GPU, but you have one available, you'll need\n-            to explicitly set `cuda_device=-1`.\n-\n-        !!! Note\n-            If you intend to use a GPU, your model already needs to be on the correct device,\n-            which you can do with `model = model.cuda()`.\n-\n-        !!! Note\n-            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n-\n-    grad_norm : `float`, optional, (default = `None`).\n-        If provided, gradient norms will be rescaled to have a maximum of this value.\n-\n-    grad_clipping : `float`, optional (default = `None`).\n-        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n-        maximum of this value.  If you are getting `NaNs` in your gradients during training\n-        that are not solved by using `grad_norm`, you may need this.\n-\n-    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n-        If specified, the learning rate will be decayed with respect to\n-        this schedule at the end of each epoch (or batch, if the scheduler implements\n-        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n-        this will use the `validation_metric` provided to determine if learning has plateaued.\n-        To support updating the learning rate on every batch, this can optionally implement\n-        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n-\n-    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n-        If specified, the momentum will be updated at the end of each batch or epoch\n-        according to the schedule.\n-\n-    moving_average : `MovingAverage`, optional, (default = `None`)\n-        If provided, we will maintain moving averages for all parameters. During training, we\n-        employ a shadow variable for each parameter, which maintains the moving average. During\n-        evaluation, we backup the original parameters and assign the moving averages to corresponding\n-        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n-        parameters. This is necessary because we want the saved model to perform as well as the validated\n-        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n-\n-    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n-        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n-        and end of training, etc.\n-\n-    distributed : `bool`, optional, (default = `False`)\n-        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n-        requires `world_size` to be greater than 1.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n-        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n-\n-    local_rank : `int`, optional, (default = `0`)\n-        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n-        used as the rank.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    world_size : `int`, (default = `1`)\n-        The number of `Trainer` workers participating in the distributed training.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n-        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n-        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n-        post][0] for details on Gradient Accumulation.\n-\n-    use_amp : `bool`, optional, (default = `False`)\n-        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n-\n-    enable_default_callbacks : `bool`, optional (default = `True`)\n-        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n-        addition to any other callbacks listed in the `callbacks` parameter.\n-        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n-\n-    run_confidence_checks : `bool`, optional (default = `True`)\n-        Determines whether model confidence checks, such as\n-        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n-        are run.\n-\n-    run_sanity_checks : `bool`, optional (default = `True`)\n-        This parameter is deprecated. Please use `run_confidence_checks` instead.\n-\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        model: Model,\n-        optimizer: torch.optim.Optimizer,\n-        data_loader: DataLoader,\n-        patience: Optional[int] = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        validation_data_loader: DataLoader = None,\n-        num_epochs: int = 20,\n-        serialization_dir: Optional[str] = None,\n-        checkpointer: Checkpointer = None,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: Optional[float] = None,\n-        grad_clipping: Optional[float] = None,\n-        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n-        momentum_scheduler: Optional[MomentumScheduler] = None,\n-        moving_average: Optional[MovingAverage] = None,\n-        callbacks: List[TrainerCallback] = None,\n-        distributed: bool = False,\n-        local_rank: int = 0,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-        )\n-\n-        if \"run_sanity_checks\" in kwargs:\n-            warnings.warn(\n-                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n-                DeprecationWarning,\n-            )\n-            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n-\n-        # I am not calling move_to_gpu here, because if the model is\n-        # not already on the GPU then the optimizer is going to be wrong.\n-        self.model = model\n-\n-        self.data_loader = data_loader\n-        self.data_loader.set_target_device(self.cuda_device)\n-        self._validation_data_loader = validation_data_loader\n-        if self._validation_data_loader is not None:\n-            self._validation_data_loader.set_target_device(self.cuda_device)\n-        self.optimizer = optimizer\n-\n-        if patience is None:  # no early stopping\n-            if validation_data_loader is not None:\n-                logger.warning(\n-                    \"You provided a validation dataset but patience was set to None, \"\n-                    \"meaning that early stopping is disabled\"\n-                )\n-        elif (not isinstance(patience, int)) or patience <= 0:\n-            raise ConfigurationError(\n-                '{} is an invalid value for \"patience\": it must be a positive integer '\n-                \"or None (if you want to disable early stopping)\".format(patience)\n-            )\n-\n-        # For tracking is_best_so_far and should_stop_early\n-        self._metric_tracker = MetricTracker(validation_metric, patience)\n-\n-        self._num_epochs = num_epochs\n-\n-        self._checkpointer: Optional[Checkpointer] = checkpointer\n-        if checkpointer is None and serialization_dir is not None:\n-            self._checkpointer = Checkpointer(serialization_dir)\n-\n-        self._grad_norm = grad_norm\n-        self._grad_clipping = grad_clipping\n-\n-        self._learning_rate_scheduler = learning_rate_scheduler\n-        self._momentum_scheduler = momentum_scheduler\n-        self._moving_average = moving_average\n-\n-        self._callbacks = callbacks or []\n-        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n-\n-        if run_confidence_checks:\n-            default_callbacks.append(ConfidenceChecksCallback)\n-        for callback_cls in default_callbacks:\n-            for callback in self._callbacks:\n-                if callback.__class__ == callback_cls:\n-                    break\n-            else:\n-                self._callbacks.append(callback_cls(self._serialization_dir))\n-\n-        self._batch_num_total = 0\n-        self._last_log = 0.0  # time of last logging\n-        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n-\n-        # Enable automatic mixed precision training.\n-        self._scaler: Optional[amp.GradScaler] = None\n-        self._use_amp = use_amp\n-        if self._use_amp:\n-            if self.cuda_device == torch.device(\"cpu\"):\n-                raise ValueError(\"Using AMP requires a cuda device\")\n-            self._scaler = amp.GradScaler()\n-\n-        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n-        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n-        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n-        #\n-        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n-        # normal case, reference to `Model` is retained. This reference is only used in\n-        # these places: `model.__call__`, `model.train` and `model.eval`.\n-        if self._distributed:\n-            self._pytorch_model = DistributedDataParallel(\n-                self.model,\n-                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n-                find_unused_parameters=True,\n-            )\n-        else:\n-            self._pytorch_model = self.model\n-\n-    def rescale_gradients(self) -> float:\n-        \"\"\"\n-        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n-\n-        Returns the norm of the gradients.\n-        \"\"\"\n-        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n-        if self._grad_norm:\n-            if self._scaler is not None:\n-                # Need to first unscale gradients in order to clip as usual.\n-                self._scaler.unscale_(self.optimizer)\n-            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n-        else:\n-            return torch.norm(\n-                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n-            )\n-\n-    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n-        \"\"\"\n-        Does a forward pass on the given batch and returns the output dictionary that the model\n-        returns, after adding any specified regularization penalty to the loss (if training).\n-        \"\"\"\n-        output_dict = self._pytorch_model(**batch)\n-\n-        if for_training:\n-            try:\n-                assert \"loss\" in output_dict\n-                regularization_penalty = self.model.get_regularization_penalty()\n-\n-                if regularization_penalty is not None:\n-                    output_dict[\"reg_loss\"] = regularization_penalty\n-                    output_dict[\"loss\"] += regularization_penalty\n-\n-            except AssertionError:\n-                if for_training:\n-                    raise RuntimeError(\n-                        \"The model you are trying to optimize does not contain a\"\n-                        \" 'loss' key in the output of model.forward(inputs).\"\n-                    )\n-\n-        return output_dict\n-\n-    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n-        \"\"\"\n-        Trains one epoch and returns metrics.\n-        \"\"\"\n-        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n-        cpu_memory_usage = []\n-        for worker, memory in common_util.peak_cpu_memory().items():\n-            cpu_memory_usage.append((worker, memory))\n-            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n-        gpu_memory_usage = []\n-        for gpu, memory in common_util.peak_gpu_memory().items():\n-            gpu_memory_usage.append((gpu, memory))\n-            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        train_loss = 0.0\n-        batch_loss = 0.0\n-        train_reg_loss = None if regularization_penalty is None else 0.0\n-        batch_reg_loss = None if regularization_penalty is None else 0.0\n-\n-        # Set the model to \"train\" mode.\n-        self._pytorch_model.train()\n-\n-        # Get tqdm for the training batches\n-        batch_generator = iter(self.data_loader)\n-        batch_group_generator = common_util.lazy_groups_of(\n-            batch_generator, self._num_gradient_accumulation_steps\n-        )\n-\n-        logger.info(\"Training\")\n-\n-        num_training_batches: Union[int, float]\n-        try:\n-            len_data_loader = len(self.data_loader)\n-            num_training_batches = math.ceil(\n-                len_data_loader / self._num_gradient_accumulation_steps\n-            )\n-        except TypeError:\n-            num_training_batches = float(\"inf\")\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            batch_group_generator_tqdm = Tqdm.tqdm(\n-                batch_group_generator, total=num_training_batches\n-            )\n-        else:\n-            batch_group_generator_tqdm = batch_group_generator\n-\n-        self._last_log = time.time()\n-\n-        batches_this_epoch = 0\n-        if self._batch_num_total is None:\n-            self._batch_num_total = 0\n-\n-        done_early = False\n-        for batch_group in batch_group_generator_tqdm:\n-            if done_early:\n-                break\n-\n-            batches_this_epoch += 1\n-            self._batch_num_total += 1\n-            batch_num_total = self._batch_num_total\n-\n-            # Zero gradients.\n-            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n-            # because it avoids a read op when the gradients are first updated below.\n-            for param_group in self.optimizer.param_groups:\n-                for p in param_group[\"params\"]:\n-                    p.grad = None\n-\n-            batch_loss = 0.0\n-            batch_group_outputs = []\n-            for batch in batch_group:\n-                if self._distributed:\n-                    # Check whether the other workers have stopped already (due to differing amounts of\n-                    # data in each). If so, we can't proceed because we would hang when we hit the\n-                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                    # here because NCCL process groups apparently don't support BoolTensor.\n-                    done = torch.tensor(0, device=self.cuda_device)\n-                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                    if done.item() > 0:\n-                        done_early = True\n-                        logger.warning(\n-                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n-                            \"This implies that there is an imbalance in your training \"\n-                            \"data across the workers and that some amount of it will be \"\n-                            \"ignored. A small amount of this is fine, but a major imbalance \"\n-                            \"should be avoided. Note: This warning will appear unless your \"\n-                            \"data is perfectly balanced.\"\n-                        )\n-                        break\n-\n-                with amp.autocast(self._use_amp):\n-                    batch_outputs = self.batch_outputs(batch, for_training=True)\n-                    batch_group_outputs.append(batch_outputs)\n-                    loss = batch_outputs[\"loss\"]\n-                    reg_loss = batch_outputs.get(\"reg_loss\")\n-                    if torch.isnan(loss):\n-                        raise ValueError(\"nan loss encountered\")\n-                    loss = loss / len(batch_group)\n-\n-                    batch_loss += loss.item()\n-                    if reg_loss is not None:\n-                        reg_loss = reg_loss / len(batch_group)\n-                        batch_reg_loss = reg_loss.item()\n-                        train_reg_loss += batch_reg_loss  # type: ignore\n-\n-                if self._scaler is not None:\n-                    self._scaler.scale(loss).backward()\n-                else:\n-                    loss.backward()\n-            if len(batch_group_outputs) <= 0:\n-                continue\n-\n-            train_loss += batch_loss\n-\n-            batch_grad_norm = self.rescale_gradients()\n-\n-            # This does nothing if batch_num_total is None or you are using a\n-            # scheduler which doesn't update per batch.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step_batch(batch_num_total)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step_batch(batch_num_total)\n-\n-            if self._scaler is not None:\n-                self._scaler.step(self.optimizer)\n-                self._scaler.update()\n-            else:\n-                self.optimizer.step()\n-\n-            # Update moving averages\n-            if self._moving_average is not None:\n-                self._moving_average.apply(batch_num_total)\n-\n-            # Update the description with the latest metrics\n-            metrics = training_util.get_metrics(\n-                self.model,\n-                train_loss,\n-                train_reg_loss,\n-                batch_loss,\n-                batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            if self._primary:\n-                # Updating tqdm only for the primary as the trainers wouldn't have one\n-                description = training_util.description_from_metrics(metrics)\n-                batch_group_generator_tqdm.set_description(description, refresh=False)\n-\n-                if self._checkpointer is not None:\n-                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    batch_group,\n-                    batch_group_outputs,\n-                    metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=True,\n-                    is_primary=self._primary,\n-                    batch_grad_norm=batch_grad_norm,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Let all workers finish their epoch before computing\n-        # the final statistics for the epoch.\n-        if self._distributed:\n-            dist.barrier()\n-\n-        metrics = training_util.get_metrics(\n-            self.model,\n-            train_loss,\n-            train_reg_loss,\n-            batch_loss=None,\n-            batch_reg_loss=None,\n-            num_batches=batches_this_epoch,\n-            reset=True,\n-            world_size=self._world_size,\n-            cuda_device=self.cuda_device,\n-        )\n-\n-        for (worker, memory) in cpu_memory_usage:\n-            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        for (gpu_num, memory) in gpu_memory_usage:\n-            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        return metrics\n-\n-    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n-        \"\"\"\n-        Computes the validation loss. Returns it and the number of batches.\n-        \"\"\"\n-        logger.info(\"Validating\")\n-\n-        self._pytorch_model.eval()\n-\n-        # Replace parameter values with the shadow values from the moving averages.\n-        if self._moving_average is not None:\n-            self._moving_average.assign_average_value()\n-\n-        if self._validation_data_loader is not None:\n-            validation_data_loader = self._validation_data_loader\n-        else:\n-            raise ConfigurationError(\n-                \"Validation results cannot be calculated without a validation_data_loader\"\n-            )\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n-        else:\n-            val_generator_tqdm = validation_data_loader\n-\n-        batches_this_epoch = 0\n-        val_loss = 0.0\n-        val_batch_loss = 0.0\n-        val_reg_loss = None if regularization_penalty is None else 0.0\n-        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n-        done_early = False\n-        for batch in val_generator_tqdm:\n-            if self._distributed:\n-                # Check whether the other workers have stopped already (due to differing amounts of\n-                # data in each). If so, we can't proceed because we would hang when we hit the\n-                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                # here because NCCL process groups apparently don't support BoolTensor.\n-                done = torch.tensor(0, device=self.cuda_device)\n-                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                if done.item() > 0:\n-                    done_early = True\n-                    logger.warning(\n-                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n-                        \"This implies that there is an imbalance in your validation \"\n-                        \"data across the workers and that some amount of it will be \"\n-                        \"ignored. A small amount of this is fine, but a major imbalance \"\n-                        \"should be avoided. Note: This warning will appear unless your \"\n-                        \"data is perfectly balanced.\"\n-                    )\n-                    break\n-\n-            with amp.autocast(self._use_amp):\n-                batch_outputs = self.batch_outputs(batch, for_training=False)\n-                loss = batch_outputs.get(\"loss\")\n-                reg_loss = batch_outputs.get(\"reg_loss\")\n-                if loss is not None:\n-                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n-                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n-                    # currently only used as the divisor for the loss function, so we can safely only\n-                    # count those batches for which we actually have a loss.  If this variable ever\n-                    # gets used for something else, we might need to change things around a bit.\n-                    batches_this_epoch += 1\n-                    val_batch_loss = loss.item()\n-                    val_loss += val_batch_loss\n-                    if reg_loss is not None:\n-                        val_batch_reg_loss = reg_loss.item()\n-                        val_reg_loss += val_batch_reg_loss  # type: ignore\n-\n-            # Update the description with the latest metrics\n-            val_metrics = training_util.get_metrics(\n-                self.model,\n-                val_loss,\n-                val_reg_loss,\n-                val_batch_loss,\n-                val_batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            description = training_util.description_from_metrics(val_metrics)\n-            if self._primary:\n-                val_generator_tqdm.set_description(description, refresh=False)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    [batch],\n-                    [batch_outputs],\n-                    val_metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=False,\n-                    is_primary=self._primary,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop validation early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Now restore the original parameter values.\n-        if self._moving_average is not None:\n-            self._moving_average.restore()\n-\n-        return val_loss, val_reg_loss, batches_this_epoch\n-\n-    def train(self) -> Dict[str, Any]:\n-        \"\"\"\n-        Trains the supplied model with the supplied parameters.\n-        \"\"\"\n-\n-        for callback in self._callbacks:\n-            callback.on_start(self, is_primary=self._primary)\n-\n-        # Set default values in case of failure\n-        epoch = None\n-        metrics = None\n-\n-        try:\n-            metrics, epoch = self._try_train()\n-            return metrics\n-        finally:\n-            for callback in self._callbacks:\n-                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n-        try:\n-            epoch_counter = self._restore_checkpoint()\n-        except RuntimeError:\n-            traceback.print_exc()\n-            raise ConfigurationError(\n-                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n-                \"a different serialization directory or delete the existing serialization \"\n-                \"directory?\"\n-            )\n-\n-        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n-\n-        logger.info(\"Beginning training.\")\n-\n-        val_metrics: Dict[str, float] = {}\n-        metrics: Dict[str, Any] = {}\n-        epochs_trained = 0\n-        training_start_time = time.time()\n-\n-        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n-        for key, value in self._metric_tracker.best_epoch_metrics.items():\n-            metrics[\"best_validation_\" + key] = value\n-\n-        for epoch in range(epoch_counter, self._num_epochs):\n-            epoch_start_time = time.time()\n-            train_metrics = self._train_epoch(epoch)\n-\n-            # Back up the model now, in case something goes wrong later with the evaluation\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.shelve_model(epoch, self)\n-            # Wait for the primary process to finish saving the model checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            # get peak of memory usage\n-            for key, value in train_metrics.items():\n-                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-\n-            this_epoch_val_metric: float = 0.0\n-            if self._validation_data_loader is not None:\n-                with torch.no_grad():\n-                    # We have a validation set, so compute all the metrics on it.\n-                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n-\n-                    # It is safe again to wait till the validation is done. This is\n-                    # important to get the metrics right.\n-                    if self._distributed:\n-                        dist.barrier()\n-\n-                    val_metrics = training_util.get_metrics(\n-                        self.model,\n-                        val_loss,\n-                        val_reg_loss,\n-                        batch_loss=None,\n-                        batch_reg_loss=None,\n-                        num_batches=num_batches,\n-                        reset=True,\n-                        world_size=self._world_size,\n-                        cuda_device=self.cuda_device,\n-                    )\n-\n-                    # Check validation metric for early stopping\n-                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n-                    self._metric_tracker.add_metrics(val_metrics)\n-\n-            # Create overall metrics dict\n-            training_elapsed_time = time.time() - training_start_time\n-            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n-            metrics[\"training_start_epoch\"] = epoch_counter\n-            metrics[\"training_epochs\"] = epochs_trained\n-            metrics[\"epoch\"] = epoch\n-\n-            for key, value in train_metrics.items():\n-                metrics[\"training_\" + key] = value\n-            for key, value in val_metrics.items():\n-                metrics[\"validation_\" + key] = value\n-\n-            if self._metric_tracker.is_best_so_far():\n-                # Update all the best_ metrics.\n-                # (Otherwise they just stay the same as they were.)\n-                metrics[\"best_epoch\"] = epoch\n-                for key, value in val_metrics.items():\n-                    metrics[\"best_validation_\" + key] = value\n-\n-                self._metric_tracker.best_epoch_metrics = val_metrics\n-\n-            if self._serialization_dir and self._primary:\n-                common_util.dump_metrics(\n-                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n-                    metrics,\n-                )\n-\n-            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n-            # if it doesn't, the validation metric passed here is ignored.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step(this_epoch_val_metric)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step(this_epoch_val_metric)\n-\n-            # The checkpointer saves state from the learning rate scheduler and the momentum\n-            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.save_checkpoint(\n-                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n-                )\n-            # Wait for the primary process to finish saving the checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            for callback in self._callbacks:\n-                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-            epoch_elapsed_time = time.time() - epoch_start_time\n-            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n-\n-            if epoch < self._num_epochs - 1:\n-                training_elapsed_time = time.time() - training_start_time\n-                estimated_time_remaining = training_elapsed_time * (\n-                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n-                )\n-                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n-                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n-\n-            epochs_trained += 1\n-\n-            if self._metric_tracker.should_stop_early():\n-                logger.info(\"Ran out of patience. Stopping training.\")\n-                break\n-        else:\n-            epoch = self._num_epochs - 1\n-\n-        # Load the best model state before returning\n-        best_model_state = (\n-            None if self._checkpointer is None else self._checkpointer.best_model_state()\n-        )\n-        if best_model_state:\n-            self.model.load_state_dict(best_model_state)\n-\n-        return metrics, epoch\n-\n-    @contextmanager\n-    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n-        if self._moving_average is not None:\n-            # Assigning average value to model parameters.  The checkpointer will call\n-            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n-            self._moving_average.assign_average_value()\n-\n-        model_state = self.model.state_dict()\n-\n-        # These are the training states we need to persist.\n-        training_states = {\n-            \"metric_tracker\": self._metric_tracker.state_dict(),\n-            \"optimizer\": self.optimizer.state_dict(),\n-            \"batch_num_total\": self._batch_num_total,\n-        }\n-\n-        # If we have a learning rate or momentum scheduler, we should persist them too.\n-        if self._learning_rate_scheduler is not None:\n-            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n-        if self._momentum_scheduler is not None:\n-            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n-\n-        try:\n-            yield model_state, training_states\n-        finally:\n-            if self._moving_average is not None:\n-                self._moving_average.restore()\n-\n-    def _restore_checkpoint(self) -> int:\n-        \"\"\"\n-        Restores the model and training state from the last saved checkpoint.\n-        This includes an epoch count and optimizer state, which is serialized separately\n-        from model parameters. This function should only be used to continue training -\n-        if you wish to load a model for inference/load parts of a model into a new\n-        computation graph, you should use the native Pytorch functions:\n-        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n-\n-        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n-        this function will do nothing and return 0.\n-\n-        # Returns\n-\n-        epoch: `int`\n-            The epoch at which to resume training, which should be one after the epoch\n-            in the saved training state.\n-        \"\"\"\n-        if self._checkpointer is None:\n-            return 0\n-\n-        model_state, training_state = self._checkpointer.restore_checkpoint()\n-\n-        if not training_state:\n-            # No checkpoint to restore, start at 0\n-            return 0\n-\n-        self.model.load_state_dict(model_state)\n-        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n-        if (\n-            self._learning_rate_scheduler is not None\n-            and \"learning_rate_scheduler\" in training_state\n-        ):\n-            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n-        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n-            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n-        training_util.move_optimizer_to_cuda(self.optimizer)\n-\n-        # Currently the `training_state` contains a serialized `MetricTracker`.\n-        if \"metric_tracker\" in training_state:\n-            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n-        else:\n-            self._metric_tracker.clear()\n-\n-        if isinstance(training_state[\"epoch\"], int):\n-            epoch_to_return = training_state[\"epoch\"] + 1\n-        else:\n-            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n-\n-        # For older checkpoints with batch_num_total missing, default to old behavior where\n-        # it is unchanged.\n-        batch_num_total = training_state.get(\"batch_num_total\")\n-        if batch_num_total is not None:\n-            self._batch_num_total = batch_num_total\n-\n-        return epoch_to_return\n-\n-    @classmethod\n-    def from_partial_objects(\n-        cls,\n-        model: Model,\n-        serialization_dir: str,\n-        data_loader: DataLoader,\n-        validation_data_loader: DataLoader = None,\n-        local_rank: int = 0,\n-        patience: int = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        num_epochs: int = 20,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: float = None,\n-        grad_clipping: float = None,\n-        distributed: bool = False,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        no_grad: List[str] = None,\n-        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n-        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n-        momentum_scheduler: Lazy[MomentumScheduler] = None,\n-        moving_average: Lazy[MovingAverage] = None,\n-        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n-        callbacks: List[Lazy[TrainerCallback]] = None,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> \"Trainer\":\n-        \"\"\"\n-        This method exists so that we can have a documented method to construct this class using\n-        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n-        method.\n-\n-        The reason we can't just use `__init__` with `FromParams` here is because there are\n-        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n-        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n-        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n-        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n-        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n-\n-        If you're not using `FromParams`, you can just construct these arguments in the right order\n-        yourself in your code and call the constructor directly.\n-        \"\"\"\n-        if cuda_device is None:\n-            from torch import cuda\n-\n-            if cuda.device_count() > 0:\n-                cuda_device = 0\n-            else:\n-                cuda_device = -1\n-\n-        check_for_gpu(cuda_device)\n-        if cuda_device >= 0:\n-            # Moving model to GPU here so that the optimizer state gets constructed on\n-            # the right device.\n-            model = model.cuda(cuda_device)\n-\n-        if no_grad:\n-            for name, parameter in model.named_parameters():\n-                if any(re.search(regex, name) for regex in no_grad):\n-                    parameter.requires_grad_(False)\n-\n-        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n-        optimizer_ = optimizer.construct(model_parameters=parameters)\n-\n-        common_util.log_frozen_and_tunable_parameter_names(model)\n-\n-        batches_per_epoch: Optional[int]\n-        try:\n-            batches_per_epoch = len(data_loader)\n-            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n-        except TypeError:\n-            batches_per_epoch = None\n-\n-        moving_average_ = (\n-            None if moving_average is None else moving_average.construct(parameters=parameters)\n-        )\n-        learning_rate_scheduler_ = (\n-            None\n-            if learning_rate_scheduler is None\n-            else learning_rate_scheduler.construct(\n-                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n-            )\n-        )\n-        momentum_scheduler_ = (\n-            None\n-            if momentum_scheduler is None\n-            else momentum_scheduler.construct(optimizer=optimizer_)\n-        )\n-        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n-\n-        callbacks_: List[TrainerCallback] = []\n-        for callback_ in callbacks or []:\n-            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n-\n-        return cls(\n-            model,\n-            optimizer_,\n-            data_loader,\n-            patience=patience,\n-            validation_metric=validation_metric,\n-            validation_data_loader=validation_data_loader,\n-            num_epochs=num_epochs,\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            grad_norm=grad_norm,\n-            grad_clipping=grad_clipping,\n-            learning_rate_scheduler=learning_rate_scheduler_,\n-            momentum_scheduler=momentum_scheduler_,\n-            checkpointer=checkpointer_,\n-            moving_average=moving_average_,\n-            callbacks=callbacks_,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n-            use_amp=use_amp,\n-            enable_default_callbacks=enable_default_callbacks,\n-            run_confidence_checks=run_confidence_checks,\n-            **kwargs,\n-        )\n-\n-\n-DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n-\"\"\"\n-The default callbacks used by `GradientDescentTrainer`.\n-\"\"\"\n+    def get_best_weights_path(self) -> Optional[str]:\n+        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n+        return None\n",
        "source_code_with_indent": "\n\n<DED><DED>@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    <IND>\"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        <IND>super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            <IND>warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        <DED>self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            <IND>self._validation_data_loader.set_target_device(self.cuda_device)\n        <DED>self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            <IND>if validation_data_loader is not None:\n                <IND>logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        <DED><DED>elif (not isinstance(patience, int)) or patience <= 0:\n            <IND>raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        <DED>self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            <IND>self._checkpointer = Checkpointer(serialization_dir)\n\n        <DED>self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            <IND>default_callbacks.append(ConfidenceChecksCallback)\n        <DED>for callback_cls in default_callbacks:\n            <IND>for callback in self._callbacks:\n                <IND>if callback.__class__ == callback_cls:\n                    <IND>break\n            <DED><DED>else:\n                <IND>self._callbacks.append(callback_cls(self._serialization_dir))\n\n        <DED><DED>self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            <IND>if self.cuda_device == torch.device(\"cpu\"):\n                <IND>raise ValueError(\"Using AMP requires a cuda device\")\n            <DED>self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        <DED>if self._distributed:\n            <IND>self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        <DED>else:\n            <IND>self._pytorch_model = self.model\n\n    <DED><DED>def rescale_gradients(self) -> float:\n        <IND>\"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            <IND>if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                <IND>self._scaler.unscale_(self.optimizer)\n            <DED>return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        <DED>else:\n            <IND>return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    <DED><DED>def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        <IND>\"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            <IND>try:\n                <IND>assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    <IND>output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            <DED><DED>except AssertionError:\n                <IND>if for_training:\n                    <IND>raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        <DED><DED><DED>return output_dict\n\n    <DED>def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        <IND>\"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            <IND>cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        <DED>gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            <IND>gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            <IND>len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        <DED>except TypeError:\n            <IND>num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        <DED>if self._primary:\n            <IND>batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        <DED>else:\n            <IND>batch_group_generator_tqdm = batch_group_generator\n\n        <DED>self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            <IND>self._batch_num_total = 0\n\n        <DED>done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            <IND>if done_early:\n                <IND>break\n\n            <DED>batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                <IND>for p in param_group[\"params\"]:\n                    <IND>p.grad = None\n\n            <DED><DED>batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                <IND>if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    <IND>done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        <IND>done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                <DED><DED>with amp.autocast(self._use_amp):\n                    <IND>batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        <IND>raise ValueError(\"nan loss encountered\")\n                    <DED>loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        <IND>reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                <DED><DED>if self._scaler is not None:\n                    <IND>self._scaler.scale(loss).backward()\n                <DED>else:\n                    <IND>loss.backward()\n            <DED><DED>if len(batch_group_outputs) <= 0:\n                <IND>continue\n\n            <DED>train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step_batch(batch_num_total)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step_batch(batch_num_total)\n\n            <DED>if self._scaler is not None:\n                <IND>self._scaler.step(self.optimizer)\n                self._scaler.update()\n            <DED>else:\n                <IND>self.optimizer.step()\n\n            # Update moving averages\n            <DED>if self._moving_average is not None:\n                <IND>self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            <DED>metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                <IND>description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    <IND>self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            <DED><DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        <DED>if self._distributed:\n            <IND>dist.barrier()\n\n        <DED>metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            <IND>metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>for (gpu_num, memory) in gpu_memory_usage:\n            <IND>metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>return metrics\n\n    <DED>def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        <IND>\"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>if self._validation_data_loader is not None:\n            <IND>validation_data_loader = self._validation_data_loader\n        <DED>else:\n            <IND>raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            <IND>val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        <DED>else:\n            <IND>val_generator_tqdm = validation_data_loader\n\n        <DED>batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            <IND>if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                <IND>done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    <IND>done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            <DED><DED>with amp.autocast(self._use_amp):\n                <IND>batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    <IND>batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        <IND>val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            <DED><DED><DED>val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                <IND>val_generator_tqdm.set_description(description, refresh=False)\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        <DED>if self._moving_average is not None:\n            <IND>self._moving_average.restore()\n\n        <DED>return val_loss, val_reg_loss, batches_this_epoch\n\n    <DED>def train(self) -> Dict[str, Any]:\n        <IND>\"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            <IND>callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        <DED>epoch = None\n        metrics = None\n\n        try:\n            <IND>metrics, epoch = self._try_train()\n            return metrics\n        <DED>finally:\n            <IND>for callback in self._callbacks:\n                <IND>callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    <DED><DED><DED>def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        <IND>try:\n            <IND>epoch_counter = self._restore_checkpoint()\n        <DED>except RuntimeError:\n            <IND>traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        <DED>training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            <IND>metrics[\"best_validation_\" + key] = value\n\n        <DED>for epoch in range(epoch_counter, self._num_epochs):\n            <IND>epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            # get peak of memory usage\n            <DED>for key, value in train_metrics.items():\n                <IND>if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                <DED>elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            <DED><DED>this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                <IND>with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    <IND>val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        <IND>dist.barrier()\n\n                    <DED>val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            <DED><DED>training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                <IND>metrics[\"training_\" + key] = value\n            <DED>for key, value in val_metrics.items():\n                <IND>metrics[\"validation_\" + key] = value\n\n            <DED>if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                <IND>metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    <IND>metrics[\"best_validation_\" + key] = value\n\n                <DED>self._metric_tracker.best_epoch_metrics = val_metrics\n\n            <DED>if self._serialization_dir and self._primary:\n                <IND>common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            <DED>if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step(this_epoch_val_metric)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            <DED>if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            <DED>epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                <IND>training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            <DED>epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                <IND>logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        <DED><DED>else:\n            <IND>epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        <DED>best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            <IND>self.model.load_state_dict(best_model_state)\n\n        <DED>return metrics, epoch\n\n    <DED>@contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        <IND>if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            <IND>training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        <DED>if self._momentum_scheduler is not None:\n            <IND>training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        <DED>try:\n            <IND>yield model_state, training_states\n        <DED>finally:\n            <IND>if self._moving_average is not None:\n                <IND>self._moving_average.restore()\n\n    <DED><DED><DED>def _restore_checkpoint(self) -> int:\n        <IND>\"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            <IND>return 0\n\n        <DED>model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            <IND>return 0\n\n        <DED>self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            <IND>self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        <DED>if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            <IND>self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        <DED>training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            <IND>self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        <DED>else:\n            <IND>self._metric_tracker.clear()\n\n        <DED>if isinstance(training_state[\"epoch\"], int):\n            <IND>epoch_to_return = training_state[\"epoch\"] + 1\n        <DED>else:\n            <IND>epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        <DED>batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            <IND>self._batch_num_total = batch_num_total\n\n        <DED>return epoch_to_return\n\n    <DED>@classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        <IND>\"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            <IND>from torch import cuda\n\n            if cuda.device_count() > 0:\n                <IND>cuda_device = 0\n            <DED>else:\n                <IND>cuda_device = -1\n\n        <DED><DED>check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            <IND>model = model.cuda(cuda_device)\n\n        <DED>if no_grad:\n            <IND>for name, parameter in model.named_parameters():\n                <IND>if any(re.search(regex, name) for regex in no_grad):\n                    <IND>parameter.requires_grad_(False)\n\n        <DED><DED><DED>parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            <IND>batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        <DED>except TypeError:\n            <IND>batches_per_epoch = None\n\n        <DED>moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            <IND>callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        <DED>return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\n<DED><DED>DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def get_best_weights_path(self) -> Optional[str]:\n        <IND>\"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "c5bff8ba0d835eb03931f10f4f427ffe936cf796",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:1026:8 Incompatible variable type [9]: no_grad is declared to have type `List[str]` but is used as type `None`.",
    "message": " no_grad is declared to have type `List[str]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 1026,
    "warning_line": "        no_grad: List[str] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n\n@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    \"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            self._validation_data_loader.set_target_device(self.cuda_device)\n        self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            if validation_data_loader is not None:\n                logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        elif (not isinstance(patience, int)) or patience <= 0:\n            raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            self._checkpointer = Checkpointer(serialization_dir)\n\n        self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            default_callbacks.append(ConfidenceChecksCallback)\n        for callback_cls in default_callbacks:\n            for callback in self._callbacks:\n                if callback.__class__ == callback_cls:\n                    break\n            else:\n                self._callbacks.append(callback_cls(self._serialization_dir))\n\n        self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            if self.cuda_device == torch.device(\"cpu\"):\n                raise ValueError(\"Using AMP requires a cuda device\")\n            self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        if self._distributed:\n            self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        else:\n            self._pytorch_model = self.model\n\n    def rescale_gradients(self) -> float:\n        \"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                self._scaler.unscale_(self.optimizer)\n            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        else:\n            return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            try:\n                assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            except AssertionError:\n                if for_training:\n                    raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        return output_dict\n\n    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        \"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        except TypeError:\n            num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        else:\n            batch_group_generator_tqdm = batch_group_generator\n\n        self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            self._batch_num_total = 0\n\n        done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            if done_early:\n                break\n\n            batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                for p in param_group[\"params\"]:\n                    p.grad = None\n\n            batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                with amp.autocast(self._use_amp):\n                    batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        raise ValueError(\"nan loss encountered\")\n                    loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                if self._scaler is not None:\n                    self._scaler.scale(loss).backward()\n                else:\n                    loss.backward()\n            if len(batch_group_outputs) <= 0:\n                continue\n\n            train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step_batch(batch_num_total)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step_batch(batch_num_total)\n\n            if self._scaler is not None:\n                self._scaler.step(self.optimizer)\n                self._scaler.update()\n            else:\n                self.optimizer.step()\n\n            # Update moving averages\n            if self._moving_average is not None:\n                self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        if self._distributed:\n            dist.barrier()\n\n        metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        for (gpu_num, memory) in gpu_memory_usage:\n            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        return metrics\n\n    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        \"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            self._moving_average.assign_average_value()\n\n        if self._validation_data_loader is not None:\n            validation_data_loader = self._validation_data_loader\n        else:\n            raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        else:\n            val_generator_tqdm = validation_data_loader\n\n        batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            with amp.autocast(self._use_amp):\n                batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                val_generator_tqdm.set_description(description, refresh=False)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        if self._moving_average is not None:\n            self._moving_average.restore()\n\n        return val_loss, val_reg_loss, batches_this_epoch\n\n    def train(self) -> Dict[str, Any]:\n        \"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        epoch = None\n        metrics = None\n\n        try:\n            metrics, epoch = self._try_train()\n            return metrics\n        finally:\n            for callback in self._callbacks:\n                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        try:\n            epoch_counter = self._restore_checkpoint()\n        except RuntimeError:\n            traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            metrics[\"best_validation_\" + key] = value\n\n        for epoch in range(epoch_counter, self._num_epochs):\n            epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            # get peak of memory usage\n            for key, value in train_metrics.items():\n                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        dist.barrier()\n\n                    val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                metrics[\"training_\" + key] = value\n            for key, value in val_metrics.items():\n                metrics[\"validation_\" + key] = value\n\n            if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    metrics[\"best_validation_\" + key] = value\n\n                self._metric_tracker.best_epoch_metrics = val_metrics\n\n            if self._serialization_dir and self._primary:\n                common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step(this_epoch_val_metric)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            for callback in self._callbacks:\n                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        else:\n            epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            self.model.load_state_dict(best_model_state)\n\n        return metrics, epoch\n\n    @contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            self._moving_average.assign_average_value()\n\n        model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        if self._momentum_scheduler is not None:\n            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        try:\n            yield model_state, training_states\n        finally:\n            if self._moving_average is not None:\n                self._moving_average.restore()\n\n    def _restore_checkpoint(self) -> int:\n        \"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            return 0\n\n        model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            return 0\n\n        self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        else:\n            self._metric_tracker.clear()\n\n        if isinstance(training_state[\"epoch\"], int):\n            epoch_to_return = training_state[\"epoch\"] + 1\n        else:\n            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            self._batch_num_total = batch_num_total\n\n        return epoch_to_return\n\n    @classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        \"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            from torch import cuda\n\n            if cuda.device_count() > 0:\n                cuda_device = 0\n            else:\n                cuda_device = -1\n\n        check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            model = model.cuda(cuda_device)\n\n        if no_grad:\n            for name, parameter in model.named_parameters():\n                if any(re.search(regex, name) for regex in no_grad):\n                    parameter.requires_grad_(False)\n\n        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        except TypeError:\n            batches_per_epoch = None\n\n        moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\nDEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_len": 45826,
        "target_code": "\n    def get_best_weights_path(self) -> Optional[str]:\n        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_len": 151,
        "diff_format": "@@ -115,1021 +89,4 @@\n \n-\n-@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\n-class GradientDescentTrainer(Trainer):\n-    \"\"\"\n-    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n-    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n-    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n-    stopping. There are many other bells and whistles as well.\n-\n-    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n-    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n-    see the arguments to that function for the exact keys that should be used, if you are using\n-    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n-    docstrings in `from_partial_objects`.\n-\n-    [0]: https://tinyurl.com/y5mv44fw\n-\n-    # Parameters\n-\n-    model : `Model`, required.\n-        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n-        their `forward` method returns a dictionary with a \"loss\" key, containing a\n-        scalar tensor representing the loss function to be optimized.\n-\n-        If you are training your model using GPUs, your model should already be\n-        on the correct device. (If you are using our `train` command this will be\n-        handled for you.)\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    optimizer : `torch.nn.Optimizer`, required.\n-        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n-        model to be optimized.\n-\n-    data_loader : `DataLoader`, required.\n-        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    patience : `Optional[int] > 0`, optional (default=`None`)\n-        Number of epochs to be patient before early stopping: the training is stopped\n-        after `patience` epochs with no improvement. If given, it must be `> 0`.\n-        If None, early stopping is disabled.\n-\n-    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n-        Validation metric to measure for whether to stop training using patience\n-        and whether to serialize an `is_best` model each epoch. The metric name\n-        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n-        is an increasing or decreasing function. If you specify more than one metric,\n-        the metrics will be summed to make the `is_best` decision.\n-\n-    validation_data_loader : `DataLoader`, optional (default=`None`)\n-        A `DataLoader` to use for the validation set.  If `None`, then\n-        use the training `DataLoader` with the validation data.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_epochs : `int`, optional (default = `20`)\n-        Number of training epochs.\n-\n-    serialization_dir : `str`, optional (default=`None`)\n-        Path to directory for saving and loading model files. Models will not be saved if\n-        this parameter is not passed.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    checkpointer : `Checkpointer`, optional (default=`None`)\n-        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n-        here, we will construct one with default parameters.\n-\n-    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n-        An integer or `torch.device` specifying the CUDA device to use for this process.\n-        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n-\n-        !!! Note\n-            If you *don't* intend to use a GPU, but you have one available, you'll need\n-            to explicitly set `cuda_device=-1`.\n-\n-        !!! Note\n-            If you intend to use a GPU, your model already needs to be on the correct device,\n-            which you can do with `model = model.cuda()`.\n-\n-        !!! Note\n-            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n-\n-    grad_norm : `float`, optional, (default = `None`).\n-        If provided, gradient norms will be rescaled to have a maximum of this value.\n-\n-    grad_clipping : `float`, optional (default = `None`).\n-        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n-        maximum of this value.  If you are getting `NaNs` in your gradients during training\n-        that are not solved by using `grad_norm`, you may need this.\n-\n-    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n-        If specified, the learning rate will be decayed with respect to\n-        this schedule at the end of each epoch (or batch, if the scheduler implements\n-        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n-        this will use the `validation_metric` provided to determine if learning has plateaued.\n-        To support updating the learning rate on every batch, this can optionally implement\n-        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n-\n-    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n-        If specified, the momentum will be updated at the end of each batch or epoch\n-        according to the schedule.\n-\n-    moving_average : `MovingAverage`, optional, (default = `None`)\n-        If provided, we will maintain moving averages for all parameters. During training, we\n-        employ a shadow variable for each parameter, which maintains the moving average. During\n-        evaluation, we backup the original parameters and assign the moving averages to corresponding\n-        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n-        parameters. This is necessary because we want the saved model to perform as well as the validated\n-        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n-\n-    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n-        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n-        and end of training, etc.\n-\n-    distributed : `bool`, optional, (default = `False`)\n-        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n-        requires `world_size` to be greater than 1.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n-        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n-\n-    local_rank : `int`, optional, (default = `0`)\n-        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n-        used as the rank.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    world_size : `int`, (default = `1`)\n-        The number of `Trainer` workers participating in the distributed training.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n-        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n-        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n-        post][0] for details on Gradient Accumulation.\n-\n-    use_amp : `bool`, optional, (default = `False`)\n-        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n-\n-    enable_default_callbacks : `bool`, optional (default = `True`)\n-        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n-        addition to any other callbacks listed in the `callbacks` parameter.\n-        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n-\n-    run_confidence_checks : `bool`, optional (default = `True`)\n-        Determines whether model confidence checks, such as\n-        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n-        are run.\n-\n-    run_sanity_checks : `bool`, optional (default = `True`)\n-        This parameter is deprecated. Please use `run_confidence_checks` instead.\n-\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        model: Model,\n-        optimizer: torch.optim.Optimizer,\n-        data_loader: DataLoader,\n-        patience: Optional[int] = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        validation_data_loader: DataLoader = None,\n-        num_epochs: int = 20,\n-        serialization_dir: Optional[str] = None,\n-        checkpointer: Checkpointer = None,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: Optional[float] = None,\n-        grad_clipping: Optional[float] = None,\n-        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n-        momentum_scheduler: Optional[MomentumScheduler] = None,\n-        moving_average: Optional[MovingAverage] = None,\n-        callbacks: List[TrainerCallback] = None,\n-        distributed: bool = False,\n-        local_rank: int = 0,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-        )\n-\n-        if \"run_sanity_checks\" in kwargs:\n-            warnings.warn(\n-                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n-                DeprecationWarning,\n-            )\n-            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n-\n-        # I am not calling move_to_gpu here, because if the model is\n-        # not already on the GPU then the optimizer is going to be wrong.\n-        self.model = model\n-\n-        self.data_loader = data_loader\n-        self.data_loader.set_target_device(self.cuda_device)\n-        self._validation_data_loader = validation_data_loader\n-        if self._validation_data_loader is not None:\n-            self._validation_data_loader.set_target_device(self.cuda_device)\n-        self.optimizer = optimizer\n-\n-        if patience is None:  # no early stopping\n-            if validation_data_loader is not None:\n-                logger.warning(\n-                    \"You provided a validation dataset but patience was set to None, \"\n-                    \"meaning that early stopping is disabled\"\n-                )\n-        elif (not isinstance(patience, int)) or patience <= 0:\n-            raise ConfigurationError(\n-                '{} is an invalid value for \"patience\": it must be a positive integer '\n-                \"or None (if you want to disable early stopping)\".format(patience)\n-            )\n-\n-        # For tracking is_best_so_far and should_stop_early\n-        self._metric_tracker = MetricTracker(validation_metric, patience)\n-\n-        self._num_epochs = num_epochs\n-\n-        self._checkpointer: Optional[Checkpointer] = checkpointer\n-        if checkpointer is None and serialization_dir is not None:\n-            self._checkpointer = Checkpointer(serialization_dir)\n-\n-        self._grad_norm = grad_norm\n-        self._grad_clipping = grad_clipping\n-\n-        self._learning_rate_scheduler = learning_rate_scheduler\n-        self._momentum_scheduler = momentum_scheduler\n-        self._moving_average = moving_average\n-\n-        self._callbacks = callbacks or []\n-        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n-\n-        if run_confidence_checks:\n-            default_callbacks.append(ConfidenceChecksCallback)\n-        for callback_cls in default_callbacks:\n-            for callback in self._callbacks:\n-                if callback.__class__ == callback_cls:\n-                    break\n-            else:\n-                self._callbacks.append(callback_cls(self._serialization_dir))\n-\n-        self._batch_num_total = 0\n-        self._last_log = 0.0  # time of last logging\n-        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n-\n-        # Enable automatic mixed precision training.\n-        self._scaler: Optional[amp.GradScaler] = None\n-        self._use_amp = use_amp\n-        if self._use_amp:\n-            if self.cuda_device == torch.device(\"cpu\"):\n-                raise ValueError(\"Using AMP requires a cuda device\")\n-            self._scaler = amp.GradScaler()\n-\n-        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n-        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n-        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n-        #\n-        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n-        # normal case, reference to `Model` is retained. This reference is only used in\n-        # these places: `model.__call__`, `model.train` and `model.eval`.\n-        if self._distributed:\n-            self._pytorch_model = DistributedDataParallel(\n-                self.model,\n-                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n-                find_unused_parameters=True,\n-            )\n-        else:\n-            self._pytorch_model = self.model\n-\n-    def rescale_gradients(self) -> float:\n-        \"\"\"\n-        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n-\n-        Returns the norm of the gradients.\n-        \"\"\"\n-        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n-        if self._grad_norm:\n-            if self._scaler is not None:\n-                # Need to first unscale gradients in order to clip as usual.\n-                self._scaler.unscale_(self.optimizer)\n-            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n-        else:\n-            return torch.norm(\n-                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n-            )\n-\n-    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n-        \"\"\"\n-        Does a forward pass on the given batch and returns the output dictionary that the model\n-        returns, after adding any specified regularization penalty to the loss (if training).\n-        \"\"\"\n-        output_dict = self._pytorch_model(**batch)\n-\n-        if for_training:\n-            try:\n-                assert \"loss\" in output_dict\n-                regularization_penalty = self.model.get_regularization_penalty()\n-\n-                if regularization_penalty is not None:\n-                    output_dict[\"reg_loss\"] = regularization_penalty\n-                    output_dict[\"loss\"] += regularization_penalty\n-\n-            except AssertionError:\n-                if for_training:\n-                    raise RuntimeError(\n-                        \"The model you are trying to optimize does not contain a\"\n-                        \" 'loss' key in the output of model.forward(inputs).\"\n-                    )\n-\n-        return output_dict\n-\n-    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n-        \"\"\"\n-        Trains one epoch and returns metrics.\n-        \"\"\"\n-        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n-        cpu_memory_usage = []\n-        for worker, memory in common_util.peak_cpu_memory().items():\n-            cpu_memory_usage.append((worker, memory))\n-            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n-        gpu_memory_usage = []\n-        for gpu, memory in common_util.peak_gpu_memory().items():\n-            gpu_memory_usage.append((gpu, memory))\n-            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        train_loss = 0.0\n-        batch_loss = 0.0\n-        train_reg_loss = None if regularization_penalty is None else 0.0\n-        batch_reg_loss = None if regularization_penalty is None else 0.0\n-\n-        # Set the model to \"train\" mode.\n-        self._pytorch_model.train()\n-\n-        # Get tqdm for the training batches\n-        batch_generator = iter(self.data_loader)\n-        batch_group_generator = common_util.lazy_groups_of(\n-            batch_generator, self._num_gradient_accumulation_steps\n-        )\n-\n-        logger.info(\"Training\")\n-\n-        num_training_batches: Union[int, float]\n-        try:\n-            len_data_loader = len(self.data_loader)\n-            num_training_batches = math.ceil(\n-                len_data_loader / self._num_gradient_accumulation_steps\n-            )\n-        except TypeError:\n-            num_training_batches = float(\"inf\")\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            batch_group_generator_tqdm = Tqdm.tqdm(\n-                batch_group_generator, total=num_training_batches\n-            )\n-        else:\n-            batch_group_generator_tqdm = batch_group_generator\n-\n-        self._last_log = time.time()\n-\n-        batches_this_epoch = 0\n-        if self._batch_num_total is None:\n-            self._batch_num_total = 0\n-\n-        done_early = False\n-        for batch_group in batch_group_generator_tqdm:\n-            if done_early:\n-                break\n-\n-            batches_this_epoch += 1\n-            self._batch_num_total += 1\n-            batch_num_total = self._batch_num_total\n-\n-            # Zero gradients.\n-            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n-            # because it avoids a read op when the gradients are first updated below.\n-            for param_group in self.optimizer.param_groups:\n-                for p in param_group[\"params\"]:\n-                    p.grad = None\n-\n-            batch_loss = 0.0\n-            batch_group_outputs = []\n-            for batch in batch_group:\n-                if self._distributed:\n-                    # Check whether the other workers have stopped already (due to differing amounts of\n-                    # data in each). If so, we can't proceed because we would hang when we hit the\n-                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                    # here because NCCL process groups apparently don't support BoolTensor.\n-                    done = torch.tensor(0, device=self.cuda_device)\n-                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                    if done.item() > 0:\n-                        done_early = True\n-                        logger.warning(\n-                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n-                            \"This implies that there is an imbalance in your training \"\n-                            \"data across the workers and that some amount of it will be \"\n-                            \"ignored. A small amount of this is fine, but a major imbalance \"\n-                            \"should be avoided. Note: This warning will appear unless your \"\n-                            \"data is perfectly balanced.\"\n-                        )\n-                        break\n-\n-                with amp.autocast(self._use_amp):\n-                    batch_outputs = self.batch_outputs(batch, for_training=True)\n-                    batch_group_outputs.append(batch_outputs)\n-                    loss = batch_outputs[\"loss\"]\n-                    reg_loss = batch_outputs.get(\"reg_loss\")\n-                    if torch.isnan(loss):\n-                        raise ValueError(\"nan loss encountered\")\n-                    loss = loss / len(batch_group)\n-\n-                    batch_loss += loss.item()\n-                    if reg_loss is not None:\n-                        reg_loss = reg_loss / len(batch_group)\n-                        batch_reg_loss = reg_loss.item()\n-                        train_reg_loss += batch_reg_loss  # type: ignore\n-\n-                if self._scaler is not None:\n-                    self._scaler.scale(loss).backward()\n-                else:\n-                    loss.backward()\n-            if len(batch_group_outputs) <= 0:\n-                continue\n-\n-            train_loss += batch_loss\n-\n-            batch_grad_norm = self.rescale_gradients()\n-\n-            # This does nothing if batch_num_total is None or you are using a\n-            # scheduler which doesn't update per batch.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step_batch(batch_num_total)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step_batch(batch_num_total)\n-\n-            if self._scaler is not None:\n-                self._scaler.step(self.optimizer)\n-                self._scaler.update()\n-            else:\n-                self.optimizer.step()\n-\n-            # Update moving averages\n-            if self._moving_average is not None:\n-                self._moving_average.apply(batch_num_total)\n-\n-            # Update the description with the latest metrics\n-            metrics = training_util.get_metrics(\n-                self.model,\n-                train_loss,\n-                train_reg_loss,\n-                batch_loss,\n-                batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            if self._primary:\n-                # Updating tqdm only for the primary as the trainers wouldn't have one\n-                description = training_util.description_from_metrics(metrics)\n-                batch_group_generator_tqdm.set_description(description, refresh=False)\n-\n-                if self._checkpointer is not None:\n-                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    batch_group,\n-                    batch_group_outputs,\n-                    metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=True,\n-                    is_primary=self._primary,\n-                    batch_grad_norm=batch_grad_norm,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Let all workers finish their epoch before computing\n-        # the final statistics for the epoch.\n-        if self._distributed:\n-            dist.barrier()\n-\n-        metrics = training_util.get_metrics(\n-            self.model,\n-            train_loss,\n-            train_reg_loss,\n-            batch_loss=None,\n-            batch_reg_loss=None,\n-            num_batches=batches_this_epoch,\n-            reset=True,\n-            world_size=self._world_size,\n-            cuda_device=self.cuda_device,\n-        )\n-\n-        for (worker, memory) in cpu_memory_usage:\n-            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        for (gpu_num, memory) in gpu_memory_usage:\n-            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        return metrics\n-\n-    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n-        \"\"\"\n-        Computes the validation loss. Returns it and the number of batches.\n-        \"\"\"\n-        logger.info(\"Validating\")\n-\n-        self._pytorch_model.eval()\n-\n-        # Replace parameter values with the shadow values from the moving averages.\n-        if self._moving_average is not None:\n-            self._moving_average.assign_average_value()\n-\n-        if self._validation_data_loader is not None:\n-            validation_data_loader = self._validation_data_loader\n-        else:\n-            raise ConfigurationError(\n-                \"Validation results cannot be calculated without a validation_data_loader\"\n-            )\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n-        else:\n-            val_generator_tqdm = validation_data_loader\n-\n-        batches_this_epoch = 0\n-        val_loss = 0.0\n-        val_batch_loss = 0.0\n-        val_reg_loss = None if regularization_penalty is None else 0.0\n-        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n-        done_early = False\n-        for batch in val_generator_tqdm:\n-            if self._distributed:\n-                # Check whether the other workers have stopped already (due to differing amounts of\n-                # data in each). If so, we can't proceed because we would hang when we hit the\n-                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                # here because NCCL process groups apparently don't support BoolTensor.\n-                done = torch.tensor(0, device=self.cuda_device)\n-                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                if done.item() > 0:\n-                    done_early = True\n-                    logger.warning(\n-                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n-                        \"This implies that there is an imbalance in your validation \"\n-                        \"data across the workers and that some amount of it will be \"\n-                        \"ignored. A small amount of this is fine, but a major imbalance \"\n-                        \"should be avoided. Note: This warning will appear unless your \"\n-                        \"data is perfectly balanced.\"\n-                    )\n-                    break\n-\n-            with amp.autocast(self._use_amp):\n-                batch_outputs = self.batch_outputs(batch, for_training=False)\n-                loss = batch_outputs.get(\"loss\")\n-                reg_loss = batch_outputs.get(\"reg_loss\")\n-                if loss is not None:\n-                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n-                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n-                    # currently only used as the divisor for the loss function, so we can safely only\n-                    # count those batches for which we actually have a loss.  If this variable ever\n-                    # gets used for something else, we might need to change things around a bit.\n-                    batches_this_epoch += 1\n-                    val_batch_loss = loss.item()\n-                    val_loss += val_batch_loss\n-                    if reg_loss is not None:\n-                        val_batch_reg_loss = reg_loss.item()\n-                        val_reg_loss += val_batch_reg_loss  # type: ignore\n-\n-            # Update the description with the latest metrics\n-            val_metrics = training_util.get_metrics(\n-                self.model,\n-                val_loss,\n-                val_reg_loss,\n-                val_batch_loss,\n-                val_batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            description = training_util.description_from_metrics(val_metrics)\n-            if self._primary:\n-                val_generator_tqdm.set_description(description, refresh=False)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    [batch],\n-                    [batch_outputs],\n-                    val_metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=False,\n-                    is_primary=self._primary,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop validation early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Now restore the original parameter values.\n-        if self._moving_average is not None:\n-            self._moving_average.restore()\n-\n-        return val_loss, val_reg_loss, batches_this_epoch\n-\n-    def train(self) -> Dict[str, Any]:\n-        \"\"\"\n-        Trains the supplied model with the supplied parameters.\n-        \"\"\"\n-\n-        for callback in self._callbacks:\n-            callback.on_start(self, is_primary=self._primary)\n-\n-        # Set default values in case of failure\n-        epoch = None\n-        metrics = None\n-\n-        try:\n-            metrics, epoch = self._try_train()\n-            return metrics\n-        finally:\n-            for callback in self._callbacks:\n-                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n-        try:\n-            epoch_counter = self._restore_checkpoint()\n-        except RuntimeError:\n-            traceback.print_exc()\n-            raise ConfigurationError(\n-                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n-                \"a different serialization directory or delete the existing serialization \"\n-                \"directory?\"\n-            )\n-\n-        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n-\n-        logger.info(\"Beginning training.\")\n-\n-        val_metrics: Dict[str, float] = {}\n-        metrics: Dict[str, Any] = {}\n-        epochs_trained = 0\n-        training_start_time = time.time()\n-\n-        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n-        for key, value in self._metric_tracker.best_epoch_metrics.items():\n-            metrics[\"best_validation_\" + key] = value\n-\n-        for epoch in range(epoch_counter, self._num_epochs):\n-            epoch_start_time = time.time()\n-            train_metrics = self._train_epoch(epoch)\n-\n-            # Back up the model now, in case something goes wrong later with the evaluation\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.shelve_model(epoch, self)\n-            # Wait for the primary process to finish saving the model checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            # get peak of memory usage\n-            for key, value in train_metrics.items():\n-                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-\n-            this_epoch_val_metric: float = 0.0\n-            if self._validation_data_loader is not None:\n-                with torch.no_grad():\n-                    # We have a validation set, so compute all the metrics on it.\n-                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n-\n-                    # It is safe again to wait till the validation is done. This is\n-                    # important to get the metrics right.\n-                    if self._distributed:\n-                        dist.barrier()\n-\n-                    val_metrics = training_util.get_metrics(\n-                        self.model,\n-                        val_loss,\n-                        val_reg_loss,\n-                        batch_loss=None,\n-                        batch_reg_loss=None,\n-                        num_batches=num_batches,\n-                        reset=True,\n-                        world_size=self._world_size,\n-                        cuda_device=self.cuda_device,\n-                    )\n-\n-                    # Check validation metric for early stopping\n-                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n-                    self._metric_tracker.add_metrics(val_metrics)\n-\n-            # Create overall metrics dict\n-            training_elapsed_time = time.time() - training_start_time\n-            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n-            metrics[\"training_start_epoch\"] = epoch_counter\n-            metrics[\"training_epochs\"] = epochs_trained\n-            metrics[\"epoch\"] = epoch\n-\n-            for key, value in train_metrics.items():\n-                metrics[\"training_\" + key] = value\n-            for key, value in val_metrics.items():\n-                metrics[\"validation_\" + key] = value\n-\n-            if self._metric_tracker.is_best_so_far():\n-                # Update all the best_ metrics.\n-                # (Otherwise they just stay the same as they were.)\n-                metrics[\"best_epoch\"] = epoch\n-                for key, value in val_metrics.items():\n-                    metrics[\"best_validation_\" + key] = value\n-\n-                self._metric_tracker.best_epoch_metrics = val_metrics\n-\n-            if self._serialization_dir and self._primary:\n-                common_util.dump_metrics(\n-                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n-                    metrics,\n-                )\n-\n-            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n-            # if it doesn't, the validation metric passed here is ignored.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step(this_epoch_val_metric)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step(this_epoch_val_metric)\n-\n-            # The checkpointer saves state from the learning rate scheduler and the momentum\n-            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.save_checkpoint(\n-                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n-                )\n-            # Wait for the primary process to finish saving the checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            for callback in self._callbacks:\n-                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-            epoch_elapsed_time = time.time() - epoch_start_time\n-            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n-\n-            if epoch < self._num_epochs - 1:\n-                training_elapsed_time = time.time() - training_start_time\n-                estimated_time_remaining = training_elapsed_time * (\n-                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n-                )\n-                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n-                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n-\n-            epochs_trained += 1\n-\n-            if self._metric_tracker.should_stop_early():\n-                logger.info(\"Ran out of patience. Stopping training.\")\n-                break\n-        else:\n-            epoch = self._num_epochs - 1\n-\n-        # Load the best model state before returning\n-        best_model_state = (\n-            None if self._checkpointer is None else self._checkpointer.best_model_state()\n-        )\n-        if best_model_state:\n-            self.model.load_state_dict(best_model_state)\n-\n-        return metrics, epoch\n-\n-    @contextmanager\n-    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n-        if self._moving_average is not None:\n-            # Assigning average value to model parameters.  The checkpointer will call\n-            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n-            self._moving_average.assign_average_value()\n-\n-        model_state = self.model.state_dict()\n-\n-        # These are the training states we need to persist.\n-        training_states = {\n-            \"metric_tracker\": self._metric_tracker.state_dict(),\n-            \"optimizer\": self.optimizer.state_dict(),\n-            \"batch_num_total\": self._batch_num_total,\n-        }\n-\n-        # If we have a learning rate or momentum scheduler, we should persist them too.\n-        if self._learning_rate_scheduler is not None:\n-            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n-        if self._momentum_scheduler is not None:\n-            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n-\n-        try:\n-            yield model_state, training_states\n-        finally:\n-            if self._moving_average is not None:\n-                self._moving_average.restore()\n-\n-    def _restore_checkpoint(self) -> int:\n-        \"\"\"\n-        Restores the model and training state from the last saved checkpoint.\n-        This includes an epoch count and optimizer state, which is serialized separately\n-        from model parameters. This function should only be used to continue training -\n-        if you wish to load a model for inference/load parts of a model into a new\n-        computation graph, you should use the native Pytorch functions:\n-        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n-\n-        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n-        this function will do nothing and return 0.\n-\n-        # Returns\n-\n-        epoch: `int`\n-            The epoch at which to resume training, which should be one after the epoch\n-            in the saved training state.\n-        \"\"\"\n-        if self._checkpointer is None:\n-            return 0\n-\n-        model_state, training_state = self._checkpointer.restore_checkpoint()\n-\n-        if not training_state:\n-            # No checkpoint to restore, start at 0\n-            return 0\n-\n-        self.model.load_state_dict(model_state)\n-        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n-        if (\n-            self._learning_rate_scheduler is not None\n-            and \"learning_rate_scheduler\" in training_state\n-        ):\n-            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n-        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n-            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n-        training_util.move_optimizer_to_cuda(self.optimizer)\n-\n-        # Currently the `training_state` contains a serialized `MetricTracker`.\n-        if \"metric_tracker\" in training_state:\n-            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n-        else:\n-            self._metric_tracker.clear()\n-\n-        if isinstance(training_state[\"epoch\"], int):\n-            epoch_to_return = training_state[\"epoch\"] + 1\n-        else:\n-            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n-\n-        # For older checkpoints with batch_num_total missing, default to old behavior where\n-        # it is unchanged.\n-        batch_num_total = training_state.get(\"batch_num_total\")\n-        if batch_num_total is not None:\n-            self._batch_num_total = batch_num_total\n-\n-        return epoch_to_return\n-\n-    @classmethod\n-    def from_partial_objects(\n-        cls,\n-        model: Model,\n-        serialization_dir: str,\n-        data_loader: DataLoader,\n-        validation_data_loader: DataLoader = None,\n-        local_rank: int = 0,\n-        patience: int = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        num_epochs: int = 20,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: float = None,\n-        grad_clipping: float = None,\n-        distributed: bool = False,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        no_grad: List[str] = None,\n-        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n-        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n-        momentum_scheduler: Lazy[MomentumScheduler] = None,\n-        moving_average: Lazy[MovingAverage] = None,\n-        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n-        callbacks: List[Lazy[TrainerCallback]] = None,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> \"Trainer\":\n-        \"\"\"\n-        This method exists so that we can have a documented method to construct this class using\n-        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n-        method.\n-\n-        The reason we can't just use `__init__` with `FromParams` here is because there are\n-        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n-        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n-        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n-        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n-        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n-\n-        If you're not using `FromParams`, you can just construct these arguments in the right order\n-        yourself in your code and call the constructor directly.\n-        \"\"\"\n-        if cuda_device is None:\n-            from torch import cuda\n-\n-            if cuda.device_count() > 0:\n-                cuda_device = 0\n-            else:\n-                cuda_device = -1\n-\n-        check_for_gpu(cuda_device)\n-        if cuda_device >= 0:\n-            # Moving model to GPU here so that the optimizer state gets constructed on\n-            # the right device.\n-            model = model.cuda(cuda_device)\n-\n-        if no_grad:\n-            for name, parameter in model.named_parameters():\n-                if any(re.search(regex, name) for regex in no_grad):\n-                    parameter.requires_grad_(False)\n-\n-        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n-        optimizer_ = optimizer.construct(model_parameters=parameters)\n-\n-        common_util.log_frozen_and_tunable_parameter_names(model)\n-\n-        batches_per_epoch: Optional[int]\n-        try:\n-            batches_per_epoch = len(data_loader)\n-            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n-        except TypeError:\n-            batches_per_epoch = None\n-\n-        moving_average_ = (\n-            None if moving_average is None else moving_average.construct(parameters=parameters)\n-        )\n-        learning_rate_scheduler_ = (\n-            None\n-            if learning_rate_scheduler is None\n-            else learning_rate_scheduler.construct(\n-                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n-            )\n-        )\n-        momentum_scheduler_ = (\n-            None\n-            if momentum_scheduler is None\n-            else momentum_scheduler.construct(optimizer=optimizer_)\n-        )\n-        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n-\n-        callbacks_: List[TrainerCallback] = []\n-        for callback_ in callbacks or []:\n-            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n-\n-        return cls(\n-            model,\n-            optimizer_,\n-            data_loader,\n-            patience=patience,\n-            validation_metric=validation_metric,\n-            validation_data_loader=validation_data_loader,\n-            num_epochs=num_epochs,\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            grad_norm=grad_norm,\n-            grad_clipping=grad_clipping,\n-            learning_rate_scheduler=learning_rate_scheduler_,\n-            momentum_scheduler=momentum_scheduler_,\n-            checkpointer=checkpointer_,\n-            moving_average=moving_average_,\n-            callbacks=callbacks_,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n-            use_amp=use_amp,\n-            enable_default_callbacks=enable_default_callbacks,\n-            run_confidence_checks=run_confidence_checks,\n-            **kwargs,\n-        )\n-\n-\n-DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n-\"\"\"\n-The default callbacks used by `GradientDescentTrainer`.\n-\"\"\"\n+    def get_best_weights_path(self) -> Optional[str]:\n+        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n+        return None\n",
        "source_code_with_indent": "\n\n<DED><DED>@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    <IND>\"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        <IND>super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            <IND>warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        <DED>self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            <IND>self._validation_data_loader.set_target_device(self.cuda_device)\n        <DED>self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            <IND>if validation_data_loader is not None:\n                <IND>logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        <DED><DED>elif (not isinstance(patience, int)) or patience <= 0:\n            <IND>raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        <DED>self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            <IND>self._checkpointer = Checkpointer(serialization_dir)\n\n        <DED>self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            <IND>default_callbacks.append(ConfidenceChecksCallback)\n        <DED>for callback_cls in default_callbacks:\n            <IND>for callback in self._callbacks:\n                <IND>if callback.__class__ == callback_cls:\n                    <IND>break\n            <DED><DED>else:\n                <IND>self._callbacks.append(callback_cls(self._serialization_dir))\n\n        <DED><DED>self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            <IND>if self.cuda_device == torch.device(\"cpu\"):\n                <IND>raise ValueError(\"Using AMP requires a cuda device\")\n            <DED>self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        <DED>if self._distributed:\n            <IND>self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        <DED>else:\n            <IND>self._pytorch_model = self.model\n\n    <DED><DED>def rescale_gradients(self) -> float:\n        <IND>\"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            <IND>if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                <IND>self._scaler.unscale_(self.optimizer)\n            <DED>return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        <DED>else:\n            <IND>return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    <DED><DED>def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        <IND>\"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            <IND>try:\n                <IND>assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    <IND>output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            <DED><DED>except AssertionError:\n                <IND>if for_training:\n                    <IND>raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        <DED><DED><DED>return output_dict\n\n    <DED>def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        <IND>\"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            <IND>cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        <DED>gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            <IND>gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            <IND>len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        <DED>except TypeError:\n            <IND>num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        <DED>if self._primary:\n            <IND>batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        <DED>else:\n            <IND>batch_group_generator_tqdm = batch_group_generator\n\n        <DED>self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            <IND>self._batch_num_total = 0\n\n        <DED>done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            <IND>if done_early:\n                <IND>break\n\n            <DED>batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                <IND>for p in param_group[\"params\"]:\n                    <IND>p.grad = None\n\n            <DED><DED>batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                <IND>if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    <IND>done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        <IND>done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                <DED><DED>with amp.autocast(self._use_amp):\n                    <IND>batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        <IND>raise ValueError(\"nan loss encountered\")\n                    <DED>loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        <IND>reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                <DED><DED>if self._scaler is not None:\n                    <IND>self._scaler.scale(loss).backward()\n                <DED>else:\n                    <IND>loss.backward()\n            <DED><DED>if len(batch_group_outputs) <= 0:\n                <IND>continue\n\n            <DED>train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step_batch(batch_num_total)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step_batch(batch_num_total)\n\n            <DED>if self._scaler is not None:\n                <IND>self._scaler.step(self.optimizer)\n                self._scaler.update()\n            <DED>else:\n                <IND>self.optimizer.step()\n\n            # Update moving averages\n            <DED>if self._moving_average is not None:\n                <IND>self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            <DED>metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                <IND>description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    <IND>self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            <DED><DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        <DED>if self._distributed:\n            <IND>dist.barrier()\n\n        <DED>metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            <IND>metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>for (gpu_num, memory) in gpu_memory_usage:\n            <IND>metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>return metrics\n\n    <DED>def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        <IND>\"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>if self._validation_data_loader is not None:\n            <IND>validation_data_loader = self._validation_data_loader\n        <DED>else:\n            <IND>raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            <IND>val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        <DED>else:\n            <IND>val_generator_tqdm = validation_data_loader\n\n        <DED>batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            <IND>if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                <IND>done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    <IND>done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            <DED><DED>with amp.autocast(self._use_amp):\n                <IND>batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    <IND>batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        <IND>val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            <DED><DED><DED>val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                <IND>val_generator_tqdm.set_description(description, refresh=False)\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        <DED>if self._moving_average is not None:\n            <IND>self._moving_average.restore()\n\n        <DED>return val_loss, val_reg_loss, batches_this_epoch\n\n    <DED>def train(self) -> Dict[str, Any]:\n        <IND>\"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            <IND>callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        <DED>epoch = None\n        metrics = None\n\n        try:\n            <IND>metrics, epoch = self._try_train()\n            return metrics\n        <DED>finally:\n            <IND>for callback in self._callbacks:\n                <IND>callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    <DED><DED><DED>def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        <IND>try:\n            <IND>epoch_counter = self._restore_checkpoint()\n        <DED>except RuntimeError:\n            <IND>traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        <DED>training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            <IND>metrics[\"best_validation_\" + key] = value\n\n        <DED>for epoch in range(epoch_counter, self._num_epochs):\n            <IND>epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            # get peak of memory usage\n            <DED>for key, value in train_metrics.items():\n                <IND>if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                <DED>elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            <DED><DED>this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                <IND>with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    <IND>val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        <IND>dist.barrier()\n\n                    <DED>val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            <DED><DED>training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                <IND>metrics[\"training_\" + key] = value\n            <DED>for key, value in val_metrics.items():\n                <IND>metrics[\"validation_\" + key] = value\n\n            <DED>if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                <IND>metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    <IND>metrics[\"best_validation_\" + key] = value\n\n                <DED>self._metric_tracker.best_epoch_metrics = val_metrics\n\n            <DED>if self._serialization_dir and self._primary:\n                <IND>common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            <DED>if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step(this_epoch_val_metric)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            <DED>if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            <DED>epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                <IND>training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            <DED>epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                <IND>logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        <DED><DED>else:\n            <IND>epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        <DED>best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            <IND>self.model.load_state_dict(best_model_state)\n\n        <DED>return metrics, epoch\n\n    <DED>@contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        <IND>if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            <IND>training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        <DED>if self._momentum_scheduler is not None:\n            <IND>training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        <DED>try:\n            <IND>yield model_state, training_states\n        <DED>finally:\n            <IND>if self._moving_average is not None:\n                <IND>self._moving_average.restore()\n\n    <DED><DED><DED>def _restore_checkpoint(self) -> int:\n        <IND>\"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            <IND>return 0\n\n        <DED>model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            <IND>return 0\n\n        <DED>self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            <IND>self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        <DED>if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            <IND>self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        <DED>training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            <IND>self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        <DED>else:\n            <IND>self._metric_tracker.clear()\n\n        <DED>if isinstance(training_state[\"epoch\"], int):\n            <IND>epoch_to_return = training_state[\"epoch\"] + 1\n        <DED>else:\n            <IND>epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        <DED>batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            <IND>self._batch_num_total = batch_num_total\n\n        <DED>return epoch_to_return\n\n    <DED>@classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        <IND>\"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            <IND>from torch import cuda\n\n            if cuda.device_count() > 0:\n                <IND>cuda_device = 0\n            <DED>else:\n                <IND>cuda_device = -1\n\n        <DED><DED>check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            <IND>model = model.cuda(cuda_device)\n\n        <DED>if no_grad:\n            <IND>for name, parameter in model.named_parameters():\n                <IND>if any(re.search(regex, name) for regex in no_grad):\n                    <IND>parameter.requires_grad_(False)\n\n        <DED><DED><DED>parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            <IND>batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        <DED>except TypeError:\n            <IND>batches_per_epoch = None\n\n        <DED>moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            <IND>callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        <DED>return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\n<DED><DED>DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def get_best_weights_path(self) -> Optional[str]:\n        <IND>\"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "c5bff8ba0d835eb03931f10f4f427ffe936cf796",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:1028:8 Incompatible variable type [9]: learning_rate_scheduler is declared to have type `allennlp.common.lazy.Lazy[allennlp.training.learning_rate_schedulers.learning_rate_scheduler.LearningRateScheduler]` but is used as type `None`.",
    "message": " learning_rate_scheduler is declared to have type `allennlp.common.lazy.Lazy[allennlp.training.learning_rate_schedulers.learning_rate_scheduler.LearningRateScheduler]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 1028,
    "warning_line": "        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n\n@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    \"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            self._validation_data_loader.set_target_device(self.cuda_device)\n        self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            if validation_data_loader is not None:\n                logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        elif (not isinstance(patience, int)) or patience <= 0:\n            raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            self._checkpointer = Checkpointer(serialization_dir)\n\n        self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            default_callbacks.append(ConfidenceChecksCallback)\n        for callback_cls in default_callbacks:\n            for callback in self._callbacks:\n                if callback.__class__ == callback_cls:\n                    break\n            else:\n                self._callbacks.append(callback_cls(self._serialization_dir))\n\n        self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            if self.cuda_device == torch.device(\"cpu\"):\n                raise ValueError(\"Using AMP requires a cuda device\")\n            self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        if self._distributed:\n            self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        else:\n            self._pytorch_model = self.model\n\n    def rescale_gradients(self) -> float:\n        \"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                self._scaler.unscale_(self.optimizer)\n            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        else:\n            return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            try:\n                assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            except AssertionError:\n                if for_training:\n                    raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        return output_dict\n\n    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        \"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        except TypeError:\n            num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        else:\n            batch_group_generator_tqdm = batch_group_generator\n\n        self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            self._batch_num_total = 0\n\n        done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            if done_early:\n                break\n\n            batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                for p in param_group[\"params\"]:\n                    p.grad = None\n\n            batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                with amp.autocast(self._use_amp):\n                    batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        raise ValueError(\"nan loss encountered\")\n                    loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                if self._scaler is not None:\n                    self._scaler.scale(loss).backward()\n                else:\n                    loss.backward()\n            if len(batch_group_outputs) <= 0:\n                continue\n\n            train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step_batch(batch_num_total)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step_batch(batch_num_total)\n\n            if self._scaler is not None:\n                self._scaler.step(self.optimizer)\n                self._scaler.update()\n            else:\n                self.optimizer.step()\n\n            # Update moving averages\n            if self._moving_average is not None:\n                self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        if self._distributed:\n            dist.barrier()\n\n        metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        for (gpu_num, memory) in gpu_memory_usage:\n            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        return metrics\n\n    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        \"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            self._moving_average.assign_average_value()\n\n        if self._validation_data_loader is not None:\n            validation_data_loader = self._validation_data_loader\n        else:\n            raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        else:\n            val_generator_tqdm = validation_data_loader\n\n        batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            with amp.autocast(self._use_amp):\n                batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                val_generator_tqdm.set_description(description, refresh=False)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        if self._moving_average is not None:\n            self._moving_average.restore()\n\n        return val_loss, val_reg_loss, batches_this_epoch\n\n    def train(self) -> Dict[str, Any]:\n        \"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        epoch = None\n        metrics = None\n\n        try:\n            metrics, epoch = self._try_train()\n            return metrics\n        finally:\n            for callback in self._callbacks:\n                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        try:\n            epoch_counter = self._restore_checkpoint()\n        except RuntimeError:\n            traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            metrics[\"best_validation_\" + key] = value\n\n        for epoch in range(epoch_counter, self._num_epochs):\n            epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            # get peak of memory usage\n            for key, value in train_metrics.items():\n                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        dist.barrier()\n\n                    val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                metrics[\"training_\" + key] = value\n            for key, value in val_metrics.items():\n                metrics[\"validation_\" + key] = value\n\n            if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    metrics[\"best_validation_\" + key] = value\n\n                self._metric_tracker.best_epoch_metrics = val_metrics\n\n            if self._serialization_dir and self._primary:\n                common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step(this_epoch_val_metric)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            for callback in self._callbacks:\n                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        else:\n            epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            self.model.load_state_dict(best_model_state)\n\n        return metrics, epoch\n\n    @contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            self._moving_average.assign_average_value()\n\n        model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        if self._momentum_scheduler is not None:\n            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        try:\n            yield model_state, training_states\n        finally:\n            if self._moving_average is not None:\n                self._moving_average.restore()\n\n    def _restore_checkpoint(self) -> int:\n        \"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            return 0\n\n        model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            return 0\n\n        self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        else:\n            self._metric_tracker.clear()\n\n        if isinstance(training_state[\"epoch\"], int):\n            epoch_to_return = training_state[\"epoch\"] + 1\n        else:\n            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            self._batch_num_total = batch_num_total\n\n        return epoch_to_return\n\n    @classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        \"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            from torch import cuda\n\n            if cuda.device_count() > 0:\n                cuda_device = 0\n            else:\n                cuda_device = -1\n\n        check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            model = model.cuda(cuda_device)\n\n        if no_grad:\n            for name, parameter in model.named_parameters():\n                if any(re.search(regex, name) for regex in no_grad):\n                    parameter.requires_grad_(False)\n\n        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        except TypeError:\n            batches_per_epoch = None\n\n        moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\nDEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_len": 45826,
        "target_code": "\n    def get_best_weights_path(self) -> Optional[str]:\n        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_len": 151,
        "diff_format": "@@ -115,1021 +89,4 @@\n \n-\n-@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\n-class GradientDescentTrainer(Trainer):\n-    \"\"\"\n-    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n-    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n-    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n-    stopping. There are many other bells and whistles as well.\n-\n-    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n-    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n-    see the arguments to that function for the exact keys that should be used, if you are using\n-    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n-    docstrings in `from_partial_objects`.\n-\n-    [0]: https://tinyurl.com/y5mv44fw\n-\n-    # Parameters\n-\n-    model : `Model`, required.\n-        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n-        their `forward` method returns a dictionary with a \"loss\" key, containing a\n-        scalar tensor representing the loss function to be optimized.\n-\n-        If you are training your model using GPUs, your model should already be\n-        on the correct device. (If you are using our `train` command this will be\n-        handled for you.)\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    optimizer : `torch.nn.Optimizer`, required.\n-        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n-        model to be optimized.\n-\n-    data_loader : `DataLoader`, required.\n-        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    patience : `Optional[int] > 0`, optional (default=`None`)\n-        Number of epochs to be patient before early stopping: the training is stopped\n-        after `patience` epochs with no improvement. If given, it must be `> 0`.\n-        If None, early stopping is disabled.\n-\n-    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n-        Validation metric to measure for whether to stop training using patience\n-        and whether to serialize an `is_best` model each epoch. The metric name\n-        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n-        is an increasing or decreasing function. If you specify more than one metric,\n-        the metrics will be summed to make the `is_best` decision.\n-\n-    validation_data_loader : `DataLoader`, optional (default=`None`)\n-        A `DataLoader` to use for the validation set.  If `None`, then\n-        use the training `DataLoader` with the validation data.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_epochs : `int`, optional (default = `20`)\n-        Number of training epochs.\n-\n-    serialization_dir : `str`, optional (default=`None`)\n-        Path to directory for saving and loading model files. Models will not be saved if\n-        this parameter is not passed.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    checkpointer : `Checkpointer`, optional (default=`None`)\n-        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n-        here, we will construct one with default parameters.\n-\n-    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n-        An integer or `torch.device` specifying the CUDA device to use for this process.\n-        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n-\n-        !!! Note\n-            If you *don't* intend to use a GPU, but you have one available, you'll need\n-            to explicitly set `cuda_device=-1`.\n-\n-        !!! Note\n-            If you intend to use a GPU, your model already needs to be on the correct device,\n-            which you can do with `model = model.cuda()`.\n-\n-        !!! Note\n-            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n-\n-    grad_norm : `float`, optional, (default = `None`).\n-        If provided, gradient norms will be rescaled to have a maximum of this value.\n-\n-    grad_clipping : `float`, optional (default = `None`).\n-        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n-        maximum of this value.  If you are getting `NaNs` in your gradients during training\n-        that are not solved by using `grad_norm`, you may need this.\n-\n-    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n-        If specified, the learning rate will be decayed with respect to\n-        this schedule at the end of each epoch (or batch, if the scheduler implements\n-        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n-        this will use the `validation_metric` provided to determine if learning has plateaued.\n-        To support updating the learning rate on every batch, this can optionally implement\n-        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n-\n-    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n-        If specified, the momentum will be updated at the end of each batch or epoch\n-        according to the schedule.\n-\n-    moving_average : `MovingAverage`, optional, (default = `None`)\n-        If provided, we will maintain moving averages for all parameters. During training, we\n-        employ a shadow variable for each parameter, which maintains the moving average. During\n-        evaluation, we backup the original parameters and assign the moving averages to corresponding\n-        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n-        parameters. This is necessary because we want the saved model to perform as well as the validated\n-        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n-\n-    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n-        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n-        and end of training, etc.\n-\n-    distributed : `bool`, optional, (default = `False`)\n-        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n-        requires `world_size` to be greater than 1.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n-        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n-\n-    local_rank : `int`, optional, (default = `0`)\n-        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n-        used as the rank.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    world_size : `int`, (default = `1`)\n-        The number of `Trainer` workers participating in the distributed training.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n-        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n-        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n-        post][0] for details on Gradient Accumulation.\n-\n-    use_amp : `bool`, optional, (default = `False`)\n-        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n-\n-    enable_default_callbacks : `bool`, optional (default = `True`)\n-        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n-        addition to any other callbacks listed in the `callbacks` parameter.\n-        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n-\n-    run_confidence_checks : `bool`, optional (default = `True`)\n-        Determines whether model confidence checks, such as\n-        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n-        are run.\n-\n-    run_sanity_checks : `bool`, optional (default = `True`)\n-        This parameter is deprecated. Please use `run_confidence_checks` instead.\n-\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        model: Model,\n-        optimizer: torch.optim.Optimizer,\n-        data_loader: DataLoader,\n-        patience: Optional[int] = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        validation_data_loader: DataLoader = None,\n-        num_epochs: int = 20,\n-        serialization_dir: Optional[str] = None,\n-        checkpointer: Checkpointer = None,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: Optional[float] = None,\n-        grad_clipping: Optional[float] = None,\n-        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n-        momentum_scheduler: Optional[MomentumScheduler] = None,\n-        moving_average: Optional[MovingAverage] = None,\n-        callbacks: List[TrainerCallback] = None,\n-        distributed: bool = False,\n-        local_rank: int = 0,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-        )\n-\n-        if \"run_sanity_checks\" in kwargs:\n-            warnings.warn(\n-                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n-                DeprecationWarning,\n-            )\n-            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n-\n-        # I am not calling move_to_gpu here, because if the model is\n-        # not already on the GPU then the optimizer is going to be wrong.\n-        self.model = model\n-\n-        self.data_loader = data_loader\n-        self.data_loader.set_target_device(self.cuda_device)\n-        self._validation_data_loader = validation_data_loader\n-        if self._validation_data_loader is not None:\n-            self._validation_data_loader.set_target_device(self.cuda_device)\n-        self.optimizer = optimizer\n-\n-        if patience is None:  # no early stopping\n-            if validation_data_loader is not None:\n-                logger.warning(\n-                    \"You provided a validation dataset but patience was set to None, \"\n-                    \"meaning that early stopping is disabled\"\n-                )\n-        elif (not isinstance(patience, int)) or patience <= 0:\n-            raise ConfigurationError(\n-                '{} is an invalid value for \"patience\": it must be a positive integer '\n-                \"or None (if you want to disable early stopping)\".format(patience)\n-            )\n-\n-        # For tracking is_best_so_far and should_stop_early\n-        self._metric_tracker = MetricTracker(validation_metric, patience)\n-\n-        self._num_epochs = num_epochs\n-\n-        self._checkpointer: Optional[Checkpointer] = checkpointer\n-        if checkpointer is None and serialization_dir is not None:\n-            self._checkpointer = Checkpointer(serialization_dir)\n-\n-        self._grad_norm = grad_norm\n-        self._grad_clipping = grad_clipping\n-\n-        self._learning_rate_scheduler = learning_rate_scheduler\n-        self._momentum_scheduler = momentum_scheduler\n-        self._moving_average = moving_average\n-\n-        self._callbacks = callbacks or []\n-        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n-\n-        if run_confidence_checks:\n-            default_callbacks.append(ConfidenceChecksCallback)\n-        for callback_cls in default_callbacks:\n-            for callback in self._callbacks:\n-                if callback.__class__ == callback_cls:\n-                    break\n-            else:\n-                self._callbacks.append(callback_cls(self._serialization_dir))\n-\n-        self._batch_num_total = 0\n-        self._last_log = 0.0  # time of last logging\n-        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n-\n-        # Enable automatic mixed precision training.\n-        self._scaler: Optional[amp.GradScaler] = None\n-        self._use_amp = use_amp\n-        if self._use_amp:\n-            if self.cuda_device == torch.device(\"cpu\"):\n-                raise ValueError(\"Using AMP requires a cuda device\")\n-            self._scaler = amp.GradScaler()\n-\n-        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n-        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n-        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n-        #\n-        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n-        # normal case, reference to `Model` is retained. This reference is only used in\n-        # these places: `model.__call__`, `model.train` and `model.eval`.\n-        if self._distributed:\n-            self._pytorch_model = DistributedDataParallel(\n-                self.model,\n-                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n-                find_unused_parameters=True,\n-            )\n-        else:\n-            self._pytorch_model = self.model\n-\n-    def rescale_gradients(self) -> float:\n-        \"\"\"\n-        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n-\n-        Returns the norm of the gradients.\n-        \"\"\"\n-        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n-        if self._grad_norm:\n-            if self._scaler is not None:\n-                # Need to first unscale gradients in order to clip as usual.\n-                self._scaler.unscale_(self.optimizer)\n-            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n-        else:\n-            return torch.norm(\n-                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n-            )\n-\n-    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n-        \"\"\"\n-        Does a forward pass on the given batch and returns the output dictionary that the model\n-        returns, after adding any specified regularization penalty to the loss (if training).\n-        \"\"\"\n-        output_dict = self._pytorch_model(**batch)\n-\n-        if for_training:\n-            try:\n-                assert \"loss\" in output_dict\n-                regularization_penalty = self.model.get_regularization_penalty()\n-\n-                if regularization_penalty is not None:\n-                    output_dict[\"reg_loss\"] = regularization_penalty\n-                    output_dict[\"loss\"] += regularization_penalty\n-\n-            except AssertionError:\n-                if for_training:\n-                    raise RuntimeError(\n-                        \"The model you are trying to optimize does not contain a\"\n-                        \" 'loss' key in the output of model.forward(inputs).\"\n-                    )\n-\n-        return output_dict\n-\n-    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n-        \"\"\"\n-        Trains one epoch and returns metrics.\n-        \"\"\"\n-        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n-        cpu_memory_usage = []\n-        for worker, memory in common_util.peak_cpu_memory().items():\n-            cpu_memory_usage.append((worker, memory))\n-            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n-        gpu_memory_usage = []\n-        for gpu, memory in common_util.peak_gpu_memory().items():\n-            gpu_memory_usage.append((gpu, memory))\n-            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        train_loss = 0.0\n-        batch_loss = 0.0\n-        train_reg_loss = None if regularization_penalty is None else 0.0\n-        batch_reg_loss = None if regularization_penalty is None else 0.0\n-\n-        # Set the model to \"train\" mode.\n-        self._pytorch_model.train()\n-\n-        # Get tqdm for the training batches\n-        batch_generator = iter(self.data_loader)\n-        batch_group_generator = common_util.lazy_groups_of(\n-            batch_generator, self._num_gradient_accumulation_steps\n-        )\n-\n-        logger.info(\"Training\")\n-\n-        num_training_batches: Union[int, float]\n-        try:\n-            len_data_loader = len(self.data_loader)\n-            num_training_batches = math.ceil(\n-                len_data_loader / self._num_gradient_accumulation_steps\n-            )\n-        except TypeError:\n-            num_training_batches = float(\"inf\")\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            batch_group_generator_tqdm = Tqdm.tqdm(\n-                batch_group_generator, total=num_training_batches\n-            )\n-        else:\n-            batch_group_generator_tqdm = batch_group_generator\n-\n-        self._last_log = time.time()\n-\n-        batches_this_epoch = 0\n-        if self._batch_num_total is None:\n-            self._batch_num_total = 0\n-\n-        done_early = False\n-        for batch_group in batch_group_generator_tqdm:\n-            if done_early:\n-                break\n-\n-            batches_this_epoch += 1\n-            self._batch_num_total += 1\n-            batch_num_total = self._batch_num_total\n-\n-            # Zero gradients.\n-            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n-            # because it avoids a read op when the gradients are first updated below.\n-            for param_group in self.optimizer.param_groups:\n-                for p in param_group[\"params\"]:\n-                    p.grad = None\n-\n-            batch_loss = 0.0\n-            batch_group_outputs = []\n-            for batch in batch_group:\n-                if self._distributed:\n-                    # Check whether the other workers have stopped already (due to differing amounts of\n-                    # data in each). If so, we can't proceed because we would hang when we hit the\n-                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                    # here because NCCL process groups apparently don't support BoolTensor.\n-                    done = torch.tensor(0, device=self.cuda_device)\n-                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                    if done.item() > 0:\n-                        done_early = True\n-                        logger.warning(\n-                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n-                            \"This implies that there is an imbalance in your training \"\n-                            \"data across the workers and that some amount of it will be \"\n-                            \"ignored. A small amount of this is fine, but a major imbalance \"\n-                            \"should be avoided. Note: This warning will appear unless your \"\n-                            \"data is perfectly balanced.\"\n-                        )\n-                        break\n-\n-                with amp.autocast(self._use_amp):\n-                    batch_outputs = self.batch_outputs(batch, for_training=True)\n-                    batch_group_outputs.append(batch_outputs)\n-                    loss = batch_outputs[\"loss\"]\n-                    reg_loss = batch_outputs.get(\"reg_loss\")\n-                    if torch.isnan(loss):\n-                        raise ValueError(\"nan loss encountered\")\n-                    loss = loss / len(batch_group)\n-\n-                    batch_loss += loss.item()\n-                    if reg_loss is not None:\n-                        reg_loss = reg_loss / len(batch_group)\n-                        batch_reg_loss = reg_loss.item()\n-                        train_reg_loss += batch_reg_loss  # type: ignore\n-\n-                if self._scaler is not None:\n-                    self._scaler.scale(loss).backward()\n-                else:\n-                    loss.backward()\n-            if len(batch_group_outputs) <= 0:\n-                continue\n-\n-            train_loss += batch_loss\n-\n-            batch_grad_norm = self.rescale_gradients()\n-\n-            # This does nothing if batch_num_total is None or you are using a\n-            # scheduler which doesn't update per batch.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step_batch(batch_num_total)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step_batch(batch_num_total)\n-\n-            if self._scaler is not None:\n-                self._scaler.step(self.optimizer)\n-                self._scaler.update()\n-            else:\n-                self.optimizer.step()\n-\n-            # Update moving averages\n-            if self._moving_average is not None:\n-                self._moving_average.apply(batch_num_total)\n-\n-            # Update the description with the latest metrics\n-            metrics = training_util.get_metrics(\n-                self.model,\n-                train_loss,\n-                train_reg_loss,\n-                batch_loss,\n-                batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            if self._primary:\n-                # Updating tqdm only for the primary as the trainers wouldn't have one\n-                description = training_util.description_from_metrics(metrics)\n-                batch_group_generator_tqdm.set_description(description, refresh=False)\n-\n-                if self._checkpointer is not None:\n-                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    batch_group,\n-                    batch_group_outputs,\n-                    metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=True,\n-                    is_primary=self._primary,\n-                    batch_grad_norm=batch_grad_norm,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Let all workers finish their epoch before computing\n-        # the final statistics for the epoch.\n-        if self._distributed:\n-            dist.barrier()\n-\n-        metrics = training_util.get_metrics(\n-            self.model,\n-            train_loss,\n-            train_reg_loss,\n-            batch_loss=None,\n-            batch_reg_loss=None,\n-            num_batches=batches_this_epoch,\n-            reset=True,\n-            world_size=self._world_size,\n-            cuda_device=self.cuda_device,\n-        )\n-\n-        for (worker, memory) in cpu_memory_usage:\n-            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        for (gpu_num, memory) in gpu_memory_usage:\n-            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        return metrics\n-\n-    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n-        \"\"\"\n-        Computes the validation loss. Returns it and the number of batches.\n-        \"\"\"\n-        logger.info(\"Validating\")\n-\n-        self._pytorch_model.eval()\n-\n-        # Replace parameter values with the shadow values from the moving averages.\n-        if self._moving_average is not None:\n-            self._moving_average.assign_average_value()\n-\n-        if self._validation_data_loader is not None:\n-            validation_data_loader = self._validation_data_loader\n-        else:\n-            raise ConfigurationError(\n-                \"Validation results cannot be calculated without a validation_data_loader\"\n-            )\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n-        else:\n-            val_generator_tqdm = validation_data_loader\n-\n-        batches_this_epoch = 0\n-        val_loss = 0.0\n-        val_batch_loss = 0.0\n-        val_reg_loss = None if regularization_penalty is None else 0.0\n-        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n-        done_early = False\n-        for batch in val_generator_tqdm:\n-            if self._distributed:\n-                # Check whether the other workers have stopped already (due to differing amounts of\n-                # data in each). If so, we can't proceed because we would hang when we hit the\n-                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                # here because NCCL process groups apparently don't support BoolTensor.\n-                done = torch.tensor(0, device=self.cuda_device)\n-                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                if done.item() > 0:\n-                    done_early = True\n-                    logger.warning(\n-                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n-                        \"This implies that there is an imbalance in your validation \"\n-                        \"data across the workers and that some amount of it will be \"\n-                        \"ignored. A small amount of this is fine, but a major imbalance \"\n-                        \"should be avoided. Note: This warning will appear unless your \"\n-                        \"data is perfectly balanced.\"\n-                    )\n-                    break\n-\n-            with amp.autocast(self._use_amp):\n-                batch_outputs = self.batch_outputs(batch, for_training=False)\n-                loss = batch_outputs.get(\"loss\")\n-                reg_loss = batch_outputs.get(\"reg_loss\")\n-                if loss is not None:\n-                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n-                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n-                    # currently only used as the divisor for the loss function, so we can safely only\n-                    # count those batches for which we actually have a loss.  If this variable ever\n-                    # gets used for something else, we might need to change things around a bit.\n-                    batches_this_epoch += 1\n-                    val_batch_loss = loss.item()\n-                    val_loss += val_batch_loss\n-                    if reg_loss is not None:\n-                        val_batch_reg_loss = reg_loss.item()\n-                        val_reg_loss += val_batch_reg_loss  # type: ignore\n-\n-            # Update the description with the latest metrics\n-            val_metrics = training_util.get_metrics(\n-                self.model,\n-                val_loss,\n-                val_reg_loss,\n-                val_batch_loss,\n-                val_batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            description = training_util.description_from_metrics(val_metrics)\n-            if self._primary:\n-                val_generator_tqdm.set_description(description, refresh=False)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    [batch],\n-                    [batch_outputs],\n-                    val_metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=False,\n-                    is_primary=self._primary,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop validation early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Now restore the original parameter values.\n-        if self._moving_average is not None:\n-            self._moving_average.restore()\n-\n-        return val_loss, val_reg_loss, batches_this_epoch\n-\n-    def train(self) -> Dict[str, Any]:\n-        \"\"\"\n-        Trains the supplied model with the supplied parameters.\n-        \"\"\"\n-\n-        for callback in self._callbacks:\n-            callback.on_start(self, is_primary=self._primary)\n-\n-        # Set default values in case of failure\n-        epoch = None\n-        metrics = None\n-\n-        try:\n-            metrics, epoch = self._try_train()\n-            return metrics\n-        finally:\n-            for callback in self._callbacks:\n-                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n-        try:\n-            epoch_counter = self._restore_checkpoint()\n-        except RuntimeError:\n-            traceback.print_exc()\n-            raise ConfigurationError(\n-                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n-                \"a different serialization directory or delete the existing serialization \"\n-                \"directory?\"\n-            )\n-\n-        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n-\n-        logger.info(\"Beginning training.\")\n-\n-        val_metrics: Dict[str, float] = {}\n-        metrics: Dict[str, Any] = {}\n-        epochs_trained = 0\n-        training_start_time = time.time()\n-\n-        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n-        for key, value in self._metric_tracker.best_epoch_metrics.items():\n-            metrics[\"best_validation_\" + key] = value\n-\n-        for epoch in range(epoch_counter, self._num_epochs):\n-            epoch_start_time = time.time()\n-            train_metrics = self._train_epoch(epoch)\n-\n-            # Back up the model now, in case something goes wrong later with the evaluation\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.shelve_model(epoch, self)\n-            # Wait for the primary process to finish saving the model checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            # get peak of memory usage\n-            for key, value in train_metrics.items():\n-                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-\n-            this_epoch_val_metric: float = 0.0\n-            if self._validation_data_loader is not None:\n-                with torch.no_grad():\n-                    # We have a validation set, so compute all the metrics on it.\n-                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n-\n-                    # It is safe again to wait till the validation is done. This is\n-                    # important to get the metrics right.\n-                    if self._distributed:\n-                        dist.barrier()\n-\n-                    val_metrics = training_util.get_metrics(\n-                        self.model,\n-                        val_loss,\n-                        val_reg_loss,\n-                        batch_loss=None,\n-                        batch_reg_loss=None,\n-                        num_batches=num_batches,\n-                        reset=True,\n-                        world_size=self._world_size,\n-                        cuda_device=self.cuda_device,\n-                    )\n-\n-                    # Check validation metric for early stopping\n-                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n-                    self._metric_tracker.add_metrics(val_metrics)\n-\n-            # Create overall metrics dict\n-            training_elapsed_time = time.time() - training_start_time\n-            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n-            metrics[\"training_start_epoch\"] = epoch_counter\n-            metrics[\"training_epochs\"] = epochs_trained\n-            metrics[\"epoch\"] = epoch\n-\n-            for key, value in train_metrics.items():\n-                metrics[\"training_\" + key] = value\n-            for key, value in val_metrics.items():\n-                metrics[\"validation_\" + key] = value\n-\n-            if self._metric_tracker.is_best_so_far():\n-                # Update all the best_ metrics.\n-                # (Otherwise they just stay the same as they were.)\n-                metrics[\"best_epoch\"] = epoch\n-                for key, value in val_metrics.items():\n-                    metrics[\"best_validation_\" + key] = value\n-\n-                self._metric_tracker.best_epoch_metrics = val_metrics\n-\n-            if self._serialization_dir and self._primary:\n-                common_util.dump_metrics(\n-                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n-                    metrics,\n-                )\n-\n-            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n-            # if it doesn't, the validation metric passed here is ignored.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step(this_epoch_val_metric)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step(this_epoch_val_metric)\n-\n-            # The checkpointer saves state from the learning rate scheduler and the momentum\n-            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.save_checkpoint(\n-                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n-                )\n-            # Wait for the primary process to finish saving the checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            for callback in self._callbacks:\n-                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-            epoch_elapsed_time = time.time() - epoch_start_time\n-            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n-\n-            if epoch < self._num_epochs - 1:\n-                training_elapsed_time = time.time() - training_start_time\n-                estimated_time_remaining = training_elapsed_time * (\n-                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n-                )\n-                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n-                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n-\n-            epochs_trained += 1\n-\n-            if self._metric_tracker.should_stop_early():\n-                logger.info(\"Ran out of patience. Stopping training.\")\n-                break\n-        else:\n-            epoch = self._num_epochs - 1\n-\n-        # Load the best model state before returning\n-        best_model_state = (\n-            None if self._checkpointer is None else self._checkpointer.best_model_state()\n-        )\n-        if best_model_state:\n-            self.model.load_state_dict(best_model_state)\n-\n-        return metrics, epoch\n-\n-    @contextmanager\n-    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n-        if self._moving_average is not None:\n-            # Assigning average value to model parameters.  The checkpointer will call\n-            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n-            self._moving_average.assign_average_value()\n-\n-        model_state = self.model.state_dict()\n-\n-        # These are the training states we need to persist.\n-        training_states = {\n-            \"metric_tracker\": self._metric_tracker.state_dict(),\n-            \"optimizer\": self.optimizer.state_dict(),\n-            \"batch_num_total\": self._batch_num_total,\n-        }\n-\n-        # If we have a learning rate or momentum scheduler, we should persist them too.\n-        if self._learning_rate_scheduler is not None:\n-            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n-        if self._momentum_scheduler is not None:\n-            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n-\n-        try:\n-            yield model_state, training_states\n-        finally:\n-            if self._moving_average is not None:\n-                self._moving_average.restore()\n-\n-    def _restore_checkpoint(self) -> int:\n-        \"\"\"\n-        Restores the model and training state from the last saved checkpoint.\n-        This includes an epoch count and optimizer state, which is serialized separately\n-        from model parameters. This function should only be used to continue training -\n-        if you wish to load a model for inference/load parts of a model into a new\n-        computation graph, you should use the native Pytorch functions:\n-        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n-\n-        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n-        this function will do nothing and return 0.\n-\n-        # Returns\n-\n-        epoch: `int`\n-            The epoch at which to resume training, which should be one after the epoch\n-            in the saved training state.\n-        \"\"\"\n-        if self._checkpointer is None:\n-            return 0\n-\n-        model_state, training_state = self._checkpointer.restore_checkpoint()\n-\n-        if not training_state:\n-            # No checkpoint to restore, start at 0\n-            return 0\n-\n-        self.model.load_state_dict(model_state)\n-        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n-        if (\n-            self._learning_rate_scheduler is not None\n-            and \"learning_rate_scheduler\" in training_state\n-        ):\n-            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n-        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n-            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n-        training_util.move_optimizer_to_cuda(self.optimizer)\n-\n-        # Currently the `training_state` contains a serialized `MetricTracker`.\n-        if \"metric_tracker\" in training_state:\n-            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n-        else:\n-            self._metric_tracker.clear()\n-\n-        if isinstance(training_state[\"epoch\"], int):\n-            epoch_to_return = training_state[\"epoch\"] + 1\n-        else:\n-            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n-\n-        # For older checkpoints with batch_num_total missing, default to old behavior where\n-        # it is unchanged.\n-        batch_num_total = training_state.get(\"batch_num_total\")\n-        if batch_num_total is not None:\n-            self._batch_num_total = batch_num_total\n-\n-        return epoch_to_return\n-\n-    @classmethod\n-    def from_partial_objects(\n-        cls,\n-        model: Model,\n-        serialization_dir: str,\n-        data_loader: DataLoader,\n-        validation_data_loader: DataLoader = None,\n-        local_rank: int = 0,\n-        patience: int = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        num_epochs: int = 20,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: float = None,\n-        grad_clipping: float = None,\n-        distributed: bool = False,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        no_grad: List[str] = None,\n-        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n-        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n-        momentum_scheduler: Lazy[MomentumScheduler] = None,\n-        moving_average: Lazy[MovingAverage] = None,\n-        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n-        callbacks: List[Lazy[TrainerCallback]] = None,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> \"Trainer\":\n-        \"\"\"\n-        This method exists so that we can have a documented method to construct this class using\n-        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n-        method.\n-\n-        The reason we can't just use `__init__` with `FromParams` here is because there are\n-        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n-        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n-        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n-        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n-        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n-\n-        If you're not using `FromParams`, you can just construct these arguments in the right order\n-        yourself in your code and call the constructor directly.\n-        \"\"\"\n-        if cuda_device is None:\n-            from torch import cuda\n-\n-            if cuda.device_count() > 0:\n-                cuda_device = 0\n-            else:\n-                cuda_device = -1\n-\n-        check_for_gpu(cuda_device)\n-        if cuda_device >= 0:\n-            # Moving model to GPU here so that the optimizer state gets constructed on\n-            # the right device.\n-            model = model.cuda(cuda_device)\n-\n-        if no_grad:\n-            for name, parameter in model.named_parameters():\n-                if any(re.search(regex, name) for regex in no_grad):\n-                    parameter.requires_grad_(False)\n-\n-        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n-        optimizer_ = optimizer.construct(model_parameters=parameters)\n-\n-        common_util.log_frozen_and_tunable_parameter_names(model)\n-\n-        batches_per_epoch: Optional[int]\n-        try:\n-            batches_per_epoch = len(data_loader)\n-            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n-        except TypeError:\n-            batches_per_epoch = None\n-\n-        moving_average_ = (\n-            None if moving_average is None else moving_average.construct(parameters=parameters)\n-        )\n-        learning_rate_scheduler_ = (\n-            None\n-            if learning_rate_scheduler is None\n-            else learning_rate_scheduler.construct(\n-                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n-            )\n-        )\n-        momentum_scheduler_ = (\n-            None\n-            if momentum_scheduler is None\n-            else momentum_scheduler.construct(optimizer=optimizer_)\n-        )\n-        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n-\n-        callbacks_: List[TrainerCallback] = []\n-        for callback_ in callbacks or []:\n-            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n-\n-        return cls(\n-            model,\n-            optimizer_,\n-            data_loader,\n-            patience=patience,\n-            validation_metric=validation_metric,\n-            validation_data_loader=validation_data_loader,\n-            num_epochs=num_epochs,\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            grad_norm=grad_norm,\n-            grad_clipping=grad_clipping,\n-            learning_rate_scheduler=learning_rate_scheduler_,\n-            momentum_scheduler=momentum_scheduler_,\n-            checkpointer=checkpointer_,\n-            moving_average=moving_average_,\n-            callbacks=callbacks_,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n-            use_amp=use_amp,\n-            enable_default_callbacks=enable_default_callbacks,\n-            run_confidence_checks=run_confidence_checks,\n-            **kwargs,\n-        )\n-\n-\n-DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n-\"\"\"\n-The default callbacks used by `GradientDescentTrainer`.\n-\"\"\"\n+    def get_best_weights_path(self) -> Optional[str]:\n+        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n+        return None\n",
        "source_code_with_indent": "\n\n<DED><DED>@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    <IND>\"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        <IND>super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            <IND>warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        <DED>self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            <IND>self._validation_data_loader.set_target_device(self.cuda_device)\n        <DED>self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            <IND>if validation_data_loader is not None:\n                <IND>logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        <DED><DED>elif (not isinstance(patience, int)) or patience <= 0:\n            <IND>raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        <DED>self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            <IND>self._checkpointer = Checkpointer(serialization_dir)\n\n        <DED>self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            <IND>default_callbacks.append(ConfidenceChecksCallback)\n        <DED>for callback_cls in default_callbacks:\n            <IND>for callback in self._callbacks:\n                <IND>if callback.__class__ == callback_cls:\n                    <IND>break\n            <DED><DED>else:\n                <IND>self._callbacks.append(callback_cls(self._serialization_dir))\n\n        <DED><DED>self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            <IND>if self.cuda_device == torch.device(\"cpu\"):\n                <IND>raise ValueError(\"Using AMP requires a cuda device\")\n            <DED>self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        <DED>if self._distributed:\n            <IND>self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        <DED>else:\n            <IND>self._pytorch_model = self.model\n\n    <DED><DED>def rescale_gradients(self) -> float:\n        <IND>\"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            <IND>if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                <IND>self._scaler.unscale_(self.optimizer)\n            <DED>return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        <DED>else:\n            <IND>return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    <DED><DED>def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        <IND>\"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            <IND>try:\n                <IND>assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    <IND>output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            <DED><DED>except AssertionError:\n                <IND>if for_training:\n                    <IND>raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        <DED><DED><DED>return output_dict\n\n    <DED>def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        <IND>\"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            <IND>cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        <DED>gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            <IND>gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            <IND>len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        <DED>except TypeError:\n            <IND>num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        <DED>if self._primary:\n            <IND>batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        <DED>else:\n            <IND>batch_group_generator_tqdm = batch_group_generator\n\n        <DED>self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            <IND>self._batch_num_total = 0\n\n        <DED>done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            <IND>if done_early:\n                <IND>break\n\n            <DED>batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                <IND>for p in param_group[\"params\"]:\n                    <IND>p.grad = None\n\n            <DED><DED>batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                <IND>if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    <IND>done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        <IND>done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                <DED><DED>with amp.autocast(self._use_amp):\n                    <IND>batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        <IND>raise ValueError(\"nan loss encountered\")\n                    <DED>loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        <IND>reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                <DED><DED>if self._scaler is not None:\n                    <IND>self._scaler.scale(loss).backward()\n                <DED>else:\n                    <IND>loss.backward()\n            <DED><DED>if len(batch_group_outputs) <= 0:\n                <IND>continue\n\n            <DED>train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step_batch(batch_num_total)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step_batch(batch_num_total)\n\n            <DED>if self._scaler is not None:\n                <IND>self._scaler.step(self.optimizer)\n                self._scaler.update()\n            <DED>else:\n                <IND>self.optimizer.step()\n\n            # Update moving averages\n            <DED>if self._moving_average is not None:\n                <IND>self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            <DED>metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                <IND>description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    <IND>self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            <DED><DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        <DED>if self._distributed:\n            <IND>dist.barrier()\n\n        <DED>metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            <IND>metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>for (gpu_num, memory) in gpu_memory_usage:\n            <IND>metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>return metrics\n\n    <DED>def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        <IND>\"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>if self._validation_data_loader is not None:\n            <IND>validation_data_loader = self._validation_data_loader\n        <DED>else:\n            <IND>raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            <IND>val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        <DED>else:\n            <IND>val_generator_tqdm = validation_data_loader\n\n        <DED>batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            <IND>if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                <IND>done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    <IND>done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            <DED><DED>with amp.autocast(self._use_amp):\n                <IND>batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    <IND>batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        <IND>val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            <DED><DED><DED>val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                <IND>val_generator_tqdm.set_description(description, refresh=False)\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        <DED>if self._moving_average is not None:\n            <IND>self._moving_average.restore()\n\n        <DED>return val_loss, val_reg_loss, batches_this_epoch\n\n    <DED>def train(self) -> Dict[str, Any]:\n        <IND>\"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            <IND>callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        <DED>epoch = None\n        metrics = None\n\n        try:\n            <IND>metrics, epoch = self._try_train()\n            return metrics\n        <DED>finally:\n            <IND>for callback in self._callbacks:\n                <IND>callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    <DED><DED><DED>def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        <IND>try:\n            <IND>epoch_counter = self._restore_checkpoint()\n        <DED>except RuntimeError:\n            <IND>traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        <DED>training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            <IND>metrics[\"best_validation_\" + key] = value\n\n        <DED>for epoch in range(epoch_counter, self._num_epochs):\n            <IND>epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            # get peak of memory usage\n            <DED>for key, value in train_metrics.items():\n                <IND>if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                <DED>elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            <DED><DED>this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                <IND>with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    <IND>val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        <IND>dist.barrier()\n\n                    <DED>val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            <DED><DED>training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                <IND>metrics[\"training_\" + key] = value\n            <DED>for key, value in val_metrics.items():\n                <IND>metrics[\"validation_\" + key] = value\n\n            <DED>if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                <IND>metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    <IND>metrics[\"best_validation_\" + key] = value\n\n                <DED>self._metric_tracker.best_epoch_metrics = val_metrics\n\n            <DED>if self._serialization_dir and self._primary:\n                <IND>common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            <DED>if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step(this_epoch_val_metric)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            <DED>if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            <DED>epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                <IND>training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            <DED>epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                <IND>logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        <DED><DED>else:\n            <IND>epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        <DED>best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            <IND>self.model.load_state_dict(best_model_state)\n\n        <DED>return metrics, epoch\n\n    <DED>@contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        <IND>if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            <IND>training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        <DED>if self._momentum_scheduler is not None:\n            <IND>training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        <DED>try:\n            <IND>yield model_state, training_states\n        <DED>finally:\n            <IND>if self._moving_average is not None:\n                <IND>self._moving_average.restore()\n\n    <DED><DED><DED>def _restore_checkpoint(self) -> int:\n        <IND>\"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            <IND>return 0\n\n        <DED>model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            <IND>return 0\n\n        <DED>self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            <IND>self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        <DED>if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            <IND>self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        <DED>training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            <IND>self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        <DED>else:\n            <IND>self._metric_tracker.clear()\n\n        <DED>if isinstance(training_state[\"epoch\"], int):\n            <IND>epoch_to_return = training_state[\"epoch\"] + 1\n        <DED>else:\n            <IND>epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        <DED>batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            <IND>self._batch_num_total = batch_num_total\n\n        <DED>return epoch_to_return\n\n    <DED>@classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        <IND>\"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            <IND>from torch import cuda\n\n            if cuda.device_count() > 0:\n                <IND>cuda_device = 0\n            <DED>else:\n                <IND>cuda_device = -1\n\n        <DED><DED>check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            <IND>model = model.cuda(cuda_device)\n\n        <DED>if no_grad:\n            <IND>for name, parameter in model.named_parameters():\n                <IND>if any(re.search(regex, name) for regex in no_grad):\n                    <IND>parameter.requires_grad_(False)\n\n        <DED><DED><DED>parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            <IND>batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        <DED>except TypeError:\n            <IND>batches_per_epoch = None\n\n        <DED>moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            <IND>callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        <DED>return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\n<DED><DED>DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def get_best_weights_path(self) -> Optional[str]:\n        <IND>\"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "c5bff8ba0d835eb03931f10f4f427ffe936cf796",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:1029:8 Incompatible variable type [9]: momentum_scheduler is declared to have type `allennlp.common.lazy.Lazy[allennlp.training.momentum_schedulers.momentum_scheduler.MomentumScheduler]` but is used as type `None`.",
    "message": " momentum_scheduler is declared to have type `allennlp.common.lazy.Lazy[allennlp.training.momentum_schedulers.momentum_scheduler.MomentumScheduler]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 1029,
    "warning_line": "        momentum_scheduler: Lazy[MomentumScheduler] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n\n@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    \"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            self._validation_data_loader.set_target_device(self.cuda_device)\n        self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            if validation_data_loader is not None:\n                logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        elif (not isinstance(patience, int)) or patience <= 0:\n            raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            self._checkpointer = Checkpointer(serialization_dir)\n\n        self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            default_callbacks.append(ConfidenceChecksCallback)\n        for callback_cls in default_callbacks:\n            for callback in self._callbacks:\n                if callback.__class__ == callback_cls:\n                    break\n            else:\n                self._callbacks.append(callback_cls(self._serialization_dir))\n\n        self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            if self.cuda_device == torch.device(\"cpu\"):\n                raise ValueError(\"Using AMP requires a cuda device\")\n            self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        if self._distributed:\n            self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        else:\n            self._pytorch_model = self.model\n\n    def rescale_gradients(self) -> float:\n        \"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                self._scaler.unscale_(self.optimizer)\n            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        else:\n            return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            try:\n                assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            except AssertionError:\n                if for_training:\n                    raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        return output_dict\n\n    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        \"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        except TypeError:\n            num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        else:\n            batch_group_generator_tqdm = batch_group_generator\n\n        self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            self._batch_num_total = 0\n\n        done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            if done_early:\n                break\n\n            batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                for p in param_group[\"params\"]:\n                    p.grad = None\n\n            batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                with amp.autocast(self._use_amp):\n                    batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        raise ValueError(\"nan loss encountered\")\n                    loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                if self._scaler is not None:\n                    self._scaler.scale(loss).backward()\n                else:\n                    loss.backward()\n            if len(batch_group_outputs) <= 0:\n                continue\n\n            train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step_batch(batch_num_total)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step_batch(batch_num_total)\n\n            if self._scaler is not None:\n                self._scaler.step(self.optimizer)\n                self._scaler.update()\n            else:\n                self.optimizer.step()\n\n            # Update moving averages\n            if self._moving_average is not None:\n                self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        if self._distributed:\n            dist.barrier()\n\n        metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        for (gpu_num, memory) in gpu_memory_usage:\n            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        return metrics\n\n    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        \"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            self._moving_average.assign_average_value()\n\n        if self._validation_data_loader is not None:\n            validation_data_loader = self._validation_data_loader\n        else:\n            raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        else:\n            val_generator_tqdm = validation_data_loader\n\n        batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            with amp.autocast(self._use_amp):\n                batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                val_generator_tqdm.set_description(description, refresh=False)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        if self._moving_average is not None:\n            self._moving_average.restore()\n\n        return val_loss, val_reg_loss, batches_this_epoch\n\n    def train(self) -> Dict[str, Any]:\n        \"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        epoch = None\n        metrics = None\n\n        try:\n            metrics, epoch = self._try_train()\n            return metrics\n        finally:\n            for callback in self._callbacks:\n                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        try:\n            epoch_counter = self._restore_checkpoint()\n        except RuntimeError:\n            traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            metrics[\"best_validation_\" + key] = value\n\n        for epoch in range(epoch_counter, self._num_epochs):\n            epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            # get peak of memory usage\n            for key, value in train_metrics.items():\n                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        dist.barrier()\n\n                    val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                metrics[\"training_\" + key] = value\n            for key, value in val_metrics.items():\n                metrics[\"validation_\" + key] = value\n\n            if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    metrics[\"best_validation_\" + key] = value\n\n                self._metric_tracker.best_epoch_metrics = val_metrics\n\n            if self._serialization_dir and self._primary:\n                common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step(this_epoch_val_metric)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            for callback in self._callbacks:\n                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        else:\n            epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            self.model.load_state_dict(best_model_state)\n\n        return metrics, epoch\n\n    @contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            self._moving_average.assign_average_value()\n\n        model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        if self._momentum_scheduler is not None:\n            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        try:\n            yield model_state, training_states\n        finally:\n            if self._moving_average is not None:\n                self._moving_average.restore()\n\n    def _restore_checkpoint(self) -> int:\n        \"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            return 0\n\n        model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            return 0\n\n        self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        else:\n            self._metric_tracker.clear()\n\n        if isinstance(training_state[\"epoch\"], int):\n            epoch_to_return = training_state[\"epoch\"] + 1\n        else:\n            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            self._batch_num_total = batch_num_total\n\n        return epoch_to_return\n\n    @classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        \"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            from torch import cuda\n\n            if cuda.device_count() > 0:\n                cuda_device = 0\n            else:\n                cuda_device = -1\n\n        check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            model = model.cuda(cuda_device)\n\n        if no_grad:\n            for name, parameter in model.named_parameters():\n                if any(re.search(regex, name) for regex in no_grad):\n                    parameter.requires_grad_(False)\n\n        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        except TypeError:\n            batches_per_epoch = None\n\n        moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\nDEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_len": 45826,
        "target_code": "\n    def get_best_weights_path(self) -> Optional[str]:\n        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_len": 151,
        "diff_format": "@@ -115,1021 +89,4 @@\n \n-\n-@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\n-class GradientDescentTrainer(Trainer):\n-    \"\"\"\n-    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n-    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n-    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n-    stopping. There are many other bells and whistles as well.\n-\n-    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n-    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n-    see the arguments to that function for the exact keys that should be used, if you are using\n-    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n-    docstrings in `from_partial_objects`.\n-\n-    [0]: https://tinyurl.com/y5mv44fw\n-\n-    # Parameters\n-\n-    model : `Model`, required.\n-        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n-        their `forward` method returns a dictionary with a \"loss\" key, containing a\n-        scalar tensor representing the loss function to be optimized.\n-\n-        If you are training your model using GPUs, your model should already be\n-        on the correct device. (If you are using our `train` command this will be\n-        handled for you.)\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    optimizer : `torch.nn.Optimizer`, required.\n-        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n-        model to be optimized.\n-\n-    data_loader : `DataLoader`, required.\n-        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    patience : `Optional[int] > 0`, optional (default=`None`)\n-        Number of epochs to be patient before early stopping: the training is stopped\n-        after `patience` epochs with no improvement. If given, it must be `> 0`.\n-        If None, early stopping is disabled.\n-\n-    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n-        Validation metric to measure for whether to stop training using patience\n-        and whether to serialize an `is_best` model each epoch. The metric name\n-        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n-        is an increasing or decreasing function. If you specify more than one metric,\n-        the metrics will be summed to make the `is_best` decision.\n-\n-    validation_data_loader : `DataLoader`, optional (default=`None`)\n-        A `DataLoader` to use for the validation set.  If `None`, then\n-        use the training `DataLoader` with the validation data.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_epochs : `int`, optional (default = `20`)\n-        Number of training epochs.\n-\n-    serialization_dir : `str`, optional (default=`None`)\n-        Path to directory for saving and loading model files. Models will not be saved if\n-        this parameter is not passed.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    checkpointer : `Checkpointer`, optional (default=`None`)\n-        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n-        here, we will construct one with default parameters.\n-\n-    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n-        An integer or `torch.device` specifying the CUDA device to use for this process.\n-        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n-\n-        !!! Note\n-            If you *don't* intend to use a GPU, but you have one available, you'll need\n-            to explicitly set `cuda_device=-1`.\n-\n-        !!! Note\n-            If you intend to use a GPU, your model already needs to be on the correct device,\n-            which you can do with `model = model.cuda()`.\n-\n-        !!! Note\n-            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n-\n-    grad_norm : `float`, optional, (default = `None`).\n-        If provided, gradient norms will be rescaled to have a maximum of this value.\n-\n-    grad_clipping : `float`, optional (default = `None`).\n-        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n-        maximum of this value.  If you are getting `NaNs` in your gradients during training\n-        that are not solved by using `grad_norm`, you may need this.\n-\n-    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n-        If specified, the learning rate will be decayed with respect to\n-        this schedule at the end of each epoch (or batch, if the scheduler implements\n-        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n-        this will use the `validation_metric` provided to determine if learning has plateaued.\n-        To support updating the learning rate on every batch, this can optionally implement\n-        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n-\n-    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n-        If specified, the momentum will be updated at the end of each batch or epoch\n-        according to the schedule.\n-\n-    moving_average : `MovingAverage`, optional, (default = `None`)\n-        If provided, we will maintain moving averages for all parameters. During training, we\n-        employ a shadow variable for each parameter, which maintains the moving average. During\n-        evaluation, we backup the original parameters and assign the moving averages to corresponding\n-        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n-        parameters. This is necessary because we want the saved model to perform as well as the validated\n-        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n-\n-    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n-        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n-        and end of training, etc.\n-\n-    distributed : `bool`, optional, (default = `False`)\n-        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n-        requires `world_size` to be greater than 1.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n-        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n-\n-    local_rank : `int`, optional, (default = `0`)\n-        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n-        used as the rank.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    world_size : `int`, (default = `1`)\n-        The number of `Trainer` workers participating in the distributed training.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n-        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n-        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n-        post][0] for details on Gradient Accumulation.\n-\n-    use_amp : `bool`, optional, (default = `False`)\n-        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n-\n-    enable_default_callbacks : `bool`, optional (default = `True`)\n-        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n-        addition to any other callbacks listed in the `callbacks` parameter.\n-        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n-\n-    run_confidence_checks : `bool`, optional (default = `True`)\n-        Determines whether model confidence checks, such as\n-        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n-        are run.\n-\n-    run_sanity_checks : `bool`, optional (default = `True`)\n-        This parameter is deprecated. Please use `run_confidence_checks` instead.\n-\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        model: Model,\n-        optimizer: torch.optim.Optimizer,\n-        data_loader: DataLoader,\n-        patience: Optional[int] = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        validation_data_loader: DataLoader = None,\n-        num_epochs: int = 20,\n-        serialization_dir: Optional[str] = None,\n-        checkpointer: Checkpointer = None,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: Optional[float] = None,\n-        grad_clipping: Optional[float] = None,\n-        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n-        momentum_scheduler: Optional[MomentumScheduler] = None,\n-        moving_average: Optional[MovingAverage] = None,\n-        callbacks: List[TrainerCallback] = None,\n-        distributed: bool = False,\n-        local_rank: int = 0,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-        )\n-\n-        if \"run_sanity_checks\" in kwargs:\n-            warnings.warn(\n-                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n-                DeprecationWarning,\n-            )\n-            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n-\n-        # I am not calling move_to_gpu here, because if the model is\n-        # not already on the GPU then the optimizer is going to be wrong.\n-        self.model = model\n-\n-        self.data_loader = data_loader\n-        self.data_loader.set_target_device(self.cuda_device)\n-        self._validation_data_loader = validation_data_loader\n-        if self._validation_data_loader is not None:\n-            self._validation_data_loader.set_target_device(self.cuda_device)\n-        self.optimizer = optimizer\n-\n-        if patience is None:  # no early stopping\n-            if validation_data_loader is not None:\n-                logger.warning(\n-                    \"You provided a validation dataset but patience was set to None, \"\n-                    \"meaning that early stopping is disabled\"\n-                )\n-        elif (not isinstance(patience, int)) or patience <= 0:\n-            raise ConfigurationError(\n-                '{} is an invalid value for \"patience\": it must be a positive integer '\n-                \"or None (if you want to disable early stopping)\".format(patience)\n-            )\n-\n-        # For tracking is_best_so_far and should_stop_early\n-        self._metric_tracker = MetricTracker(validation_metric, patience)\n-\n-        self._num_epochs = num_epochs\n-\n-        self._checkpointer: Optional[Checkpointer] = checkpointer\n-        if checkpointer is None and serialization_dir is not None:\n-            self._checkpointer = Checkpointer(serialization_dir)\n-\n-        self._grad_norm = grad_norm\n-        self._grad_clipping = grad_clipping\n-\n-        self._learning_rate_scheduler = learning_rate_scheduler\n-        self._momentum_scheduler = momentum_scheduler\n-        self._moving_average = moving_average\n-\n-        self._callbacks = callbacks or []\n-        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n-\n-        if run_confidence_checks:\n-            default_callbacks.append(ConfidenceChecksCallback)\n-        for callback_cls in default_callbacks:\n-            for callback in self._callbacks:\n-                if callback.__class__ == callback_cls:\n-                    break\n-            else:\n-                self._callbacks.append(callback_cls(self._serialization_dir))\n-\n-        self._batch_num_total = 0\n-        self._last_log = 0.0  # time of last logging\n-        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n-\n-        # Enable automatic mixed precision training.\n-        self._scaler: Optional[amp.GradScaler] = None\n-        self._use_amp = use_amp\n-        if self._use_amp:\n-            if self.cuda_device == torch.device(\"cpu\"):\n-                raise ValueError(\"Using AMP requires a cuda device\")\n-            self._scaler = amp.GradScaler()\n-\n-        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n-        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n-        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n-        #\n-        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n-        # normal case, reference to `Model` is retained. This reference is only used in\n-        # these places: `model.__call__`, `model.train` and `model.eval`.\n-        if self._distributed:\n-            self._pytorch_model = DistributedDataParallel(\n-                self.model,\n-                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n-                find_unused_parameters=True,\n-            )\n-        else:\n-            self._pytorch_model = self.model\n-\n-    def rescale_gradients(self) -> float:\n-        \"\"\"\n-        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n-\n-        Returns the norm of the gradients.\n-        \"\"\"\n-        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n-        if self._grad_norm:\n-            if self._scaler is not None:\n-                # Need to first unscale gradients in order to clip as usual.\n-                self._scaler.unscale_(self.optimizer)\n-            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n-        else:\n-            return torch.norm(\n-                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n-            )\n-\n-    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n-        \"\"\"\n-        Does a forward pass on the given batch and returns the output dictionary that the model\n-        returns, after adding any specified regularization penalty to the loss (if training).\n-        \"\"\"\n-        output_dict = self._pytorch_model(**batch)\n-\n-        if for_training:\n-            try:\n-                assert \"loss\" in output_dict\n-                regularization_penalty = self.model.get_regularization_penalty()\n-\n-                if regularization_penalty is not None:\n-                    output_dict[\"reg_loss\"] = regularization_penalty\n-                    output_dict[\"loss\"] += regularization_penalty\n-\n-            except AssertionError:\n-                if for_training:\n-                    raise RuntimeError(\n-                        \"The model you are trying to optimize does not contain a\"\n-                        \" 'loss' key in the output of model.forward(inputs).\"\n-                    )\n-\n-        return output_dict\n-\n-    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n-        \"\"\"\n-        Trains one epoch and returns metrics.\n-        \"\"\"\n-        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n-        cpu_memory_usage = []\n-        for worker, memory in common_util.peak_cpu_memory().items():\n-            cpu_memory_usage.append((worker, memory))\n-            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n-        gpu_memory_usage = []\n-        for gpu, memory in common_util.peak_gpu_memory().items():\n-            gpu_memory_usage.append((gpu, memory))\n-            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        train_loss = 0.0\n-        batch_loss = 0.0\n-        train_reg_loss = None if regularization_penalty is None else 0.0\n-        batch_reg_loss = None if regularization_penalty is None else 0.0\n-\n-        # Set the model to \"train\" mode.\n-        self._pytorch_model.train()\n-\n-        # Get tqdm for the training batches\n-        batch_generator = iter(self.data_loader)\n-        batch_group_generator = common_util.lazy_groups_of(\n-            batch_generator, self._num_gradient_accumulation_steps\n-        )\n-\n-        logger.info(\"Training\")\n-\n-        num_training_batches: Union[int, float]\n-        try:\n-            len_data_loader = len(self.data_loader)\n-            num_training_batches = math.ceil(\n-                len_data_loader / self._num_gradient_accumulation_steps\n-            )\n-        except TypeError:\n-            num_training_batches = float(\"inf\")\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            batch_group_generator_tqdm = Tqdm.tqdm(\n-                batch_group_generator, total=num_training_batches\n-            )\n-        else:\n-            batch_group_generator_tqdm = batch_group_generator\n-\n-        self._last_log = time.time()\n-\n-        batches_this_epoch = 0\n-        if self._batch_num_total is None:\n-            self._batch_num_total = 0\n-\n-        done_early = False\n-        for batch_group in batch_group_generator_tqdm:\n-            if done_early:\n-                break\n-\n-            batches_this_epoch += 1\n-            self._batch_num_total += 1\n-            batch_num_total = self._batch_num_total\n-\n-            # Zero gradients.\n-            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n-            # because it avoids a read op when the gradients are first updated below.\n-            for param_group in self.optimizer.param_groups:\n-                for p in param_group[\"params\"]:\n-                    p.grad = None\n-\n-            batch_loss = 0.0\n-            batch_group_outputs = []\n-            for batch in batch_group:\n-                if self._distributed:\n-                    # Check whether the other workers have stopped already (due to differing amounts of\n-                    # data in each). If so, we can't proceed because we would hang when we hit the\n-                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                    # here because NCCL process groups apparently don't support BoolTensor.\n-                    done = torch.tensor(0, device=self.cuda_device)\n-                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                    if done.item() > 0:\n-                        done_early = True\n-                        logger.warning(\n-                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n-                            \"This implies that there is an imbalance in your training \"\n-                            \"data across the workers and that some amount of it will be \"\n-                            \"ignored. A small amount of this is fine, but a major imbalance \"\n-                            \"should be avoided. Note: This warning will appear unless your \"\n-                            \"data is perfectly balanced.\"\n-                        )\n-                        break\n-\n-                with amp.autocast(self._use_amp):\n-                    batch_outputs = self.batch_outputs(batch, for_training=True)\n-                    batch_group_outputs.append(batch_outputs)\n-                    loss = batch_outputs[\"loss\"]\n-                    reg_loss = batch_outputs.get(\"reg_loss\")\n-                    if torch.isnan(loss):\n-                        raise ValueError(\"nan loss encountered\")\n-                    loss = loss / len(batch_group)\n-\n-                    batch_loss += loss.item()\n-                    if reg_loss is not None:\n-                        reg_loss = reg_loss / len(batch_group)\n-                        batch_reg_loss = reg_loss.item()\n-                        train_reg_loss += batch_reg_loss  # type: ignore\n-\n-                if self._scaler is not None:\n-                    self._scaler.scale(loss).backward()\n-                else:\n-                    loss.backward()\n-            if len(batch_group_outputs) <= 0:\n-                continue\n-\n-            train_loss += batch_loss\n-\n-            batch_grad_norm = self.rescale_gradients()\n-\n-            # This does nothing if batch_num_total is None or you are using a\n-            # scheduler which doesn't update per batch.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step_batch(batch_num_total)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step_batch(batch_num_total)\n-\n-            if self._scaler is not None:\n-                self._scaler.step(self.optimizer)\n-                self._scaler.update()\n-            else:\n-                self.optimizer.step()\n-\n-            # Update moving averages\n-            if self._moving_average is not None:\n-                self._moving_average.apply(batch_num_total)\n-\n-            # Update the description with the latest metrics\n-            metrics = training_util.get_metrics(\n-                self.model,\n-                train_loss,\n-                train_reg_loss,\n-                batch_loss,\n-                batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            if self._primary:\n-                # Updating tqdm only for the primary as the trainers wouldn't have one\n-                description = training_util.description_from_metrics(metrics)\n-                batch_group_generator_tqdm.set_description(description, refresh=False)\n-\n-                if self._checkpointer is not None:\n-                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    batch_group,\n-                    batch_group_outputs,\n-                    metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=True,\n-                    is_primary=self._primary,\n-                    batch_grad_norm=batch_grad_norm,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Let all workers finish their epoch before computing\n-        # the final statistics for the epoch.\n-        if self._distributed:\n-            dist.barrier()\n-\n-        metrics = training_util.get_metrics(\n-            self.model,\n-            train_loss,\n-            train_reg_loss,\n-            batch_loss=None,\n-            batch_reg_loss=None,\n-            num_batches=batches_this_epoch,\n-            reset=True,\n-            world_size=self._world_size,\n-            cuda_device=self.cuda_device,\n-        )\n-\n-        for (worker, memory) in cpu_memory_usage:\n-            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        for (gpu_num, memory) in gpu_memory_usage:\n-            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        return metrics\n-\n-    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n-        \"\"\"\n-        Computes the validation loss. Returns it and the number of batches.\n-        \"\"\"\n-        logger.info(\"Validating\")\n-\n-        self._pytorch_model.eval()\n-\n-        # Replace parameter values with the shadow values from the moving averages.\n-        if self._moving_average is not None:\n-            self._moving_average.assign_average_value()\n-\n-        if self._validation_data_loader is not None:\n-            validation_data_loader = self._validation_data_loader\n-        else:\n-            raise ConfigurationError(\n-                \"Validation results cannot be calculated without a validation_data_loader\"\n-            )\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n-        else:\n-            val_generator_tqdm = validation_data_loader\n-\n-        batches_this_epoch = 0\n-        val_loss = 0.0\n-        val_batch_loss = 0.0\n-        val_reg_loss = None if regularization_penalty is None else 0.0\n-        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n-        done_early = False\n-        for batch in val_generator_tqdm:\n-            if self._distributed:\n-                # Check whether the other workers have stopped already (due to differing amounts of\n-                # data in each). If so, we can't proceed because we would hang when we hit the\n-                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                # here because NCCL process groups apparently don't support BoolTensor.\n-                done = torch.tensor(0, device=self.cuda_device)\n-                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                if done.item() > 0:\n-                    done_early = True\n-                    logger.warning(\n-                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n-                        \"This implies that there is an imbalance in your validation \"\n-                        \"data across the workers and that some amount of it will be \"\n-                        \"ignored. A small amount of this is fine, but a major imbalance \"\n-                        \"should be avoided. Note: This warning will appear unless your \"\n-                        \"data is perfectly balanced.\"\n-                    )\n-                    break\n-\n-            with amp.autocast(self._use_amp):\n-                batch_outputs = self.batch_outputs(batch, for_training=False)\n-                loss = batch_outputs.get(\"loss\")\n-                reg_loss = batch_outputs.get(\"reg_loss\")\n-                if loss is not None:\n-                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n-                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n-                    # currently only used as the divisor for the loss function, so we can safely only\n-                    # count those batches for which we actually have a loss.  If this variable ever\n-                    # gets used for something else, we might need to change things around a bit.\n-                    batches_this_epoch += 1\n-                    val_batch_loss = loss.item()\n-                    val_loss += val_batch_loss\n-                    if reg_loss is not None:\n-                        val_batch_reg_loss = reg_loss.item()\n-                        val_reg_loss += val_batch_reg_loss  # type: ignore\n-\n-            # Update the description with the latest metrics\n-            val_metrics = training_util.get_metrics(\n-                self.model,\n-                val_loss,\n-                val_reg_loss,\n-                val_batch_loss,\n-                val_batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            description = training_util.description_from_metrics(val_metrics)\n-            if self._primary:\n-                val_generator_tqdm.set_description(description, refresh=False)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    [batch],\n-                    [batch_outputs],\n-                    val_metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=False,\n-                    is_primary=self._primary,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop validation early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Now restore the original parameter values.\n-        if self._moving_average is not None:\n-            self._moving_average.restore()\n-\n-        return val_loss, val_reg_loss, batches_this_epoch\n-\n-    def train(self) -> Dict[str, Any]:\n-        \"\"\"\n-        Trains the supplied model with the supplied parameters.\n-        \"\"\"\n-\n-        for callback in self._callbacks:\n-            callback.on_start(self, is_primary=self._primary)\n-\n-        # Set default values in case of failure\n-        epoch = None\n-        metrics = None\n-\n-        try:\n-            metrics, epoch = self._try_train()\n-            return metrics\n-        finally:\n-            for callback in self._callbacks:\n-                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n-        try:\n-            epoch_counter = self._restore_checkpoint()\n-        except RuntimeError:\n-            traceback.print_exc()\n-            raise ConfigurationError(\n-                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n-                \"a different serialization directory or delete the existing serialization \"\n-                \"directory?\"\n-            )\n-\n-        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n-\n-        logger.info(\"Beginning training.\")\n-\n-        val_metrics: Dict[str, float] = {}\n-        metrics: Dict[str, Any] = {}\n-        epochs_trained = 0\n-        training_start_time = time.time()\n-\n-        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n-        for key, value in self._metric_tracker.best_epoch_metrics.items():\n-            metrics[\"best_validation_\" + key] = value\n-\n-        for epoch in range(epoch_counter, self._num_epochs):\n-            epoch_start_time = time.time()\n-            train_metrics = self._train_epoch(epoch)\n-\n-            # Back up the model now, in case something goes wrong later with the evaluation\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.shelve_model(epoch, self)\n-            # Wait for the primary process to finish saving the model checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            # get peak of memory usage\n-            for key, value in train_metrics.items():\n-                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-\n-            this_epoch_val_metric: float = 0.0\n-            if self._validation_data_loader is not None:\n-                with torch.no_grad():\n-                    # We have a validation set, so compute all the metrics on it.\n-                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n-\n-                    # It is safe again to wait till the validation is done. This is\n-                    # important to get the metrics right.\n-                    if self._distributed:\n-                        dist.barrier()\n-\n-                    val_metrics = training_util.get_metrics(\n-                        self.model,\n-                        val_loss,\n-                        val_reg_loss,\n-                        batch_loss=None,\n-                        batch_reg_loss=None,\n-                        num_batches=num_batches,\n-                        reset=True,\n-                        world_size=self._world_size,\n-                        cuda_device=self.cuda_device,\n-                    )\n-\n-                    # Check validation metric for early stopping\n-                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n-                    self._metric_tracker.add_metrics(val_metrics)\n-\n-            # Create overall metrics dict\n-            training_elapsed_time = time.time() - training_start_time\n-            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n-            metrics[\"training_start_epoch\"] = epoch_counter\n-            metrics[\"training_epochs\"] = epochs_trained\n-            metrics[\"epoch\"] = epoch\n-\n-            for key, value in train_metrics.items():\n-                metrics[\"training_\" + key] = value\n-            for key, value in val_metrics.items():\n-                metrics[\"validation_\" + key] = value\n-\n-            if self._metric_tracker.is_best_so_far():\n-                # Update all the best_ metrics.\n-                # (Otherwise they just stay the same as they were.)\n-                metrics[\"best_epoch\"] = epoch\n-                for key, value in val_metrics.items():\n-                    metrics[\"best_validation_\" + key] = value\n-\n-                self._metric_tracker.best_epoch_metrics = val_metrics\n-\n-            if self._serialization_dir and self._primary:\n-                common_util.dump_metrics(\n-                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n-                    metrics,\n-                )\n-\n-            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n-            # if it doesn't, the validation metric passed here is ignored.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step(this_epoch_val_metric)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step(this_epoch_val_metric)\n-\n-            # The checkpointer saves state from the learning rate scheduler and the momentum\n-            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.save_checkpoint(\n-                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n-                )\n-            # Wait for the primary process to finish saving the checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            for callback in self._callbacks:\n-                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-            epoch_elapsed_time = time.time() - epoch_start_time\n-            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n-\n-            if epoch < self._num_epochs - 1:\n-                training_elapsed_time = time.time() - training_start_time\n-                estimated_time_remaining = training_elapsed_time * (\n-                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n-                )\n-                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n-                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n-\n-            epochs_trained += 1\n-\n-            if self._metric_tracker.should_stop_early():\n-                logger.info(\"Ran out of patience. Stopping training.\")\n-                break\n-        else:\n-            epoch = self._num_epochs - 1\n-\n-        # Load the best model state before returning\n-        best_model_state = (\n-            None if self._checkpointer is None else self._checkpointer.best_model_state()\n-        )\n-        if best_model_state:\n-            self.model.load_state_dict(best_model_state)\n-\n-        return metrics, epoch\n-\n-    @contextmanager\n-    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n-        if self._moving_average is not None:\n-            # Assigning average value to model parameters.  The checkpointer will call\n-            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n-            self._moving_average.assign_average_value()\n-\n-        model_state = self.model.state_dict()\n-\n-        # These are the training states we need to persist.\n-        training_states = {\n-            \"metric_tracker\": self._metric_tracker.state_dict(),\n-            \"optimizer\": self.optimizer.state_dict(),\n-            \"batch_num_total\": self._batch_num_total,\n-        }\n-\n-        # If we have a learning rate or momentum scheduler, we should persist them too.\n-        if self._learning_rate_scheduler is not None:\n-            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n-        if self._momentum_scheduler is not None:\n-            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n-\n-        try:\n-            yield model_state, training_states\n-        finally:\n-            if self._moving_average is not None:\n-                self._moving_average.restore()\n-\n-    def _restore_checkpoint(self) -> int:\n-        \"\"\"\n-        Restores the model and training state from the last saved checkpoint.\n-        This includes an epoch count and optimizer state, which is serialized separately\n-        from model parameters. This function should only be used to continue training -\n-        if you wish to load a model for inference/load parts of a model into a new\n-        computation graph, you should use the native Pytorch functions:\n-        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n-\n-        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n-        this function will do nothing and return 0.\n-\n-        # Returns\n-\n-        epoch: `int`\n-            The epoch at which to resume training, which should be one after the epoch\n-            in the saved training state.\n-        \"\"\"\n-        if self._checkpointer is None:\n-            return 0\n-\n-        model_state, training_state = self._checkpointer.restore_checkpoint()\n-\n-        if not training_state:\n-            # No checkpoint to restore, start at 0\n-            return 0\n-\n-        self.model.load_state_dict(model_state)\n-        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n-        if (\n-            self._learning_rate_scheduler is not None\n-            and \"learning_rate_scheduler\" in training_state\n-        ):\n-            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n-        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n-            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n-        training_util.move_optimizer_to_cuda(self.optimizer)\n-\n-        # Currently the `training_state` contains a serialized `MetricTracker`.\n-        if \"metric_tracker\" in training_state:\n-            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n-        else:\n-            self._metric_tracker.clear()\n-\n-        if isinstance(training_state[\"epoch\"], int):\n-            epoch_to_return = training_state[\"epoch\"] + 1\n-        else:\n-            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n-\n-        # For older checkpoints with batch_num_total missing, default to old behavior where\n-        # it is unchanged.\n-        batch_num_total = training_state.get(\"batch_num_total\")\n-        if batch_num_total is not None:\n-            self._batch_num_total = batch_num_total\n-\n-        return epoch_to_return\n-\n-    @classmethod\n-    def from_partial_objects(\n-        cls,\n-        model: Model,\n-        serialization_dir: str,\n-        data_loader: DataLoader,\n-        validation_data_loader: DataLoader = None,\n-        local_rank: int = 0,\n-        patience: int = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        num_epochs: int = 20,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: float = None,\n-        grad_clipping: float = None,\n-        distributed: bool = False,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        no_grad: List[str] = None,\n-        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n-        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n-        momentum_scheduler: Lazy[MomentumScheduler] = None,\n-        moving_average: Lazy[MovingAverage] = None,\n-        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n-        callbacks: List[Lazy[TrainerCallback]] = None,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> \"Trainer\":\n-        \"\"\"\n-        This method exists so that we can have a documented method to construct this class using\n-        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n-        method.\n-\n-        The reason we can't just use `__init__` with `FromParams` here is because there are\n-        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n-        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n-        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n-        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n-        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n-\n-        If you're not using `FromParams`, you can just construct these arguments in the right order\n-        yourself in your code and call the constructor directly.\n-        \"\"\"\n-        if cuda_device is None:\n-            from torch import cuda\n-\n-            if cuda.device_count() > 0:\n-                cuda_device = 0\n-            else:\n-                cuda_device = -1\n-\n-        check_for_gpu(cuda_device)\n-        if cuda_device >= 0:\n-            # Moving model to GPU here so that the optimizer state gets constructed on\n-            # the right device.\n-            model = model.cuda(cuda_device)\n-\n-        if no_grad:\n-            for name, parameter in model.named_parameters():\n-                if any(re.search(regex, name) for regex in no_grad):\n-                    parameter.requires_grad_(False)\n-\n-        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n-        optimizer_ = optimizer.construct(model_parameters=parameters)\n-\n-        common_util.log_frozen_and_tunable_parameter_names(model)\n-\n-        batches_per_epoch: Optional[int]\n-        try:\n-            batches_per_epoch = len(data_loader)\n-            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n-        except TypeError:\n-            batches_per_epoch = None\n-\n-        moving_average_ = (\n-            None if moving_average is None else moving_average.construct(parameters=parameters)\n-        )\n-        learning_rate_scheduler_ = (\n-            None\n-            if learning_rate_scheduler is None\n-            else learning_rate_scheduler.construct(\n-                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n-            )\n-        )\n-        momentum_scheduler_ = (\n-            None\n-            if momentum_scheduler is None\n-            else momentum_scheduler.construct(optimizer=optimizer_)\n-        )\n-        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n-\n-        callbacks_: List[TrainerCallback] = []\n-        for callback_ in callbacks or []:\n-            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n-\n-        return cls(\n-            model,\n-            optimizer_,\n-            data_loader,\n-            patience=patience,\n-            validation_metric=validation_metric,\n-            validation_data_loader=validation_data_loader,\n-            num_epochs=num_epochs,\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            grad_norm=grad_norm,\n-            grad_clipping=grad_clipping,\n-            learning_rate_scheduler=learning_rate_scheduler_,\n-            momentum_scheduler=momentum_scheduler_,\n-            checkpointer=checkpointer_,\n-            moving_average=moving_average_,\n-            callbacks=callbacks_,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n-            use_amp=use_amp,\n-            enable_default_callbacks=enable_default_callbacks,\n-            run_confidence_checks=run_confidence_checks,\n-            **kwargs,\n-        )\n-\n-\n-DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n-\"\"\"\n-The default callbacks used by `GradientDescentTrainer`.\n-\"\"\"\n+    def get_best_weights_path(self) -> Optional[str]:\n+        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n+        return None\n",
        "source_code_with_indent": "\n\n<DED><DED>@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    <IND>\"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        <IND>super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            <IND>warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        <DED>self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            <IND>self._validation_data_loader.set_target_device(self.cuda_device)\n        <DED>self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            <IND>if validation_data_loader is not None:\n                <IND>logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        <DED><DED>elif (not isinstance(patience, int)) or patience <= 0:\n            <IND>raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        <DED>self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            <IND>self._checkpointer = Checkpointer(serialization_dir)\n\n        <DED>self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            <IND>default_callbacks.append(ConfidenceChecksCallback)\n        <DED>for callback_cls in default_callbacks:\n            <IND>for callback in self._callbacks:\n                <IND>if callback.__class__ == callback_cls:\n                    <IND>break\n            <DED><DED>else:\n                <IND>self._callbacks.append(callback_cls(self._serialization_dir))\n\n        <DED><DED>self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            <IND>if self.cuda_device == torch.device(\"cpu\"):\n                <IND>raise ValueError(\"Using AMP requires a cuda device\")\n            <DED>self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        <DED>if self._distributed:\n            <IND>self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        <DED>else:\n            <IND>self._pytorch_model = self.model\n\n    <DED><DED>def rescale_gradients(self) -> float:\n        <IND>\"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            <IND>if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                <IND>self._scaler.unscale_(self.optimizer)\n            <DED>return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        <DED>else:\n            <IND>return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    <DED><DED>def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        <IND>\"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            <IND>try:\n                <IND>assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    <IND>output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            <DED><DED>except AssertionError:\n                <IND>if for_training:\n                    <IND>raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        <DED><DED><DED>return output_dict\n\n    <DED>def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        <IND>\"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            <IND>cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        <DED>gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            <IND>gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            <IND>len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        <DED>except TypeError:\n            <IND>num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        <DED>if self._primary:\n            <IND>batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        <DED>else:\n            <IND>batch_group_generator_tqdm = batch_group_generator\n\n        <DED>self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            <IND>self._batch_num_total = 0\n\n        <DED>done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            <IND>if done_early:\n                <IND>break\n\n            <DED>batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                <IND>for p in param_group[\"params\"]:\n                    <IND>p.grad = None\n\n            <DED><DED>batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                <IND>if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    <IND>done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        <IND>done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                <DED><DED>with amp.autocast(self._use_amp):\n                    <IND>batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        <IND>raise ValueError(\"nan loss encountered\")\n                    <DED>loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        <IND>reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                <DED><DED>if self._scaler is not None:\n                    <IND>self._scaler.scale(loss).backward()\n                <DED>else:\n                    <IND>loss.backward()\n            <DED><DED>if len(batch_group_outputs) <= 0:\n                <IND>continue\n\n            <DED>train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step_batch(batch_num_total)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step_batch(batch_num_total)\n\n            <DED>if self._scaler is not None:\n                <IND>self._scaler.step(self.optimizer)\n                self._scaler.update()\n            <DED>else:\n                <IND>self.optimizer.step()\n\n            # Update moving averages\n            <DED>if self._moving_average is not None:\n                <IND>self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            <DED>metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                <IND>description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    <IND>self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            <DED><DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        <DED>if self._distributed:\n            <IND>dist.barrier()\n\n        <DED>metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            <IND>metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>for (gpu_num, memory) in gpu_memory_usage:\n            <IND>metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>return metrics\n\n    <DED>def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        <IND>\"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>if self._validation_data_loader is not None:\n            <IND>validation_data_loader = self._validation_data_loader\n        <DED>else:\n            <IND>raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            <IND>val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        <DED>else:\n            <IND>val_generator_tqdm = validation_data_loader\n\n        <DED>batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            <IND>if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                <IND>done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    <IND>done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            <DED><DED>with amp.autocast(self._use_amp):\n                <IND>batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    <IND>batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        <IND>val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            <DED><DED><DED>val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                <IND>val_generator_tqdm.set_description(description, refresh=False)\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        <DED>if self._moving_average is not None:\n            <IND>self._moving_average.restore()\n\n        <DED>return val_loss, val_reg_loss, batches_this_epoch\n\n    <DED>def train(self) -> Dict[str, Any]:\n        <IND>\"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            <IND>callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        <DED>epoch = None\n        metrics = None\n\n        try:\n            <IND>metrics, epoch = self._try_train()\n            return metrics\n        <DED>finally:\n            <IND>for callback in self._callbacks:\n                <IND>callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    <DED><DED><DED>def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        <IND>try:\n            <IND>epoch_counter = self._restore_checkpoint()\n        <DED>except RuntimeError:\n            <IND>traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        <DED>training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            <IND>metrics[\"best_validation_\" + key] = value\n\n        <DED>for epoch in range(epoch_counter, self._num_epochs):\n            <IND>epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            # get peak of memory usage\n            <DED>for key, value in train_metrics.items():\n                <IND>if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                <DED>elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            <DED><DED>this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                <IND>with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    <IND>val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        <IND>dist.barrier()\n\n                    <DED>val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            <DED><DED>training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                <IND>metrics[\"training_\" + key] = value\n            <DED>for key, value in val_metrics.items():\n                <IND>metrics[\"validation_\" + key] = value\n\n            <DED>if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                <IND>metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    <IND>metrics[\"best_validation_\" + key] = value\n\n                <DED>self._metric_tracker.best_epoch_metrics = val_metrics\n\n            <DED>if self._serialization_dir and self._primary:\n                <IND>common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            <DED>if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step(this_epoch_val_metric)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            <DED>if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            <DED>epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                <IND>training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            <DED>epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                <IND>logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        <DED><DED>else:\n            <IND>epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        <DED>best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            <IND>self.model.load_state_dict(best_model_state)\n\n        <DED>return metrics, epoch\n\n    <DED>@contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        <IND>if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            <IND>training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        <DED>if self._momentum_scheduler is not None:\n            <IND>training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        <DED>try:\n            <IND>yield model_state, training_states\n        <DED>finally:\n            <IND>if self._moving_average is not None:\n                <IND>self._moving_average.restore()\n\n    <DED><DED><DED>def _restore_checkpoint(self) -> int:\n        <IND>\"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            <IND>return 0\n\n        <DED>model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            <IND>return 0\n\n        <DED>self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            <IND>self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        <DED>if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            <IND>self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        <DED>training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            <IND>self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        <DED>else:\n            <IND>self._metric_tracker.clear()\n\n        <DED>if isinstance(training_state[\"epoch\"], int):\n            <IND>epoch_to_return = training_state[\"epoch\"] + 1\n        <DED>else:\n            <IND>epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        <DED>batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            <IND>self._batch_num_total = batch_num_total\n\n        <DED>return epoch_to_return\n\n    <DED>@classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        <IND>\"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            <IND>from torch import cuda\n\n            if cuda.device_count() > 0:\n                <IND>cuda_device = 0\n            <DED>else:\n                <IND>cuda_device = -1\n\n        <DED><DED>check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            <IND>model = model.cuda(cuda_device)\n\n        <DED>if no_grad:\n            <IND>for name, parameter in model.named_parameters():\n                <IND>if any(re.search(regex, name) for regex in no_grad):\n                    <IND>parameter.requires_grad_(False)\n\n        <DED><DED><DED>parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            <IND>batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        <DED>except TypeError:\n            <IND>batches_per_epoch = None\n\n        <DED>moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            <IND>callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        <DED>return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\n<DED><DED>DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def get_best_weights_path(self) -> Optional[str]:\n        <IND>\"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "c5bff8ba0d835eb03931f10f4f427ffe936cf796",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:1030:8 Incompatible variable type [9]: moving_average is declared to have type `allennlp.common.lazy.Lazy[MovingAverage]` but is used as type `None`.",
    "message": " moving_average is declared to have type `allennlp.common.lazy.Lazy[MovingAverage]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 1030,
    "warning_line": "        moving_average: Lazy[MovingAverage] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n\n@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    \"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            self._validation_data_loader.set_target_device(self.cuda_device)\n        self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            if validation_data_loader is not None:\n                logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        elif (not isinstance(patience, int)) or patience <= 0:\n            raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            self._checkpointer = Checkpointer(serialization_dir)\n\n        self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            default_callbacks.append(ConfidenceChecksCallback)\n        for callback_cls in default_callbacks:\n            for callback in self._callbacks:\n                if callback.__class__ == callback_cls:\n                    break\n            else:\n                self._callbacks.append(callback_cls(self._serialization_dir))\n\n        self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            if self.cuda_device == torch.device(\"cpu\"):\n                raise ValueError(\"Using AMP requires a cuda device\")\n            self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        if self._distributed:\n            self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        else:\n            self._pytorch_model = self.model\n\n    def rescale_gradients(self) -> float:\n        \"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                self._scaler.unscale_(self.optimizer)\n            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        else:\n            return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            try:\n                assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            except AssertionError:\n                if for_training:\n                    raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        return output_dict\n\n    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        \"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        except TypeError:\n            num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        else:\n            batch_group_generator_tqdm = batch_group_generator\n\n        self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            self._batch_num_total = 0\n\n        done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            if done_early:\n                break\n\n            batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                for p in param_group[\"params\"]:\n                    p.grad = None\n\n            batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                with amp.autocast(self._use_amp):\n                    batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        raise ValueError(\"nan loss encountered\")\n                    loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                if self._scaler is not None:\n                    self._scaler.scale(loss).backward()\n                else:\n                    loss.backward()\n            if len(batch_group_outputs) <= 0:\n                continue\n\n            train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step_batch(batch_num_total)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step_batch(batch_num_total)\n\n            if self._scaler is not None:\n                self._scaler.step(self.optimizer)\n                self._scaler.update()\n            else:\n                self.optimizer.step()\n\n            # Update moving averages\n            if self._moving_average is not None:\n                self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        if self._distributed:\n            dist.barrier()\n\n        metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        for (gpu_num, memory) in gpu_memory_usage:\n            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        return metrics\n\n    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        \"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            self._moving_average.assign_average_value()\n\n        if self._validation_data_loader is not None:\n            validation_data_loader = self._validation_data_loader\n        else:\n            raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        else:\n            val_generator_tqdm = validation_data_loader\n\n        batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            with amp.autocast(self._use_amp):\n                batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                val_generator_tqdm.set_description(description, refresh=False)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        if self._moving_average is not None:\n            self._moving_average.restore()\n\n        return val_loss, val_reg_loss, batches_this_epoch\n\n    def train(self) -> Dict[str, Any]:\n        \"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        epoch = None\n        metrics = None\n\n        try:\n            metrics, epoch = self._try_train()\n            return metrics\n        finally:\n            for callback in self._callbacks:\n                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        try:\n            epoch_counter = self._restore_checkpoint()\n        except RuntimeError:\n            traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            metrics[\"best_validation_\" + key] = value\n\n        for epoch in range(epoch_counter, self._num_epochs):\n            epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            # get peak of memory usage\n            for key, value in train_metrics.items():\n                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        dist.barrier()\n\n                    val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                metrics[\"training_\" + key] = value\n            for key, value in val_metrics.items():\n                metrics[\"validation_\" + key] = value\n\n            if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    metrics[\"best_validation_\" + key] = value\n\n                self._metric_tracker.best_epoch_metrics = val_metrics\n\n            if self._serialization_dir and self._primary:\n                common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step(this_epoch_val_metric)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            for callback in self._callbacks:\n                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        else:\n            epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            self.model.load_state_dict(best_model_state)\n\n        return metrics, epoch\n\n    @contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            self._moving_average.assign_average_value()\n\n        model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        if self._momentum_scheduler is not None:\n            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        try:\n            yield model_state, training_states\n        finally:\n            if self._moving_average is not None:\n                self._moving_average.restore()\n\n    def _restore_checkpoint(self) -> int:\n        \"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            return 0\n\n        model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            return 0\n\n        self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        else:\n            self._metric_tracker.clear()\n\n        if isinstance(training_state[\"epoch\"], int):\n            epoch_to_return = training_state[\"epoch\"] + 1\n        else:\n            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            self._batch_num_total = batch_num_total\n\n        return epoch_to_return\n\n    @classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        \"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            from torch import cuda\n\n            if cuda.device_count() > 0:\n                cuda_device = 0\n            else:\n                cuda_device = -1\n\n        check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            model = model.cuda(cuda_device)\n\n        if no_grad:\n            for name, parameter in model.named_parameters():\n                if any(re.search(regex, name) for regex in no_grad):\n                    parameter.requires_grad_(False)\n\n        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        except TypeError:\n            batches_per_epoch = None\n\n        moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\nDEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_len": 45826,
        "target_code": "\n    def get_best_weights_path(self) -> Optional[str]:\n        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_len": 151,
        "diff_format": "@@ -115,1021 +89,4 @@\n \n-\n-@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\n-class GradientDescentTrainer(Trainer):\n-    \"\"\"\n-    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n-    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n-    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n-    stopping. There are many other bells and whistles as well.\n-\n-    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n-    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n-    see the arguments to that function for the exact keys that should be used, if you are using\n-    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n-    docstrings in `from_partial_objects`.\n-\n-    [0]: https://tinyurl.com/y5mv44fw\n-\n-    # Parameters\n-\n-    model : `Model`, required.\n-        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n-        their `forward` method returns a dictionary with a \"loss\" key, containing a\n-        scalar tensor representing the loss function to be optimized.\n-\n-        If you are training your model using GPUs, your model should already be\n-        on the correct device. (If you are using our `train` command this will be\n-        handled for you.)\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    optimizer : `torch.nn.Optimizer`, required.\n-        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n-        model to be optimized.\n-\n-    data_loader : `DataLoader`, required.\n-        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    patience : `Optional[int] > 0`, optional (default=`None`)\n-        Number of epochs to be patient before early stopping: the training is stopped\n-        after `patience` epochs with no improvement. If given, it must be `> 0`.\n-        If None, early stopping is disabled.\n-\n-    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n-        Validation metric to measure for whether to stop training using patience\n-        and whether to serialize an `is_best` model each epoch. The metric name\n-        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n-        is an increasing or decreasing function. If you specify more than one metric,\n-        the metrics will be summed to make the `is_best` decision.\n-\n-    validation_data_loader : `DataLoader`, optional (default=`None`)\n-        A `DataLoader` to use for the validation set.  If `None`, then\n-        use the training `DataLoader` with the validation data.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_epochs : `int`, optional (default = `20`)\n-        Number of training epochs.\n-\n-    serialization_dir : `str`, optional (default=`None`)\n-        Path to directory for saving and loading model files. Models will not be saved if\n-        this parameter is not passed.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    checkpointer : `Checkpointer`, optional (default=`None`)\n-        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n-        here, we will construct one with default parameters.\n-\n-    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n-        An integer or `torch.device` specifying the CUDA device to use for this process.\n-        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n-\n-        !!! Note\n-            If you *don't* intend to use a GPU, but you have one available, you'll need\n-            to explicitly set `cuda_device=-1`.\n-\n-        !!! Note\n-            If you intend to use a GPU, your model already needs to be on the correct device,\n-            which you can do with `model = model.cuda()`.\n-\n-        !!! Note\n-            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n-\n-    grad_norm : `float`, optional, (default = `None`).\n-        If provided, gradient norms will be rescaled to have a maximum of this value.\n-\n-    grad_clipping : `float`, optional (default = `None`).\n-        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n-        maximum of this value.  If you are getting `NaNs` in your gradients during training\n-        that are not solved by using `grad_norm`, you may need this.\n-\n-    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n-        If specified, the learning rate will be decayed with respect to\n-        this schedule at the end of each epoch (or batch, if the scheduler implements\n-        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n-        this will use the `validation_metric` provided to determine if learning has plateaued.\n-        To support updating the learning rate on every batch, this can optionally implement\n-        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n-\n-    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n-        If specified, the momentum will be updated at the end of each batch or epoch\n-        according to the schedule.\n-\n-    moving_average : `MovingAverage`, optional, (default = `None`)\n-        If provided, we will maintain moving averages for all parameters. During training, we\n-        employ a shadow variable for each parameter, which maintains the moving average. During\n-        evaluation, we backup the original parameters and assign the moving averages to corresponding\n-        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n-        parameters. This is necessary because we want the saved model to perform as well as the validated\n-        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n-\n-    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n-        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n-        and end of training, etc.\n-\n-    distributed : `bool`, optional, (default = `False`)\n-        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n-        requires `world_size` to be greater than 1.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n-        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n-\n-    local_rank : `int`, optional, (default = `0`)\n-        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n-        used as the rank.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    world_size : `int`, (default = `1`)\n-        The number of `Trainer` workers participating in the distributed training.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n-        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n-        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n-        post][0] for details on Gradient Accumulation.\n-\n-    use_amp : `bool`, optional, (default = `False`)\n-        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n-\n-    enable_default_callbacks : `bool`, optional (default = `True`)\n-        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n-        addition to any other callbacks listed in the `callbacks` parameter.\n-        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n-\n-    run_confidence_checks : `bool`, optional (default = `True`)\n-        Determines whether model confidence checks, such as\n-        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n-        are run.\n-\n-    run_sanity_checks : `bool`, optional (default = `True`)\n-        This parameter is deprecated. Please use `run_confidence_checks` instead.\n-\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        model: Model,\n-        optimizer: torch.optim.Optimizer,\n-        data_loader: DataLoader,\n-        patience: Optional[int] = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        validation_data_loader: DataLoader = None,\n-        num_epochs: int = 20,\n-        serialization_dir: Optional[str] = None,\n-        checkpointer: Checkpointer = None,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: Optional[float] = None,\n-        grad_clipping: Optional[float] = None,\n-        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n-        momentum_scheduler: Optional[MomentumScheduler] = None,\n-        moving_average: Optional[MovingAverage] = None,\n-        callbacks: List[TrainerCallback] = None,\n-        distributed: bool = False,\n-        local_rank: int = 0,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-        )\n-\n-        if \"run_sanity_checks\" in kwargs:\n-            warnings.warn(\n-                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n-                DeprecationWarning,\n-            )\n-            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n-\n-        # I am not calling move_to_gpu here, because if the model is\n-        # not already on the GPU then the optimizer is going to be wrong.\n-        self.model = model\n-\n-        self.data_loader = data_loader\n-        self.data_loader.set_target_device(self.cuda_device)\n-        self._validation_data_loader = validation_data_loader\n-        if self._validation_data_loader is not None:\n-            self._validation_data_loader.set_target_device(self.cuda_device)\n-        self.optimizer = optimizer\n-\n-        if patience is None:  # no early stopping\n-            if validation_data_loader is not None:\n-                logger.warning(\n-                    \"You provided a validation dataset but patience was set to None, \"\n-                    \"meaning that early stopping is disabled\"\n-                )\n-        elif (not isinstance(patience, int)) or patience <= 0:\n-            raise ConfigurationError(\n-                '{} is an invalid value for \"patience\": it must be a positive integer '\n-                \"or None (if you want to disable early stopping)\".format(patience)\n-            )\n-\n-        # For tracking is_best_so_far and should_stop_early\n-        self._metric_tracker = MetricTracker(validation_metric, patience)\n-\n-        self._num_epochs = num_epochs\n-\n-        self._checkpointer: Optional[Checkpointer] = checkpointer\n-        if checkpointer is None and serialization_dir is not None:\n-            self._checkpointer = Checkpointer(serialization_dir)\n-\n-        self._grad_norm = grad_norm\n-        self._grad_clipping = grad_clipping\n-\n-        self._learning_rate_scheduler = learning_rate_scheduler\n-        self._momentum_scheduler = momentum_scheduler\n-        self._moving_average = moving_average\n-\n-        self._callbacks = callbacks or []\n-        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n-\n-        if run_confidence_checks:\n-            default_callbacks.append(ConfidenceChecksCallback)\n-        for callback_cls in default_callbacks:\n-            for callback in self._callbacks:\n-                if callback.__class__ == callback_cls:\n-                    break\n-            else:\n-                self._callbacks.append(callback_cls(self._serialization_dir))\n-\n-        self._batch_num_total = 0\n-        self._last_log = 0.0  # time of last logging\n-        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n-\n-        # Enable automatic mixed precision training.\n-        self._scaler: Optional[amp.GradScaler] = None\n-        self._use_amp = use_amp\n-        if self._use_amp:\n-            if self.cuda_device == torch.device(\"cpu\"):\n-                raise ValueError(\"Using AMP requires a cuda device\")\n-            self._scaler = amp.GradScaler()\n-\n-        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n-        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n-        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n-        #\n-        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n-        # normal case, reference to `Model` is retained. This reference is only used in\n-        # these places: `model.__call__`, `model.train` and `model.eval`.\n-        if self._distributed:\n-            self._pytorch_model = DistributedDataParallel(\n-                self.model,\n-                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n-                find_unused_parameters=True,\n-            )\n-        else:\n-            self._pytorch_model = self.model\n-\n-    def rescale_gradients(self) -> float:\n-        \"\"\"\n-        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n-\n-        Returns the norm of the gradients.\n-        \"\"\"\n-        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n-        if self._grad_norm:\n-            if self._scaler is not None:\n-                # Need to first unscale gradients in order to clip as usual.\n-                self._scaler.unscale_(self.optimizer)\n-            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n-        else:\n-            return torch.norm(\n-                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n-            )\n-\n-    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n-        \"\"\"\n-        Does a forward pass on the given batch and returns the output dictionary that the model\n-        returns, after adding any specified regularization penalty to the loss (if training).\n-        \"\"\"\n-        output_dict = self._pytorch_model(**batch)\n-\n-        if for_training:\n-            try:\n-                assert \"loss\" in output_dict\n-                regularization_penalty = self.model.get_regularization_penalty()\n-\n-                if regularization_penalty is not None:\n-                    output_dict[\"reg_loss\"] = regularization_penalty\n-                    output_dict[\"loss\"] += regularization_penalty\n-\n-            except AssertionError:\n-                if for_training:\n-                    raise RuntimeError(\n-                        \"The model you are trying to optimize does not contain a\"\n-                        \" 'loss' key in the output of model.forward(inputs).\"\n-                    )\n-\n-        return output_dict\n-\n-    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n-        \"\"\"\n-        Trains one epoch and returns metrics.\n-        \"\"\"\n-        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n-        cpu_memory_usage = []\n-        for worker, memory in common_util.peak_cpu_memory().items():\n-            cpu_memory_usage.append((worker, memory))\n-            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n-        gpu_memory_usage = []\n-        for gpu, memory in common_util.peak_gpu_memory().items():\n-            gpu_memory_usage.append((gpu, memory))\n-            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        train_loss = 0.0\n-        batch_loss = 0.0\n-        train_reg_loss = None if regularization_penalty is None else 0.0\n-        batch_reg_loss = None if regularization_penalty is None else 0.0\n-\n-        # Set the model to \"train\" mode.\n-        self._pytorch_model.train()\n-\n-        # Get tqdm for the training batches\n-        batch_generator = iter(self.data_loader)\n-        batch_group_generator = common_util.lazy_groups_of(\n-            batch_generator, self._num_gradient_accumulation_steps\n-        )\n-\n-        logger.info(\"Training\")\n-\n-        num_training_batches: Union[int, float]\n-        try:\n-            len_data_loader = len(self.data_loader)\n-            num_training_batches = math.ceil(\n-                len_data_loader / self._num_gradient_accumulation_steps\n-            )\n-        except TypeError:\n-            num_training_batches = float(\"inf\")\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            batch_group_generator_tqdm = Tqdm.tqdm(\n-                batch_group_generator, total=num_training_batches\n-            )\n-        else:\n-            batch_group_generator_tqdm = batch_group_generator\n-\n-        self._last_log = time.time()\n-\n-        batches_this_epoch = 0\n-        if self._batch_num_total is None:\n-            self._batch_num_total = 0\n-\n-        done_early = False\n-        for batch_group in batch_group_generator_tqdm:\n-            if done_early:\n-                break\n-\n-            batches_this_epoch += 1\n-            self._batch_num_total += 1\n-            batch_num_total = self._batch_num_total\n-\n-            # Zero gradients.\n-            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n-            # because it avoids a read op when the gradients are first updated below.\n-            for param_group in self.optimizer.param_groups:\n-                for p in param_group[\"params\"]:\n-                    p.grad = None\n-\n-            batch_loss = 0.0\n-            batch_group_outputs = []\n-            for batch in batch_group:\n-                if self._distributed:\n-                    # Check whether the other workers have stopped already (due to differing amounts of\n-                    # data in each). If so, we can't proceed because we would hang when we hit the\n-                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                    # here because NCCL process groups apparently don't support BoolTensor.\n-                    done = torch.tensor(0, device=self.cuda_device)\n-                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                    if done.item() > 0:\n-                        done_early = True\n-                        logger.warning(\n-                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n-                            \"This implies that there is an imbalance in your training \"\n-                            \"data across the workers and that some amount of it will be \"\n-                            \"ignored. A small amount of this is fine, but a major imbalance \"\n-                            \"should be avoided. Note: This warning will appear unless your \"\n-                            \"data is perfectly balanced.\"\n-                        )\n-                        break\n-\n-                with amp.autocast(self._use_amp):\n-                    batch_outputs = self.batch_outputs(batch, for_training=True)\n-                    batch_group_outputs.append(batch_outputs)\n-                    loss = batch_outputs[\"loss\"]\n-                    reg_loss = batch_outputs.get(\"reg_loss\")\n-                    if torch.isnan(loss):\n-                        raise ValueError(\"nan loss encountered\")\n-                    loss = loss / len(batch_group)\n-\n-                    batch_loss += loss.item()\n-                    if reg_loss is not None:\n-                        reg_loss = reg_loss / len(batch_group)\n-                        batch_reg_loss = reg_loss.item()\n-                        train_reg_loss += batch_reg_loss  # type: ignore\n-\n-                if self._scaler is not None:\n-                    self._scaler.scale(loss).backward()\n-                else:\n-                    loss.backward()\n-            if len(batch_group_outputs) <= 0:\n-                continue\n-\n-            train_loss += batch_loss\n-\n-            batch_grad_norm = self.rescale_gradients()\n-\n-            # This does nothing if batch_num_total is None or you are using a\n-            # scheduler which doesn't update per batch.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step_batch(batch_num_total)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step_batch(batch_num_total)\n-\n-            if self._scaler is not None:\n-                self._scaler.step(self.optimizer)\n-                self._scaler.update()\n-            else:\n-                self.optimizer.step()\n-\n-            # Update moving averages\n-            if self._moving_average is not None:\n-                self._moving_average.apply(batch_num_total)\n-\n-            # Update the description with the latest metrics\n-            metrics = training_util.get_metrics(\n-                self.model,\n-                train_loss,\n-                train_reg_loss,\n-                batch_loss,\n-                batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            if self._primary:\n-                # Updating tqdm only for the primary as the trainers wouldn't have one\n-                description = training_util.description_from_metrics(metrics)\n-                batch_group_generator_tqdm.set_description(description, refresh=False)\n-\n-                if self._checkpointer is not None:\n-                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    batch_group,\n-                    batch_group_outputs,\n-                    metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=True,\n-                    is_primary=self._primary,\n-                    batch_grad_norm=batch_grad_norm,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Let all workers finish their epoch before computing\n-        # the final statistics for the epoch.\n-        if self._distributed:\n-            dist.barrier()\n-\n-        metrics = training_util.get_metrics(\n-            self.model,\n-            train_loss,\n-            train_reg_loss,\n-            batch_loss=None,\n-            batch_reg_loss=None,\n-            num_batches=batches_this_epoch,\n-            reset=True,\n-            world_size=self._world_size,\n-            cuda_device=self.cuda_device,\n-        )\n-\n-        for (worker, memory) in cpu_memory_usage:\n-            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        for (gpu_num, memory) in gpu_memory_usage:\n-            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        return metrics\n-\n-    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n-        \"\"\"\n-        Computes the validation loss. Returns it and the number of batches.\n-        \"\"\"\n-        logger.info(\"Validating\")\n-\n-        self._pytorch_model.eval()\n-\n-        # Replace parameter values with the shadow values from the moving averages.\n-        if self._moving_average is not None:\n-            self._moving_average.assign_average_value()\n-\n-        if self._validation_data_loader is not None:\n-            validation_data_loader = self._validation_data_loader\n-        else:\n-            raise ConfigurationError(\n-                \"Validation results cannot be calculated without a validation_data_loader\"\n-            )\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n-        else:\n-            val_generator_tqdm = validation_data_loader\n-\n-        batches_this_epoch = 0\n-        val_loss = 0.0\n-        val_batch_loss = 0.0\n-        val_reg_loss = None if regularization_penalty is None else 0.0\n-        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n-        done_early = False\n-        for batch in val_generator_tqdm:\n-            if self._distributed:\n-                # Check whether the other workers have stopped already (due to differing amounts of\n-                # data in each). If so, we can't proceed because we would hang when we hit the\n-                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                # here because NCCL process groups apparently don't support BoolTensor.\n-                done = torch.tensor(0, device=self.cuda_device)\n-                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                if done.item() > 0:\n-                    done_early = True\n-                    logger.warning(\n-                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n-                        \"This implies that there is an imbalance in your validation \"\n-                        \"data across the workers and that some amount of it will be \"\n-                        \"ignored. A small amount of this is fine, but a major imbalance \"\n-                        \"should be avoided. Note: This warning will appear unless your \"\n-                        \"data is perfectly balanced.\"\n-                    )\n-                    break\n-\n-            with amp.autocast(self._use_amp):\n-                batch_outputs = self.batch_outputs(batch, for_training=False)\n-                loss = batch_outputs.get(\"loss\")\n-                reg_loss = batch_outputs.get(\"reg_loss\")\n-                if loss is not None:\n-                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n-                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n-                    # currently only used as the divisor for the loss function, so we can safely only\n-                    # count those batches for which we actually have a loss.  If this variable ever\n-                    # gets used for something else, we might need to change things around a bit.\n-                    batches_this_epoch += 1\n-                    val_batch_loss = loss.item()\n-                    val_loss += val_batch_loss\n-                    if reg_loss is not None:\n-                        val_batch_reg_loss = reg_loss.item()\n-                        val_reg_loss += val_batch_reg_loss  # type: ignore\n-\n-            # Update the description with the latest metrics\n-            val_metrics = training_util.get_metrics(\n-                self.model,\n-                val_loss,\n-                val_reg_loss,\n-                val_batch_loss,\n-                val_batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            description = training_util.description_from_metrics(val_metrics)\n-            if self._primary:\n-                val_generator_tqdm.set_description(description, refresh=False)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    [batch],\n-                    [batch_outputs],\n-                    val_metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=False,\n-                    is_primary=self._primary,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop validation early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Now restore the original parameter values.\n-        if self._moving_average is not None:\n-            self._moving_average.restore()\n-\n-        return val_loss, val_reg_loss, batches_this_epoch\n-\n-    def train(self) -> Dict[str, Any]:\n-        \"\"\"\n-        Trains the supplied model with the supplied parameters.\n-        \"\"\"\n-\n-        for callback in self._callbacks:\n-            callback.on_start(self, is_primary=self._primary)\n-\n-        # Set default values in case of failure\n-        epoch = None\n-        metrics = None\n-\n-        try:\n-            metrics, epoch = self._try_train()\n-            return metrics\n-        finally:\n-            for callback in self._callbacks:\n-                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n-        try:\n-            epoch_counter = self._restore_checkpoint()\n-        except RuntimeError:\n-            traceback.print_exc()\n-            raise ConfigurationError(\n-                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n-                \"a different serialization directory or delete the existing serialization \"\n-                \"directory?\"\n-            )\n-\n-        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n-\n-        logger.info(\"Beginning training.\")\n-\n-        val_metrics: Dict[str, float] = {}\n-        metrics: Dict[str, Any] = {}\n-        epochs_trained = 0\n-        training_start_time = time.time()\n-\n-        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n-        for key, value in self._metric_tracker.best_epoch_metrics.items():\n-            metrics[\"best_validation_\" + key] = value\n-\n-        for epoch in range(epoch_counter, self._num_epochs):\n-            epoch_start_time = time.time()\n-            train_metrics = self._train_epoch(epoch)\n-\n-            # Back up the model now, in case something goes wrong later with the evaluation\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.shelve_model(epoch, self)\n-            # Wait for the primary process to finish saving the model checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            # get peak of memory usage\n-            for key, value in train_metrics.items():\n-                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-\n-            this_epoch_val_metric: float = 0.0\n-            if self._validation_data_loader is not None:\n-                with torch.no_grad():\n-                    # We have a validation set, so compute all the metrics on it.\n-                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n-\n-                    # It is safe again to wait till the validation is done. This is\n-                    # important to get the metrics right.\n-                    if self._distributed:\n-                        dist.barrier()\n-\n-                    val_metrics = training_util.get_metrics(\n-                        self.model,\n-                        val_loss,\n-                        val_reg_loss,\n-                        batch_loss=None,\n-                        batch_reg_loss=None,\n-                        num_batches=num_batches,\n-                        reset=True,\n-                        world_size=self._world_size,\n-                        cuda_device=self.cuda_device,\n-                    )\n-\n-                    # Check validation metric for early stopping\n-                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n-                    self._metric_tracker.add_metrics(val_metrics)\n-\n-            # Create overall metrics dict\n-            training_elapsed_time = time.time() - training_start_time\n-            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n-            metrics[\"training_start_epoch\"] = epoch_counter\n-            metrics[\"training_epochs\"] = epochs_trained\n-            metrics[\"epoch\"] = epoch\n-\n-            for key, value in train_metrics.items():\n-                metrics[\"training_\" + key] = value\n-            for key, value in val_metrics.items():\n-                metrics[\"validation_\" + key] = value\n-\n-            if self._metric_tracker.is_best_so_far():\n-                # Update all the best_ metrics.\n-                # (Otherwise they just stay the same as they were.)\n-                metrics[\"best_epoch\"] = epoch\n-                for key, value in val_metrics.items():\n-                    metrics[\"best_validation_\" + key] = value\n-\n-                self._metric_tracker.best_epoch_metrics = val_metrics\n-\n-            if self._serialization_dir and self._primary:\n-                common_util.dump_metrics(\n-                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n-                    metrics,\n-                )\n-\n-            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n-            # if it doesn't, the validation metric passed here is ignored.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step(this_epoch_val_metric)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step(this_epoch_val_metric)\n-\n-            # The checkpointer saves state from the learning rate scheduler and the momentum\n-            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.save_checkpoint(\n-                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n-                )\n-            # Wait for the primary process to finish saving the checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            for callback in self._callbacks:\n-                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-            epoch_elapsed_time = time.time() - epoch_start_time\n-            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n-\n-            if epoch < self._num_epochs - 1:\n-                training_elapsed_time = time.time() - training_start_time\n-                estimated_time_remaining = training_elapsed_time * (\n-                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n-                )\n-                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n-                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n-\n-            epochs_trained += 1\n-\n-            if self._metric_tracker.should_stop_early():\n-                logger.info(\"Ran out of patience. Stopping training.\")\n-                break\n-        else:\n-            epoch = self._num_epochs - 1\n-\n-        # Load the best model state before returning\n-        best_model_state = (\n-            None if self._checkpointer is None else self._checkpointer.best_model_state()\n-        )\n-        if best_model_state:\n-            self.model.load_state_dict(best_model_state)\n-\n-        return metrics, epoch\n-\n-    @contextmanager\n-    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n-        if self._moving_average is not None:\n-            # Assigning average value to model parameters.  The checkpointer will call\n-            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n-            self._moving_average.assign_average_value()\n-\n-        model_state = self.model.state_dict()\n-\n-        # These are the training states we need to persist.\n-        training_states = {\n-            \"metric_tracker\": self._metric_tracker.state_dict(),\n-            \"optimizer\": self.optimizer.state_dict(),\n-            \"batch_num_total\": self._batch_num_total,\n-        }\n-\n-        # If we have a learning rate or momentum scheduler, we should persist them too.\n-        if self._learning_rate_scheduler is not None:\n-            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n-        if self._momentum_scheduler is not None:\n-            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n-\n-        try:\n-            yield model_state, training_states\n-        finally:\n-            if self._moving_average is not None:\n-                self._moving_average.restore()\n-\n-    def _restore_checkpoint(self) -> int:\n-        \"\"\"\n-        Restores the model and training state from the last saved checkpoint.\n-        This includes an epoch count and optimizer state, which is serialized separately\n-        from model parameters. This function should only be used to continue training -\n-        if you wish to load a model for inference/load parts of a model into a new\n-        computation graph, you should use the native Pytorch functions:\n-        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n-\n-        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n-        this function will do nothing and return 0.\n-\n-        # Returns\n-\n-        epoch: `int`\n-            The epoch at which to resume training, which should be one after the epoch\n-            in the saved training state.\n-        \"\"\"\n-        if self._checkpointer is None:\n-            return 0\n-\n-        model_state, training_state = self._checkpointer.restore_checkpoint()\n-\n-        if not training_state:\n-            # No checkpoint to restore, start at 0\n-            return 0\n-\n-        self.model.load_state_dict(model_state)\n-        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n-        if (\n-            self._learning_rate_scheduler is not None\n-            and \"learning_rate_scheduler\" in training_state\n-        ):\n-            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n-        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n-            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n-        training_util.move_optimizer_to_cuda(self.optimizer)\n-\n-        # Currently the `training_state` contains a serialized `MetricTracker`.\n-        if \"metric_tracker\" in training_state:\n-            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n-        else:\n-            self._metric_tracker.clear()\n-\n-        if isinstance(training_state[\"epoch\"], int):\n-            epoch_to_return = training_state[\"epoch\"] + 1\n-        else:\n-            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n-\n-        # For older checkpoints with batch_num_total missing, default to old behavior where\n-        # it is unchanged.\n-        batch_num_total = training_state.get(\"batch_num_total\")\n-        if batch_num_total is not None:\n-            self._batch_num_total = batch_num_total\n-\n-        return epoch_to_return\n-\n-    @classmethod\n-    def from_partial_objects(\n-        cls,\n-        model: Model,\n-        serialization_dir: str,\n-        data_loader: DataLoader,\n-        validation_data_loader: DataLoader = None,\n-        local_rank: int = 0,\n-        patience: int = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        num_epochs: int = 20,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: float = None,\n-        grad_clipping: float = None,\n-        distributed: bool = False,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        no_grad: List[str] = None,\n-        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n-        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n-        momentum_scheduler: Lazy[MomentumScheduler] = None,\n-        moving_average: Lazy[MovingAverage] = None,\n-        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n-        callbacks: List[Lazy[TrainerCallback]] = None,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> \"Trainer\":\n-        \"\"\"\n-        This method exists so that we can have a documented method to construct this class using\n-        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n-        method.\n-\n-        The reason we can't just use `__init__` with `FromParams` here is because there are\n-        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n-        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n-        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n-        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n-        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n-\n-        If you're not using `FromParams`, you can just construct these arguments in the right order\n-        yourself in your code and call the constructor directly.\n-        \"\"\"\n-        if cuda_device is None:\n-            from torch import cuda\n-\n-            if cuda.device_count() > 0:\n-                cuda_device = 0\n-            else:\n-                cuda_device = -1\n-\n-        check_for_gpu(cuda_device)\n-        if cuda_device >= 0:\n-            # Moving model to GPU here so that the optimizer state gets constructed on\n-            # the right device.\n-            model = model.cuda(cuda_device)\n-\n-        if no_grad:\n-            for name, parameter in model.named_parameters():\n-                if any(re.search(regex, name) for regex in no_grad):\n-                    parameter.requires_grad_(False)\n-\n-        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n-        optimizer_ = optimizer.construct(model_parameters=parameters)\n-\n-        common_util.log_frozen_and_tunable_parameter_names(model)\n-\n-        batches_per_epoch: Optional[int]\n-        try:\n-            batches_per_epoch = len(data_loader)\n-            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n-        except TypeError:\n-            batches_per_epoch = None\n-\n-        moving_average_ = (\n-            None if moving_average is None else moving_average.construct(parameters=parameters)\n-        )\n-        learning_rate_scheduler_ = (\n-            None\n-            if learning_rate_scheduler is None\n-            else learning_rate_scheduler.construct(\n-                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n-            )\n-        )\n-        momentum_scheduler_ = (\n-            None\n-            if momentum_scheduler is None\n-            else momentum_scheduler.construct(optimizer=optimizer_)\n-        )\n-        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n-\n-        callbacks_: List[TrainerCallback] = []\n-        for callback_ in callbacks or []:\n-            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n-\n-        return cls(\n-            model,\n-            optimizer_,\n-            data_loader,\n-            patience=patience,\n-            validation_metric=validation_metric,\n-            validation_data_loader=validation_data_loader,\n-            num_epochs=num_epochs,\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            grad_norm=grad_norm,\n-            grad_clipping=grad_clipping,\n-            learning_rate_scheduler=learning_rate_scheduler_,\n-            momentum_scheduler=momentum_scheduler_,\n-            checkpointer=checkpointer_,\n-            moving_average=moving_average_,\n-            callbacks=callbacks_,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n-            use_amp=use_amp,\n-            enable_default_callbacks=enable_default_callbacks,\n-            run_confidence_checks=run_confidence_checks,\n-            **kwargs,\n-        )\n-\n-\n-DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n-\"\"\"\n-The default callbacks used by `GradientDescentTrainer`.\n-\"\"\"\n+    def get_best_weights_path(self) -> Optional[str]:\n+        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n+        return None\n",
        "source_code_with_indent": "\n\n<DED><DED>@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    <IND>\"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        <IND>super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            <IND>warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        <DED>self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            <IND>self._validation_data_loader.set_target_device(self.cuda_device)\n        <DED>self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            <IND>if validation_data_loader is not None:\n                <IND>logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        <DED><DED>elif (not isinstance(patience, int)) or patience <= 0:\n            <IND>raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        <DED>self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            <IND>self._checkpointer = Checkpointer(serialization_dir)\n\n        <DED>self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            <IND>default_callbacks.append(ConfidenceChecksCallback)\n        <DED>for callback_cls in default_callbacks:\n            <IND>for callback in self._callbacks:\n                <IND>if callback.__class__ == callback_cls:\n                    <IND>break\n            <DED><DED>else:\n                <IND>self._callbacks.append(callback_cls(self._serialization_dir))\n\n        <DED><DED>self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            <IND>if self.cuda_device == torch.device(\"cpu\"):\n                <IND>raise ValueError(\"Using AMP requires a cuda device\")\n            <DED>self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        <DED>if self._distributed:\n            <IND>self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        <DED>else:\n            <IND>self._pytorch_model = self.model\n\n    <DED><DED>def rescale_gradients(self) -> float:\n        <IND>\"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            <IND>if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                <IND>self._scaler.unscale_(self.optimizer)\n            <DED>return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        <DED>else:\n            <IND>return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    <DED><DED>def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        <IND>\"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            <IND>try:\n                <IND>assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    <IND>output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            <DED><DED>except AssertionError:\n                <IND>if for_training:\n                    <IND>raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        <DED><DED><DED>return output_dict\n\n    <DED>def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        <IND>\"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            <IND>cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        <DED>gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            <IND>gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            <IND>len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        <DED>except TypeError:\n            <IND>num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        <DED>if self._primary:\n            <IND>batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        <DED>else:\n            <IND>batch_group_generator_tqdm = batch_group_generator\n\n        <DED>self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            <IND>self._batch_num_total = 0\n\n        <DED>done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            <IND>if done_early:\n                <IND>break\n\n            <DED>batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                <IND>for p in param_group[\"params\"]:\n                    <IND>p.grad = None\n\n            <DED><DED>batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                <IND>if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    <IND>done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        <IND>done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                <DED><DED>with amp.autocast(self._use_amp):\n                    <IND>batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        <IND>raise ValueError(\"nan loss encountered\")\n                    <DED>loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        <IND>reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                <DED><DED>if self._scaler is not None:\n                    <IND>self._scaler.scale(loss).backward()\n                <DED>else:\n                    <IND>loss.backward()\n            <DED><DED>if len(batch_group_outputs) <= 0:\n                <IND>continue\n\n            <DED>train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step_batch(batch_num_total)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step_batch(batch_num_total)\n\n            <DED>if self._scaler is not None:\n                <IND>self._scaler.step(self.optimizer)\n                self._scaler.update()\n            <DED>else:\n                <IND>self.optimizer.step()\n\n            # Update moving averages\n            <DED>if self._moving_average is not None:\n                <IND>self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            <DED>metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                <IND>description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    <IND>self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            <DED><DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        <DED>if self._distributed:\n            <IND>dist.barrier()\n\n        <DED>metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            <IND>metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>for (gpu_num, memory) in gpu_memory_usage:\n            <IND>metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>return metrics\n\n    <DED>def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        <IND>\"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>if self._validation_data_loader is not None:\n            <IND>validation_data_loader = self._validation_data_loader\n        <DED>else:\n            <IND>raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            <IND>val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        <DED>else:\n            <IND>val_generator_tqdm = validation_data_loader\n\n        <DED>batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            <IND>if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                <IND>done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    <IND>done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            <DED><DED>with amp.autocast(self._use_amp):\n                <IND>batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    <IND>batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        <IND>val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            <DED><DED><DED>val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                <IND>val_generator_tqdm.set_description(description, refresh=False)\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        <DED>if self._moving_average is not None:\n            <IND>self._moving_average.restore()\n\n        <DED>return val_loss, val_reg_loss, batches_this_epoch\n\n    <DED>def train(self) -> Dict[str, Any]:\n        <IND>\"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            <IND>callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        <DED>epoch = None\n        metrics = None\n\n        try:\n            <IND>metrics, epoch = self._try_train()\n            return metrics\n        <DED>finally:\n            <IND>for callback in self._callbacks:\n                <IND>callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    <DED><DED><DED>def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        <IND>try:\n            <IND>epoch_counter = self._restore_checkpoint()\n        <DED>except RuntimeError:\n            <IND>traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        <DED>training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            <IND>metrics[\"best_validation_\" + key] = value\n\n        <DED>for epoch in range(epoch_counter, self._num_epochs):\n            <IND>epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            # get peak of memory usage\n            <DED>for key, value in train_metrics.items():\n                <IND>if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                <DED>elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            <DED><DED>this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                <IND>with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    <IND>val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        <IND>dist.barrier()\n\n                    <DED>val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            <DED><DED>training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                <IND>metrics[\"training_\" + key] = value\n            <DED>for key, value in val_metrics.items():\n                <IND>metrics[\"validation_\" + key] = value\n\n            <DED>if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                <IND>metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    <IND>metrics[\"best_validation_\" + key] = value\n\n                <DED>self._metric_tracker.best_epoch_metrics = val_metrics\n\n            <DED>if self._serialization_dir and self._primary:\n                <IND>common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            <DED>if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step(this_epoch_val_metric)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            <DED>if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            <DED>epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                <IND>training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            <DED>epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                <IND>logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        <DED><DED>else:\n            <IND>epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        <DED>best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            <IND>self.model.load_state_dict(best_model_state)\n\n        <DED>return metrics, epoch\n\n    <DED>@contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        <IND>if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            <IND>training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        <DED>if self._momentum_scheduler is not None:\n            <IND>training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        <DED>try:\n            <IND>yield model_state, training_states\n        <DED>finally:\n            <IND>if self._moving_average is not None:\n                <IND>self._moving_average.restore()\n\n    <DED><DED><DED>def _restore_checkpoint(self) -> int:\n        <IND>\"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            <IND>return 0\n\n        <DED>model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            <IND>return 0\n\n        <DED>self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            <IND>self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        <DED>if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            <IND>self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        <DED>training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            <IND>self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        <DED>else:\n            <IND>self._metric_tracker.clear()\n\n        <DED>if isinstance(training_state[\"epoch\"], int):\n            <IND>epoch_to_return = training_state[\"epoch\"] + 1\n        <DED>else:\n            <IND>epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        <DED>batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            <IND>self._batch_num_total = batch_num_total\n\n        <DED>return epoch_to_return\n\n    <DED>@classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        <IND>\"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            <IND>from torch import cuda\n\n            if cuda.device_count() > 0:\n                <IND>cuda_device = 0\n            <DED>else:\n                <IND>cuda_device = -1\n\n        <DED><DED>check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            <IND>model = model.cuda(cuda_device)\n\n        <DED>if no_grad:\n            <IND>for name, parameter in model.named_parameters():\n                <IND>if any(re.search(regex, name) for regex in no_grad):\n                    <IND>parameter.requires_grad_(False)\n\n        <DED><DED><DED>parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            <IND>batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        <DED>except TypeError:\n            <IND>batches_per_epoch = None\n\n        <DED>moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            <IND>callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        <DED>return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\n<DED><DED>DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def get_best_weights_path(self) -> Optional[str]:\n        <IND>\"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "c5bff8ba0d835eb03931f10f4f427ffe936cf796",
    "filename": "allennlp/training/trainer.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/training/trainer.py",
    "file_hunks_size": 5,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/training/trainer.py:1032:8 Incompatible variable type [9]: callbacks is declared to have type `List[allennlp.common.lazy.Lazy[allennlp.training.callbacks.callback.TrainerCallback]]` but is used as type `None`.",
    "message": " callbacks is declared to have type `List[allennlp.common.lazy.Lazy[allennlp.training.callbacks.callback.TrainerCallback]]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 1032,
    "warning_line": "        callbacks: List[Lazy[TrainerCallback]] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n\n@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    \"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            self._validation_data_loader.set_target_device(self.cuda_device)\n        self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            if validation_data_loader is not None:\n                logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        elif (not isinstance(patience, int)) or patience <= 0:\n            raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            self._checkpointer = Checkpointer(serialization_dir)\n\n        self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            default_callbacks.append(ConfidenceChecksCallback)\n        for callback_cls in default_callbacks:\n            for callback in self._callbacks:\n                if callback.__class__ == callback_cls:\n                    break\n            else:\n                self._callbacks.append(callback_cls(self._serialization_dir))\n\n        self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            if self.cuda_device == torch.device(\"cpu\"):\n                raise ValueError(\"Using AMP requires a cuda device\")\n            self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        if self._distributed:\n            self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        else:\n            self._pytorch_model = self.model\n\n    def rescale_gradients(self) -> float:\n        \"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                self._scaler.unscale_(self.optimizer)\n            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        else:\n            return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            try:\n                assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            except AssertionError:\n                if for_training:\n                    raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        return output_dict\n\n    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        \"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        except TypeError:\n            num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        else:\n            batch_group_generator_tqdm = batch_group_generator\n\n        self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            self._batch_num_total = 0\n\n        done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            if done_early:\n                break\n\n            batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                for p in param_group[\"params\"]:\n                    p.grad = None\n\n            batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                with amp.autocast(self._use_amp):\n                    batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        raise ValueError(\"nan loss encountered\")\n                    loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                if self._scaler is not None:\n                    self._scaler.scale(loss).backward()\n                else:\n                    loss.backward()\n            if len(batch_group_outputs) <= 0:\n                continue\n\n            train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step_batch(batch_num_total)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step_batch(batch_num_total)\n\n            if self._scaler is not None:\n                self._scaler.step(self.optimizer)\n                self._scaler.update()\n            else:\n                self.optimizer.step()\n\n            # Update moving averages\n            if self._moving_average is not None:\n                self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        if self._distributed:\n            dist.barrier()\n\n        metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        for (gpu_num, memory) in gpu_memory_usage:\n            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        return metrics\n\n    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        \"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            self._moving_average.assign_average_value()\n\n        if self._validation_data_loader is not None:\n            validation_data_loader = self._validation_data_loader\n        else:\n            raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        else:\n            val_generator_tqdm = validation_data_loader\n\n        batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            with amp.autocast(self._use_amp):\n                batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                val_generator_tqdm.set_description(description, refresh=False)\n\n            for callback in self._callbacks:\n                callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        if self._distributed and not done_early:\n            logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        if self._moving_average is not None:\n            self._moving_average.restore()\n\n        return val_loss, val_reg_loss, batches_this_epoch\n\n    def train(self) -> Dict[str, Any]:\n        \"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        epoch = None\n        metrics = None\n\n        try:\n            metrics, epoch = self._try_train()\n            return metrics\n        finally:\n            for callback in self._callbacks:\n                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        try:\n            epoch_counter = self._restore_checkpoint()\n        except RuntimeError:\n            traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            metrics[\"best_validation_\" + key] = value\n\n        for epoch in range(epoch_counter, self._num_epochs):\n            epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            # get peak of memory usage\n            for key, value in train_metrics.items():\n                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        dist.barrier()\n\n                    val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                metrics[\"training_\" + key] = value\n            for key, value in val_metrics.items():\n                metrics[\"validation_\" + key] = value\n\n            if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    metrics[\"best_validation_\" + key] = value\n\n                self._metric_tracker.best_epoch_metrics = val_metrics\n\n            if self._serialization_dir and self._primary:\n                common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            if self._learning_rate_scheduler:\n                self._learning_rate_scheduler.step(this_epoch_val_metric)\n            if self._momentum_scheduler:\n                self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            if self._primary and self._checkpointer is not None:\n                self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            if self._distributed:\n                dist.barrier()\n\n            for callback in self._callbacks:\n                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        else:\n            epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            self.model.load_state_dict(best_model_state)\n\n        return metrics, epoch\n\n    @contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            self._moving_average.assign_average_value()\n\n        model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        if self._momentum_scheduler is not None:\n            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        try:\n            yield model_state, training_states\n        finally:\n            if self._moving_average is not None:\n                self._moving_average.restore()\n\n    def _restore_checkpoint(self) -> int:\n        \"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            return 0\n\n        model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            return 0\n\n        self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        else:\n            self._metric_tracker.clear()\n\n        if isinstance(training_state[\"epoch\"], int):\n            epoch_to_return = training_state[\"epoch\"] + 1\n        else:\n            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            self._batch_num_total = batch_num_total\n\n        return epoch_to_return\n\n    @classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        \"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            from torch import cuda\n\n            if cuda.device_count() > 0:\n                cuda_device = 0\n            else:\n                cuda_device = -1\n\n        check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            model = model.cuda(cuda_device)\n\n        if no_grad:\n            for name, parameter in model.named_parameters():\n                if any(re.search(regex, name) for regex in no_grad):\n                    parameter.requires_grad_(False)\n\n        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        except TypeError:\n            batches_per_epoch = None\n\n        moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\nDEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_len": 45826,
        "target_code": "\n    def get_best_weights_path(self) -> Optional[str]:\n        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_len": 151,
        "diff_format": "@@ -115,1021 +89,4 @@\n \n-\n-@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\n-class GradientDescentTrainer(Trainer):\n-    \"\"\"\n-    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n-    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n-    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n-    stopping. There are many other bells and whistles as well.\n-\n-    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n-    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n-    see the arguments to that function for the exact keys that should be used, if you are using\n-    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n-    docstrings in `from_partial_objects`.\n-\n-    [0]: https://tinyurl.com/y5mv44fw\n-\n-    # Parameters\n-\n-    model : `Model`, required.\n-        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n-        their `forward` method returns a dictionary with a \"loss\" key, containing a\n-        scalar tensor representing the loss function to be optimized.\n-\n-        If you are training your model using GPUs, your model should already be\n-        on the correct device. (If you are using our `train` command this will be\n-        handled for you.)\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    optimizer : `torch.nn.Optimizer`, required.\n-        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n-        model to be optimized.\n-\n-    data_loader : `DataLoader`, required.\n-        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    patience : `Optional[int] > 0`, optional (default=`None`)\n-        Number of epochs to be patient before early stopping: the training is stopped\n-        after `patience` epochs with no improvement. If given, it must be `> 0`.\n-        If None, early stopping is disabled.\n-\n-    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n-        Validation metric to measure for whether to stop training using patience\n-        and whether to serialize an `is_best` model each epoch. The metric name\n-        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n-        is an increasing or decreasing function. If you specify more than one metric,\n-        the metrics will be summed to make the `is_best` decision.\n-\n-    validation_data_loader : `DataLoader`, optional (default=`None`)\n-        A `DataLoader` to use for the validation set.  If `None`, then\n-        use the training `DataLoader` with the validation data.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_epochs : `int`, optional (default = `20`)\n-        Number of training epochs.\n-\n-    serialization_dir : `str`, optional (default=`None`)\n-        Path to directory for saving and loading model files. Models will not be saved if\n-        this parameter is not passed.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    checkpointer : `Checkpointer`, optional (default=`None`)\n-        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n-        here, we will construct one with default parameters.\n-\n-    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n-        An integer or `torch.device` specifying the CUDA device to use for this process.\n-        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n-\n-        !!! Note\n-            If you *don't* intend to use a GPU, but you have one available, you'll need\n-            to explicitly set `cuda_device=-1`.\n-\n-        !!! Note\n-            If you intend to use a GPU, your model already needs to be on the correct device,\n-            which you can do with `model = model.cuda()`.\n-\n-        !!! Note\n-            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n-\n-    grad_norm : `float`, optional, (default = `None`).\n-        If provided, gradient norms will be rescaled to have a maximum of this value.\n-\n-    grad_clipping : `float`, optional (default = `None`).\n-        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n-        maximum of this value.  If you are getting `NaNs` in your gradients during training\n-        that are not solved by using `grad_norm`, you may need this.\n-\n-    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n-        If specified, the learning rate will be decayed with respect to\n-        this schedule at the end of each epoch (or batch, if the scheduler implements\n-        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n-        this will use the `validation_metric` provided to determine if learning has plateaued.\n-        To support updating the learning rate on every batch, this can optionally implement\n-        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n-\n-    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n-        If specified, the momentum will be updated at the end of each batch or epoch\n-        according to the schedule.\n-\n-    moving_average : `MovingAverage`, optional, (default = `None`)\n-        If provided, we will maintain moving averages for all parameters. During training, we\n-        employ a shadow variable for each parameter, which maintains the moving average. During\n-        evaluation, we backup the original parameters and assign the moving averages to corresponding\n-        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n-        parameters. This is necessary because we want the saved model to perform as well as the validated\n-        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n-\n-    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n-        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n-        and end of training, etc.\n-\n-    distributed : `bool`, optional, (default = `False`)\n-        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n-        requires `world_size` to be greater than 1.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n-        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n-\n-    local_rank : `int`, optional, (default = `0`)\n-        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n-        used as the rank.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    world_size : `int`, (default = `1`)\n-        The number of `Trainer` workers participating in the distributed training.\n-\n-        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n-        \"trainer\", it gets constructed separately.\n-\n-    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n-        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n-        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n-        post][0] for details on Gradient Accumulation.\n-\n-    use_amp : `bool`, optional, (default = `False`)\n-        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n-\n-    enable_default_callbacks : `bool`, optional (default = `True`)\n-        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n-        addition to any other callbacks listed in the `callbacks` parameter.\n-        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n-\n-    run_confidence_checks : `bool`, optional (default = `True`)\n-        Determines whether model confidence checks, such as\n-        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n-        are run.\n-\n-    run_sanity_checks : `bool`, optional (default = `True`)\n-        This parameter is deprecated. Please use `run_confidence_checks` instead.\n-\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        model: Model,\n-        optimizer: torch.optim.Optimizer,\n-        data_loader: DataLoader,\n-        patience: Optional[int] = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        validation_data_loader: DataLoader = None,\n-        num_epochs: int = 20,\n-        serialization_dir: Optional[str] = None,\n-        checkpointer: Checkpointer = None,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: Optional[float] = None,\n-        grad_clipping: Optional[float] = None,\n-        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n-        momentum_scheduler: Optional[MomentumScheduler] = None,\n-        moving_average: Optional[MovingAverage] = None,\n-        callbacks: List[TrainerCallback] = None,\n-        distributed: bool = False,\n-        local_rank: int = 0,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-        )\n-\n-        if \"run_sanity_checks\" in kwargs:\n-            warnings.warn(\n-                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n-                DeprecationWarning,\n-            )\n-            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n-\n-        # I am not calling move_to_gpu here, because if the model is\n-        # not already on the GPU then the optimizer is going to be wrong.\n-        self.model = model\n-\n-        self.data_loader = data_loader\n-        self.data_loader.set_target_device(self.cuda_device)\n-        self._validation_data_loader = validation_data_loader\n-        if self._validation_data_loader is not None:\n-            self._validation_data_loader.set_target_device(self.cuda_device)\n-        self.optimizer = optimizer\n-\n-        if patience is None:  # no early stopping\n-            if validation_data_loader is not None:\n-                logger.warning(\n-                    \"You provided a validation dataset but patience was set to None, \"\n-                    \"meaning that early stopping is disabled\"\n-                )\n-        elif (not isinstance(patience, int)) or patience <= 0:\n-            raise ConfigurationError(\n-                '{} is an invalid value for \"patience\": it must be a positive integer '\n-                \"or None (if you want to disable early stopping)\".format(patience)\n-            )\n-\n-        # For tracking is_best_so_far and should_stop_early\n-        self._metric_tracker = MetricTracker(validation_metric, patience)\n-\n-        self._num_epochs = num_epochs\n-\n-        self._checkpointer: Optional[Checkpointer] = checkpointer\n-        if checkpointer is None and serialization_dir is not None:\n-            self._checkpointer = Checkpointer(serialization_dir)\n-\n-        self._grad_norm = grad_norm\n-        self._grad_clipping = grad_clipping\n-\n-        self._learning_rate_scheduler = learning_rate_scheduler\n-        self._momentum_scheduler = momentum_scheduler\n-        self._moving_average = moving_average\n-\n-        self._callbacks = callbacks or []\n-        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n-\n-        if run_confidence_checks:\n-            default_callbacks.append(ConfidenceChecksCallback)\n-        for callback_cls in default_callbacks:\n-            for callback in self._callbacks:\n-                if callback.__class__ == callback_cls:\n-                    break\n-            else:\n-                self._callbacks.append(callback_cls(self._serialization_dir))\n-\n-        self._batch_num_total = 0\n-        self._last_log = 0.0  # time of last logging\n-        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n-\n-        # Enable automatic mixed precision training.\n-        self._scaler: Optional[amp.GradScaler] = None\n-        self._use_amp = use_amp\n-        if self._use_amp:\n-            if self.cuda_device == torch.device(\"cpu\"):\n-                raise ValueError(\"Using AMP requires a cuda device\")\n-            self._scaler = amp.GradScaler()\n-\n-        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n-        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n-        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n-        #\n-        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n-        # normal case, reference to `Model` is retained. This reference is only used in\n-        # these places: `model.__call__`, `model.train` and `model.eval`.\n-        if self._distributed:\n-            self._pytorch_model = DistributedDataParallel(\n-                self.model,\n-                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n-                find_unused_parameters=True,\n-            )\n-        else:\n-            self._pytorch_model = self.model\n-\n-    def rescale_gradients(self) -> float:\n-        \"\"\"\n-        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n-\n-        Returns the norm of the gradients.\n-        \"\"\"\n-        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n-        if self._grad_norm:\n-            if self._scaler is not None:\n-                # Need to first unscale gradients in order to clip as usual.\n-                self._scaler.unscale_(self.optimizer)\n-            return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n-        else:\n-            return torch.norm(\n-                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n-            )\n-\n-    def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n-        \"\"\"\n-        Does a forward pass on the given batch and returns the output dictionary that the model\n-        returns, after adding any specified regularization penalty to the loss (if training).\n-        \"\"\"\n-        output_dict = self._pytorch_model(**batch)\n-\n-        if for_training:\n-            try:\n-                assert \"loss\" in output_dict\n-                regularization_penalty = self.model.get_regularization_penalty()\n-\n-                if regularization_penalty is not None:\n-                    output_dict[\"reg_loss\"] = regularization_penalty\n-                    output_dict[\"loss\"] += regularization_penalty\n-\n-            except AssertionError:\n-                if for_training:\n-                    raise RuntimeError(\n-                        \"The model you are trying to optimize does not contain a\"\n-                        \" 'loss' key in the output of model.forward(inputs).\"\n-                    )\n-\n-        return output_dict\n-\n-    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n-        \"\"\"\n-        Trains one epoch and returns metrics.\n-        \"\"\"\n-        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n-        cpu_memory_usage = []\n-        for worker, memory in common_util.peak_cpu_memory().items():\n-            cpu_memory_usage.append((worker, memory))\n-            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n-        gpu_memory_usage = []\n-        for gpu, memory in common_util.peak_gpu_memory().items():\n-            gpu_memory_usage.append((gpu, memory))\n-            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        train_loss = 0.0\n-        batch_loss = 0.0\n-        train_reg_loss = None if regularization_penalty is None else 0.0\n-        batch_reg_loss = None if regularization_penalty is None else 0.0\n-\n-        # Set the model to \"train\" mode.\n-        self._pytorch_model.train()\n-\n-        # Get tqdm for the training batches\n-        batch_generator = iter(self.data_loader)\n-        batch_group_generator = common_util.lazy_groups_of(\n-            batch_generator, self._num_gradient_accumulation_steps\n-        )\n-\n-        logger.info(\"Training\")\n-\n-        num_training_batches: Union[int, float]\n-        try:\n-            len_data_loader = len(self.data_loader)\n-            num_training_batches = math.ceil(\n-                len_data_loader / self._num_gradient_accumulation_steps\n-            )\n-        except TypeError:\n-            num_training_batches = float(\"inf\")\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            batch_group_generator_tqdm = Tqdm.tqdm(\n-                batch_group_generator, total=num_training_batches\n-            )\n-        else:\n-            batch_group_generator_tqdm = batch_group_generator\n-\n-        self._last_log = time.time()\n-\n-        batches_this_epoch = 0\n-        if self._batch_num_total is None:\n-            self._batch_num_total = 0\n-\n-        done_early = False\n-        for batch_group in batch_group_generator_tqdm:\n-            if done_early:\n-                break\n-\n-            batches_this_epoch += 1\n-            self._batch_num_total += 1\n-            batch_num_total = self._batch_num_total\n-\n-            # Zero gradients.\n-            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n-            # because it avoids a read op when the gradients are first updated below.\n-            for param_group in self.optimizer.param_groups:\n-                for p in param_group[\"params\"]:\n-                    p.grad = None\n-\n-            batch_loss = 0.0\n-            batch_group_outputs = []\n-            for batch in batch_group:\n-                if self._distributed:\n-                    # Check whether the other workers have stopped already (due to differing amounts of\n-                    # data in each). If so, we can't proceed because we would hang when we hit the\n-                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                    # here because NCCL process groups apparently don't support BoolTensor.\n-                    done = torch.tensor(0, device=self.cuda_device)\n-                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                    if done.item() > 0:\n-                        done_early = True\n-                        logger.warning(\n-                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n-                            \"This implies that there is an imbalance in your training \"\n-                            \"data across the workers and that some amount of it will be \"\n-                            \"ignored. A small amount of this is fine, but a major imbalance \"\n-                            \"should be avoided. Note: This warning will appear unless your \"\n-                            \"data is perfectly balanced.\"\n-                        )\n-                        break\n-\n-                with amp.autocast(self._use_amp):\n-                    batch_outputs = self.batch_outputs(batch, for_training=True)\n-                    batch_group_outputs.append(batch_outputs)\n-                    loss = batch_outputs[\"loss\"]\n-                    reg_loss = batch_outputs.get(\"reg_loss\")\n-                    if torch.isnan(loss):\n-                        raise ValueError(\"nan loss encountered\")\n-                    loss = loss / len(batch_group)\n-\n-                    batch_loss += loss.item()\n-                    if reg_loss is not None:\n-                        reg_loss = reg_loss / len(batch_group)\n-                        batch_reg_loss = reg_loss.item()\n-                        train_reg_loss += batch_reg_loss  # type: ignore\n-\n-                if self._scaler is not None:\n-                    self._scaler.scale(loss).backward()\n-                else:\n-                    loss.backward()\n-            if len(batch_group_outputs) <= 0:\n-                continue\n-\n-            train_loss += batch_loss\n-\n-            batch_grad_norm = self.rescale_gradients()\n-\n-            # This does nothing if batch_num_total is None or you are using a\n-            # scheduler which doesn't update per batch.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step_batch(batch_num_total)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step_batch(batch_num_total)\n-\n-            if self._scaler is not None:\n-                self._scaler.step(self.optimizer)\n-                self._scaler.update()\n-            else:\n-                self.optimizer.step()\n-\n-            # Update moving averages\n-            if self._moving_average is not None:\n-                self._moving_average.apply(batch_num_total)\n-\n-            # Update the description with the latest metrics\n-            metrics = training_util.get_metrics(\n-                self.model,\n-                train_loss,\n-                train_reg_loss,\n-                batch_loss,\n-                batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            if self._primary:\n-                # Updating tqdm only for the primary as the trainers wouldn't have one\n-                description = training_util.description_from_metrics(metrics)\n-                batch_group_generator_tqdm.set_description(description, refresh=False)\n-\n-                if self._checkpointer is not None:\n-                    self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    batch_group,\n-                    batch_group_outputs,\n-                    metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=True,\n-                    is_primary=self._primary,\n-                    batch_grad_norm=batch_grad_norm,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Let all workers finish their epoch before computing\n-        # the final statistics for the epoch.\n-        if self._distributed:\n-            dist.barrier()\n-\n-        metrics = training_util.get_metrics(\n-            self.model,\n-            train_loss,\n-            train_reg_loss,\n-            batch_loss=None,\n-            batch_reg_loss=None,\n-            num_batches=batches_this_epoch,\n-            reset=True,\n-            world_size=self._world_size,\n-            cuda_device=self.cuda_device,\n-        )\n-\n-        for (worker, memory) in cpu_memory_usage:\n-            metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        for (gpu_num, memory) in gpu_memory_usage:\n-            metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n-        return metrics\n-\n-    def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n-        \"\"\"\n-        Computes the validation loss. Returns it and the number of batches.\n-        \"\"\"\n-        logger.info(\"Validating\")\n-\n-        self._pytorch_model.eval()\n-\n-        # Replace parameter values with the shadow values from the moving averages.\n-        if self._moving_average is not None:\n-            self._moving_average.assign_average_value()\n-\n-        if self._validation_data_loader is not None:\n-            validation_data_loader = self._validation_data_loader\n-        else:\n-            raise ConfigurationError(\n-                \"Validation results cannot be calculated without a validation_data_loader\"\n-            )\n-\n-        regularization_penalty = self.model.get_regularization_penalty()\n-\n-        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n-        # progress is shown\n-        if self._primary:\n-            val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n-        else:\n-            val_generator_tqdm = validation_data_loader\n-\n-        batches_this_epoch = 0\n-        val_loss = 0.0\n-        val_batch_loss = 0.0\n-        val_reg_loss = None if regularization_penalty is None else 0.0\n-        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n-        done_early = False\n-        for batch in val_generator_tqdm:\n-            if self._distributed:\n-                # Check whether the other workers have stopped already (due to differing amounts of\n-                # data in each). If so, we can't proceed because we would hang when we hit the\n-                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n-                # here because NCCL process groups apparently don't support BoolTensor.\n-                done = torch.tensor(0, device=self.cuda_device)\n-                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-                if done.item() > 0:\n-                    done_early = True\n-                    logger.warning(\n-                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n-                        \"This implies that there is an imbalance in your validation \"\n-                        \"data across the workers and that some amount of it will be \"\n-                        \"ignored. A small amount of this is fine, but a major imbalance \"\n-                        \"should be avoided. Note: This warning will appear unless your \"\n-                        \"data is perfectly balanced.\"\n-                    )\n-                    break\n-\n-            with amp.autocast(self._use_amp):\n-                batch_outputs = self.batch_outputs(batch, for_training=False)\n-                loss = batch_outputs.get(\"loss\")\n-                reg_loss = batch_outputs.get(\"reg_loss\")\n-                if loss is not None:\n-                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n-                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n-                    # currently only used as the divisor for the loss function, so we can safely only\n-                    # count those batches for which we actually have a loss.  If this variable ever\n-                    # gets used for something else, we might need to change things around a bit.\n-                    batches_this_epoch += 1\n-                    val_batch_loss = loss.item()\n-                    val_loss += val_batch_loss\n-                    if reg_loss is not None:\n-                        val_batch_reg_loss = reg_loss.item()\n-                        val_reg_loss += val_batch_reg_loss  # type: ignore\n-\n-            # Update the description with the latest metrics\n-            val_metrics = training_util.get_metrics(\n-                self.model,\n-                val_loss,\n-                val_reg_loss,\n-                val_batch_loss,\n-                val_batch_reg_loss,\n-                batches_this_epoch,\n-                world_size=self._world_size,\n-                cuda_device=self.cuda_device,\n-            )\n-\n-            description = training_util.description_from_metrics(val_metrics)\n-            if self._primary:\n-                val_generator_tqdm.set_description(description, refresh=False)\n-\n-            for callback in self._callbacks:\n-                callback.on_batch(\n-                    self,\n-                    [batch],\n-                    [batch_outputs],\n-                    val_metrics,\n-                    epoch,\n-                    batches_this_epoch,\n-                    is_training=False,\n-                    is_primary=self._primary,\n-                )\n-\n-        if self._distributed and not done_early:\n-            logger.warning(\n-                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n-            )\n-            # Indicate that we're done so that any workers that have remaining data stop validation early.\n-            done = torch.tensor(1, device=self.cuda_device)\n-            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n-            assert done.item()\n-\n-        # Now restore the original parameter values.\n-        if self._moving_average is not None:\n-            self._moving_average.restore()\n-\n-        return val_loss, val_reg_loss, batches_this_epoch\n-\n-    def train(self) -> Dict[str, Any]:\n-        \"\"\"\n-        Trains the supplied model with the supplied parameters.\n-        \"\"\"\n-\n-        for callback in self._callbacks:\n-            callback.on_start(self, is_primary=self._primary)\n-\n-        # Set default values in case of failure\n-        epoch = None\n-        metrics = None\n-\n-        try:\n-            metrics, epoch = self._try_train()\n-            return metrics\n-        finally:\n-            for callback in self._callbacks:\n-                callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-    def _try_train(self) -> Tuple[Dict[str, Any], int]:\n-        try:\n-            epoch_counter = self._restore_checkpoint()\n-        except RuntimeError:\n-            traceback.print_exc()\n-            raise ConfigurationError(\n-                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n-                \"a different serialization directory or delete the existing serialization \"\n-                \"directory?\"\n-            )\n-\n-        training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n-\n-        logger.info(\"Beginning training.\")\n-\n-        val_metrics: Dict[str, float] = {}\n-        metrics: Dict[str, Any] = {}\n-        epochs_trained = 0\n-        training_start_time = time.time()\n-\n-        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n-        for key, value in self._metric_tracker.best_epoch_metrics.items():\n-            metrics[\"best_validation_\" + key] = value\n-\n-        for epoch in range(epoch_counter, self._num_epochs):\n-            epoch_start_time = time.time()\n-            train_metrics = self._train_epoch(epoch)\n-\n-            # Back up the model now, in case something goes wrong later with the evaluation\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.shelve_model(epoch, self)\n-            # Wait for the primary process to finish saving the model checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            # get peak of memory usage\n-            for key, value in train_metrics.items():\n-                if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-                elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n-                    metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n-\n-            this_epoch_val_metric: float = 0.0\n-            if self._validation_data_loader is not None:\n-                with torch.no_grad():\n-                    # We have a validation set, so compute all the metrics on it.\n-                    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n-\n-                    # It is safe again to wait till the validation is done. This is\n-                    # important to get the metrics right.\n-                    if self._distributed:\n-                        dist.barrier()\n-\n-                    val_metrics = training_util.get_metrics(\n-                        self.model,\n-                        val_loss,\n-                        val_reg_loss,\n-                        batch_loss=None,\n-                        batch_reg_loss=None,\n-                        num_batches=num_batches,\n-                        reset=True,\n-                        world_size=self._world_size,\n-                        cuda_device=self.cuda_device,\n-                    )\n-\n-                    # Check validation metric for early stopping\n-                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n-                    self._metric_tracker.add_metrics(val_metrics)\n-\n-            # Create overall metrics dict\n-            training_elapsed_time = time.time() - training_start_time\n-            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n-            metrics[\"training_start_epoch\"] = epoch_counter\n-            metrics[\"training_epochs\"] = epochs_trained\n-            metrics[\"epoch\"] = epoch\n-\n-            for key, value in train_metrics.items():\n-                metrics[\"training_\" + key] = value\n-            for key, value in val_metrics.items():\n-                metrics[\"validation_\" + key] = value\n-\n-            if self._metric_tracker.is_best_so_far():\n-                # Update all the best_ metrics.\n-                # (Otherwise they just stay the same as they were.)\n-                metrics[\"best_epoch\"] = epoch\n-                for key, value in val_metrics.items():\n-                    metrics[\"best_validation_\" + key] = value\n-\n-                self._metric_tracker.best_epoch_metrics = val_metrics\n-\n-            if self._serialization_dir and self._primary:\n-                common_util.dump_metrics(\n-                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n-                    metrics,\n-                )\n-\n-            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n-            # if it doesn't, the validation metric passed here is ignored.\n-            if self._learning_rate_scheduler:\n-                self._learning_rate_scheduler.step(this_epoch_val_metric)\n-            if self._momentum_scheduler:\n-                self._momentum_scheduler.step(this_epoch_val_metric)\n-\n-            # The checkpointer saves state from the learning rate scheduler and the momentum\n-            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n-            if self._primary and self._checkpointer is not None:\n-                self._checkpointer.save_checkpoint(\n-                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n-                )\n-            # Wait for the primary process to finish saving the checkpoint\n-            if self._distributed:\n-                dist.barrier()\n-\n-            for callback in self._callbacks:\n-                callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n-\n-            epoch_elapsed_time = time.time() - epoch_start_time\n-            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n-\n-            if epoch < self._num_epochs - 1:\n-                training_elapsed_time = time.time() - training_start_time\n-                estimated_time_remaining = training_elapsed_time * (\n-                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n-                )\n-                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n-                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n-\n-            epochs_trained += 1\n-\n-            if self._metric_tracker.should_stop_early():\n-                logger.info(\"Ran out of patience. Stopping training.\")\n-                break\n-        else:\n-            epoch = self._num_epochs - 1\n-\n-        # Load the best model state before returning\n-        best_model_state = (\n-            None if self._checkpointer is None else self._checkpointer.best_model_state()\n-        )\n-        if best_model_state:\n-            self.model.load_state_dict(best_model_state)\n-\n-        return metrics, epoch\n-\n-    @contextmanager\n-    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n-        if self._moving_average is not None:\n-            # Assigning average value to model parameters.  The checkpointer will call\n-            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n-            self._moving_average.assign_average_value()\n-\n-        model_state = self.model.state_dict()\n-\n-        # These are the training states we need to persist.\n-        training_states = {\n-            \"metric_tracker\": self._metric_tracker.state_dict(),\n-            \"optimizer\": self.optimizer.state_dict(),\n-            \"batch_num_total\": self._batch_num_total,\n-        }\n-\n-        # If we have a learning rate or momentum scheduler, we should persist them too.\n-        if self._learning_rate_scheduler is not None:\n-            training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n-        if self._momentum_scheduler is not None:\n-            training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n-\n-        try:\n-            yield model_state, training_states\n-        finally:\n-            if self._moving_average is not None:\n-                self._moving_average.restore()\n-\n-    def _restore_checkpoint(self) -> int:\n-        \"\"\"\n-        Restores the model and training state from the last saved checkpoint.\n-        This includes an epoch count and optimizer state, which is serialized separately\n-        from model parameters. This function should only be used to continue training -\n-        if you wish to load a model for inference/load parts of a model into a new\n-        computation graph, you should use the native Pytorch functions:\n-        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n-\n-        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n-        this function will do nothing and return 0.\n-\n-        # Returns\n-\n-        epoch: `int`\n-            The epoch at which to resume training, which should be one after the epoch\n-            in the saved training state.\n-        \"\"\"\n-        if self._checkpointer is None:\n-            return 0\n-\n-        model_state, training_state = self._checkpointer.restore_checkpoint()\n-\n-        if not training_state:\n-            # No checkpoint to restore, start at 0\n-            return 0\n-\n-        self.model.load_state_dict(model_state)\n-        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n-        if (\n-            self._learning_rate_scheduler is not None\n-            and \"learning_rate_scheduler\" in training_state\n-        ):\n-            self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n-        if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n-            self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n-        training_util.move_optimizer_to_cuda(self.optimizer)\n-\n-        # Currently the `training_state` contains a serialized `MetricTracker`.\n-        if \"metric_tracker\" in training_state:\n-            self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n-        else:\n-            self._metric_tracker.clear()\n-\n-        if isinstance(training_state[\"epoch\"], int):\n-            epoch_to_return = training_state[\"epoch\"] + 1\n-        else:\n-            epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n-\n-        # For older checkpoints with batch_num_total missing, default to old behavior where\n-        # it is unchanged.\n-        batch_num_total = training_state.get(\"batch_num_total\")\n-        if batch_num_total is not None:\n-            self._batch_num_total = batch_num_total\n-\n-        return epoch_to_return\n-\n-    @classmethod\n-    def from_partial_objects(\n-        cls,\n-        model: Model,\n-        serialization_dir: str,\n-        data_loader: DataLoader,\n-        validation_data_loader: DataLoader = None,\n-        local_rank: int = 0,\n-        patience: int = None,\n-        validation_metric: Union[str, List[str]] = \"-loss\",\n-        num_epochs: int = 20,\n-        cuda_device: Optional[Union[int, torch.device]] = None,\n-        grad_norm: float = None,\n-        grad_clipping: float = None,\n-        distributed: bool = False,\n-        world_size: int = 1,\n-        num_gradient_accumulation_steps: int = 1,\n-        use_amp: bool = False,\n-        no_grad: List[str] = None,\n-        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n-        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n-        momentum_scheduler: Lazy[MomentumScheduler] = None,\n-        moving_average: Lazy[MovingAverage] = None,\n-        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n-        callbacks: List[Lazy[TrainerCallback]] = None,\n-        enable_default_callbacks: bool = True,\n-        run_confidence_checks: bool = True,\n-        **kwargs,\n-    ) -> \"Trainer\":\n-        \"\"\"\n-        This method exists so that we can have a documented method to construct this class using\n-        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n-        method.\n-\n-        The reason we can't just use `__init__` with `FromParams` here is because there are\n-        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n-        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n-        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n-        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n-        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n-\n-        If you're not using `FromParams`, you can just construct these arguments in the right order\n-        yourself in your code and call the constructor directly.\n-        \"\"\"\n-        if cuda_device is None:\n-            from torch import cuda\n-\n-            if cuda.device_count() > 0:\n-                cuda_device = 0\n-            else:\n-                cuda_device = -1\n-\n-        check_for_gpu(cuda_device)\n-        if cuda_device >= 0:\n-            # Moving model to GPU here so that the optimizer state gets constructed on\n-            # the right device.\n-            model = model.cuda(cuda_device)\n-\n-        if no_grad:\n-            for name, parameter in model.named_parameters():\n-                if any(re.search(regex, name) for regex in no_grad):\n-                    parameter.requires_grad_(False)\n-\n-        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n-        optimizer_ = optimizer.construct(model_parameters=parameters)\n-\n-        common_util.log_frozen_and_tunable_parameter_names(model)\n-\n-        batches_per_epoch: Optional[int]\n-        try:\n-            batches_per_epoch = len(data_loader)\n-            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n-        except TypeError:\n-            batches_per_epoch = None\n-\n-        moving_average_ = (\n-            None if moving_average is None else moving_average.construct(parameters=parameters)\n-        )\n-        learning_rate_scheduler_ = (\n-            None\n-            if learning_rate_scheduler is None\n-            else learning_rate_scheduler.construct(\n-                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n-            )\n-        )\n-        momentum_scheduler_ = (\n-            None\n-            if momentum_scheduler is None\n-            else momentum_scheduler.construct(optimizer=optimizer_)\n-        )\n-        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n-\n-        callbacks_: List[TrainerCallback] = []\n-        for callback_ in callbacks or []:\n-            callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n-\n-        return cls(\n-            model,\n-            optimizer_,\n-            data_loader,\n-            patience=patience,\n-            validation_metric=validation_metric,\n-            validation_data_loader=validation_data_loader,\n-            num_epochs=num_epochs,\n-            serialization_dir=serialization_dir,\n-            cuda_device=cuda_device,\n-            grad_norm=grad_norm,\n-            grad_clipping=grad_clipping,\n-            learning_rate_scheduler=learning_rate_scheduler_,\n-            momentum_scheduler=momentum_scheduler_,\n-            checkpointer=checkpointer_,\n-            moving_average=moving_average_,\n-            callbacks=callbacks_,\n-            distributed=distributed,\n-            local_rank=local_rank,\n-            world_size=world_size,\n-            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n-            use_amp=use_amp,\n-            enable_default_callbacks=enable_default_callbacks,\n-            run_confidence_checks=run_confidence_checks,\n-            **kwargs,\n-        )\n-\n-\n-DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n-\"\"\"\n-The default callbacks used by `GradientDescentTrainer`.\n-\"\"\"\n+    def get_best_weights_path(self) -> Optional[str]:\n+        \"\"\"Returns the path to file containing the current best weights.\"\"\"\n+        return None\n",
        "source_code_with_indent": "\n\n<DED><DED>@Trainer.register(\"gradient_descent\", constructor=\"from_partial_objects\")\nclass GradientDescentTrainer(Trainer):\n    <IND>\"\"\"\n    A trainer for doing supervised learning with gradient descent. It just takes a labeled dataset\n    and a `DataLoader`, and uses the supplied `Optimizer` to learn the weights for your model over\n    some fixed number of epochs. You can also pass in a validation data_loader and enable early\n    stopping. There are many other bells and whistles as well.\n\n    Registered as a `Trainer` with the name \"gradient_descent\" (and is also the default `Trainer`).\n    The constructor that is registered is [`from_partial_objects`](#from_partial_objects) -\n    see the arguments to that function for the exact keys that should be used, if you are using\n    a configuration file. They largely match the arguments to `__init__`, and we don't repeat their\n    docstrings in `from_partial_objects`.\n\n    [0]: https://tinyurl.com/y5mv44fw\n\n    # Parameters\n\n    model : `Model`, required.\n        An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n        their `forward` method returns a dictionary with a \"loss\" key, containing a\n        scalar tensor representing the loss function to be optimized.\n\n        If you are training your model using GPUs, your model should already be\n        on the correct device. (If you are using our `train` command this will be\n        handled for you.)\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    optimizer : `torch.nn.Optimizer`, required.\n        An instance of a Pytorch Optimizer, instantiated with the parameters of the\n        model to be optimized.\n\n    data_loader : `DataLoader`, required.\n        A `DataLoader` containing your `Dataset`, yielding padded indexed batches.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    patience : `Optional[int] > 0`, optional (default=`None`)\n        Number of epochs to be patient before early stopping: the training is stopped\n        after `patience` epochs with no improvement. If given, it must be `> 0`.\n        If None, early stopping is disabled.\n\n    validation_metric : `Union[str, List[str]]`, optional (default=`\"-loss\"`)\n        Validation metric to measure for whether to stop training using patience\n        and whether to serialize an `is_best` model each epoch. The metric name\n        must be prepended with either \"+\" or \"-\", which specifies whether the metric\n        is an increasing or decreasing function. If you specify more than one metric,\n        the metrics will be summed to make the `is_best` decision.\n\n    validation_data_loader : `DataLoader`, optional (default=`None`)\n        A `DataLoader` to use for the validation set.  If `None`, then\n        use the training `DataLoader` with the validation data.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_epochs : `int`, optional (default = `20`)\n        Number of training epochs.\n\n    serialization_dir : `str`, optional (default=`None`)\n        Path to directory for saving and loading model files. Models will not be saved if\n        this parameter is not passed.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    checkpointer : `Checkpointer`, optional (default=`None`)\n        A `Checkpointer` is responsible for periodically saving model weights.  If none is given\n        here, we will construct one with default parameters.\n\n    cuda_device : `Optional[Union[int, torch.device]]`, optional (default = `None`)\n        An integer or `torch.device` specifying the CUDA device to use for this process.\n        If -1, the CPU is used. If `None` and you have a GPU available, that GPU will be used.\n\n        !!! Note\n            If you *don't* intend to use a GPU, but you have one available, you'll need\n            to explicitly set `cuda_device=-1`.\n\n        !!! Note\n            If you intend to use a GPU, your model already needs to be on the correct device,\n            which you can do with `model = model.cuda()`.\n\n        !!! Note\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single GPU.\n\n    grad_norm : `float`, optional, (default = `None`).\n        If provided, gradient norms will be rescaled to have a maximum of this value.\n\n    grad_clipping : `float`, optional (default = `None`).\n        If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n        maximum of this value.  If you are getting `NaNs` in your gradients during training\n        that are not solved by using `grad_norm`, you may need this.\n\n    learning_rate_scheduler : `LearningRateScheduler`, optional (default = `None`)\n        If specified, the learning rate will be decayed with respect to\n        this schedule at the end of each epoch (or batch, if the scheduler implements\n        the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n        this will use the `validation_metric` provided to determine if learning has plateaued.\n        To support updating the learning rate on every batch, this can optionally implement\n        `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n\n    momentum_scheduler : `MomentumScheduler`, optional (default = `None`)\n        If specified, the momentum will be updated at the end of each batch or epoch\n        according to the schedule.\n\n    moving_average : `MovingAverage`, optional, (default = `None`)\n        If provided, we will maintain moving averages for all parameters. During training, we\n        employ a shadow variable for each parameter, which maintains the moving average. During\n        evaluation, we backup the original parameters and assign the moving averages to corresponding\n        parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n        parameters. This is necessary because we want the saved model to perform as well as the validated\n        model if we load it later. But this may cause problems if you restart the training from checkpoint.\n\n    callbacks : `List[Lazy[TrainerCallback]]`, optional (default = `None`)\n        A list of callbacks that can be called at certain events: e.g. each batch, epoch, and at the start\n        and end of training, etc.\n\n    distributed : `bool`, optional, (default = `False`)\n        If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n        requires `world_size` to be greater than 1.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately (you need a top-level \"distributed\" key, next to\n        the \"trainer\" entry, that specifies a list of \"cuda_devices\").\n\n    local_rank : `int`, optional, (default = `0`)\n        This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n        used as the rank.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    world_size : `int`, (default = `1`)\n        The number of `Trainer` workers participating in the distributed training.\n\n        In a typical AllenNLP configuration file, this parameter does not get an entry under the\n        \"trainer\", it gets constructed separately.\n\n    num_gradient_accumulation_steps : `int`, optional, (default = `1`)\n        Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n        be useful to accommodate batches that are larger than the RAM size. Refer [Thomas Wolf's\n        post][0] for details on Gradient Accumulation.\n\n    use_amp : `bool`, optional, (default = `False`)\n        If `True`, we'll train using [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html).\n\n    enable_default_callbacks : `bool`, optional (default = `True`)\n        When `True`, the [`DEFAULT_CALLBACKS`](#default_callbacks) will be used in\n        addition to any other callbacks listed in the `callbacks` parameter.\n        When set to `False`, `DEFAULT_CALLBACKS` are not used.\n\n    run_confidence_checks : `bool`, optional (default = `True`)\n        Determines whether model confidence checks, such as\n        [`NormalizationBiasVerification`](../../confidence_checks/normalization_bias_verification/),\n        are run.\n\n    run_sanity_checks : `bool`, optional (default = `True`)\n        This parameter is deprecated. Please use `run_confidence_checks` instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        optimizer: torch.optim.Optimizer,\n        data_loader: DataLoader,\n        patience: Optional[int] = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        validation_data_loader: DataLoader = None,\n        num_epochs: int = 20,\n        serialization_dir: Optional[str] = None,\n        checkpointer: Checkpointer = None,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: Optional[float] = None,\n        grad_clipping: Optional[float] = None,\n        learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n        momentum_scheduler: Optional[MomentumScheduler] = None,\n        moving_average: Optional[MovingAverage] = None,\n        callbacks: List[TrainerCallback] = None,\n        distributed: bool = False,\n        local_rank: int = 0,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> None:\n        <IND>super().__init__(\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n        )\n\n        if \"run_sanity_checks\" in kwargs:\n            <IND>warnings.warn(\n                \"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.\",\n                DeprecationWarning,\n            )\n            run_confidence_checks = kwargs[\"run_sanity_checks\"]\n\n        # I am not calling move_to_gpu here, because if the model is\n        # not already on the GPU then the optimizer is going to be wrong.\n        <DED>self.model = model\n\n        self.data_loader = data_loader\n        self.data_loader.set_target_device(self.cuda_device)\n        self._validation_data_loader = validation_data_loader\n        if self._validation_data_loader is not None:\n            <IND>self._validation_data_loader.set_target_device(self.cuda_device)\n        <DED>self.optimizer = optimizer\n\n        if patience is None:  # no early stopping\n            <IND>if validation_data_loader is not None:\n                <IND>logger.warning(\n                    \"You provided a validation dataset but patience was set to None, \"\n                    \"meaning that early stopping is disabled\"\n                )\n        <DED><DED>elif (not isinstance(patience, int)) or patience <= 0:\n            <IND>raise ConfigurationError(\n                '{} is an invalid value for \"patience\": it must be a positive integer '\n                \"or None (if you want to disable early stopping)\".format(patience)\n            )\n\n        # For tracking is_best_so_far and should_stop_early\n        <DED>self._metric_tracker = MetricTracker(validation_metric, patience)\n\n        self._num_epochs = num_epochs\n\n        self._checkpointer: Optional[Checkpointer] = checkpointer\n        if checkpointer is None and serialization_dir is not None:\n            <IND>self._checkpointer = Checkpointer(serialization_dir)\n\n        <DED>self._grad_norm = grad_norm\n        self._grad_clipping = grad_clipping\n\n        self._learning_rate_scheduler = learning_rate_scheduler\n        self._momentum_scheduler = momentum_scheduler\n        self._moving_average = moving_average\n\n        self._callbacks = callbacks or []\n        default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []\n\n        if run_confidence_checks:\n            <IND>default_callbacks.append(ConfidenceChecksCallback)\n        <DED>for callback_cls in default_callbacks:\n            <IND>for callback in self._callbacks:\n                <IND>if callback.__class__ == callback_cls:\n                    <IND>break\n            <DED><DED>else:\n                <IND>self._callbacks.append(callback_cls(self._serialization_dir))\n\n        <DED><DED>self._batch_num_total = 0\n        self._last_log = 0.0  # time of last logging\n        self._num_gradient_accumulation_steps = num_gradient_accumulation_steps\n\n        # Enable automatic mixed precision training.\n        self._scaler: Optional[amp.GradScaler] = None\n        self._use_amp = use_amp\n        if self._use_amp:\n            <IND>if self.cuda_device == torch.device(\"cpu\"):\n                <IND>raise ValueError(\"Using AMP requires a cuda device\")\n            <DED>self._scaler = amp.GradScaler()\n\n        # Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its\n        # usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`\n        # will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.\n        #\n        # Hence a reference to Pytorch's object is maintained in the case of distributed training and in the\n        # normal case, reference to `Model` is retained. This reference is only used in\n        # these places: `model.__call__`, `model.train` and `model.eval`.\n        <DED>if self._distributed:\n            <IND>self._pytorch_model = DistributedDataParallel(\n                self.model,\n                device_ids=None if self.cuda_device == torch.device(\"cpu\") else [self.cuda_device],\n                find_unused_parameters=True,\n            )\n        <DED>else:\n            <IND>self._pytorch_model = self.model\n\n    <DED><DED>def rescale_gradients(self) -> float:\n        <IND>\"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n\n        Returns the norm of the gradients.\n        \"\"\"\n        parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]\n        if self._grad_norm:\n            <IND>if self._scaler is not None:\n                # Need to first unscale gradients in order to clip as usual.\n                <IND>self._scaler.unscale_(self.optimizer)\n            <DED>return clip_grad_norm_(parameters_to_clip, self._grad_norm)\n        <DED>else:\n            <IND>return torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])\n            )\n\n    <DED><DED>def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:\n        <IND>\"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"\n        output_dict = self._pytorch_model(**batch)\n\n        if for_training:\n            <IND>try:\n                <IND>assert \"loss\" in output_dict\n                regularization_penalty = self.model.get_regularization_penalty()\n\n                if regularization_penalty is not None:\n                    <IND>output_dict[\"reg_loss\"] = regularization_penalty\n                    output_dict[\"loss\"] += regularization_penalty\n\n            <DED><DED>except AssertionError:\n                <IND>if for_training:\n                    <IND>raise RuntimeError(\n                        \"The model you are trying to optimize does not contain a\"\n                        \" 'loss' key in the output of model.forward(inputs).\"\n                    )\n\n        <DED><DED><DED>return output_dict\n\n    <DED>def _train_epoch(self, epoch: int) -> Dict[str, float]:\n        <IND>\"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"\n        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n        cpu_memory_usage = []\n        for worker, memory in common_util.peak_cpu_memory().items():\n            <IND>cpu_memory_usage.append((worker, memory))\n            logger.info(f\"Worker {worker} memory usage: {common_util.format_size(memory)}\")\n        <DED>gpu_memory_usage = []\n        for gpu, memory in common_util.peak_gpu_memory().items():\n            <IND>gpu_memory_usage.append((gpu, memory))\n            logger.info(f\"GPU {gpu} memory usage: {common_util.format_size(memory)}\")\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        train_loss = 0.0\n        batch_loss = 0.0\n        train_reg_loss = None if regularization_penalty is None else 0.0\n        batch_reg_loss = None if regularization_penalty is None else 0.0\n\n        # Set the model to \"train\" mode.\n        self._pytorch_model.train()\n\n        # Get tqdm for the training batches\n        batch_generator = iter(self.data_loader)\n        batch_group_generator = common_util.lazy_groups_of(\n            batch_generator, self._num_gradient_accumulation_steps\n        )\n\n        logger.info(\"Training\")\n\n        num_training_batches: Union[int, float]\n        try:\n            <IND>len_data_loader = len(self.data_loader)\n            num_training_batches = math.ceil(\n                len_data_loader / self._num_gradient_accumulation_steps\n            )\n        <DED>except TypeError:\n            <IND>num_training_batches = float(\"inf\")\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        <DED>if self._primary:\n            <IND>batch_group_generator_tqdm = Tqdm.tqdm(\n                batch_group_generator, total=num_training_batches\n            )\n        <DED>else:\n            <IND>batch_group_generator_tqdm = batch_group_generator\n\n        <DED>self._last_log = time.time()\n\n        batches_this_epoch = 0\n        if self._batch_num_total is None:\n            <IND>self._batch_num_total = 0\n\n        <DED>done_early = False\n        for batch_group in batch_group_generator_tqdm:\n            <IND>if done_early:\n                <IND>break\n\n            <DED>batches_this_epoch += 1\n            self._batch_num_total += 1\n            batch_num_total = self._batch_num_total\n\n            # Zero gradients.\n            # NOTE: this is actually more efficient than calling `self.optimizer.zero_grad()`\n            # because it avoids a read op when the gradients are first updated below.\n            for param_group in self.optimizer.param_groups:\n                <IND>for p in param_group[\"params\"]:\n                    <IND>p.grad = None\n\n            <DED><DED>batch_loss = 0.0\n            batch_group_outputs = []\n            for batch in batch_group:\n                <IND>if self._distributed:\n                    # Check whether the other workers have stopped already (due to differing amounts of\n                    # data in each). If so, we can't proceed because we would hang when we hit the\n                    # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                    # here because NCCL process groups apparently don't support BoolTensor.\n                    <IND>done = torch.tensor(0, device=self.cuda_device)\n                    torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                    if done.item() > 0:\n                        <IND>done_early = True\n                        logger.warning(\n                            f\"Worker {torch.distributed.get_rank()} finishing training early! \"\n                            \"This implies that there is an imbalance in your training \"\n                            \"data across the workers and that some amount of it will be \"\n                            \"ignored. A small amount of this is fine, but a major imbalance \"\n                            \"should be avoided. Note: This warning will appear unless your \"\n                            \"data is perfectly balanced.\"\n                        )\n                        break\n\n                <DED><DED>with amp.autocast(self._use_amp):\n                    <IND>batch_outputs = self.batch_outputs(batch, for_training=True)\n                    batch_group_outputs.append(batch_outputs)\n                    loss = batch_outputs[\"loss\"]\n                    reg_loss = batch_outputs.get(\"reg_loss\")\n                    if torch.isnan(loss):\n                        <IND>raise ValueError(\"nan loss encountered\")\n                    <DED>loss = loss / len(batch_group)\n\n                    batch_loss += loss.item()\n                    if reg_loss is not None:\n                        <IND>reg_loss = reg_loss / len(batch_group)\n                        batch_reg_loss = reg_loss.item()\n                        train_reg_loss += batch_reg_loss  # type: ignore\n\n                <DED><DED>if self._scaler is not None:\n                    <IND>self._scaler.scale(loss).backward()\n                <DED>else:\n                    <IND>loss.backward()\n            <DED><DED>if len(batch_group_outputs) <= 0:\n                <IND>continue\n\n            <DED>train_loss += batch_loss\n\n            batch_grad_norm = self.rescale_gradients()\n\n            # This does nothing if batch_num_total is None or you are using a\n            # scheduler which doesn't update per batch.\n            if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step_batch(batch_num_total)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step_batch(batch_num_total)\n\n            <DED>if self._scaler is not None:\n                <IND>self._scaler.step(self.optimizer)\n                self._scaler.update()\n            <DED>else:\n                <IND>self.optimizer.step()\n\n            # Update moving averages\n            <DED>if self._moving_average is not None:\n                <IND>self._moving_average.apply(batch_num_total)\n\n            # Update the description with the latest metrics\n            <DED>metrics = training_util.get_metrics(\n                self.model,\n                train_loss,\n                train_reg_loss,\n                batch_loss,\n                batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            if self._primary:\n                # Updating tqdm only for the primary as the trainers wouldn't have one\n                <IND>description = training_util.description_from_metrics(metrics)\n                batch_group_generator_tqdm.set_description(description, refresh=False)\n\n                if self._checkpointer is not None:\n                    <IND>self._checkpointer.maybe_save_checkpoint(self, epoch, batches_this_epoch)\n\n            <DED><DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    batch_group,\n                    batch_group_outputs,\n                    metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=True,\n                    is_primary=self._primary,\n                    batch_grad_norm=batch_grad_norm,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop the epoch early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Let all workers finish their epoch before computing\n        # the final statistics for the epoch.\n        <DED>if self._distributed:\n            <IND>dist.barrier()\n\n        <DED>metrics = training_util.get_metrics(\n            self.model,\n            train_loss,\n            train_reg_loss,\n            batch_loss=None,\n            batch_reg_loss=None,\n            num_batches=batches_this_epoch,\n            reset=True,\n            world_size=self._world_size,\n            cuda_device=self.cuda_device,\n        )\n\n        for (worker, memory) in cpu_memory_usage:\n            <IND>metrics[\"worker_\" + str(worker) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>for (gpu_num, memory) in gpu_memory_usage:\n            <IND>metrics[\"gpu_\" + str(gpu_num) + \"_memory_MB\"] = memory / (1024 * 1024)\n        <DED>return metrics\n\n    <DED>def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:\n        <IND>\"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"\n        logger.info(\"Validating\")\n\n        self._pytorch_model.eval()\n\n        # Replace parameter values with the shadow values from the moving averages.\n        if self._moving_average is not None:\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>if self._validation_data_loader is not None:\n            <IND>validation_data_loader = self._validation_data_loader\n        <DED>else:\n            <IND>raise ConfigurationError(\n                \"Validation results cannot be calculated without a validation_data_loader\"\n            )\n\n        <DED>regularization_penalty = self.model.get_regularization_penalty()\n\n        # Having multiple tqdm bars in case of distributed training will be a mess. Hence only the primary's\n        # progress is shown\n        if self._primary:\n            <IND>val_generator_tqdm = Tqdm.tqdm(validation_data_loader)\n        <DED>else:\n            <IND>val_generator_tqdm = validation_data_loader\n\n        <DED>batches_this_epoch = 0\n        val_loss = 0.0\n        val_batch_loss = 0.0\n        val_reg_loss = None if regularization_penalty is None else 0.0\n        val_batch_reg_loss = None if regularization_penalty is None else 0.0\n        done_early = False\n        for batch in val_generator_tqdm:\n            <IND>if self._distributed:\n                # Check whether the other workers have stopped already (due to differing amounts of\n                # data in each). If so, we can't proceed because we would hang when we hit the\n                # barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor\n                # here because NCCL process groups apparently don't support BoolTensor.\n                <IND>done = torch.tensor(0, device=self.cuda_device)\n                torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n                if done.item() > 0:\n                    <IND>done_early = True\n                    logger.warning(\n                        f\"Worker {torch.distributed.get_rank()} finishing validation early! \"\n                        \"This implies that there is an imbalance in your validation \"\n                        \"data across the workers and that some amount of it will be \"\n                        \"ignored. A small amount of this is fine, but a major imbalance \"\n                        \"should be avoided. Note: This warning will appear unless your \"\n                        \"data is perfectly balanced.\"\n                    )\n                    break\n\n            <DED><DED>with amp.autocast(self._use_amp):\n                <IND>batch_outputs = self.batch_outputs(batch, for_training=False)\n                loss = batch_outputs.get(\"loss\")\n                reg_loss = batch_outputs.get(\"reg_loss\")\n                if loss is not None:\n                    # You shouldn't necessarily have to compute a loss for validation, so we allow for\n                    # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n                    # currently only used as the divisor for the loss function, so we can safely only\n                    # count those batches for which we actually have a loss.  If this variable ever\n                    # gets used for something else, we might need to change things around a bit.\n                    <IND>batches_this_epoch += 1\n                    val_batch_loss = loss.item()\n                    val_loss += val_batch_loss\n                    if reg_loss is not None:\n                        <IND>val_batch_reg_loss = reg_loss.item()\n                        val_reg_loss += val_batch_reg_loss  # type: ignore\n\n            # Update the description with the latest metrics\n            <DED><DED><DED>val_metrics = training_util.get_metrics(\n                self.model,\n                val_loss,\n                val_reg_loss,\n                val_batch_loss,\n                val_batch_reg_loss,\n                batches_this_epoch,\n                world_size=self._world_size,\n                cuda_device=self.cuda_device,\n            )\n\n            description = training_util.description_from_metrics(val_metrics)\n            if self._primary:\n                <IND>val_generator_tqdm.set_description(description, refresh=False)\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_batch(\n                    self,\n                    [batch],\n                    [batch_outputs],\n                    val_metrics,\n                    epoch,\n                    batches_this_epoch,\n                    is_training=False,\n                    is_primary=self._primary,\n                )\n\n        <DED><DED>if self._distributed and not done_early:\n            <IND>logger.warning(\n                f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"\n            )\n            # Indicate that we're done so that any workers that have remaining data stop validation early.\n            done = torch.tensor(1, device=self.cuda_device)\n            torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)\n            assert done.item()\n\n        # Now restore the original parameter values.\n        <DED>if self._moving_average is not None:\n            <IND>self._moving_average.restore()\n\n        <DED>return val_loss, val_reg_loss, batches_this_epoch\n\n    <DED>def train(self) -> Dict[str, Any]:\n        <IND>\"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"\n\n        for callback in self._callbacks:\n            <IND>callback.on_start(self, is_primary=self._primary)\n\n        # Set default values in case of failure\n        <DED>epoch = None\n        metrics = None\n\n        try:\n            <IND>metrics, epoch = self._try_train()\n            return metrics\n        <DED>finally:\n            <IND>for callback in self._callbacks:\n                <IND>callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n    <DED><DED><DED>def _try_train(self) -> Tuple[Dict[str, Any], int]:\n        <IND>try:\n            <IND>epoch_counter = self._restore_checkpoint()\n        <DED>except RuntimeError:\n            <IND>traceback.print_exc()\n            raise ConfigurationError(\n                \"Could not recover training from the checkpoint.  Did you mean to output to \"\n                \"a different serialization directory or delete the existing serialization \"\n                \"directory?\"\n            )\n\n        <DED>training_util.enable_gradient_clipping(self.model, self._grad_clipping)\n\n        logger.info(\"Beginning training.\")\n\n        val_metrics: Dict[str, float] = {}\n        metrics: Dict[str, Any] = {}\n        epochs_trained = 0\n        training_start_time = time.time()\n\n        metrics[\"best_epoch\"] = self._metric_tracker.best_epoch\n        for key, value in self._metric_tracker.best_epoch_metrics.items():\n            <IND>metrics[\"best_validation_\" + key] = value\n\n        <DED>for epoch in range(epoch_counter, self._num_epochs):\n            <IND>epoch_start_time = time.time()\n            train_metrics = self._train_epoch(epoch)\n\n            # Back up the model now, in case something goes wrong later with the evaluation\n            if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.shelve_model(epoch, self)\n            # Wait for the primary process to finish saving the model checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            # get peak of memory usage\n            <DED>for key, value in train_metrics.items():\n                <IND>if key.startswith(\"gpu_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n                <DED>elif key.startswith(\"worker_\") and key.endswith(\"_memory_MB\"):\n                    <IND>metrics[\"peak_\" + key] = max(metrics.get(\"peak_\" + key, 0), value)\n\n            <DED><DED>this_epoch_val_metric: float = 0.0\n            if self._validation_data_loader is not None:\n                <IND>with torch.no_grad():\n                    # We have a validation set, so compute all the metrics on it.\n                    <IND>val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)\n\n                    # It is safe again to wait till the validation is done. This is\n                    # important to get the metrics right.\n                    if self._distributed:\n                        <IND>dist.barrier()\n\n                    <DED>val_metrics = training_util.get_metrics(\n                        self.model,\n                        val_loss,\n                        val_reg_loss,\n                        batch_loss=None,\n                        batch_reg_loss=None,\n                        num_batches=num_batches,\n                        reset=True,\n                        world_size=self._world_size,\n                        cuda_device=self.cuda_device,\n                    )\n\n                    # Check validation metric for early stopping\n                    this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)\n                    self._metric_tracker.add_metrics(val_metrics)\n\n            # Create overall metrics dict\n            <DED><DED>training_elapsed_time = time.time() - training_start_time\n            metrics[\"training_duration\"] = str(datetime.timedelta(seconds=training_elapsed_time))\n            metrics[\"training_start_epoch\"] = epoch_counter\n            metrics[\"training_epochs\"] = epochs_trained\n            metrics[\"epoch\"] = epoch\n\n            for key, value in train_metrics.items():\n                <IND>metrics[\"training_\" + key] = value\n            <DED>for key, value in val_metrics.items():\n                <IND>metrics[\"validation_\" + key] = value\n\n            <DED>if self._metric_tracker.is_best_so_far():\n                # Update all the best_ metrics.\n                # (Otherwise they just stay the same as they were.)\n                <IND>metrics[\"best_epoch\"] = epoch\n                for key, value in val_metrics.items():\n                    <IND>metrics[\"best_validation_\" + key] = value\n\n                <DED>self._metric_tracker.best_epoch_metrics = val_metrics\n\n            <DED>if self._serialization_dir and self._primary:\n                <IND>common_util.dump_metrics(\n                    os.path.join(self._serialization_dir, f\"metrics_epoch_{epoch}.json\"),\n                    metrics,\n                )\n\n            # The Scheduler API is agnostic to whether your schedule requires a validation metric -\n            # if it doesn't, the validation metric passed here is ignored.\n            <DED>if self._learning_rate_scheduler:\n                <IND>self._learning_rate_scheduler.step(this_epoch_val_metric)\n            <DED>if self._momentum_scheduler:\n                <IND>self._momentum_scheduler.step(this_epoch_val_metric)\n\n            # The checkpointer saves state from the learning rate scheduler and the momentum\n            # scheduler, so we have to make sure those are updated before we save the checkpoint here.\n            <DED>if self._primary and self._checkpointer is not None:\n                <IND>self._checkpointer.save_checkpoint(\n                    epoch, self, is_best_so_far=self._metric_tracker.is_best_so_far()\n                )\n            # Wait for the primary process to finish saving the checkpoint\n            <DED>if self._distributed:\n                <IND>dist.barrier()\n\n            <DED>for callback in self._callbacks:\n                <IND>callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)\n\n            <DED>epoch_elapsed_time = time.time() - epoch_start_time\n            logger.info(\"Epoch duration: %s\", datetime.timedelta(seconds=epoch_elapsed_time))\n\n            if epoch < self._num_epochs - 1:\n                <IND>training_elapsed_time = time.time() - training_start_time\n                estimated_time_remaining = training_elapsed_time * (\n                    (self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1\n                )\n                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n\n            <DED>epochs_trained += 1\n\n            if self._metric_tracker.should_stop_early():\n                <IND>logger.info(\"Ran out of patience. Stopping training.\")\n                break\n        <DED><DED>else:\n            <IND>epoch = self._num_epochs - 1\n\n        # Load the best model state before returning\n        <DED>best_model_state = (\n            None if self._checkpointer is None else self._checkpointer.best_model_state()\n        )\n        if best_model_state:\n            <IND>self.model.load_state_dict(best_model_state)\n\n        <DED>return metrics, epoch\n\n    <DED>@contextmanager\n    def get_checkpoint_state(self) -> Iterator[Tuple[Dict[str, Any], Dict[str, Any]]]:\n        <IND>if self._moving_average is not None:\n            # Assigning average value to model parameters.  The checkpointer will call\n            # `restore_state_after_checkpointing` when it is done to put this back to what it was.\n            <IND>self._moving_average.assign_average_value()\n\n        <DED>model_state = self.model.state_dict()\n\n        # These are the training states we need to persist.\n        training_states = {\n            \"metric_tracker\": self._metric_tracker.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"batch_num_total\": self._batch_num_total,\n        }\n\n        # If we have a learning rate or momentum scheduler, we should persist them too.\n        if self._learning_rate_scheduler is not None:\n            <IND>training_states[\"learning_rate_scheduler\"] = self._learning_rate_scheduler.state_dict()\n        <DED>if self._momentum_scheduler is not None:\n            <IND>training_states[\"momentum_scheduler\"] = self._momentum_scheduler.state_dict()\n\n        <DED>try:\n            <IND>yield model_state, training_states\n        <DED>finally:\n            <IND>if self._moving_average is not None:\n                <IND>self._moving_average.restore()\n\n    <DED><DED><DED>def _restore_checkpoint(self) -> int:\n        <IND>\"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"\n        if self._checkpointer is None:\n            <IND>return 0\n\n        <DED>model_state, training_state = self._checkpointer.restore_checkpoint()\n\n        if not training_state:\n            # No checkpoint to restore, start at 0\n            <IND>return 0\n\n        <DED>self.model.load_state_dict(model_state)\n        self.optimizer.load_state_dict(training_state[\"optimizer\"])\n        if (\n            self._learning_rate_scheduler is not None\n            and \"learning_rate_scheduler\" in training_state\n        ):\n            <IND>self._learning_rate_scheduler.load_state_dict(training_state[\"learning_rate_scheduler\"])\n        <DED>if self._momentum_scheduler is not None and \"momentum_scheduler\" in training_state:\n            <IND>self._momentum_scheduler.load_state_dict(training_state[\"momentum_scheduler\"])\n        <DED>training_util.move_optimizer_to_cuda(self.optimizer)\n\n        # Currently the `training_state` contains a serialized `MetricTracker`.\n        if \"metric_tracker\" in training_state:\n            <IND>self._metric_tracker.load_state_dict(training_state[\"metric_tracker\"])\n        <DED>else:\n            <IND>self._metric_tracker.clear()\n\n        <DED>if isinstance(training_state[\"epoch\"], int):\n            <IND>epoch_to_return = training_state[\"epoch\"] + 1\n        <DED>else:\n            <IND>epoch_to_return = int(training_state[\"epoch\"].split(\".\")[0]) + 1\n\n        # For older checkpoints with batch_num_total missing, default to old behavior where\n        # it is unchanged.\n        <DED>batch_num_total = training_state.get(\"batch_num_total\")\n        if batch_num_total is not None:\n            <IND>self._batch_num_total = batch_num_total\n\n        <DED>return epoch_to_return\n\n    <DED>@classmethod\n    def from_partial_objects(\n        cls,\n        model: Model,\n        serialization_dir: str,\n        data_loader: DataLoader,\n        validation_data_loader: DataLoader = None,\n        local_rank: int = 0,\n        patience: int = None,\n        validation_metric: Union[str, List[str]] = \"-loss\",\n        num_epochs: int = 20,\n        cuda_device: Optional[Union[int, torch.device]] = None,\n        grad_norm: float = None,\n        grad_clipping: float = None,\n        distributed: bool = False,\n        world_size: int = 1,\n        num_gradient_accumulation_steps: int = 1,\n        use_amp: bool = False,\n        no_grad: List[str] = None,\n        optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),\n        learning_rate_scheduler: Lazy[LearningRateScheduler] = None,\n        momentum_scheduler: Lazy[MomentumScheduler] = None,\n        moving_average: Lazy[MovingAverage] = None,\n        checkpointer: Lazy[Checkpointer] = Lazy(Checkpointer),\n        callbacks: List[Lazy[TrainerCallback]] = None,\n        enable_default_callbacks: bool = True,\n        run_confidence_checks: bool = True,\n        **kwargs,\n    ) -> \"Trainer\":\n        <IND>\"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"\n        if cuda_device is None:\n            <IND>from torch import cuda\n\n            if cuda.device_count() > 0:\n                <IND>cuda_device = 0\n            <DED>else:\n                <IND>cuda_device = -1\n\n        <DED><DED>check_for_gpu(cuda_device)\n        if cuda_device >= 0:\n            # Moving model to GPU here so that the optimizer state gets constructed on\n            # the right device.\n            <IND>model = model.cuda(cuda_device)\n\n        <DED>if no_grad:\n            <IND>for name, parameter in model.named_parameters():\n                <IND>if any(re.search(regex, name) for regex in no_grad):\n                    <IND>parameter.requires_grad_(False)\n\n        <DED><DED><DED>parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n        optimizer_ = optimizer.construct(model_parameters=parameters)\n\n        common_util.log_frozen_and_tunable_parameter_names(model)\n\n        batches_per_epoch: Optional[int]\n        try:\n            <IND>batches_per_epoch = len(data_loader)\n            batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)\n        <DED>except TypeError:\n            <IND>batches_per_epoch = None\n\n        <DED>moving_average_ = (\n            None if moving_average is None else moving_average.construct(parameters=parameters)\n        )\n        learning_rate_scheduler_ = (\n            None\n            if learning_rate_scheduler is None\n            else learning_rate_scheduler.construct(\n                optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch\n            )\n        )\n        momentum_scheduler_ = (\n            None\n            if momentum_scheduler is None\n            else momentum_scheduler.construct(optimizer=optimizer_)\n        )\n        checkpointer_ = checkpointer.construct(serialization_dir=serialization_dir)\n\n        callbacks_: List[TrainerCallback] = []\n        for callback_ in callbacks or []:\n            <IND>callbacks_.append(callback_.construct(serialization_dir=serialization_dir))\n\n        <DED>return cls(\n            model,\n            optimizer_,\n            data_loader,\n            patience=patience,\n            validation_metric=validation_metric,\n            validation_data_loader=validation_data_loader,\n            num_epochs=num_epochs,\n            serialization_dir=serialization_dir,\n            cuda_device=cuda_device,\n            grad_norm=grad_norm,\n            grad_clipping=grad_clipping,\n            learning_rate_scheduler=learning_rate_scheduler_,\n            momentum_scheduler=momentum_scheduler_,\n            checkpointer=checkpointer_,\n            moving_average=moving_average_,\n            callbacks=callbacks_,\n            distributed=distributed,\n            local_rank=local_rank,\n            world_size=world_size,\n            num_gradient_accumulation_steps=num_gradient_accumulation_steps,\n            use_amp=use_amp,\n            enable_default_callbacks=enable_default_callbacks,\n            run_confidence_checks=run_confidence_checks,\n            **kwargs,\n        )\n\n\n<DED><DED>DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)\n\"\"\"\nThe default callbacks used by `GradientDescentTrainer`.\n\"\"\"\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def get_best_weights_path(self) -> Optional[str]:\n        <IND>\"\"\"Returns the path to file containing the current best weights.\"\"\"\n        return None\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  }
]