[
  {
    "project": "allenai/allennlp",
    "commit": "b4f7b9461e7f8f3b2f315f588dbb75f44c208794",
    "filename": "allennlp/data/fields/text_field.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/fields/text_field.py",
    "file_hunks_size": 12,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/data/fields/text_field.py:56:56 Incompatible parameter type [6]: Expected `typing.Iterable[Variable[_T2]]` for 2nd positional only parameter to call `zip.__new__` but got `Optional[List[List[typing.Any]]]`.",
    "message": " Expected `typing.Iterable[Variable[_T2]]` for 2nd positional only parameter to call `zip.__new__` but got `Optional[List[List[typing.Any]]]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 56,
    "warning_line": "        for indexer, array in zip(self._token_indexers, self._indexed_tokens):",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    \"\"\"\n    def __init__(self, tokens: List[str], token_indexers: List[TokenIndexer]) -> None:\n        self._tokens = tokens\n        self._token_indexers = token_indexers\n        self._indexed_tokens = None  # type: Optional[List[TokenList]]\n\n    @overrides\n    def count_vocab_items(self, counter: Dict[str, Dict[str, int]]):\n        for indexer in self._token_indexers:\n            for token in self._tokens:\n",
        "source_code_len": 411,
        "target_code": "    \"\"\"\n    def __init__(self, tokens: List[str], token_indexers: Dict[str, TokenIndexer]) -> None:\n        self._tokens = tokens\n        self._token_indexers = token_indexers\n        self._indexed_tokens = None  # type: Optional[Dict[str, TokenList]]\n\n    # @overrides\n    def count_vocab_items(self, counter: Dict[str, Dict[str, int]]):\n        for indexer in self._token_indexers.values():\n            for token in self._tokens:\n",
        "target_code_len": 432,
        "diff_format": "@@ -30,10 +31,10 @@\n     \"\"\"\n-    def __init__(self, tokens: List[str], token_indexers: List[TokenIndexer]) -> None:\n+    def __init__(self, tokens: List[str], token_indexers: Dict[str, TokenIndexer]) -> None:\n         self._tokens = tokens\n         self._token_indexers = token_indexers\n-        self._indexed_tokens = None  # type: Optional[List[TokenList]]\n+        self._indexed_tokens = None  # type: Optional[Dict[str, TokenList]]\n \n-    @overrides\n+    # @overrides\n     def count_vocab_items(self, counter: Dict[str, Dict[str, int]]):\n-        for indexer in self._token_indexers:\n+        for indexer in self._token_indexers.values():\n             for token in self._tokens:\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n    def __init__(self, tokens: List[str], token_indexers: List[TokenIndexer]) -> None:\n        <IND>self._tokens = tokens\n        self._token_indexers = token_indexers\n        self._indexed_tokens = None  # type: Optional[List[TokenList]]\n\n    <DED>@overrides\n    def count_vocab_items(self, counter: Dict[str, Dict[str, int]]):\n        <IND>for indexer in self._token_indexers:\n            <IND>for token in self._tokens:\n",
        "target_code_with_indent": "\n    def __init__(self, tokens: List[str], token_indexers: Dict[str, TokenIndexer]) -> None:\n        <IND>self._tokens = tokens\n        self._token_indexers = token_indexers\n        self._indexed_tokens = None  # type: Optional[Dict[str, TokenList]]\n\n    # @overrides\n    <DED>def count_vocab_items(self, counter: Dict[str, Dict[str, int]]):\n        <IND>for indexer in self._token_indexers.values():\n            <IND>for token in self._tokens:\n"
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    @overrides\n    def index(self, vocab: Vocabulary):\n        token_arrays = []\n        for indexer in self._token_indexers:\n            arrays = [indexer.token_to_indices(token, vocab) for token in self._tokens]\n            token_arrays.append(arrays)\n        self._indexed_tokens = token_arrays\n\n    @overrides\n    def get_padding_lengths(self) -> Dict[str, int]:\n",
        "source_code_len": 368,
        "target_code": "\n    # @overrides\n    def index(self, vocab: Vocabulary):\n        token_arrays = {}\n        for indexer_name, indexer in self._token_indexers.items():\n            arrays = [indexer.token_to_indices(token, vocab) for token in self._tokens]\n            token_arrays[indexer_name] = arrays\n        self._indexed_tokens = token_arrays\n\n    # @overrides\n    def get_padding_lengths(self) -> Dict[str, int]:\n",
        "target_code_len": 402,
        "diff_format": "@@ -41,11 +42,11 @@\n \n-    @overrides\n+    # @overrides\n     def index(self, vocab: Vocabulary):\n-        token_arrays = []\n-        for indexer in self._token_indexers:\n+        token_arrays = {}\n+        for indexer_name, indexer in self._token_indexers.items():\n             arrays = [indexer.token_to_indices(token, vocab) for token in self._tokens]\n-            token_arrays.append(arrays)\n+            token_arrays[indexer_name] = arrays\n         self._indexed_tokens = token_arrays\n \n-    @overrides\n+    # @overrides\n     def get_padding_lengths(self) -> Dict[str, int]:\n",
        "source_code_with_indent": "\n    <DED><DED><DED>@overrides\n    def index(self, vocab: Vocabulary):\n        <IND>token_arrays = []\n        for indexer in self._token_indexers:\n            <IND>arrays = [indexer.token_to_indices(token, vocab) for token in self._tokens]\n            token_arrays.append(arrays)\n        <DED>self._indexed_tokens = token_arrays\n\n    <DED>@overrides\n    def get_padding_lengths(self) -> Dict[str, int]:\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    # @overrides\n    <DED><DED><DED>def index(self, vocab: Vocabulary):\n        <IND>token_arrays = {}\n        for indexer_name, indexer in self._token_indexers.items():\n            <IND>arrays = [indexer.token_to_indices(token, vocab) for token in self._tokens]\n            token_arrays[indexer_name] = arrays\n        <DED>self._indexed_tokens = token_arrays\n\n    # @overrides\n    <DED>def get_padding_lengths(self) -> Dict[str, int]:\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "                                     \"field before determining padding lengths.\")\n        for indexer, array in zip(self._token_indexers, self._indexed_tokens):\n            indexer_lengths = {}\n",
        "source_code_len": 194,
        "target_code": "                                     \"field before determining padding lengths.\")\n        for indexer_name, indexer in self._token_indexers.items():\n            indexer_lengths = {}\n",
        "target_code_len": 182,
        "diff_format": "@@ -55,3 +56,3 @@\n                                      \"field before determining padding lengths.\")\n-        for indexer, array in zip(self._token_indexers, self._indexed_tokens):\n+        for indexer_name, indexer in self._token_indexers.items():\n             indexer_lengths = {}\n",
        "source_code_with_indent": "                                     \"field before determining padding lengths.\")\n        <DED>for indexer, array in zip(self._token_indexers, self._indexed_tokens):\n            <IND>indexer_lengths = {}\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "                                     \"field before determining padding lengths.\")\n        <DED>for indexer_name, indexer in self._token_indexers.items():\n            <IND>indexer_lengths = {}\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "            # This is a list of dicts, one for each token in the field.\n            token_lengths = [indexer.get_padding_lengths(token) for token in array]\n            # TODO(Mark): This breaks if the token list is empty, but we need to be able to have empty fields.\n",
        "source_code_len": 267,
        "target_code": "            # This is a list of dicts, one for each token in the field.\n            token_lengths = [indexer.get_padding_lengths(token) for token in self._indexed_tokens[indexer_name]]\n            # TODO(Mark): This breaks if the token list is empty, but we need to be able to have empty fields.\n",
        "target_code_len": 296,
        "diff_format": "@@ -59,3 +60,3 @@\n             # This is a list of dicts, one for each token in the field.\n-            token_lengths = [indexer.get_padding_lengths(token) for token in array]\n+            token_lengths = [indexer.get_padding_lengths(token) for token in self._indexed_tokens[indexer_name]]\n             # TODO(Mark): This breaks if the token list is empty, but we need to be able to have empty fields.\n",
        "source_code_with_indent": "            # This is a list of dicts, one for each token in the field.\n            token_lengths = [indexer.get_padding_lengths(token) for token in array]\n            # TODO(Mark): This breaks if the token list is empty, but we need to be able to have empty fields.\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "            # This is a list of dicts, one for each token in the field.\n            token_lengths = [indexer.get_padding_lengths(token) for token in self._indexed_tokens[indexer_name]]\n            # TODO(Mark): This breaks if the token list is empty, but we need to be able to have empty fields.\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    @overrides\n    def pad(self, padding_lengths: Dict[str, int]) -> List[numpy.array]:\n        arrays = []\n        desired_num_tokens = padding_lengths['num_tokens']\n        for indexer, array in zip(self._token_indexers, self._indexed_tokens):\n            padded_array = indexer.pad_token_sequence(array, desired_num_tokens, padding_lengths)\n            arrays.append(numpy.asarray(padded_array))\n        return arrays\n\n    @overrides\n    def empty_field(self):\n",
        "source_code_len": 465,
        "target_code": "\n    # @overrides\n    def as_array(self, padding_lengths: Dict[str, int]) -> Dict[str, numpy.array]:\n        arrays = {}\n        desired_num_tokens = padding_lengths['num_tokens']\n        for indexer_name, indexer in self._token_indexers.items():\n            padded_array = indexer.pad_token_sequence(self._indexed_tokens[indexer_name],\n                                                      desired_num_tokens, padding_lengths)\n            # Use the key of the indexer to recognise what the array corresponds to within the field\n            # (i.e. the result of word indexing, or the result of character indexing, for example).\n            arrays[indexer_name] = padded_array\n        return arrays\n\n    # @overrides\n    def empty_field(self):\n",
        "target_code_len": 744,
        "diff_format": "@@ -79,12 +81,15 @@\n \n-    @overrides\n-    def pad(self, padding_lengths: Dict[str, int]) -> List[numpy.array]:\n-        arrays = []\n+    # @overrides\n+    def as_array(self, padding_lengths: Dict[str, int]) -> Dict[str, numpy.array]:\n+        arrays = {}\n         desired_num_tokens = padding_lengths['num_tokens']\n-        for indexer, array in zip(self._token_indexers, self._indexed_tokens):\n-            padded_array = indexer.pad_token_sequence(array, desired_num_tokens, padding_lengths)\n-            arrays.append(numpy.asarray(padded_array))\n+        for indexer_name, indexer in self._token_indexers.items():\n+            padded_array = indexer.pad_token_sequence(self._indexed_tokens[indexer_name],\n+                                                      desired_num_tokens, padding_lengths)\n+            # Use the key of the indexer to recognise what the array corresponds to within the field\n+            # (i.e. the result of word indexing, or the result of character indexing, for example).\n+            arrays[indexer_name] = padded_array\n         return arrays\n \n-    @overrides\n+    # @overrides\n     def empty_field(self):\n",
        "source_code_with_indent": "\n    <DED>@overrides\n    def pad(self, padding_lengths: Dict[str, int]) -> List[numpy.array]:\n        <IND>arrays = []\n        desired_num_tokens = padding_lengths['num_tokens']\n        for indexer, array in zip(self._token_indexers, self._indexed_tokens):\n            <IND>padded_array = indexer.pad_token_sequence(array, desired_num_tokens, padding_lengths)\n            arrays.append(numpy.asarray(padded_array))\n        <DED>return arrays\n\n    <DED>@overrides\n    def empty_field(self):\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    # @overrides\n    <DED>def as_array(self, padding_lengths: Dict[str, int]) -> Dict[str, numpy.array]:\n        <IND>arrays = {}\n        desired_num_tokens = padding_lengths['num_tokens']\n        for indexer_name, indexer in self._token_indexers.items():\n            <IND>padded_array = indexer.pad_token_sequence(self._indexed_tokens[indexer_name],\n                                                      desired_num_tokens, padding_lengths)\n            # Use the key of the indexer to recognise what the array corresponds to within the field\n            # (i.e. the result of word indexing, or the result of character indexing, for example).\n            arrays[indexer_name] = padded_array\n        <DED>return arrays\n\n    # @overrides\n    <DED>def empty_field(self):\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "allenai/allennlp",
    "commit": "b4f7b9461e7f8f3b2f315f588dbb75f44c208794",
    "filename": "allennlp/data/fields/text_field.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/allenai-allennlp/allennlp/data/fields/text_field.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "allennlp/data/fields/text_field.py:84:56 Incompatible parameter type [6]: Expected `typing.Iterable[Variable[_T2]]` for 2nd positional only parameter to call `zip.__new__` but got `Optional[List[List[typing.Any]]]`.",
    "message": " Expected `typing.Iterable[Variable[_T2]]` for 2nd positional only parameter to call `zip.__new__` but got `Optional[List[List[typing.Any]]]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 84,
    "warning_line": "        for indexer, array in zip(self._token_indexers, self._indexed_tokens):"
  }
]