[
  {
    "project": "asyml/forte",
    "commit": "b56a6eb94977a65078b8265041af30a9ecd21bac",
    "filename": "forte/processors/ner_predictor.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asyml-forte/forte/processors/ner_predictor.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "forte/processors/ner_predictor.py:484:4 Inconsistent override [14]: `forte.processors.ner_predictor.BioBERTNERPredictor.pack` overrides method defined in `forte.processors.base.batch_processor.BaseBatchProcessor` inconsistently. Could not find parameter `inputs` in overriding signature.",
    "message": " `forte.processors.ner_predictor.BioBERTNERPredictor.pack` overrides method defined in `forte.processors.base.batch_processor.BaseBatchProcessor` inconsistently. Could not find parameter `inputs` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 484,
    "warning_line": "    def pack(self, data_pack: DataPack,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        return configs\n\n\nclass BioBERTNERPredictor(FixedSizeBatchProcessor):\n    \"\"\"\n       An Named Entity Recognizer fine-tuned on BioBERT\n\n       Note that to use :class:`BioBERTNERPredictor`, the :attr:`ontology` of\n       :class:`Pipeline` must be an ontology that include\n       ``ft.onto.base_ontology.Subword`` and ``ft.onto.base_ontology.Sentence``.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.resources = None\n        self.device = None\n\n        self.ft_configs = None\n        self.model_config = None\n        self.model = None\n        self.tokenizer = None\n\n    @staticmethod\n    def _define_context() -> Type[Annotation]:\n        return Sentence\n\n    @staticmethod\n    def _define_input_info() -> DataRequest:\n        input_info: DataRequest = {\n            Subword: [],\n            Sentence: [],\n        }\n        return input_info\n\n    def initialize(self, resources: Resources, configs: Config):\n        super().initialize(resources, configs)\n\n        if resources.get(\"device\"):\n            self.device = resources.get(\"device\")\n        else:\n            self.device = torch.device('cuda') if torch.cuda.is_available() \\\n                          else torch.device('cpu')\n\n        self.resources = resources\n        self.ft_configs = configs\n\n        model_path = self.ft_configs.model_path\n        self.model_config = AutoConfig.from_pretrained(model_path)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        self.model = AutoModelForTokenClassification.from_pretrained(\n            model_path,\n            from_tf=bool(\".ckpt\" in model_path),\n            config=self.model_config\n        )\n        self.model.to(self.device)\n\n    @torch.no_grad()\n    def predict(self, data_batch: Dict[str, Dict[str, List[str]]]) \\\n            -> Dict[str, Dict[str, List[np.array]]]:\n        sentences = data_batch['context']\n        subwords = data_batch['Subword']\n\n        inputs = self.tokenizer(sentences, return_tensors=\"pt\", padding=True)\n        inputs = {key: value.to(self.device) for key, value in inputs.items()}\n        outputs = self.model(**inputs)[0].cpu().numpy()\n        score = np.exp(outputs) / np.exp(outputs).sum(-1, keepdims=True)\n        labels_idx = score.argmax(axis=-1)[:, 1:-1]  # Remove placeholders.\n\n        pred: Dict = {\"Subword\": {\"ner\": [], \"tid\": []}}\n\n        for i in range(len(subwords[\"tid\"])):\n            tids = subwords[\"tid\"][i]\n            ner_tags = []\n            for j in range(len(tids)):\n                ner_tags.append(self.model.config.id2label[labels_idx[i, j]])\n\n            pred[\"Subword\"][\"ner\"].append(np.array(ner_tags))\n            pred[\"Subword\"][\"tid\"].append(np.array(tids))\n\n        return pred\n\n    def _fill_sub_entities(self, entity_groups_disagg, data_pack, tids):\n        first_idx = entity_groups_disagg[0]['idx']\n        first_tid = entity_groups_disagg[0]['tid']\n        while first_idx > 0 and data_pack.get_entry(first_tid).is_subword:\n            first_idx -= 1\n            first_tid -= 1\n\n        last_idx = entity_groups_disagg[-1]['idx']\n        last_tid = entity_groups_disagg[-1]['tid']\n\n        while last_idx < len(tids) and data_pack.get_entry(last_tid + 1)\\\n                                                .is_subword:\n            last_idx += 1\n            last_tid += 1\n\n        return first_tid, last_tid\n\n    def _group_entities(self, entities, data_pack, tids):\n        \"\"\"Find and group adjacent tokens considered to have the same entity\n\n        Logic: An entity starts with a \"B\" and extends to the end of the\n        word that contains last \"I\"\n\n        \"\"\"\n        entity_groups = []\n        entity_groups_disagg = []\n\n        for entity in entities:\n            subword = data_pack.get_entry(entity['tid'])\n            if entity['label'] == 'B' and not subword.is_subword:\n                if entity_groups_disagg:\n                    entity_groups_disagg = \\\n                        self._fill_sub_entities(entity_groups_disagg,\n                                                data_pack,\n                                                tids)\n                    entity_groups.append(entity_groups_disagg)\n                entity_groups_disagg = [entity]\n            else:\n                entity_groups_disagg.append(entity)\n\n        if entity_groups_disagg:\n            entity_groups_disagg = self._fill_sub_entities(entity_groups_disagg,\n                                                           data_pack,\n                                                           tids)\n            entity_groups.append(entity_groups_disagg)\n\n        return entity_groups\n\n    def pack(self, data_pack: DataPack,\n             output_dict: Optional[Dict[str, Dict[str, List[str]]]] = None):\n        \"\"\"\n        Write the prediction results back to datapack. by writing the predicted\n        ner to the original subwords and convert predictions to something that\n        makes sense in a word-by-word segmentation\n        \"\"\"\n\n        if output_dict is None:\n            return\n\n        for i in range(len(output_dict[\"Subword\"][\"tid\"])):\n            tids = output_dict[\"Subword\"][\"tid\"][i]\n            labels = output_dict[\"Subword\"][\"ner\"][i]\n\n            entities = []\n            # Filter to labels not in `self.ignore_labels`\n            entities = [dict(idx=idx, label=label, tid=tid)\n                        for idx, (label, tid) in enumerate(zip(labels, tids))\n                        if label not in self.ft_configs.ignore_labels]\n\n            entity_groups = self._group_entities(entities, data_pack, tids)\n            # Add NER tags and create EntityMention ontologies.\n            for first_tid, last_tid in entity_groups:\n                first_token: Subword = data_pack.get_entry(  # type: ignore\n                    first_tid)\n                first_token.ner = 'B-' + self.ft_configs.ner_type\n\n                for tid in range(first_tid + 1, last_tid + 1):\n                    token: Subword = data_pack.get_entry(tid)  # type: ignore\n                    token.ner = 'I-' + self.ft_configs.ner_type\n\n                begin = first_token.span.begin\n                end = data_pack.get_entry(last_tid).span.end\n                entity = EntityMention(data_pack, begin, end)\n                entity.ner_type = self.ft_configs.ner_type\n\n    @classmethod\n    def default_configs(cls):\n        r\"\"\"Default config for NER Predictor\"\"\"\n\n        configs = super().default_configs()\n        # TODO: Batcher in NER need to be update to use the sytem one.\n        configs[\"batcher\"] = {\"batch_size\": 10}\n\n        more_configs = {'model_path': None,\n                        'ner_type': 'BioEntity',\n                        'ignore_labels': ['O']}\n\n        configs.update(more_configs)\n        return configs\n",
        "source_code_len": 6739,
        "target_code": "        return configs\n",
        "target_code_len": 23,
        "diff_format": "@@ -355,180 +350,1 @@\n         return configs\n-\n-\n-class BioBERTNERPredictor(FixedSizeBatchProcessor):\n-    \"\"\"\n-       An Named Entity Recognizer fine-tuned on BioBERT\n-\n-       Note that to use :class:`BioBERTNERPredictor`, the :attr:`ontology` of\n-       :class:`Pipeline` must be an ontology that include\n-       ``ft.onto.base_ontology.Subword`` and ``ft.onto.base_ontology.Sentence``.\n-    \"\"\"\n-\n-    def __init__(self):\n-        super().__init__()\n-        self.resources = None\n-        self.device = None\n-\n-        self.ft_configs = None\n-        self.model_config = None\n-        self.model = None\n-        self.tokenizer = None\n-\n-    @staticmethod\n-    def _define_context() -> Type[Annotation]:\n-        return Sentence\n-\n-    @staticmethod\n-    def _define_input_info() -> DataRequest:\n-        input_info: DataRequest = {\n-            Subword: [],\n-            Sentence: [],\n-        }\n-        return input_info\n-\n-    def initialize(self, resources: Resources, configs: Config):\n-        super().initialize(resources, configs)\n-\n-        if resources.get(\"device\"):\n-            self.device = resources.get(\"device\")\n-        else:\n-            self.device = torch.device('cuda') if torch.cuda.is_available() \\\n-                          else torch.device('cpu')\n-\n-        self.resources = resources\n-        self.ft_configs = configs\n-\n-        model_path = self.ft_configs.model_path\n-        self.model_config = AutoConfig.from_pretrained(model_path)\n-        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n-        self.model = AutoModelForTokenClassification.from_pretrained(\n-            model_path,\n-            from_tf=bool(\".ckpt\" in model_path),\n-            config=self.model_config\n-        )\n-        self.model.to(self.device)\n-\n-    @torch.no_grad()\n-    def predict(self, data_batch: Dict[str, Dict[str, List[str]]]) \\\n-            -> Dict[str, Dict[str, List[np.array]]]:\n-        sentences = data_batch['context']\n-        subwords = data_batch['Subword']\n-\n-        inputs = self.tokenizer(sentences, return_tensors=\"pt\", padding=True)\n-        inputs = {key: value.to(self.device) for key, value in inputs.items()}\n-        outputs = self.model(**inputs)[0].cpu().numpy()\n-        score = np.exp(outputs) / np.exp(outputs).sum(-1, keepdims=True)\n-        labels_idx = score.argmax(axis=-1)[:, 1:-1]  # Remove placeholders.\n-\n-        pred: Dict = {\"Subword\": {\"ner\": [], \"tid\": []}}\n-\n-        for i in range(len(subwords[\"tid\"])):\n-            tids = subwords[\"tid\"][i]\n-            ner_tags = []\n-            for j in range(len(tids)):\n-                ner_tags.append(self.model.config.id2label[labels_idx[i, j]])\n-\n-            pred[\"Subword\"][\"ner\"].append(np.array(ner_tags))\n-            pred[\"Subword\"][\"tid\"].append(np.array(tids))\n-\n-        return pred\n-\n-    def _fill_sub_entities(self, entity_groups_disagg, data_pack, tids):\n-        first_idx = entity_groups_disagg[0]['idx']\n-        first_tid = entity_groups_disagg[0]['tid']\n-        while first_idx > 0 and data_pack.get_entry(first_tid).is_subword:\n-            first_idx -= 1\n-            first_tid -= 1\n-\n-        last_idx = entity_groups_disagg[-1]['idx']\n-        last_tid = entity_groups_disagg[-1]['tid']\n-\n-        while last_idx < len(tids) and data_pack.get_entry(last_tid + 1)\\\n-                                                .is_subword:\n-            last_idx += 1\n-            last_tid += 1\n-\n-        return first_tid, last_tid\n-\n-    def _group_entities(self, entities, data_pack, tids):\n-        \"\"\"Find and group adjacent tokens considered to have the same entity\n-\n-        Logic: An entity starts with a \"B\" and extends to the end of the\n-        word that contains last \"I\"\n-\n-        \"\"\"\n-        entity_groups = []\n-        entity_groups_disagg = []\n-\n-        for entity in entities:\n-            subword = data_pack.get_entry(entity['tid'])\n-            if entity['label'] == 'B' and not subword.is_subword:\n-                if entity_groups_disagg:\n-                    entity_groups_disagg = \\\n-                        self._fill_sub_entities(entity_groups_disagg,\n-                                                data_pack,\n-                                                tids)\n-                    entity_groups.append(entity_groups_disagg)\n-                entity_groups_disagg = [entity]\n-            else:\n-                entity_groups_disagg.append(entity)\n-\n-        if entity_groups_disagg:\n-            entity_groups_disagg = self._fill_sub_entities(entity_groups_disagg,\n-                                                           data_pack,\n-                                                           tids)\n-            entity_groups.append(entity_groups_disagg)\n-\n-        return entity_groups\n-\n-    def pack(self, data_pack: DataPack,\n-             output_dict: Optional[Dict[str, Dict[str, List[str]]]] = None):\n-        \"\"\"\n-        Write the prediction results back to datapack. by writing the predicted\n-        ner to the original subwords and convert predictions to something that\n-        makes sense in a word-by-word segmentation\n-        \"\"\"\n-\n-        if output_dict is None:\n-            return\n-\n-        for i in range(len(output_dict[\"Subword\"][\"tid\"])):\n-            tids = output_dict[\"Subword\"][\"tid\"][i]\n-            labels = output_dict[\"Subword\"][\"ner\"][i]\n-\n-            entities = []\n-            # Filter to labels not in `self.ignore_labels`\n-            entities = [dict(idx=idx, label=label, tid=tid)\n-                        for idx, (label, tid) in enumerate(zip(labels, tids))\n-                        if label not in self.ft_configs.ignore_labels]\n-\n-            entity_groups = self._group_entities(entities, data_pack, tids)\n-            # Add NER tags and create EntityMention ontologies.\n-            for first_tid, last_tid in entity_groups:\n-                first_token: Subword = data_pack.get_entry(  # type: ignore\n-                    first_tid)\n-                first_token.ner = 'B-' + self.ft_configs.ner_type\n-\n-                for tid in range(first_tid + 1, last_tid + 1):\n-                    token: Subword = data_pack.get_entry(tid)  # type: ignore\n-                    token.ner = 'I-' + self.ft_configs.ner_type\n-\n-                begin = first_token.span.begin\n-                end = data_pack.get_entry(last_tid).span.end\n-                entity = EntityMention(data_pack, begin, end)\n-                entity.ner_type = self.ft_configs.ner_type\n-\n-    @classmethod\n-    def default_configs(cls):\n-        r\"\"\"Default config for NER Predictor\"\"\"\n-\n-        configs = super().default_configs()\n-        # TODO: Batcher in NER need to be update to use the sytem one.\n-        configs[\"batcher\"] = {\"batch_size\": 10}\n-\n-        more_configs = {'model_path': None,\n-                        'ner_type': 'BioEntity',\n-                        'ignore_labels': ['O']}\n-\n-        configs.update(more_configs)\n-        return configs\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent": "        return configs\n",
        "target_code_with_indent_exact_match": true,
        "source_code_with_indent": "        return configs\n\n\n<DED><DED>class BioBERTNERPredictor(FixedSizeBatchProcessor):\n    <IND>\"\"\"\n       An Named Entity Recognizer fine-tuned on BioBERT\n\n       Note that to use :class:`BioBERTNERPredictor`, the :attr:`ontology` of\n       :class:`Pipeline` must be an ontology that include\n       ``ft.onto.base_ontology.Subword`` and ``ft.onto.base_ontology.Sentence``.\n    \"\"\"\n\n    def __init__(self):\n        <IND>super().__init__()\n        self.resources = None\n        self.device = None\n\n        self.ft_configs = None\n        self.model_config = None\n        self.model = None\n        self.tokenizer = None\n\n    <DED>@staticmethod\n    def _define_context() -> Type[Annotation]:\n        <IND>return Sentence\n\n    <DED>@staticmethod\n    def _define_input_info() -> DataRequest:\n        <IND>input_info: DataRequest = {\n            Subword: [],\n            Sentence: [],\n        }\n        return input_info\n\n    <DED>def initialize(self, resources: Resources, configs: Config):\n        <IND>super().initialize(resources, configs)\n\n        if resources.get(\"device\"):\n            <IND>self.device = resources.get(\"device\")\n        <DED>else:\n            <IND>self.device = torch.device('cuda') if torch.cuda.is_available()                          else torch.device('cpu')\n\n        <DED>self.resources = resources\n        self.ft_configs = configs\n\n        model_path = self.ft_configs.model_path\n        self.model_config = AutoConfig.from_pretrained(model_path)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        self.model = AutoModelForTokenClassification.from_pretrained(\n            model_path,\n            from_tf=bool(\".ckpt\" in model_path),\n            config=self.model_config\n        )\n        self.model.to(self.device)\n\n    <DED>@torch.no_grad()\n    def predict(self, data_batch: Dict[str, Dict[str, List[str]]])            -> Dict[str, Dict[str, List[np.array]]]:\n        <IND>sentences = data_batch['context']\n        subwords = data_batch['Subword']\n\n        inputs = self.tokenizer(sentences, return_tensors=\"pt\", padding=True)\n        inputs = {key: value.to(self.device) for key, value in inputs.items()}\n        outputs = self.model(**inputs)[0].cpu().numpy()\n        score = np.exp(outputs) / np.exp(outputs).sum(-1, keepdims=True)\n        labels_idx = score.argmax(axis=-1)[:, 1:-1]  # Remove placeholders.\n\n        pred: Dict = {\"Subword\": {\"ner\": [], \"tid\": []}}\n\n        for i in range(len(subwords[\"tid\"])):\n            <IND>tids = subwords[\"tid\"][i]\n            ner_tags = []\n            for j in range(len(tids)):\n                <IND>ner_tags.append(self.model.config.id2label[labels_idx[i, j]])\n\n            <DED>pred[\"Subword\"][\"ner\"].append(np.array(ner_tags))\n            pred[\"Subword\"][\"tid\"].append(np.array(tids))\n\n        <DED>return pred\n\n    <DED>def _fill_sub_entities(self, entity_groups_disagg, data_pack, tids):\n        <IND>first_idx = entity_groups_disagg[0]['idx']\n        first_tid = entity_groups_disagg[0]['tid']\n        while first_idx > 0 and data_pack.get_entry(first_tid).is_subword:\n            <IND>first_idx -= 1\n            first_tid -= 1\n\n        <DED>last_idx = entity_groups_disagg[-1]['idx']\n        last_tid = entity_groups_disagg[-1]['tid']\n\n        while last_idx < len(tids) and data_pack.get_entry(last_tid + 1)                                                .is_subword:\n            <IND>last_idx += 1\n            last_tid += 1\n\n        <DED>return first_tid, last_tid\n\n    <DED>def _group_entities(self, entities, data_pack, tids):\n        <IND>\"\"\"Find and group adjacent tokens considered to have the same entity\n\n        Logic: An entity starts with a \"B\" and extends to the end of the\n        word that contains last \"I\"\n\n        \"\"\"\n        entity_groups = []\n        entity_groups_disagg = []\n\n        for entity in entities:\n            <IND>subword = data_pack.get_entry(entity['tid'])\n            if entity['label'] == 'B' and not subword.is_subword:\n                <IND>if entity_groups_disagg:\n                    <IND>entity_groups_disagg =                        self._fill_sub_entities(entity_groups_disagg,\n                                                data_pack,\n                                                tids)\n                    entity_groups.append(entity_groups_disagg)\n                <DED>entity_groups_disagg = [entity]\n            <DED>else:\n                <IND>entity_groups_disagg.append(entity)\n\n        <DED><DED>if entity_groups_disagg:\n            <IND>entity_groups_disagg = self._fill_sub_entities(entity_groups_disagg,\n                                                           data_pack,\n                                                           tids)\n            entity_groups.append(entity_groups_disagg)\n\n        <DED>return entity_groups\n\n    <DED>def pack(self, data_pack: DataPack,\n             output_dict: Optional[Dict[str, Dict[str, List[str]]]] = None):\n        <IND>\"\"\"\n        Write the prediction results back to datapack. by writing the predicted\n        ner to the original subwords and convert predictions to something that\n        makes sense in a word-by-word segmentation\n        \"\"\"\n\n        if output_dict is None:\n            <IND>return\n\n        <DED>for i in range(len(output_dict[\"Subword\"][\"tid\"])):\n            <IND>tids = output_dict[\"Subword\"][\"tid\"][i]\n            labels = output_dict[\"Subword\"][\"ner\"][i]\n\n            entities = []\n            # Filter to labels not in `self.ignore_labels`\n            entities = [dict(idx=idx, label=label, tid=tid)\n                        for idx, (label, tid) in enumerate(zip(labels, tids))\n                        if label not in self.ft_configs.ignore_labels]\n\n            entity_groups = self._group_entities(entities, data_pack, tids)\n            # Add NER tags and create EntityMention ontologies.\n            for first_tid, last_tid in entity_groups:\n                <IND>first_token: Subword = data_pack.get_entry(  # type: ignore\n                    first_tid)\n                first_token.ner = 'B-' + self.ft_configs.ner_type\n\n                for tid in range(first_tid + 1, last_tid + 1):\n                    <IND>token: Subword = data_pack.get_entry(tid)  # type: ignore\n                    token.ner = 'I-' + self.ft_configs.ner_type\n\n                <DED>begin = first_token.span.begin\n                end = data_pack.get_entry(last_tid).span.end\n                entity = EntityMention(data_pack, begin, end)\n                entity.ner_type = self.ft_configs.ner_type\n\n    <DED><DED><DED>@classmethod\n    def default_configs(cls):\n        <IND>r\"\"\"Default config for NER Predictor\"\"\"\n\n        configs = super().default_configs()\n        # TODO: Batcher in NER need to be update to use the sytem one.\n        configs[\"batcher\"] = {\"batch_size\": 10}\n\n        more_configs = {'model_path': None,\n                        'ner_type': 'BioEntity',\n                        'ignore_labels': ['O']}\n\n        configs.update(more_configs)\n        return configs\n"
      }
    ]
  },
  {
    "project": "asyml/forte",
    "commit": "b56a6eb94977a65078b8265041af30a9ecd21bac",
    "filename": "forte/processors/ner_predictor.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asyml-forte/forte/processors/ner_predictor.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": true,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "forte/processors/ner_predictor.py:484:4 Inconsistent override [14]: `forte.processors.ner_predictor.BioBERTNERPredictor.pack` overrides method defined in `forte.processors.base.batch_processor.BaseBatchProcessor` inconsistently. Could not find parameter `pack` in overriding signature.",
    "message": " `forte.processors.ner_predictor.BioBERTNERPredictor.pack` overrides method defined in `forte.processors.base.batch_processor.BaseBatchProcessor` inconsistently. Could not find parameter `pack` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 484,
    "warning_line": "    def pack(self, data_pack: DataPack,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "        return configs\n\n\nclass BioBERTNERPredictor(FixedSizeBatchProcessor):\n    \"\"\"\n       An Named Entity Recognizer fine-tuned on BioBERT\n\n       Note that to use :class:`BioBERTNERPredictor`, the :attr:`ontology` of\n       :class:`Pipeline` must be an ontology that include\n       ``ft.onto.base_ontology.Subword`` and ``ft.onto.base_ontology.Sentence``.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.resources = None\n        self.device = None\n\n        self.ft_configs = None\n        self.model_config = None\n        self.model = None\n        self.tokenizer = None\n\n    @staticmethod\n    def _define_context() -> Type[Annotation]:\n        return Sentence\n\n    @staticmethod\n    def _define_input_info() -> DataRequest:\n        input_info: DataRequest = {\n            Subword: [],\n            Sentence: [],\n        }\n        return input_info\n\n    def initialize(self, resources: Resources, configs: Config):\n        super().initialize(resources, configs)\n\n        if resources.get(\"device\"):\n            self.device = resources.get(\"device\")\n        else:\n            self.device = torch.device('cuda') if torch.cuda.is_available() \\\n                          else torch.device('cpu')\n\n        self.resources = resources\n        self.ft_configs = configs\n\n        model_path = self.ft_configs.model_path\n        self.model_config = AutoConfig.from_pretrained(model_path)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        self.model = AutoModelForTokenClassification.from_pretrained(\n            model_path,\n            from_tf=bool(\".ckpt\" in model_path),\n            config=self.model_config\n        )\n        self.model.to(self.device)\n\n    @torch.no_grad()\n    def predict(self, data_batch: Dict[str, Dict[str, List[str]]]) \\\n            -> Dict[str, Dict[str, List[np.array]]]:\n        sentences = data_batch['context']\n        subwords = data_batch['Subword']\n\n        inputs = self.tokenizer(sentences, return_tensors=\"pt\", padding=True)\n        inputs = {key: value.to(self.device) for key, value in inputs.items()}\n        outputs = self.model(**inputs)[0].cpu().numpy()\n        score = np.exp(outputs) / np.exp(outputs).sum(-1, keepdims=True)\n        labels_idx = score.argmax(axis=-1)[:, 1:-1]  # Remove placeholders.\n\n        pred: Dict = {\"Subword\": {\"ner\": [], \"tid\": []}}\n\n        for i in range(len(subwords[\"tid\"])):\n            tids = subwords[\"tid\"][i]\n            ner_tags = []\n            for j in range(len(tids)):\n                ner_tags.append(self.model.config.id2label[labels_idx[i, j]])\n\n            pred[\"Subword\"][\"ner\"].append(np.array(ner_tags))\n            pred[\"Subword\"][\"tid\"].append(np.array(tids))\n\n        return pred\n\n    def _fill_sub_entities(self, entity_groups_disagg, data_pack, tids):\n        first_idx = entity_groups_disagg[0]['idx']\n        first_tid = entity_groups_disagg[0]['tid']\n        while first_idx > 0 and data_pack.get_entry(first_tid).is_subword:\n            first_idx -= 1\n            first_tid -= 1\n\n        last_idx = entity_groups_disagg[-1]['idx']\n        last_tid = entity_groups_disagg[-1]['tid']\n\n        while last_idx < len(tids) and data_pack.get_entry(last_tid + 1)\\\n                                                .is_subword:\n            last_idx += 1\n            last_tid += 1\n\n        return first_tid, last_tid\n\n    def _group_entities(self, entities, data_pack, tids):\n        \"\"\"Find and group adjacent tokens considered to have the same entity\n\n        Logic: An entity starts with a \"B\" and extends to the end of the\n        word that contains last \"I\"\n\n        \"\"\"\n        entity_groups = []\n        entity_groups_disagg = []\n\n        for entity in entities:\n            subword = data_pack.get_entry(entity['tid'])\n            if entity['label'] == 'B' and not subword.is_subword:\n                if entity_groups_disagg:\n                    entity_groups_disagg = \\\n                        self._fill_sub_entities(entity_groups_disagg,\n                                                data_pack,\n                                                tids)\n                    entity_groups.append(entity_groups_disagg)\n                entity_groups_disagg = [entity]\n            else:\n                entity_groups_disagg.append(entity)\n\n        if entity_groups_disagg:\n            entity_groups_disagg = self._fill_sub_entities(entity_groups_disagg,\n                                                           data_pack,\n                                                           tids)\n            entity_groups.append(entity_groups_disagg)\n\n        return entity_groups\n\n    def pack(self, data_pack: DataPack,\n             output_dict: Optional[Dict[str, Dict[str, List[str]]]] = None):\n        \"\"\"\n        Write the prediction results back to datapack. by writing the predicted\n        ner to the original subwords and convert predictions to something that\n        makes sense in a word-by-word segmentation\n        \"\"\"\n\n        if output_dict is None:\n            return\n\n        for i in range(len(output_dict[\"Subword\"][\"tid\"])):\n            tids = output_dict[\"Subword\"][\"tid\"][i]\n            labels = output_dict[\"Subword\"][\"ner\"][i]\n\n            entities = []\n            # Filter to labels not in `self.ignore_labels`\n            entities = [dict(idx=idx, label=label, tid=tid)\n                        for idx, (label, tid) in enumerate(zip(labels, tids))\n                        if label not in self.ft_configs.ignore_labels]\n\n            entity_groups = self._group_entities(entities, data_pack, tids)\n            # Add NER tags and create EntityMention ontologies.\n            for first_tid, last_tid in entity_groups:\n                first_token: Subword = data_pack.get_entry(  # type: ignore\n                    first_tid)\n                first_token.ner = 'B-' + self.ft_configs.ner_type\n\n                for tid in range(first_tid + 1, last_tid + 1):\n                    token: Subword = data_pack.get_entry(tid)  # type: ignore\n                    token.ner = 'I-' + self.ft_configs.ner_type\n\n                begin = first_token.span.begin\n                end = data_pack.get_entry(last_tid).span.end\n                entity = EntityMention(data_pack, begin, end)\n                entity.ner_type = self.ft_configs.ner_type\n\n    @classmethod\n    def default_configs(cls):\n        r\"\"\"Default config for NER Predictor\"\"\"\n\n        configs = super().default_configs()\n        # TODO: Batcher in NER need to be update to use the sytem one.\n        configs[\"batcher\"] = {\"batch_size\": 10}\n\n        more_configs = {'model_path': None,\n                        'ner_type': 'BioEntity',\n                        'ignore_labels': ['O']}\n\n        configs.update(more_configs)\n        return configs\n",
        "source_code_len": 6739,
        "target_code": "        return configs\n",
        "target_code_len": 23,
        "diff_format": "@@ -355,180 +350,1 @@\n         return configs\n-\n-\n-class BioBERTNERPredictor(FixedSizeBatchProcessor):\n-    \"\"\"\n-       An Named Entity Recognizer fine-tuned on BioBERT\n-\n-       Note that to use :class:`BioBERTNERPredictor`, the :attr:`ontology` of\n-       :class:`Pipeline` must be an ontology that include\n-       ``ft.onto.base_ontology.Subword`` and ``ft.onto.base_ontology.Sentence``.\n-    \"\"\"\n-\n-    def __init__(self):\n-        super().__init__()\n-        self.resources = None\n-        self.device = None\n-\n-        self.ft_configs = None\n-        self.model_config = None\n-        self.model = None\n-        self.tokenizer = None\n-\n-    @staticmethod\n-    def _define_context() -> Type[Annotation]:\n-        return Sentence\n-\n-    @staticmethod\n-    def _define_input_info() -> DataRequest:\n-        input_info: DataRequest = {\n-            Subword: [],\n-            Sentence: [],\n-        }\n-        return input_info\n-\n-    def initialize(self, resources: Resources, configs: Config):\n-        super().initialize(resources, configs)\n-\n-        if resources.get(\"device\"):\n-            self.device = resources.get(\"device\")\n-        else:\n-            self.device = torch.device('cuda') if torch.cuda.is_available() \\\n-                          else torch.device('cpu')\n-\n-        self.resources = resources\n-        self.ft_configs = configs\n-\n-        model_path = self.ft_configs.model_path\n-        self.model_config = AutoConfig.from_pretrained(model_path)\n-        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n-        self.model = AutoModelForTokenClassification.from_pretrained(\n-            model_path,\n-            from_tf=bool(\".ckpt\" in model_path),\n-            config=self.model_config\n-        )\n-        self.model.to(self.device)\n-\n-    @torch.no_grad()\n-    def predict(self, data_batch: Dict[str, Dict[str, List[str]]]) \\\n-            -> Dict[str, Dict[str, List[np.array]]]:\n-        sentences = data_batch['context']\n-        subwords = data_batch['Subword']\n-\n-        inputs = self.tokenizer(sentences, return_tensors=\"pt\", padding=True)\n-        inputs = {key: value.to(self.device) for key, value in inputs.items()}\n-        outputs = self.model(**inputs)[0].cpu().numpy()\n-        score = np.exp(outputs) / np.exp(outputs).sum(-1, keepdims=True)\n-        labels_idx = score.argmax(axis=-1)[:, 1:-1]  # Remove placeholders.\n-\n-        pred: Dict = {\"Subword\": {\"ner\": [], \"tid\": []}}\n-\n-        for i in range(len(subwords[\"tid\"])):\n-            tids = subwords[\"tid\"][i]\n-            ner_tags = []\n-            for j in range(len(tids)):\n-                ner_tags.append(self.model.config.id2label[labels_idx[i, j]])\n-\n-            pred[\"Subword\"][\"ner\"].append(np.array(ner_tags))\n-            pred[\"Subword\"][\"tid\"].append(np.array(tids))\n-\n-        return pred\n-\n-    def _fill_sub_entities(self, entity_groups_disagg, data_pack, tids):\n-        first_idx = entity_groups_disagg[0]['idx']\n-        first_tid = entity_groups_disagg[0]['tid']\n-        while first_idx > 0 and data_pack.get_entry(first_tid).is_subword:\n-            first_idx -= 1\n-            first_tid -= 1\n-\n-        last_idx = entity_groups_disagg[-1]['idx']\n-        last_tid = entity_groups_disagg[-1]['tid']\n-\n-        while last_idx < len(tids) and data_pack.get_entry(last_tid + 1)\\\n-                                                .is_subword:\n-            last_idx += 1\n-            last_tid += 1\n-\n-        return first_tid, last_tid\n-\n-    def _group_entities(self, entities, data_pack, tids):\n-        \"\"\"Find and group adjacent tokens considered to have the same entity\n-\n-        Logic: An entity starts with a \"B\" and extends to the end of the\n-        word that contains last \"I\"\n-\n-        \"\"\"\n-        entity_groups = []\n-        entity_groups_disagg = []\n-\n-        for entity in entities:\n-            subword = data_pack.get_entry(entity['tid'])\n-            if entity['label'] == 'B' and not subword.is_subword:\n-                if entity_groups_disagg:\n-                    entity_groups_disagg = \\\n-                        self._fill_sub_entities(entity_groups_disagg,\n-                                                data_pack,\n-                                                tids)\n-                    entity_groups.append(entity_groups_disagg)\n-                entity_groups_disagg = [entity]\n-            else:\n-                entity_groups_disagg.append(entity)\n-\n-        if entity_groups_disagg:\n-            entity_groups_disagg = self._fill_sub_entities(entity_groups_disagg,\n-                                                           data_pack,\n-                                                           tids)\n-            entity_groups.append(entity_groups_disagg)\n-\n-        return entity_groups\n-\n-    def pack(self, data_pack: DataPack,\n-             output_dict: Optional[Dict[str, Dict[str, List[str]]]] = None):\n-        \"\"\"\n-        Write the prediction results back to datapack. by writing the predicted\n-        ner to the original subwords and convert predictions to something that\n-        makes sense in a word-by-word segmentation\n-        \"\"\"\n-\n-        if output_dict is None:\n-            return\n-\n-        for i in range(len(output_dict[\"Subword\"][\"tid\"])):\n-            tids = output_dict[\"Subword\"][\"tid\"][i]\n-            labels = output_dict[\"Subword\"][\"ner\"][i]\n-\n-            entities = []\n-            # Filter to labels not in `self.ignore_labels`\n-            entities = [dict(idx=idx, label=label, tid=tid)\n-                        for idx, (label, tid) in enumerate(zip(labels, tids))\n-                        if label not in self.ft_configs.ignore_labels]\n-\n-            entity_groups = self._group_entities(entities, data_pack, tids)\n-            # Add NER tags and create EntityMention ontologies.\n-            for first_tid, last_tid in entity_groups:\n-                first_token: Subword = data_pack.get_entry(  # type: ignore\n-                    first_tid)\n-                first_token.ner = 'B-' + self.ft_configs.ner_type\n-\n-                for tid in range(first_tid + 1, last_tid + 1):\n-                    token: Subword = data_pack.get_entry(tid)  # type: ignore\n-                    token.ner = 'I-' + self.ft_configs.ner_type\n-\n-                begin = first_token.span.begin\n-                end = data_pack.get_entry(last_tid).span.end\n-                entity = EntityMention(data_pack, begin, end)\n-                entity.ner_type = self.ft_configs.ner_type\n-\n-    @classmethod\n-    def default_configs(cls):\n-        r\"\"\"Default config for NER Predictor\"\"\"\n-\n-        configs = super().default_configs()\n-        # TODO: Batcher in NER need to be update to use the sytem one.\n-        configs[\"batcher\"] = {\"batch_size\": 10}\n-\n-        more_configs = {'model_path': None,\n-                        'ner_type': 'BioEntity',\n-                        'ignore_labels': ['O']}\n-\n-        configs.update(more_configs)\n-        return configs\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent": "        return configs\n",
        "target_code_with_indent_exact_match": true,
        "source_code_with_indent": "        return configs\n\n\n<DED><DED>class BioBERTNERPredictor(FixedSizeBatchProcessor):\n    <IND>\"\"\"\n       An Named Entity Recognizer fine-tuned on BioBERT\n\n       Note that to use :class:`BioBERTNERPredictor`, the :attr:`ontology` of\n       :class:`Pipeline` must be an ontology that include\n       ``ft.onto.base_ontology.Subword`` and ``ft.onto.base_ontology.Sentence``.\n    \"\"\"\n\n    def __init__(self):\n        <IND>super().__init__()\n        self.resources = None\n        self.device = None\n\n        self.ft_configs = None\n        self.model_config = None\n        self.model = None\n        self.tokenizer = None\n\n    <DED>@staticmethod\n    def _define_context() -> Type[Annotation]:\n        <IND>return Sentence\n\n    <DED>@staticmethod\n    def _define_input_info() -> DataRequest:\n        <IND>input_info: DataRequest = {\n            Subword: [],\n            Sentence: [],\n        }\n        return input_info\n\n    <DED>def initialize(self, resources: Resources, configs: Config):\n        <IND>super().initialize(resources, configs)\n\n        if resources.get(\"device\"):\n            <IND>self.device = resources.get(\"device\")\n        <DED>else:\n            <IND>self.device = torch.device('cuda') if torch.cuda.is_available()                          else torch.device('cpu')\n\n        <DED>self.resources = resources\n        self.ft_configs = configs\n\n        model_path = self.ft_configs.model_path\n        self.model_config = AutoConfig.from_pretrained(model_path)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        self.model = AutoModelForTokenClassification.from_pretrained(\n            model_path,\n            from_tf=bool(\".ckpt\" in model_path),\n            config=self.model_config\n        )\n        self.model.to(self.device)\n\n    <DED>@torch.no_grad()\n    def predict(self, data_batch: Dict[str, Dict[str, List[str]]])            -> Dict[str, Dict[str, List[np.array]]]:\n        <IND>sentences = data_batch['context']\n        subwords = data_batch['Subword']\n\n        inputs = self.tokenizer(sentences, return_tensors=\"pt\", padding=True)\n        inputs = {key: value.to(self.device) for key, value in inputs.items()}\n        outputs = self.model(**inputs)[0].cpu().numpy()\n        score = np.exp(outputs) / np.exp(outputs).sum(-1, keepdims=True)\n        labels_idx = score.argmax(axis=-1)[:, 1:-1]  # Remove placeholders.\n\n        pred: Dict = {\"Subword\": {\"ner\": [], \"tid\": []}}\n\n        for i in range(len(subwords[\"tid\"])):\n            <IND>tids = subwords[\"tid\"][i]\n            ner_tags = []\n            for j in range(len(tids)):\n                <IND>ner_tags.append(self.model.config.id2label[labels_idx[i, j]])\n\n            <DED>pred[\"Subword\"][\"ner\"].append(np.array(ner_tags))\n            pred[\"Subword\"][\"tid\"].append(np.array(tids))\n\n        <DED>return pred\n\n    <DED>def _fill_sub_entities(self, entity_groups_disagg, data_pack, tids):\n        <IND>first_idx = entity_groups_disagg[0]['idx']\n        first_tid = entity_groups_disagg[0]['tid']\n        while first_idx > 0 and data_pack.get_entry(first_tid).is_subword:\n            <IND>first_idx -= 1\n            first_tid -= 1\n\n        <DED>last_idx = entity_groups_disagg[-1]['idx']\n        last_tid = entity_groups_disagg[-1]['tid']\n\n        while last_idx < len(tids) and data_pack.get_entry(last_tid + 1)                                                .is_subword:\n            <IND>last_idx += 1\n            last_tid += 1\n\n        <DED>return first_tid, last_tid\n\n    <DED>def _group_entities(self, entities, data_pack, tids):\n        <IND>\"\"\"Find and group adjacent tokens considered to have the same entity\n\n        Logic: An entity starts with a \"B\" and extends to the end of the\n        word that contains last \"I\"\n\n        \"\"\"\n        entity_groups = []\n        entity_groups_disagg = []\n\n        for entity in entities:\n            <IND>subword = data_pack.get_entry(entity['tid'])\n            if entity['label'] == 'B' and not subword.is_subword:\n                <IND>if entity_groups_disagg:\n                    <IND>entity_groups_disagg =                        self._fill_sub_entities(entity_groups_disagg,\n                                                data_pack,\n                                                tids)\n                    entity_groups.append(entity_groups_disagg)\n                <DED>entity_groups_disagg = [entity]\n            <DED>else:\n                <IND>entity_groups_disagg.append(entity)\n\n        <DED><DED>if entity_groups_disagg:\n            <IND>entity_groups_disagg = self._fill_sub_entities(entity_groups_disagg,\n                                                           data_pack,\n                                                           tids)\n            entity_groups.append(entity_groups_disagg)\n\n        <DED>return entity_groups\n\n    <DED>def pack(self, data_pack: DataPack,\n             output_dict: Optional[Dict[str, Dict[str, List[str]]]] = None):\n        <IND>\"\"\"\n        Write the prediction results back to datapack. by writing the predicted\n        ner to the original subwords and convert predictions to something that\n        makes sense in a word-by-word segmentation\n        \"\"\"\n\n        if output_dict is None:\n            <IND>return\n\n        <DED>for i in range(len(output_dict[\"Subword\"][\"tid\"])):\n            <IND>tids = output_dict[\"Subword\"][\"tid\"][i]\n            labels = output_dict[\"Subword\"][\"ner\"][i]\n\n            entities = []\n            # Filter to labels not in `self.ignore_labels`\n            entities = [dict(idx=idx, label=label, tid=tid)\n                        for idx, (label, tid) in enumerate(zip(labels, tids))\n                        if label not in self.ft_configs.ignore_labels]\n\n            entity_groups = self._group_entities(entities, data_pack, tids)\n            # Add NER tags and create EntityMention ontologies.\n            for first_tid, last_tid in entity_groups:\n                <IND>first_token: Subword = data_pack.get_entry(  # type: ignore\n                    first_tid)\n                first_token.ner = 'B-' + self.ft_configs.ner_type\n\n                for tid in range(first_tid + 1, last_tid + 1):\n                    <IND>token: Subword = data_pack.get_entry(tid)  # type: ignore\n                    token.ner = 'I-' + self.ft_configs.ner_type\n\n                <DED>begin = first_token.span.begin\n                end = data_pack.get_entry(last_tid).span.end\n                entity = EntityMention(data_pack, begin, end)\n                entity.ner_type = self.ft_configs.ner_type\n\n    <DED><DED><DED>@classmethod\n    def default_configs(cls):\n        <IND>r\"\"\"Default config for NER Predictor\"\"\"\n\n        configs = super().default_configs()\n        # TODO: Batcher in NER need to be update to use the sytem one.\n        configs[\"batcher\"] = {\"batch_size\": 10}\n\n        more_configs = {'model_path': None,\n                        'ner_type': 'BioEntity',\n                        'ignore_labels': ['O']}\n\n        configs.update(more_configs)\n        return configs\n"
      }
    ]
  }
]