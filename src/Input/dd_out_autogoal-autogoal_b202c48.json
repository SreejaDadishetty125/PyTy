[
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:28:29 Invalid type [31]: Expression `autogoal.grammar.Categorical(\"mean\", \"max\")` is not a valid type.",
    "message": " Expression `autogoal.grammar.Categorical(\"mean\", \"max\")` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 28,
    "warning_line": "    def __init__(self, mode: Categorical(\"mean\", \"max\")):"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:31:4 Inconsistent override [14]: `autogoal.contrib.wrappers.VectorAggregator.run` overrides method defined in `autogoal.experimental.pipeline.Algorithm` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `autogoal.contrib.wrappers.VectorAggregator.run` overrides method defined in `autogoal.experimental.pipeline.Algorithm` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 31,
    "warning_line": "    def run(self, input: List(ContinuousVector())) -> ContinuousVector():"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:31:25 Invalid type [31]: Expression `autogoal.kb.List(autogoal.kb.ContinuousVector())` is not a valid type.",
    "message": " Expression `autogoal.kb.List(autogoal.kb.ContinuousVector())` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 31,
    "warning_line": "    def run(self, input: List(ContinuousVector())) -> ContinuousVector():"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:31:54 Invalid type [31]: Expression `autogoal.kb.ContinuousVector()` is not a valid type.",
    "message": " Expression `autogoal.kb.ContinuousVector()` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 31,
    "warning_line": "    def run(self, input: List(ContinuousVector())) -> ContinuousVector():"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:62:4 Inconsistent override [14]: `autogoal.contrib.wrappers.MatrixBuilder.run` overrides method defined in `autogoal.experimental.pipeline.Algorithm` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `autogoal.contrib.wrappers.MatrixBuilder.run` overrides method defined in `autogoal.experimental.pipeline.Algorithm` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 62,
    "warning_line": "    def run(self, input: List(ContinuousVector())) -> MatrixContinuousDense():"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:62:25 Invalid type [31]: Expression `autogoal.kb.List(autogoal.kb.ContinuousVector())` is not a valid type.",
    "message": " Expression `autogoal.kb.List(autogoal.kb.ContinuousVector())` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 62,
    "warning_line": "    def run(self, input: List(ContinuousVector())) -> MatrixContinuousDense():"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:62:54 Invalid type [31]: Expression `autogoal.kb.MatrixContinuousDense()` is not a valid type.",
    "message": " Expression `autogoal.kb.MatrixContinuousDense()` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 62,
    "warning_line": "    def run(self, input: List(ContinuousVector())) -> MatrixContinuousDense():"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:91:4 Inconsistent override [14]: `autogoal.contrib.wrappers.TensorBuilder.run` overrides method defined in `autogoal.experimental.pipeline.Algorithm` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `autogoal.contrib.wrappers.TensorBuilder.run` overrides method defined in `autogoal.experimental.pipeline.Algorithm` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 91,
    "warning_line": "    def run(self, input: List(MatrixContinuousDense())) -> Tensor3():"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:91:25 Invalid type [31]: Expression `autogoal.kb.List(autogoal.kb.MatrixContinuousDense())` is not a valid type.",
    "message": " Expression `autogoal.kb.List(autogoal.kb.MatrixContinuousDense())` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 91,
    "warning_line": "    def run(self, input: List(MatrixContinuousDense())) -> Tensor3():"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:91:59 Invalid type [31]: Expression `autogoal.kb.Tensor3()` is not a valid type.",
    "message": " Expression `autogoal.kb.Tensor3()` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 91,
    "warning_line": "    def run(self, input: List(MatrixContinuousDense())) -> Tensor3():"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:97:4 Inconsistent override [14]: `autogoal.contrib.wrappers.FlagsMerger.run` overrides method defined in `autogoal.experimental.pipeline.Algorithm` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `autogoal.contrib.wrappers.FlagsMerger.run` overrides method defined in `autogoal.experimental.pipeline.Algorithm` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 97,
    "warning_line": "    def run(self, input: List(Flags())) -> Flags():"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:97:25 Invalid type [31]: Expression `autogoal.kb.List(autogoal.kb.Flags())` is not a valid type.",
    "message": " Expression `autogoal.kb.List(autogoal.kb.Flags())` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 97,
    "warning_line": "    def run(self, input: List(Flags())) -> Flags():"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:97:43 Invalid type [31]: Expression `autogoal.kb.Flags()` is not a valid type.",
    "message": " Expression `autogoal.kb.Flags()` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 97,
    "warning_line": "    def run(self, input: List(Flags())) -> Flags():"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:110:20 Invalid type [31]: Expression `autogoal.kb.Distinct(autogoal.kb.algorithm(autogoal.kb.Word(), autogoal.kb.Flags()), $parameter$exceptions = [\"MultipleFeatureExtractor\"])` is not a valid type.",
    "message": " Expression `autogoal.kb.Distinct(autogoal.kb.algorithm(autogoal.kb.Word(), autogoal.kb.Flags()), $parameter$exceptions = [\"MultipleFeatureExtractor\"])` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 110,
    "warning_line": "        extractors: Distinct("
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:113:16 Invalid type [31]: Expression `autogoal.kb.algorithm(autogoal.kb.List(autogoal.kb.Flags()), autogoal.kb.Flags())` is not a valid type.",
    "message": " Expression `autogoal.kb.algorithm(autogoal.kb.List(autogoal.kb.Flags()), autogoal.kb.Flags())` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 113,
    "warning_line": "        merger: algorithm(List(Flags()), Flags()),"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:118:4 Inconsistent override [14]: `autogoal.contrib.wrappers.MultipleFeatureExtractor.run` overrides method defined in `autogoal.experimental.pipeline.Algorithm` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `autogoal.contrib.wrappers.MultipleFeatureExtractor.run` overrides method defined in `autogoal.experimental.pipeline.Algorithm` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 118,
    "warning_line": "    def run(self, input: Word()) -> Flags():"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:118:25 Invalid type [31]: Expression `autogoal.kb.Word()` is not a valid type.",
    "message": " Expression `autogoal.kb.Word()` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 118,
    "warning_line": "    def run(self, input: Word()) -> Flags():"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:118:36 Invalid type [31]: Expression `autogoal.kb.Flags()` is not a valid type.",
    "message": " Expression `autogoal.kb.Flags()` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 118,
    "warning_line": "    def run(self, input: Word()) -> Flags():"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:127:19 Invalid type [31]: Expression `autogoal.kb.algorithm(autogoal.kb.Sentence(), autogoal.kb.List(autogoal.kb.Word()))` is not a valid type.",
    "message": " Expression `autogoal.kb.algorithm(autogoal.kb.Sentence(), autogoal.kb.List(autogoal.kb.Word()))` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 127,
    "warning_line": "        tokenizer: algorithm(Sentence(), List(Word())),"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:128:27 Invalid type [31]: Expression `autogoal.kb.algorithm(autogoal.kb.Word(), autogoal.kb.Flags())` is not a valid type.",
    "message": " Expression `autogoal.kb.algorithm(autogoal.kb.Word(), autogoal.kb.Flags())` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 128,
    "warning_line": "        feature_extractor: algorithm(Word(), Flags()),"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:129:22 Invalid type [31]: Expression `autogoal.grammar.Boolean()` is not a valid type.",
    "message": " Expression `autogoal.grammar.Boolean()` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 129,
    "warning_line": "        include_text: Boolean(),"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:135:4 Inconsistent override [14]: `autogoal.contrib.wrappers.SentenceFeatureExtractor.run` overrides method defined in `autogoal.experimental.pipeline.Algorithm` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `autogoal.contrib.wrappers.SentenceFeatureExtractor.run` overrides method defined in `autogoal.experimental.pipeline.Algorithm` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 135,
    "warning_line": "    def run(self, input: Sentence()) -> Flags():"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:135:25 Invalid type [31]: Expression `autogoal.kb.Sentence()` is not a valid type.",
    "message": " Expression `autogoal.kb.Sentence()` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 135,
    "warning_line": "    def run(self, input: Sentence()) -> Flags():"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:135:40 Invalid type [31]: Expression `autogoal.kb.Flags()` is not a valid type.",
    "message": " Expression `autogoal.kb.Flags()` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 135,
    "warning_line": "    def run(self, input: Sentence()) -> Flags():"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:151:19 Invalid type [31]: Expression `autogoal.kb.algorithm(autogoal.kb.Document(), autogoal.kb.List(autogoal.kb.Sentence()))` is not a valid type.",
    "message": " Expression `autogoal.kb.algorithm(autogoal.kb.Document(), autogoal.kb.List(autogoal.kb.Sentence()))` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 151,
    "warning_line": "        tokenizer: algorithm(Document(), List(Sentence())),"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:152:27 Invalid type [31]: Expression `autogoal.kb.algorithm(autogoal.kb.Sentence(), autogoal.kb.Flags())` is not a valid type.",
    "message": " Expression `autogoal.kb.algorithm(autogoal.kb.Sentence(), autogoal.kb.Flags())` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 152,
    "warning_line": "        feature_extractor: algorithm(Sentence(), Flags()),"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:157:4 Inconsistent override [14]: `autogoal.contrib.wrappers.DocumentFeatureExtractor.run` overrides method defined in `autogoal.experimental.pipeline.Algorithm` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `autogoal.contrib.wrappers.DocumentFeatureExtractor.run` overrides method defined in `autogoal.experimental.pipeline.Algorithm` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 157,
    "warning_line": "    def run(self, input: Document()) -> List(Flags()):"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:157:25 Invalid type [31]: Expression `autogoal.kb.Document()` is not a valid type.",
    "message": " Expression `autogoal.kb.Document()` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 157,
    "warning_line": "    def run(self, input: Document()) -> List(Flags()):"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:157:40 Invalid type [31]: Expression `autogoal.kb.List(autogoal.kb.Flags())` is not a valid type.",
    "message": " Expression `autogoal.kb.List(autogoal.kb.Flags())` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 157,
    "warning_line": "    def run(self, input: Document()) -> List(Flags()):"
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:171:34 Invalid type [31]: Expression `autogoal.kb.algorithm(autogoal.kb.Sentence(), autogoal.kb.List(autogoal.kb.Word()))` is not a valid type.",
    "message": " Expression `autogoal.kb.algorithm(autogoal.kb.Sentence(), autogoal.kb.List(autogoal.kb.Word()))` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 171,
    "warning_line": "    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n@nice_repr\nclass TextEntityEncoder(AlgorithmBase):\n    \"\"\"\n    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n    categorias BILOUV.\n    \"\"\"\n\n    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n        self.tokenizer = tokenizer\n\n    def run(\n        self, input: Tuple(Sentence(), List(Entity()))\n    ) -> Tuple(List(Word()), List(Postag())):\n        pass\n\n\n@nice_repr\nclass TextRelationEncoder(AlgorithmBase):\n    \"\"\"\n    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n    que se cumplen entre entidades, en una lista de ejemplos\n    por cada oraci\u00f3n.\n    \"\"\"\n\n    def __init__(self,\n        tokenizer: algorithm(Sentence(), List(Word())),\n        token_feature_extractor: algorithm(Word(), Flags()),\n        # token_sentence_encoder: algorithm(Word(), )\n    ):\n        pass\n\n    def run(\n        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n    ) -> Tuple(List(Vector()), CategoricalVector()):\n        pass\n",
        "source_code_len": 1069,
        "target_code": "\n# @nice_repr\n# class TextEntityEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n#     categorias BILOUV.\n#     \"\"\"\n\n#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n#         self.tokenizer = tokenizer\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Entity])\n#     ) -> Tuple(Seq[Word], Seq[Postag]:\n#         pass\n\n\n# @nice_repr\n# class TextRelationEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n#     que se cumplen entre entidades, en una lista de ejemplos\n#     por cada oraci\u00f3n.\n#     \"\"\"\n\n#     def __init__(self,\n#         tokenizer: algorithm(Sentence, Seq[Word]),\n#         token_feature_extractor: algorithm(Word, FeatureSet),\n#         # token_sentence_encoder: algorithm(Word, )\n#     ):\n#         pass\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n#         pass\n",
        "target_code_len": 1102,
        "diff_format": "@@ -162,37 +145,37 @@\n \n-@nice_repr\n-class TextEntityEncoder(AlgorithmBase):\n-    \"\"\"\n-    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n-    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n-    categorias BILOUV.\n-    \"\"\"\n+# @nice_repr\n+# class TextEntityEncoder(AlgorithmBase):\n+#     \"\"\"\n+#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n+#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n+#     categorias BILOUV.\n+#     \"\"\"\n \n-    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n-        self.tokenizer = tokenizer\n+#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n+#         self.tokenizer = tokenizer\n \n-    def run(\n-        self, input: Tuple(Sentence(), List(Entity()))\n-    ) -> Tuple(List(Word()), List(Postag())):\n-        pass\n+#     def run(\n+#         self, input: Tuple(Sentence, Seq[Entity])\n+#     ) -> Tuple(Seq[Word], Seq[Postag]:\n+#         pass\n \n \n-@nice_repr\n-class TextRelationEncoder(AlgorithmBase):\n-    \"\"\"\n-    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n-    que se cumplen entre entidades, en una lista de ejemplos\n-    por cada oraci\u00f3n.\n-    \"\"\"\n+# @nice_repr\n+# class TextRelationEncoder(AlgorithmBase):\n+#     \"\"\"\n+#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n+#     que se cumplen entre entidades, en una lista de ejemplos\n+#     por cada oraci\u00f3n.\n+#     \"\"\"\n \n-    def __init__(self,\n-        tokenizer: algorithm(Sentence(), List(Word())),\n-        token_feature_extractor: algorithm(Word(), Flags()),\n-        # token_sentence_encoder: algorithm(Word(), )\n-    ):\n-        pass\n+#     def __init__(self,\n+#         tokenizer: algorithm(Sentence, Seq[Word]),\n+#         token_feature_extractor: algorithm(Word, FeatureSet),\n+#         # token_sentence_encoder: algorithm(Word, )\n+#     ):\n+#         pass\n \n-    def run(\n-        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n-    ) -> Tuple(List(Vector()), CategoricalVector()):\n-        pass\n+#     def run(\n+#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n+#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n+#         pass\n",
        "source_code_with_indent": "\n<DED><DED>@nice_repr\nclass TextEntityEncoder(AlgorithmBase):\n    <IND>\"\"\"\n    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n    categorias BILOUV.\n    \"\"\"\n\n    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n        <IND>self.tokenizer = tokenizer\n\n    <DED>def run(\n        self, input: Tuple(Sentence(), List(Entity()))\n    ) -> Tuple(List(Word()), List(Postag())):\n        <IND>pass\n\n\n<DED><DED>@nice_repr\nclass TextRelationEncoder(AlgorithmBase):\n    <IND>\"\"\"\n    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n    que se cumplen entre entidades, en una lista de ejemplos\n    por cada oraci\u00f3n.\n    \"\"\"\n\n    def __init__(self,\n        tokenizer: algorithm(Sentence(), List(Word())),\n        token_feature_extractor: algorithm(Word(), Flags()),\n        # token_sentence_encoder: algorithm(Word(), )\n    ):\n        <IND>pass\n\n    <DED>def run(\n        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n    ) -> Tuple(List(Vector()), CategoricalVector()):\n        <IND>pass\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n# @nice_repr\n# class TextEntityEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n#     categorias BILOUV.\n#     \"\"\"\n\n#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n#         self.tokenizer = tokenizer\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Entity])\n#     ) -> Tuple(Seq[Word], Seq[Postag]:\n#         pass\n\n\n# @nice_repr\n# class TextRelationEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n#     que se cumplen entre entidades, en una lista de ejemplos\n#     por cada oraci\u00f3n.\n#     \"\"\"\n\n#     def __init__(self,\n#         tokenizer: algorithm(Sentence, Seq[Word]),\n#         token_feature_extractor: algorithm(Word, FeatureSet),\n#         # token_sentence_encoder: algorithm(Word, )\n#     ):\n#         pass\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n#         pass\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:174:4 Inconsistent override [14]: `autogoal.contrib.wrappers.TextEntityEncoder.run` overrides method defined in `autogoal.experimental.pipeline.Algorithm` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `autogoal.contrib.wrappers.TextEntityEncoder.run` overrides method defined in `autogoal.experimental.pipeline.Algorithm` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 174,
    "warning_line": "    def run(",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n@nice_repr\nclass TextEntityEncoder(AlgorithmBase):\n    \"\"\"\n    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n    categorias BILOUV.\n    \"\"\"\n\n    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n        self.tokenizer = tokenizer\n\n    def run(\n        self, input: Tuple(Sentence(), List(Entity()))\n    ) -> Tuple(List(Word()), List(Postag())):\n        pass\n\n\n@nice_repr\nclass TextRelationEncoder(AlgorithmBase):\n    \"\"\"\n    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n    que se cumplen entre entidades, en una lista de ejemplos\n    por cada oraci\u00f3n.\n    \"\"\"\n\n    def __init__(self,\n        tokenizer: algorithm(Sentence(), List(Word())),\n        token_feature_extractor: algorithm(Word(), Flags()),\n        # token_sentence_encoder: algorithm(Word(), )\n    ):\n        pass\n\n    def run(\n        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n    ) -> Tuple(List(Vector()), CategoricalVector()):\n        pass\n",
        "source_code_len": 1069,
        "target_code": "\n# @nice_repr\n# class TextEntityEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n#     categorias BILOUV.\n#     \"\"\"\n\n#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n#         self.tokenizer = tokenizer\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Entity])\n#     ) -> Tuple(Seq[Word], Seq[Postag]:\n#         pass\n\n\n# @nice_repr\n# class TextRelationEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n#     que se cumplen entre entidades, en una lista de ejemplos\n#     por cada oraci\u00f3n.\n#     \"\"\"\n\n#     def __init__(self,\n#         tokenizer: algorithm(Sentence, Seq[Word]),\n#         token_feature_extractor: algorithm(Word, FeatureSet),\n#         # token_sentence_encoder: algorithm(Word, )\n#     ):\n#         pass\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n#         pass\n",
        "target_code_len": 1102,
        "diff_format": "@@ -162,37 +145,37 @@\n \n-@nice_repr\n-class TextEntityEncoder(AlgorithmBase):\n-    \"\"\"\n-    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n-    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n-    categorias BILOUV.\n-    \"\"\"\n+# @nice_repr\n+# class TextEntityEncoder(AlgorithmBase):\n+#     \"\"\"\n+#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n+#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n+#     categorias BILOUV.\n+#     \"\"\"\n \n-    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n-        self.tokenizer = tokenizer\n+#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n+#         self.tokenizer = tokenizer\n \n-    def run(\n-        self, input: Tuple(Sentence(), List(Entity()))\n-    ) -> Tuple(List(Word()), List(Postag())):\n-        pass\n+#     def run(\n+#         self, input: Tuple(Sentence, Seq[Entity])\n+#     ) -> Tuple(Seq[Word], Seq[Postag]:\n+#         pass\n \n \n-@nice_repr\n-class TextRelationEncoder(AlgorithmBase):\n-    \"\"\"\n-    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n-    que se cumplen entre entidades, en una lista de ejemplos\n-    por cada oraci\u00f3n.\n-    \"\"\"\n+# @nice_repr\n+# class TextRelationEncoder(AlgorithmBase):\n+#     \"\"\"\n+#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n+#     que se cumplen entre entidades, en una lista de ejemplos\n+#     por cada oraci\u00f3n.\n+#     \"\"\"\n \n-    def __init__(self,\n-        tokenizer: algorithm(Sentence(), List(Word())),\n-        token_feature_extractor: algorithm(Word(), Flags()),\n-        # token_sentence_encoder: algorithm(Word(), )\n-    ):\n-        pass\n+#     def __init__(self,\n+#         tokenizer: algorithm(Sentence, Seq[Word]),\n+#         token_feature_extractor: algorithm(Word, FeatureSet),\n+#         # token_sentence_encoder: algorithm(Word, )\n+#     ):\n+#         pass\n \n-    def run(\n-        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n-    ) -> Tuple(List(Vector()), CategoricalVector()):\n-        pass\n+#     def run(\n+#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n+#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n+#         pass\n",
        "source_code_with_indent": "\n<DED><DED>@nice_repr\nclass TextEntityEncoder(AlgorithmBase):\n    <IND>\"\"\"\n    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n    categorias BILOUV.\n    \"\"\"\n\n    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n        <IND>self.tokenizer = tokenizer\n\n    <DED>def run(\n        self, input: Tuple(Sentence(), List(Entity()))\n    ) -> Tuple(List(Word()), List(Postag())):\n        <IND>pass\n\n\n<DED><DED>@nice_repr\nclass TextRelationEncoder(AlgorithmBase):\n    <IND>\"\"\"\n    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n    que se cumplen entre entidades, en una lista de ejemplos\n    por cada oraci\u00f3n.\n    \"\"\"\n\n    def __init__(self,\n        tokenizer: algorithm(Sentence(), List(Word())),\n        token_feature_extractor: algorithm(Word(), Flags()),\n        # token_sentence_encoder: algorithm(Word(), )\n    ):\n        <IND>pass\n\n    <DED>def run(\n        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n    ) -> Tuple(List(Vector()), CategoricalVector()):\n        <IND>pass\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n# @nice_repr\n# class TextEntityEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n#     categorias BILOUV.\n#     \"\"\"\n\n#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n#         self.tokenizer = tokenizer\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Entity])\n#     ) -> Tuple(Seq[Word], Seq[Postag]:\n#         pass\n\n\n# @nice_repr\n# class TextRelationEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n#     que se cumplen entre entidades, en una lista de ejemplos\n#     por cada oraci\u00f3n.\n#     \"\"\"\n\n#     def __init__(self,\n#         tokenizer: algorithm(Sentence, Seq[Word]),\n#         token_feature_extractor: algorithm(Word, FeatureSet),\n#         # token_sentence_encoder: algorithm(Word, )\n#     ):\n#         pass\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n#         pass\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:175:21 Invalid type [31]: Expression `autogoal.kb.Tuple(autogoal.kb.Sentence(), autogoal.kb.List(autogoal.kb.Entity()))` is not a valid type.",
    "message": " Expression `autogoal.kb.Tuple(autogoal.kb.Sentence(), autogoal.kb.List(autogoal.kb.Entity()))` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 175,
    "warning_line": "        self, input: Tuple(Sentence(), List(Entity()))",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n@nice_repr\nclass TextEntityEncoder(AlgorithmBase):\n    \"\"\"\n    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n    categorias BILOUV.\n    \"\"\"\n\n    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n        self.tokenizer = tokenizer\n\n    def run(\n        self, input: Tuple(Sentence(), List(Entity()))\n    ) -> Tuple(List(Word()), List(Postag())):\n        pass\n\n\n@nice_repr\nclass TextRelationEncoder(AlgorithmBase):\n    \"\"\"\n    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n    que se cumplen entre entidades, en una lista de ejemplos\n    por cada oraci\u00f3n.\n    \"\"\"\n\n    def __init__(self,\n        tokenizer: algorithm(Sentence(), List(Word())),\n        token_feature_extractor: algorithm(Word(), Flags()),\n        # token_sentence_encoder: algorithm(Word(), )\n    ):\n        pass\n\n    def run(\n        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n    ) -> Tuple(List(Vector()), CategoricalVector()):\n        pass\n",
        "source_code_len": 1069,
        "target_code": "\n# @nice_repr\n# class TextEntityEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n#     categorias BILOUV.\n#     \"\"\"\n\n#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n#         self.tokenizer = tokenizer\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Entity])\n#     ) -> Tuple(Seq[Word], Seq[Postag]:\n#         pass\n\n\n# @nice_repr\n# class TextRelationEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n#     que se cumplen entre entidades, en una lista de ejemplos\n#     por cada oraci\u00f3n.\n#     \"\"\"\n\n#     def __init__(self,\n#         tokenizer: algorithm(Sentence, Seq[Word]),\n#         token_feature_extractor: algorithm(Word, FeatureSet),\n#         # token_sentence_encoder: algorithm(Word, )\n#     ):\n#         pass\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n#         pass\n",
        "target_code_len": 1102,
        "diff_format": "@@ -162,37 +145,37 @@\n \n-@nice_repr\n-class TextEntityEncoder(AlgorithmBase):\n-    \"\"\"\n-    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n-    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n-    categorias BILOUV.\n-    \"\"\"\n+# @nice_repr\n+# class TextEntityEncoder(AlgorithmBase):\n+#     \"\"\"\n+#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n+#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n+#     categorias BILOUV.\n+#     \"\"\"\n \n-    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n-        self.tokenizer = tokenizer\n+#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n+#         self.tokenizer = tokenizer\n \n-    def run(\n-        self, input: Tuple(Sentence(), List(Entity()))\n-    ) -> Tuple(List(Word()), List(Postag())):\n-        pass\n+#     def run(\n+#         self, input: Tuple(Sentence, Seq[Entity])\n+#     ) -> Tuple(Seq[Word], Seq[Postag]:\n+#         pass\n \n \n-@nice_repr\n-class TextRelationEncoder(AlgorithmBase):\n-    \"\"\"\n-    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n-    que se cumplen entre entidades, en una lista de ejemplos\n-    por cada oraci\u00f3n.\n-    \"\"\"\n+# @nice_repr\n+# class TextRelationEncoder(AlgorithmBase):\n+#     \"\"\"\n+#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n+#     que se cumplen entre entidades, en una lista de ejemplos\n+#     por cada oraci\u00f3n.\n+#     \"\"\"\n \n-    def __init__(self,\n-        tokenizer: algorithm(Sentence(), List(Word())),\n-        token_feature_extractor: algorithm(Word(), Flags()),\n-        # token_sentence_encoder: algorithm(Word(), )\n-    ):\n-        pass\n+#     def __init__(self,\n+#         tokenizer: algorithm(Sentence, Seq[Word]),\n+#         token_feature_extractor: algorithm(Word, FeatureSet),\n+#         # token_sentence_encoder: algorithm(Word, )\n+#     ):\n+#         pass\n \n-    def run(\n-        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n-    ) -> Tuple(List(Vector()), CategoricalVector()):\n-        pass\n+#     def run(\n+#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n+#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n+#         pass\n",
        "source_code_with_indent": "\n<DED><DED>@nice_repr\nclass TextEntityEncoder(AlgorithmBase):\n    <IND>\"\"\"\n    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n    categorias BILOUV.\n    \"\"\"\n\n    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n        <IND>self.tokenizer = tokenizer\n\n    <DED>def run(\n        self, input: Tuple(Sentence(), List(Entity()))\n    ) -> Tuple(List(Word()), List(Postag())):\n        <IND>pass\n\n\n<DED><DED>@nice_repr\nclass TextRelationEncoder(AlgorithmBase):\n    <IND>\"\"\"\n    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n    que se cumplen entre entidades, en una lista de ejemplos\n    por cada oraci\u00f3n.\n    \"\"\"\n\n    def __init__(self,\n        tokenizer: algorithm(Sentence(), List(Word())),\n        token_feature_extractor: algorithm(Word(), Flags()),\n        # token_sentence_encoder: algorithm(Word(), )\n    ):\n        <IND>pass\n\n    <DED>def run(\n        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n    ) -> Tuple(List(Vector()), CategoricalVector()):\n        <IND>pass\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n# @nice_repr\n# class TextEntityEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n#     categorias BILOUV.\n#     \"\"\"\n\n#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n#         self.tokenizer = tokenizer\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Entity])\n#     ) -> Tuple(Seq[Word], Seq[Postag]:\n#         pass\n\n\n# @nice_repr\n# class TextRelationEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n#     que se cumplen entre entidades, en una lista de ejemplos\n#     por cada oraci\u00f3n.\n#     \"\"\"\n\n#     def __init__(self,\n#         tokenizer: algorithm(Sentence, Seq[Word]),\n#         token_feature_extractor: algorithm(Word, FeatureSet),\n#         # token_sentence_encoder: algorithm(Word, )\n#     ):\n#         pass\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n#         pass\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:176:9 Invalid type [31]: Expression `autogoal.kb.Tuple(autogoal.kb.List(autogoal.kb.Word()), autogoal.kb.List(autogoal.kb.Postag()))` is not a valid type.",
    "message": " Expression `autogoal.kb.Tuple(autogoal.kb.List(autogoal.kb.Word()), autogoal.kb.List(autogoal.kb.Postag()))` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 176,
    "warning_line": "    ) -> Tuple(List(Word()), List(Postag())):",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n@nice_repr\nclass TextEntityEncoder(AlgorithmBase):\n    \"\"\"\n    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n    categorias BILOUV.\n    \"\"\"\n\n    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n        self.tokenizer = tokenizer\n\n    def run(\n        self, input: Tuple(Sentence(), List(Entity()))\n    ) -> Tuple(List(Word()), List(Postag())):\n        pass\n\n\n@nice_repr\nclass TextRelationEncoder(AlgorithmBase):\n    \"\"\"\n    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n    que se cumplen entre entidades, en una lista de ejemplos\n    por cada oraci\u00f3n.\n    \"\"\"\n\n    def __init__(self,\n        tokenizer: algorithm(Sentence(), List(Word())),\n        token_feature_extractor: algorithm(Word(), Flags()),\n        # token_sentence_encoder: algorithm(Word(), )\n    ):\n        pass\n\n    def run(\n        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n    ) -> Tuple(List(Vector()), CategoricalVector()):\n        pass\n",
        "source_code_len": 1069,
        "target_code": "\n# @nice_repr\n# class TextEntityEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n#     categorias BILOUV.\n#     \"\"\"\n\n#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n#         self.tokenizer = tokenizer\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Entity])\n#     ) -> Tuple(Seq[Word], Seq[Postag]:\n#         pass\n\n\n# @nice_repr\n# class TextRelationEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n#     que se cumplen entre entidades, en una lista de ejemplos\n#     por cada oraci\u00f3n.\n#     \"\"\"\n\n#     def __init__(self,\n#         tokenizer: algorithm(Sentence, Seq[Word]),\n#         token_feature_extractor: algorithm(Word, FeatureSet),\n#         # token_sentence_encoder: algorithm(Word, )\n#     ):\n#         pass\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n#         pass\n",
        "target_code_len": 1102,
        "diff_format": "@@ -162,37 +145,37 @@\n \n-@nice_repr\n-class TextEntityEncoder(AlgorithmBase):\n-    \"\"\"\n-    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n-    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n-    categorias BILOUV.\n-    \"\"\"\n+# @nice_repr\n+# class TextEntityEncoder(AlgorithmBase):\n+#     \"\"\"\n+#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n+#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n+#     categorias BILOUV.\n+#     \"\"\"\n \n-    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n-        self.tokenizer = tokenizer\n+#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n+#         self.tokenizer = tokenizer\n \n-    def run(\n-        self, input: Tuple(Sentence(), List(Entity()))\n-    ) -> Tuple(List(Word()), List(Postag())):\n-        pass\n+#     def run(\n+#         self, input: Tuple(Sentence, Seq[Entity])\n+#     ) -> Tuple(Seq[Word], Seq[Postag]:\n+#         pass\n \n \n-@nice_repr\n-class TextRelationEncoder(AlgorithmBase):\n-    \"\"\"\n-    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n-    que se cumplen entre entidades, en una lista de ejemplos\n-    por cada oraci\u00f3n.\n-    \"\"\"\n+# @nice_repr\n+# class TextRelationEncoder(AlgorithmBase):\n+#     \"\"\"\n+#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n+#     que se cumplen entre entidades, en una lista de ejemplos\n+#     por cada oraci\u00f3n.\n+#     \"\"\"\n \n-    def __init__(self,\n-        tokenizer: algorithm(Sentence(), List(Word())),\n-        token_feature_extractor: algorithm(Word(), Flags()),\n-        # token_sentence_encoder: algorithm(Word(), )\n-    ):\n-        pass\n+#     def __init__(self,\n+#         tokenizer: algorithm(Sentence, Seq[Word]),\n+#         token_feature_extractor: algorithm(Word, FeatureSet),\n+#         # token_sentence_encoder: algorithm(Word, )\n+#     ):\n+#         pass\n \n-    def run(\n-        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n-    ) -> Tuple(List(Vector()), CategoricalVector()):\n-        pass\n+#     def run(\n+#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n+#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n+#         pass\n",
        "source_code_with_indent": "\n<DED><DED>@nice_repr\nclass TextEntityEncoder(AlgorithmBase):\n    <IND>\"\"\"\n    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n    categorias BILOUV.\n    \"\"\"\n\n    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n        <IND>self.tokenizer = tokenizer\n\n    <DED>def run(\n        self, input: Tuple(Sentence(), List(Entity()))\n    ) -> Tuple(List(Word()), List(Postag())):\n        <IND>pass\n\n\n<DED><DED>@nice_repr\nclass TextRelationEncoder(AlgorithmBase):\n    <IND>\"\"\"\n    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n    que se cumplen entre entidades, en una lista de ejemplos\n    por cada oraci\u00f3n.\n    \"\"\"\n\n    def __init__(self,\n        tokenizer: algorithm(Sentence(), List(Word())),\n        token_feature_extractor: algorithm(Word(), Flags()),\n        # token_sentence_encoder: algorithm(Word(), )\n    ):\n        <IND>pass\n\n    <DED>def run(\n        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n    ) -> Tuple(List(Vector()), CategoricalVector()):\n        <IND>pass\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n# @nice_repr\n# class TextEntityEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n#     categorias BILOUV.\n#     \"\"\"\n\n#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n#         self.tokenizer = tokenizer\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Entity])\n#     ) -> Tuple(Seq[Word], Seq[Postag]:\n#         pass\n\n\n# @nice_repr\n# class TextRelationEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n#     que se cumplen entre entidades, en una lista de ejemplos\n#     por cada oraci\u00f3n.\n#     \"\"\"\n\n#     def __init__(self,\n#         tokenizer: algorithm(Sentence, Seq[Word]),\n#         token_feature_extractor: algorithm(Word, FeatureSet),\n#         # token_sentence_encoder: algorithm(Word, )\n#     ):\n#         pass\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n#         pass\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:189:19 Invalid type [31]: Expression `autogoal.kb.algorithm(autogoal.kb.Sentence(), autogoal.kb.List(autogoal.kb.Word()))` is not a valid type.",
    "message": " Expression `autogoal.kb.algorithm(autogoal.kb.Sentence(), autogoal.kb.List(autogoal.kb.Word()))` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 189,
    "warning_line": "        tokenizer: algorithm(Sentence(), List(Word())),",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n@nice_repr\nclass TextEntityEncoder(AlgorithmBase):\n    \"\"\"\n    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n    categorias BILOUV.\n    \"\"\"\n\n    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n        self.tokenizer = tokenizer\n\n    def run(\n        self, input: Tuple(Sentence(), List(Entity()))\n    ) -> Tuple(List(Word()), List(Postag())):\n        pass\n\n\n@nice_repr\nclass TextRelationEncoder(AlgorithmBase):\n    \"\"\"\n    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n    que se cumplen entre entidades, en una lista de ejemplos\n    por cada oraci\u00f3n.\n    \"\"\"\n\n    def __init__(self,\n        tokenizer: algorithm(Sentence(), List(Word())),\n        token_feature_extractor: algorithm(Word(), Flags()),\n        # token_sentence_encoder: algorithm(Word(), )\n    ):\n        pass\n\n    def run(\n        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n    ) -> Tuple(List(Vector()), CategoricalVector()):\n        pass\n",
        "source_code_len": 1069,
        "target_code": "\n# @nice_repr\n# class TextEntityEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n#     categorias BILOUV.\n#     \"\"\"\n\n#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n#         self.tokenizer = tokenizer\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Entity])\n#     ) -> Tuple(Seq[Word], Seq[Postag]:\n#         pass\n\n\n# @nice_repr\n# class TextRelationEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n#     que se cumplen entre entidades, en una lista de ejemplos\n#     por cada oraci\u00f3n.\n#     \"\"\"\n\n#     def __init__(self,\n#         tokenizer: algorithm(Sentence, Seq[Word]),\n#         token_feature_extractor: algorithm(Word, FeatureSet),\n#         # token_sentence_encoder: algorithm(Word, )\n#     ):\n#         pass\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n#         pass\n",
        "target_code_len": 1102,
        "diff_format": "@@ -162,37 +145,37 @@\n \n-@nice_repr\n-class TextEntityEncoder(AlgorithmBase):\n-    \"\"\"\n-    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n-    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n-    categorias BILOUV.\n-    \"\"\"\n+# @nice_repr\n+# class TextEntityEncoder(AlgorithmBase):\n+#     \"\"\"\n+#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n+#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n+#     categorias BILOUV.\n+#     \"\"\"\n \n-    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n-        self.tokenizer = tokenizer\n+#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n+#         self.tokenizer = tokenizer\n \n-    def run(\n-        self, input: Tuple(Sentence(), List(Entity()))\n-    ) -> Tuple(List(Word()), List(Postag())):\n-        pass\n+#     def run(\n+#         self, input: Tuple(Sentence, Seq[Entity])\n+#     ) -> Tuple(Seq[Word], Seq[Postag]:\n+#         pass\n \n \n-@nice_repr\n-class TextRelationEncoder(AlgorithmBase):\n-    \"\"\"\n-    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n-    que se cumplen entre entidades, en una lista de ejemplos\n-    por cada oraci\u00f3n.\n-    \"\"\"\n+# @nice_repr\n+# class TextRelationEncoder(AlgorithmBase):\n+#     \"\"\"\n+#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n+#     que se cumplen entre entidades, en una lista de ejemplos\n+#     por cada oraci\u00f3n.\n+#     \"\"\"\n \n-    def __init__(self,\n-        tokenizer: algorithm(Sentence(), List(Word())),\n-        token_feature_extractor: algorithm(Word(), Flags()),\n-        # token_sentence_encoder: algorithm(Word(), )\n-    ):\n-        pass\n+#     def __init__(self,\n+#         tokenizer: algorithm(Sentence, Seq[Word]),\n+#         token_feature_extractor: algorithm(Word, FeatureSet),\n+#         # token_sentence_encoder: algorithm(Word, )\n+#     ):\n+#         pass\n \n-    def run(\n-        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n-    ) -> Tuple(List(Vector()), CategoricalVector()):\n-        pass\n+#     def run(\n+#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n+#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n+#         pass\n",
        "source_code_with_indent": "\n<DED><DED>@nice_repr\nclass TextEntityEncoder(AlgorithmBase):\n    <IND>\"\"\"\n    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n    categorias BILOUV.\n    \"\"\"\n\n    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n        <IND>self.tokenizer = tokenizer\n\n    <DED>def run(\n        self, input: Tuple(Sentence(), List(Entity()))\n    ) -> Tuple(List(Word()), List(Postag())):\n        <IND>pass\n\n\n<DED><DED>@nice_repr\nclass TextRelationEncoder(AlgorithmBase):\n    <IND>\"\"\"\n    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n    que se cumplen entre entidades, en una lista de ejemplos\n    por cada oraci\u00f3n.\n    \"\"\"\n\n    def __init__(self,\n        tokenizer: algorithm(Sentence(), List(Word())),\n        token_feature_extractor: algorithm(Word(), Flags()),\n        # token_sentence_encoder: algorithm(Word(), )\n    ):\n        <IND>pass\n\n    <DED>def run(\n        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n    ) -> Tuple(List(Vector()), CategoricalVector()):\n        <IND>pass\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n# @nice_repr\n# class TextEntityEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n#     categorias BILOUV.\n#     \"\"\"\n\n#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n#         self.tokenizer = tokenizer\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Entity])\n#     ) -> Tuple(Seq[Word], Seq[Postag]:\n#         pass\n\n\n# @nice_repr\n# class TextRelationEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n#     que se cumplen entre entidades, en una lista de ejemplos\n#     por cada oraci\u00f3n.\n#     \"\"\"\n\n#     def __init__(self,\n#         tokenizer: algorithm(Sentence, Seq[Word]),\n#         token_feature_extractor: algorithm(Word, FeatureSet),\n#         # token_sentence_encoder: algorithm(Word, )\n#     ):\n#         pass\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n#         pass\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:190:33 Invalid type [31]: Expression `autogoal.kb.algorithm(autogoal.kb.Word(), autogoal.kb.Flags())` is not a valid type.",
    "message": " Expression `autogoal.kb.algorithm(autogoal.kb.Word(), autogoal.kb.Flags())` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 190,
    "warning_line": "        token_feature_extractor: algorithm(Word(), Flags()),",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n@nice_repr\nclass TextEntityEncoder(AlgorithmBase):\n    \"\"\"\n    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n    categorias BILOUV.\n    \"\"\"\n\n    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n        self.tokenizer = tokenizer\n\n    def run(\n        self, input: Tuple(Sentence(), List(Entity()))\n    ) -> Tuple(List(Word()), List(Postag())):\n        pass\n\n\n@nice_repr\nclass TextRelationEncoder(AlgorithmBase):\n    \"\"\"\n    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n    que se cumplen entre entidades, en una lista de ejemplos\n    por cada oraci\u00f3n.\n    \"\"\"\n\n    def __init__(self,\n        tokenizer: algorithm(Sentence(), List(Word())),\n        token_feature_extractor: algorithm(Word(), Flags()),\n        # token_sentence_encoder: algorithm(Word(), )\n    ):\n        pass\n\n    def run(\n        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n    ) -> Tuple(List(Vector()), CategoricalVector()):\n        pass\n",
        "source_code_len": 1069,
        "target_code": "\n# @nice_repr\n# class TextEntityEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n#     categorias BILOUV.\n#     \"\"\"\n\n#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n#         self.tokenizer = tokenizer\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Entity])\n#     ) -> Tuple(Seq[Word], Seq[Postag]:\n#         pass\n\n\n# @nice_repr\n# class TextRelationEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n#     que se cumplen entre entidades, en una lista de ejemplos\n#     por cada oraci\u00f3n.\n#     \"\"\"\n\n#     def __init__(self,\n#         tokenizer: algorithm(Sentence, Seq[Word]),\n#         token_feature_extractor: algorithm(Word, FeatureSet),\n#         # token_sentence_encoder: algorithm(Word, )\n#     ):\n#         pass\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n#         pass\n",
        "target_code_len": 1102,
        "diff_format": "@@ -162,37 +145,37 @@\n \n-@nice_repr\n-class TextEntityEncoder(AlgorithmBase):\n-    \"\"\"\n-    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n-    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n-    categorias BILOUV.\n-    \"\"\"\n+# @nice_repr\n+# class TextEntityEncoder(AlgorithmBase):\n+#     \"\"\"\n+#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n+#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n+#     categorias BILOUV.\n+#     \"\"\"\n \n-    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n-        self.tokenizer = tokenizer\n+#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n+#         self.tokenizer = tokenizer\n \n-    def run(\n-        self, input: Tuple(Sentence(), List(Entity()))\n-    ) -> Tuple(List(Word()), List(Postag())):\n-        pass\n+#     def run(\n+#         self, input: Tuple(Sentence, Seq[Entity])\n+#     ) -> Tuple(Seq[Word], Seq[Postag]:\n+#         pass\n \n \n-@nice_repr\n-class TextRelationEncoder(AlgorithmBase):\n-    \"\"\"\n-    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n-    que se cumplen entre entidades, en una lista de ejemplos\n-    por cada oraci\u00f3n.\n-    \"\"\"\n+# @nice_repr\n+# class TextRelationEncoder(AlgorithmBase):\n+#     \"\"\"\n+#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n+#     que se cumplen entre entidades, en una lista de ejemplos\n+#     por cada oraci\u00f3n.\n+#     \"\"\"\n \n-    def __init__(self,\n-        tokenizer: algorithm(Sentence(), List(Word())),\n-        token_feature_extractor: algorithm(Word(), Flags()),\n-        # token_sentence_encoder: algorithm(Word(), )\n-    ):\n-        pass\n+#     def __init__(self,\n+#         tokenizer: algorithm(Sentence, Seq[Word]),\n+#         token_feature_extractor: algorithm(Word, FeatureSet),\n+#         # token_sentence_encoder: algorithm(Word, )\n+#     ):\n+#         pass\n \n-    def run(\n-        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n-    ) -> Tuple(List(Vector()), CategoricalVector()):\n-        pass\n+#     def run(\n+#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n+#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n+#         pass\n",
        "source_code_with_indent": "\n<DED><DED>@nice_repr\nclass TextEntityEncoder(AlgorithmBase):\n    <IND>\"\"\"\n    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n    categorias BILOUV.\n    \"\"\"\n\n    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n        <IND>self.tokenizer = tokenizer\n\n    <DED>def run(\n        self, input: Tuple(Sentence(), List(Entity()))\n    ) -> Tuple(List(Word()), List(Postag())):\n        <IND>pass\n\n\n<DED><DED>@nice_repr\nclass TextRelationEncoder(AlgorithmBase):\n    <IND>\"\"\"\n    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n    que se cumplen entre entidades, en una lista de ejemplos\n    por cada oraci\u00f3n.\n    \"\"\"\n\n    def __init__(self,\n        tokenizer: algorithm(Sentence(), List(Word())),\n        token_feature_extractor: algorithm(Word(), Flags()),\n        # token_sentence_encoder: algorithm(Word(), )\n    ):\n        <IND>pass\n\n    <DED>def run(\n        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n    ) -> Tuple(List(Vector()), CategoricalVector()):\n        <IND>pass\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n# @nice_repr\n# class TextEntityEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n#     categorias BILOUV.\n#     \"\"\"\n\n#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n#         self.tokenizer = tokenizer\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Entity])\n#     ) -> Tuple(Seq[Word], Seq[Postag]:\n#         pass\n\n\n# @nice_repr\n# class TextRelationEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n#     que se cumplen entre entidades, en una lista de ejemplos\n#     por cada oraci\u00f3n.\n#     \"\"\"\n\n#     def __init__(self,\n#         tokenizer: algorithm(Sentence, Seq[Word]),\n#         token_feature_extractor: algorithm(Word, FeatureSet),\n#         # token_sentence_encoder: algorithm(Word, )\n#     ):\n#         pass\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n#         pass\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:195:4 Inconsistent override [14]: `autogoal.contrib.wrappers.TextRelationEncoder.run` overrides method defined in `autogoal.experimental.pipeline.Algorithm` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `autogoal.contrib.wrappers.TextRelationEncoder.run` overrides method defined in `autogoal.experimental.pipeline.Algorithm` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 195,
    "warning_line": "    def run(",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n@nice_repr\nclass TextEntityEncoder(AlgorithmBase):\n    \"\"\"\n    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n    categorias BILOUV.\n    \"\"\"\n\n    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n        self.tokenizer = tokenizer\n\n    def run(\n        self, input: Tuple(Sentence(), List(Entity()))\n    ) -> Tuple(List(Word()), List(Postag())):\n        pass\n\n\n@nice_repr\nclass TextRelationEncoder(AlgorithmBase):\n    \"\"\"\n    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n    que se cumplen entre entidades, en una lista de ejemplos\n    por cada oraci\u00f3n.\n    \"\"\"\n\n    def __init__(self,\n        tokenizer: algorithm(Sentence(), List(Word())),\n        token_feature_extractor: algorithm(Word(), Flags()),\n        # token_sentence_encoder: algorithm(Word(), )\n    ):\n        pass\n\n    def run(\n        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n    ) -> Tuple(List(Vector()), CategoricalVector()):\n        pass\n",
        "source_code_len": 1069,
        "target_code": "\n# @nice_repr\n# class TextEntityEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n#     categorias BILOUV.\n#     \"\"\"\n\n#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n#         self.tokenizer = tokenizer\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Entity])\n#     ) -> Tuple(Seq[Word], Seq[Postag]:\n#         pass\n\n\n# @nice_repr\n# class TextRelationEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n#     que se cumplen entre entidades, en una lista de ejemplos\n#     por cada oraci\u00f3n.\n#     \"\"\"\n\n#     def __init__(self,\n#         tokenizer: algorithm(Sentence, Seq[Word]),\n#         token_feature_extractor: algorithm(Word, FeatureSet),\n#         # token_sentence_encoder: algorithm(Word, )\n#     ):\n#         pass\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n#         pass\n",
        "target_code_len": 1102,
        "diff_format": "@@ -162,37 +145,37 @@\n \n-@nice_repr\n-class TextEntityEncoder(AlgorithmBase):\n-    \"\"\"\n-    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n-    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n-    categorias BILOUV.\n-    \"\"\"\n+# @nice_repr\n+# class TextEntityEncoder(AlgorithmBase):\n+#     \"\"\"\n+#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n+#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n+#     categorias BILOUV.\n+#     \"\"\"\n \n-    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n-        self.tokenizer = tokenizer\n+#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n+#         self.tokenizer = tokenizer\n \n-    def run(\n-        self, input: Tuple(Sentence(), List(Entity()))\n-    ) -> Tuple(List(Word()), List(Postag())):\n-        pass\n+#     def run(\n+#         self, input: Tuple(Sentence, Seq[Entity])\n+#     ) -> Tuple(Seq[Word], Seq[Postag]:\n+#         pass\n \n \n-@nice_repr\n-class TextRelationEncoder(AlgorithmBase):\n-    \"\"\"\n-    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n-    que se cumplen entre entidades, en una lista de ejemplos\n-    por cada oraci\u00f3n.\n-    \"\"\"\n+# @nice_repr\n+# class TextRelationEncoder(AlgorithmBase):\n+#     \"\"\"\n+#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n+#     que se cumplen entre entidades, en una lista de ejemplos\n+#     por cada oraci\u00f3n.\n+#     \"\"\"\n \n-    def __init__(self,\n-        tokenizer: algorithm(Sentence(), List(Word())),\n-        token_feature_extractor: algorithm(Word(), Flags()),\n-        # token_sentence_encoder: algorithm(Word(), )\n-    ):\n-        pass\n+#     def __init__(self,\n+#         tokenizer: algorithm(Sentence, Seq[Word]),\n+#         token_feature_extractor: algorithm(Word, FeatureSet),\n+#         # token_sentence_encoder: algorithm(Word, )\n+#     ):\n+#         pass\n \n-    def run(\n-        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n-    ) -> Tuple(List(Vector()), CategoricalVector()):\n-        pass\n+#     def run(\n+#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n+#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n+#         pass\n",
        "source_code_with_indent": "\n<DED><DED>@nice_repr\nclass TextEntityEncoder(AlgorithmBase):\n    <IND>\"\"\"\n    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n    categorias BILOUV.\n    \"\"\"\n\n    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n        <IND>self.tokenizer = tokenizer\n\n    <DED>def run(\n        self, input: Tuple(Sentence(), List(Entity()))\n    ) -> Tuple(List(Word()), List(Postag())):\n        <IND>pass\n\n\n<DED><DED>@nice_repr\nclass TextRelationEncoder(AlgorithmBase):\n    <IND>\"\"\"\n    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n    que se cumplen entre entidades, en una lista de ejemplos\n    por cada oraci\u00f3n.\n    \"\"\"\n\n    def __init__(self,\n        tokenizer: algorithm(Sentence(), List(Word())),\n        token_feature_extractor: algorithm(Word(), Flags()),\n        # token_sentence_encoder: algorithm(Word(), )\n    ):\n        <IND>pass\n\n    <DED>def run(\n        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n    ) -> Tuple(List(Vector()), CategoricalVector()):\n        <IND>pass\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n# @nice_repr\n# class TextEntityEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n#     categorias BILOUV.\n#     \"\"\"\n\n#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n#         self.tokenizer = tokenizer\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Entity])\n#     ) -> Tuple(Seq[Word], Seq[Postag]:\n#         pass\n\n\n# @nice_repr\n# class TextRelationEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n#     que se cumplen entre entidades, en una lista de ejemplos\n#     por cada oraci\u00f3n.\n#     \"\"\"\n\n#     def __init__(self,\n#         tokenizer: algorithm(Sentence, Seq[Word]),\n#         token_feature_extractor: algorithm(Word, FeatureSet),\n#         # token_sentence_encoder: algorithm(Word, )\n#     ):\n#         pass\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n#         pass\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:196:21 Invalid type [31]: Expression `autogoal.kb.Tuple(autogoal.kb.Sentence(), autogoal.kb.List(autogoal.kb.Tuple(autogoal.kb.Entity(), autogoal.kb.Entity(), autogoal.kb.Category())))` is not a valid type.",
    "message": " Expression `autogoal.kb.Tuple(autogoal.kb.Sentence(), autogoal.kb.List(autogoal.kb.Tuple(autogoal.kb.Entity(), autogoal.kb.Entity(), autogoal.kb.Category())))` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 196,
    "warning_line": "        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n@nice_repr\nclass TextEntityEncoder(AlgorithmBase):\n    \"\"\"\n    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n    categorias BILOUV.\n    \"\"\"\n\n    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n        self.tokenizer = tokenizer\n\n    def run(\n        self, input: Tuple(Sentence(), List(Entity()))\n    ) -> Tuple(List(Word()), List(Postag())):\n        pass\n\n\n@nice_repr\nclass TextRelationEncoder(AlgorithmBase):\n    \"\"\"\n    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n    que se cumplen entre entidades, en una lista de ejemplos\n    por cada oraci\u00f3n.\n    \"\"\"\n\n    def __init__(self,\n        tokenizer: algorithm(Sentence(), List(Word())),\n        token_feature_extractor: algorithm(Word(), Flags()),\n        # token_sentence_encoder: algorithm(Word(), )\n    ):\n        pass\n\n    def run(\n        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n    ) -> Tuple(List(Vector()), CategoricalVector()):\n        pass\n",
        "source_code_len": 1069,
        "target_code": "\n# @nice_repr\n# class TextEntityEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n#     categorias BILOUV.\n#     \"\"\"\n\n#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n#         self.tokenizer = tokenizer\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Entity])\n#     ) -> Tuple(Seq[Word], Seq[Postag]:\n#         pass\n\n\n# @nice_repr\n# class TextRelationEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n#     que se cumplen entre entidades, en una lista de ejemplos\n#     por cada oraci\u00f3n.\n#     \"\"\"\n\n#     def __init__(self,\n#         tokenizer: algorithm(Sentence, Seq[Word]),\n#         token_feature_extractor: algorithm(Word, FeatureSet),\n#         # token_sentence_encoder: algorithm(Word, )\n#     ):\n#         pass\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n#         pass\n",
        "target_code_len": 1102,
        "diff_format": "@@ -162,37 +145,37 @@\n \n-@nice_repr\n-class TextEntityEncoder(AlgorithmBase):\n-    \"\"\"\n-    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n-    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n-    categorias BILOUV.\n-    \"\"\"\n+# @nice_repr\n+# class TextEntityEncoder(AlgorithmBase):\n+#     \"\"\"\n+#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n+#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n+#     categorias BILOUV.\n+#     \"\"\"\n \n-    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n-        self.tokenizer = tokenizer\n+#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n+#         self.tokenizer = tokenizer\n \n-    def run(\n-        self, input: Tuple(Sentence(), List(Entity()))\n-    ) -> Tuple(List(Word()), List(Postag())):\n-        pass\n+#     def run(\n+#         self, input: Tuple(Sentence, Seq[Entity])\n+#     ) -> Tuple(Seq[Word], Seq[Postag]:\n+#         pass\n \n \n-@nice_repr\n-class TextRelationEncoder(AlgorithmBase):\n-    \"\"\"\n-    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n-    que se cumplen entre entidades, en una lista de ejemplos\n-    por cada oraci\u00f3n.\n-    \"\"\"\n+# @nice_repr\n+# class TextRelationEncoder(AlgorithmBase):\n+#     \"\"\"\n+#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n+#     que se cumplen entre entidades, en una lista de ejemplos\n+#     por cada oraci\u00f3n.\n+#     \"\"\"\n \n-    def __init__(self,\n-        tokenizer: algorithm(Sentence(), List(Word())),\n-        token_feature_extractor: algorithm(Word(), Flags()),\n-        # token_sentence_encoder: algorithm(Word(), )\n-    ):\n-        pass\n+#     def __init__(self,\n+#         tokenizer: algorithm(Sentence, Seq[Word]),\n+#         token_feature_extractor: algorithm(Word, FeatureSet),\n+#         # token_sentence_encoder: algorithm(Word, )\n+#     ):\n+#         pass\n \n-    def run(\n-        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n-    ) -> Tuple(List(Vector()), CategoricalVector()):\n-        pass\n+#     def run(\n+#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n+#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n+#         pass\n",
        "source_code_with_indent": "\n<DED><DED>@nice_repr\nclass TextEntityEncoder(AlgorithmBase):\n    <IND>\"\"\"\n    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n    categorias BILOUV.\n    \"\"\"\n\n    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n        <IND>self.tokenizer = tokenizer\n\n    <DED>def run(\n        self, input: Tuple(Sentence(), List(Entity()))\n    ) -> Tuple(List(Word()), List(Postag())):\n        <IND>pass\n\n\n<DED><DED>@nice_repr\nclass TextRelationEncoder(AlgorithmBase):\n    <IND>\"\"\"\n    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n    que se cumplen entre entidades, en una lista de ejemplos\n    por cada oraci\u00f3n.\n    \"\"\"\n\n    def __init__(self,\n        tokenizer: algorithm(Sentence(), List(Word())),\n        token_feature_extractor: algorithm(Word(), Flags()),\n        # token_sentence_encoder: algorithm(Word(), )\n    ):\n        <IND>pass\n\n    <DED>def run(\n        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n    ) -> Tuple(List(Vector()), CategoricalVector()):\n        <IND>pass\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n# @nice_repr\n# class TextEntityEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n#     categorias BILOUV.\n#     \"\"\"\n\n#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n#         self.tokenizer = tokenizer\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Entity])\n#     ) -> Tuple(Seq[Word], Seq[Postag]:\n#         pass\n\n\n# @nice_repr\n# class TextRelationEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n#     que se cumplen entre entidades, en una lista de ejemplos\n#     por cada oraci\u00f3n.\n#     \"\"\"\n\n#     def __init__(self,\n#         tokenizer: algorithm(Sentence, Seq[Word]),\n#         token_feature_extractor: algorithm(Word, FeatureSet),\n#         # token_sentence_encoder: algorithm(Word, )\n#     ):\n#         pass\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n#         pass\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "autogoal/autogoal",
    "commit": "b202c481e447b7f7564ab602e5fa0352af2be99f",
    "filename": "autogoal/contrib/wrappers.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/autogoal-autogoal/autogoal/contrib/wrappers.py",
    "file_hunks_size": 12,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "autogoal/contrib/wrappers.py:197:9 Invalid type [31]: Expression `autogoal.kb.Tuple(autogoal.kb.List(autogoal.kb.Vector()), autogoal.kb.CategoricalVector())` is not a valid type.",
    "message": " Expression `autogoal.kb.Tuple(autogoal.kb.List(autogoal.kb.Vector()), autogoal.kb.CategoricalVector())` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 197,
    "warning_line": "    ) -> Tuple(List(Vector()), CategoricalVector()):",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n@nice_repr\nclass TextEntityEncoder(AlgorithmBase):\n    \"\"\"\n    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n    categorias BILOUV.\n    \"\"\"\n\n    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n        self.tokenizer = tokenizer\n\n    def run(\n        self, input: Tuple(Sentence(), List(Entity()))\n    ) -> Tuple(List(Word()), List(Postag())):\n        pass\n\n\n@nice_repr\nclass TextRelationEncoder(AlgorithmBase):\n    \"\"\"\n    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n    que se cumplen entre entidades, en una lista de ejemplos\n    por cada oraci\u00f3n.\n    \"\"\"\n\n    def __init__(self,\n        tokenizer: algorithm(Sentence(), List(Word())),\n        token_feature_extractor: algorithm(Word(), Flags()),\n        # token_sentence_encoder: algorithm(Word(), )\n    ):\n        pass\n\n    def run(\n        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n    ) -> Tuple(List(Vector()), CategoricalVector()):\n        pass\n",
        "source_code_len": 1069,
        "target_code": "\n# @nice_repr\n# class TextEntityEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n#     categorias BILOUV.\n#     \"\"\"\n\n#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n#         self.tokenizer = tokenizer\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Entity])\n#     ) -> Tuple(Seq[Word], Seq[Postag]:\n#         pass\n\n\n# @nice_repr\n# class TextRelationEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n#     que se cumplen entre entidades, en una lista de ejemplos\n#     por cada oraci\u00f3n.\n#     \"\"\"\n\n#     def __init__(self,\n#         tokenizer: algorithm(Sentence, Seq[Word]),\n#         token_feature_extractor: algorithm(Word, FeatureSet),\n#         # token_sentence_encoder: algorithm(Word, )\n#     ):\n#         pass\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n#         pass\n",
        "target_code_len": 1102,
        "diff_format": "@@ -162,37 +145,37 @@\n \n-@nice_repr\n-class TextEntityEncoder(AlgorithmBase):\n-    \"\"\"\n-    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n-    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n-    categorias BILOUV.\n-    \"\"\"\n+# @nice_repr\n+# class TextEntityEncoder(AlgorithmBase):\n+#     \"\"\"\n+#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n+#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n+#     categorias BILOUV.\n+#     \"\"\"\n \n-    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n-        self.tokenizer = tokenizer\n+#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n+#         self.tokenizer = tokenizer\n \n-    def run(\n-        self, input: Tuple(Sentence(), List(Entity()))\n-    ) -> Tuple(List(Word()), List(Postag())):\n-        pass\n+#     def run(\n+#         self, input: Tuple(Sentence, Seq[Entity])\n+#     ) -> Tuple(Seq[Word], Seq[Postag]:\n+#         pass\n \n \n-@nice_repr\n-class TextRelationEncoder(AlgorithmBase):\n-    \"\"\"\n-    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n-    que se cumplen entre entidades, en una lista de ejemplos\n-    por cada oraci\u00f3n.\n-    \"\"\"\n+# @nice_repr\n+# class TextRelationEncoder(AlgorithmBase):\n+#     \"\"\"\n+#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n+#     que se cumplen entre entidades, en una lista de ejemplos\n+#     por cada oraci\u00f3n.\n+#     \"\"\"\n \n-    def __init__(self,\n-        tokenizer: algorithm(Sentence(), List(Word())),\n-        token_feature_extractor: algorithm(Word(), Flags()),\n-        # token_sentence_encoder: algorithm(Word(), )\n-    ):\n-        pass\n+#     def __init__(self,\n+#         tokenizer: algorithm(Sentence, Seq[Word]),\n+#         token_feature_extractor: algorithm(Word, FeatureSet),\n+#         # token_sentence_encoder: algorithm(Word, )\n+#     ):\n+#         pass\n \n-    def run(\n-        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n-    ) -> Tuple(List(Vector()), CategoricalVector()):\n-        pass\n+#     def run(\n+#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n+#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n+#         pass\n",
        "source_code_with_indent": "\n<DED><DED>@nice_repr\nclass TextEntityEncoder(AlgorithmBase):\n    <IND>\"\"\"\n    Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n    reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n    categorias BILOUV.\n    \"\"\"\n\n    def __init__(self, tokenizer: algorithm(Sentence(), List(Word()))):\n        <IND>self.tokenizer = tokenizer\n\n    <DED>def run(\n        self, input: Tuple(Sentence(), List(Entity()))\n    ) -> Tuple(List(Word()), List(Postag())):\n        <IND>pass\n\n\n<DED><DED>@nice_repr\nclass TextRelationEncoder(AlgorithmBase):\n    <IND>\"\"\"\n    Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n    que se cumplen entre entidades, en una lista de ejemplos\n    por cada oraci\u00f3n.\n    \"\"\"\n\n    def __init__(self,\n        tokenizer: algorithm(Sentence(), List(Word())),\n        token_feature_extractor: algorithm(Word(), Flags()),\n        # token_sentence_encoder: algorithm(Word(), )\n    ):\n        <IND>pass\n\n    <DED>def run(\n        self, input: Tuple(Sentence(), List(Tuple(Entity(), Entity(), Category())))\n    ) -> Tuple(List(Vector()), CategoricalVector()):\n        <IND>pass\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n# @nice_repr\n# class TextEntityEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades\n#     reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas\n#     categorias BILOUV.\n#     \"\"\"\n\n#     def __init__(self, tokenizer: algorithm(Sentence, Seq[Word])):\n#         self.tokenizer = tokenizer\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Entity])\n#     ) -> Tuple(Seq[Word], Seq[Postag]:\n#         pass\n\n\n# @nice_repr\n# class TextRelationEncoder(AlgorithmBase):\n#     \"\"\"\n#     Convierte una oraci\u00f3n en texto plano y una lista de relaciones \n#     que se cumplen entre entidades, en una lista de ejemplos\n#     por cada oraci\u00f3n.\n#     \"\"\"\n\n#     def __init__(self,\n#         tokenizer: algorithm(Sentence, Seq[Word]),\n#         token_feature_extractor: algorithm(Word, FeatureSet),\n#         # token_sentence_encoder: algorithm(Word, )\n#     ):\n#         pass\n\n#     def run(\n#         self, input: Tuple(Sentence, Seq[Tupl](Entity(), Entity(), Category())))\n#     ) -> Tuple(Seq[Vect]r()), CategoricalVector()):\n#         pass\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  }
]