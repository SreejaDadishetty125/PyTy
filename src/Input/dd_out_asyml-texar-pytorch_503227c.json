[
  {
    "project": "asyml/texar-pytorch",
    "commit": "503227c6f58543775fb7685880aecbf2174ad1a6",
    "filename": "examples/sentence_classifier/clas_main.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asyml-texar-pytorch/examples/sentence_classifier/classifier_main.py",
    "file_hunks_size": 2,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "examples/sentence_classifier/clas_main.py:59:23 Call error [29]: `tx.modules.classifiers.conv_classifiers.Conv1DClassifier` is not a function.",
    "message": " `tx.modules.classifiers.conv_classifiers.Conv1DClassifier` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 59,
    "warning_line": "        logits, pred = self.classifier("
  },
  {
    "project": "asyml/texar-pytorch",
    "commit": "503227c6f58543775fb7685880aecbf2174ad1a6",
    "filename": "examples/sentence_classifier/clas_main.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asyml-texar-pytorch/examples/sentence_classifier/classifier_main.py",
    "file_hunks_size": 2,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "examples/sentence_classifier/clas_main.py:60:12 Call error [29]: `tx.modules.embedders.embedders.WordEmbedder` is not a function.",
    "message": " `tx.modules.embedders.embedders.WordEmbedder` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 60,
    "warning_line": "            self.embedder(batch['sentence_text_ids']))"
  },
  {
    "project": "asyml/texar-pytorch",
    "commit": "503227c6f58543775fb7685880aecbf2174ad1a6",
    "filename": "texar/torch/data/tokenizers/tokenizer_base.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/asyml-texar-pytorch/texar/torch/data/tokenizers/tokenizer_base.py",
    "file_hunks_size": 11,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "texar/torch/data/tokenizers/tokenizer_base.py:502:8 Incompatible variable type [9]: all_ids is declared to have type `List[int]` but is used as type `List[typing.Union[List[int], int]]`.",
    "message": " all_ids is declared to have type `List[int]` but is used as type `List[typing.Union[List[int], int]]`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 502,
    "warning_line": "        all_ids: List[int] = list(",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nfrom typing import Any, Dict, List, Optional, Tuple\n\n",
        "source_code_len": 54,
        "target_code": "\nfrom typing import Any, Dict, List, Optional, Tuple, overload\n\n",
        "target_code_len": 64,
        "diff_format": "@@ -20,3 +20,3 @@\n \n-from typing import Any, Dict, List, Optional, Tuple\n+from typing import Any, Dict, List, Optional, Tuple, overload\n \n",
        "source_code_with_indent": "\nfrom typing import Any, Dict, List, Optional, Tuple\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\nfrom typing import Any, Dict, List, Optional, Tuple, overload\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    def map_token_to_id(self, tokens: MaybeList[str]) -> MaybeList[int]:\n        r\"\"\"Maps a single token or a sequence of tokens to a integer id\n",
        "source_code_len": 146,
        "target_code": "\n    # TODO: Remove these once pylint supports function stubs.\n    # pylint: disable=unused-argument,function-redefined\n\n    @overload\n    def map_token_to_id(self, tokens: str) -> int:\n        ...\n\n    @overload\n    def map_token_to_id(self, tokens: List[str]) -> List[int]:\n        ...\n\n    def map_token_to_id(self, tokens):\n        r\"\"\"Maps a single token or a sequence of tokens to a integer id\n",
        "target_code_len": 400,
        "diff_format": "@@ -353,3 +353,14 @@\n \n-    def map_token_to_id(self, tokens: MaybeList[str]) -> MaybeList[int]:\n+    # TODO: Remove these once pylint supports function stubs.\n+    # pylint: disable=unused-argument,function-redefined\n+\n+    @overload\n+    def map_token_to_id(self, tokens: str) -> int:\n+        ...\n+\n+    @overload\n+    def map_token_to_id(self, tokens: List[str]) -> List[int]:\n+        ...\n+\n+    def map_token_to_id(self, tokens):\n         r\"\"\"Maps a single token or a sequence of tokens to a integer id\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n    <DED>def map_token_to_id(self, tokens: MaybeList[str]) -> MaybeList[int]:\n        <IND>",
        "target_code_with_indent": "\n    # TODO: Remove these once pylint supports function stubs.\n    # pylint: disable=unused-argument,function-redefined\n\n    <DED>@overload\n    def map_token_to_id(self, tokens: str) -> int:\n        <IND>...\n\n    <DED>@overload\n    def map_token_to_id(self, tokens: List[str]) -> List[int]:\n        <IND>...\n\n    <DED>def map_token_to_id(self, tokens):\n        <IND>"
      }
    ]
  }
]