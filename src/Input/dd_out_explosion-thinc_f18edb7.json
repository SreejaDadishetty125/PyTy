[
  {
    "project": "explosion/thinc",
    "commit": "f18edb70716b0262c211f8a8e0f6db531c1b8d55",
    "filename": "thinc/layers/add.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/explosion-thinc/thinc/layers/add.py",
    "file_hunks_size": 4,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "thinc/layers/add.py:24:8 Incompatible return type [7]: Expected `Tuple[Variable[OutT (bound to Array)], typing.Callable[..., typing.Any]]` but got `Tuple[Variable[InT (bound to Array)], typing.Callable[[Named(dY, typing.Any)], typing.Any]]`.",
    "message": " Expected `Tuple[Variable[OutT (bound to Array)], typing.Callable[..., typing.Any]]` but got `Tuple[Variable[InT (bound to Array)], typing.Callable[[Named(dY, typing.Any)], typing.Any]]`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 24,
    "warning_line": "        return X, lambda dY: dY",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\ndef forward(model: Model[InT, OutT], X: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    if not model.layers:\n",
        "source_code_len": 113,
        "target_code": "\ndef forward(model: Model[InT, InT], X: InT, is_train: bool) -> Tuple[InT, Callable]:\n    if not model.layers:\n",
        "target_code_len": 111,
        "diff_format": "@@ -21,3 +20,3 @@\n \n-def forward(model: Model[InT, OutT], X: InT, is_train: bool) -> Tuple[OutT, Callable]:\n+def forward(model: Model[InT, InT], X: InT, is_train: bool) -> Tuple[InT, Callable]:\n     if not model.layers:\n",
        "source_code_with_indent": "\n<DED>def forward(model: Model[InT, OutT], X: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    <IND>if not model.layers:\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n<DED>def forward(model: Model[InT, InT], X: InT, is_train: bool) -> Tuple[InT, Callable]:\n    <IND>if not model.layers:\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "explosion/thinc",
    "commit": "f18edb70716b0262c211f8a8e0f6db531c1b8d55",
    "filename": "thinc/layers/clone.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/explosion-thinc/thinc/layers/clone.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "thinc/layers/clone.py:18:8 Incompatible return type [7]: Expected `Model[Variable[InT], Variable[OutT]]` but got `Model[Variable[thinc.layers.noop.InOutT], Variable[thinc.layers.noop.InOutT]]`.",
    "message": " Expected `Model[Variable[InT], Variable[OutT]]` but got `Model[Variable[thinc.layers.noop.InOutT], Variable[thinc.layers.noop.InOutT]]`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 18,
    "warning_line": "        return noop()",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "from typing import TypeVar\n\n",
        "source_code_len": 28,
        "target_code": "from typing import TypeVar, cast\n\n",
        "target_code_len": 34,
        "diff_format": "@@ -1,2 +1,2 @@\n-from typing import TypeVar\n+from typing import TypeVar, cast\n \n",
        "source_code_with_indent": "from typing import TypeVar\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "from typing import TypeVar, cast\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    if n == 0:\n        return noop()\n    layers = [orig]\n",
        "source_code_len": 57,
        "target_code": "    if n == 0:\n        return cast(Model[InT, OutT], noop())\n    layers = [orig]\n",
        "target_code_len": 81,
        "diff_format": "@@ -17,3 +16,3 @@\n     if n == 0:\n-        return noop()\n+        return cast(Model[InT, OutT], noop())\n     layers = [orig]\n",
        "source_code_with_indent": "    if n == 0:\n        <IND>return noop()\n    <DED>layers = [orig]\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    if n == 0:\n        <IND>return cast(Model[InT, OutT], noop())\n    <DED>layers = [orig]\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "explosion/thinc",
    "commit": "f18edb70716b0262c211f8a8e0f6db531c1b8d55",
    "filename": "thinc/layers/lstm.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/explosion-thinc/thinc/layers/lstm.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "thinc/layers/lstm.py:26:4 Incompatible return type [7]: Expected `Model[List[Floats2d], List[Floats2d]]` but got `Model[List[Array], List[Array]]`.",
    "message": " Expected `Model[List[Floats2d], List[Floats2d]]` but got `Model[List[Array], List[Array]]`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 26,
    "warning_line": "    return with_list2padded(",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "from typing import Optional, List, Tuple, Callable\n\n",
        "source_code_len": 52,
        "target_code": "from typing import Optional, List, Tuple, Callable, cast\n\n",
        "target_code_len": 58,
        "diff_format": "@@ -1,2 +1,2 @@\n-from typing import Optional, List, Tuple, Callable\n+from typing import Optional, List, Tuple, Callable, cast\n \n",
        "source_code_with_indent": "from typing import Optional, List, Tuple, Callable\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "from typing import Optional, List, Tuple, Callable, cast\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    dropout: float = 0.0\n) -> Model[InT, OutT]:\n    return with_list2padded(\n        clone(bidirectional(recurrent(LSTM_step(nO=nO, nI=nI, dropout=dropout))), depth)\n    )\n",
        "source_code_len": 172,
        "target_code": "    dropout: float = 0.0\n) -> Model[InT, InT]:\n    return cast(\n        Model[InT, InT],\n        with_list2padded(\n            clone(\n                bidirectional(recurrent(LSTM_step(nO=nO, nI=nI, dropout=dropout))),\n                depth,\n            )\n        ),\n    )\n",
        "target_code_len": 272,
        "diff_format": "@@ -24,5 +22,11 @@\n     dropout: float = 0.0\n-) -> Model[InT, OutT]:\n-    return with_list2padded(\n-        clone(bidirectional(recurrent(LSTM_step(nO=nO, nI=nI, dropout=dropout))), depth)\n+) -> Model[InT, InT]:\n+    return cast(\n+        Model[InT, InT],\n+        with_list2padded(\n+            clone(\n+                bidirectional(recurrent(LSTM_step(nO=nO, nI=nI, dropout=dropout))),\n+                depth,\n+            )\n+        ),\n     )\n",
        "source_code_with_indent": "    dropout: float = 0.0\n) -> Model[InT, OutT]:\n    <IND>return with_list2padded(\n        clone(bidirectional(recurrent(LSTM_step(nO=nO, nI=nI, dropout=dropout))), depth)\n    )\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    dropout: float = 0.0\n) -> Model[InT, InT]:\n    <IND>return cast(\n        Model[InT, InT],\n        with_list2padded(\n            clone(\n                bidirectional(recurrent(LSTM_step(nO=nO, nI=nI, dropout=dropout))),\n                depth,\n            )\n        ),\n    )\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "explosion/thinc",
    "commit": "f18edb70716b0262c211f8a8e0f6db531c1b8d55",
    "filename": "thinc/layers/lstm.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/explosion-thinc/thinc/layers/lstm.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "thinc/layers/lstm.py:38:4 Incompatible return type [7]: Expected `Model[List[Floats2d], List[Floats2d]]` but got `Model[List[Array], List[Array]]`.",
    "message": " Expected `Model[List[Floats2d], List[Floats2d]]` but got `Model[List[Array], List[Array]]`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 38,
    "warning_line": "    return with_list2padded(",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "from typing import Optional, List, Tuple, Callable\n\n",
        "source_code_len": 52,
        "target_code": "from typing import Optional, List, Tuple, Callable, cast\n\n",
        "target_code_len": 58,
        "diff_format": "@@ -1,2 +1,2 @@\n-from typing import Optional, List, Tuple, Callable\n+from typing import Optional, List, Tuple, Callable, cast\n \n",
        "source_code_with_indent": "from typing import Optional, List, Tuple, Callable\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "from typing import Optional, List, Tuple, Callable, cast\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    dropout: float = 0.0\n) -> Model[InT, OutT]:\n    return with_list2padded(\n        clone(recurrent(LSTM_step(nO=nO, nI=nI, dropout=dropout)), depth)\n    )\n",
        "source_code_len": 157,
        "target_code": "    dropout: float = 0.0\n) -> Model[InT, InT]:\n    return cast(\n        Model[InT, InT],\n        with_list2padded(\n            clone(recurrent(LSTM_step(nO=nO, nI=nI, dropout=dropout)), depth)\n        ),\n    )\n",
        "target_code_len": 210,
        "diff_format": "@@ -36,5 +40,8 @@\n     dropout: float = 0.0\n-) -> Model[InT, OutT]:\n-    return with_list2padded(\n-        clone(recurrent(LSTM_step(nO=nO, nI=nI, dropout=dropout)), depth)\n+) -> Model[InT, InT]:\n+    return cast(\n+        Model[InT, InT],\n+        with_list2padded(\n+            clone(recurrent(LSTM_step(nO=nO, nI=nI, dropout=dropout)), depth)\n+        ),\n     )\n",
        "source_code_with_indent": "    dropout: float = 0.0\n) -> Model[InT, OutT]:\n    <IND>return with_list2padded(\n        clone(recurrent(LSTM_step(nO=nO, nI=nI, dropout=dropout)), depth)\n    )\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    dropout: float = 0.0\n) -> Model[InT, InT]:\n    <IND>return cast(\n        Model[InT, InT],\n        with_list2padded(\n            clone(recurrent(LSTM_step(nO=nO, nI=nI, dropout=dropout)), depth)\n        ),\n    )\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "explosion/thinc",
    "commit": "f18edb70716b0262c211f8a8e0f6db531c1b8d55",
    "filename": "thinc/layers/lstm.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/explosion-thinc/thinc/layers/lstm.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "thinc/layers/lstm.py:114:8 Incompatible return type [7]: Expected `Tuple[Floats2d, Floats2d]` but got `Tuple[Array, Array]`.",
    "message": " Expected `Tuple[Floats2d, Floats2d]` but got `Tuple[Array, Array]`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 114,
    "warning_line": "        return d_acts, d_prevcells",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "from typing import Optional, List, Tuple, Callable\n\n",
        "source_code_len": 52,
        "target_code": "from typing import Optional, List, Tuple, Callable, cast\n\n",
        "target_code_len": 58,
        "diff_format": "@@ -1,2 +1,2 @@\n-from typing import Optional, List, Tuple, Callable\n+from typing import Optional, List, Tuple, Callable, cast\n \n",
        "source_code_with_indent": "from typing import Optional, List, Tuple, Callable\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "from typing import Optional, List, Tuple, Callable, cast\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        d_acts = d_acts.reshape((nB, nO * 4))\n        return d_acts, d_prevcells\n\n",
        "source_code_len": 82,
        "target_code": "        d_acts = d_acts.reshape((nB, nO * 4))\n        return cast(Tuple[Floats2d, Floats2d], (d_acts, d_prevcells))\n\n",
        "target_code_len": 117,
        "diff_format": "@@ -113,3 +122,3 @@\n         d_acts = d_acts.reshape((nB, nO * 4))\n-        return d_acts, d_prevcells\n+        return cast(Tuple[Floats2d, Floats2d], (d_acts, d_prevcells))\n \n",
        "source_code_with_indent": "        d_acts = d_acts.reshape((nB, nO * 4))\n        return d_acts, d_prevcells\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        d_acts = d_acts.reshape((nB, nO * 4))\n        return cast(Tuple[Floats2d, Floats2d], (d_acts, d_prevcells))\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "explosion/thinc",
    "commit": "f18edb70716b0262c211f8a8e0f6db531c1b8d55",
    "filename": "thinc/layers/recurrent.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/explosion-thinc/thinc/layers/recurrent.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": true,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "thinc/layers/recurrent.py:48:4 Incompatible variable type [9]: backprops is declared to have type `List[Optional[typing.Callable[..., typing.Any]]]` but is used as type `List[None]`.",
    "message": " backprops is declared to have type `List[Optional[typing.Callable[..., typing.Any]]]` but is used as type `List[None]`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 48,
    "warning_line": "    backprops: List[Optional[Callable]] = [None] * X.shape[0]",
    "min_patch": [
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    Y = model.ops.allocate((X.shape[0], X.shape[1], nO))\n    backprops: List[Optional[Callable]] = [None] * X.shape[0]\n    (cell, hidden) = _get_initial_state(model, X.shape[1], nO)\n",
        "source_code_len": 182,
        "target_code": "    Y = model.ops.allocate((X.shape[0], X.shape[1], nO))\n    backprops: List[Callable] = [lambda a: a] * X.shape[0]\n    (cell, hidden) = _get_initial_state(model, X.shape[1], nO)\n",
        "target_code_len": 179,
        "diff_format": "@@ -47,3 +45,3 @@\n     Y = model.ops.allocate((X.shape[0], X.shape[1], nO))\n-    backprops: List[Optional[Callable]] = [None] * X.shape[0]\n+    backprops: List[Callable] = [lambda a: a] * X.shape[0]\n     (cell, hidden) = _get_initial_state(model, X.shape[1], nO)\n",
        "source_code_with_indent": "    Y = model.ops.allocate((X.shape[0], X.shape[1], nO))\n    backprops: List[Optional[Callable]] = [None] * X.shape[0]\n    (cell, hidden) = _get_initial_state(model, X.shape[1], nO)\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    Y = model.ops.allocate((X.shape[0], X.shape[1], nO))\n    backprops: List[Callable] = [lambda a: a] * X.shape[0]\n    (cell, hidden) = _get_initial_state(model, X.shape[1], nO)\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "explosion/thinc",
    "commit": "f18edb70716b0262c211f8a8e0f6db531c1b8d55",
    "filename": "thinc/layers/recurrent.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/explosion-thinc/thinc/layers/recurrent.py",
    "file_hunks_size": 3,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "thinc/layers/recurrent.py:68:33 Call error [29]: `Optional[typing.Callable[..., typing.Any]]` is not a function.",
    "message": " `Optional[typing.Callable[..., typing.Any]]` is not a function.",
    "rule_id": "Call error [29]",
    "warning_line_no": 68,
    "warning_line": "            d_state, dX[t, :n] = backprops[t]((d_state, dY[t]))",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "    Y = model.ops.allocate((X.shape[0], X.shape[1], nO))\n    backprops: List[Optional[Callable]] = [None] * X.shape[0]\n    (cell, hidden) = _get_initial_state(model, X.shape[1], nO)\n",
        "source_code_len": 182,
        "target_code": "    Y = model.ops.allocate((X.shape[0], X.shape[1], nO))\n    backprops: List[Callable] = [lambda a: a] * X.shape[0]\n    (cell, hidden) = _get_initial_state(model, X.shape[1], nO)\n",
        "target_code_len": 179,
        "diff_format": "@@ -47,3 +45,3 @@\n     Y = model.ops.allocate((X.shape[0], X.shape[1], nO))\n-    backprops: List[Optional[Callable]] = [None] * X.shape[0]\n+    backprops: List[Callable] = [lambda a: a] * X.shape[0]\n     (cell, hidden) = _get_initial_state(model, X.shape[1], nO)\n",
        "source_code_with_indent": "    Y = model.ops.allocate((X.shape[0], X.shape[1], nO))\n    backprops: List[Optional[Callable]] = [None] * X.shape[0]\n    (cell, hidden) = _get_initial_state(model, X.shape[1], nO)\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "    Y = model.ops.allocate((X.shape[0], X.shape[1], nO))\n    backprops: List[Callable] = [lambda a: a] * X.shape[0]\n    (cell, hidden) = _get_initial_state(model, X.shape[1], nO)\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "explosion/thinc",
    "commit": "f18edb70716b0262c211f8a8e0f6db531c1b8d55",
    "filename": "thinc/model.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/explosion-thinc/thinc/model.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "thinc/model.py:489:18 Incompatible parameter type [6]: Expected `Model[typing.Any, typing.Any]` for 1st positional only parameter to call `dict.__setitem__` but got `None`.",
    "message": " Expected `Model[typing.Any, typing.Any]` for 1st positional only parameter to call `dict.__setitem__` but got `None`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 489,
    "warning_line": "        node_to_i[None] = None",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        # we'd have no way to serialize/deserialize them.\n        node_to_i = {node: i for i, node in enumerate(nodes)}\n",
        "source_code_len": 120,
        "target_code": "        # we'd have no way to serialize/deserialize them.\n        node_to_i: Dict[Optional[Model], Optional[int]]\n        node_to_i = {node: i for i, node in enumerate(nodes)}\n",
        "target_code_len": 176,
        "diff_format": "@@ -486,2 +486,3 @@\n         # we'd have no way to serialize/deserialize them.\n+        node_to_i: Dict[Optional[Model], Optional[int]]\n         node_to_i = {node: i for i, node in enumerate(nodes)}\n",
        "source_code_with_indent": "        # we'd have no way to serialize/deserialize them.\n        node_to_i = {node: i for i, node in enumerate(nodes)}\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        # we'd have no way to serialize/deserialize them.\n        node_to_i: Dict[Optional[Model], Optional[int]]\n        node_to_i = {node: i for i, node in enumerate(nodes)}\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "explosion/thinc",
    "commit": "f18edb70716b0262c211f8a8e0f6db531c1b8d55",
    "filename": "thinc/model.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/explosion-thinc/thinc/model.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "thinc/model.py:489:26 Incompatible parameter type [6]: Expected `int` for 2nd positional only parameter to call `dict.__setitem__` but got `None`.",
    "message": " Expected `int` for 2nd positional only parameter to call `dict.__setitem__` but got `None`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 489,
    "warning_line": "        node_to_i[None] = None",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        # we'd have no way to serialize/deserialize them.\n        node_to_i = {node: i for i, node in enumerate(nodes)}\n",
        "source_code_len": 120,
        "target_code": "        # we'd have no way to serialize/deserialize them.\n        node_to_i: Dict[Optional[Model], Optional[int]]\n        node_to_i = {node: i for i, node in enumerate(nodes)}\n",
        "target_code_len": 176,
        "diff_format": "@@ -486,2 +486,3 @@\n         # we'd have no way to serialize/deserialize them.\n+        node_to_i: Dict[Optional[Model], Optional[int]]\n         node_to_i = {node: i for i, node in enumerate(nodes)}\n",
        "source_code_with_indent": "        # we'd have no way to serialize/deserialize them.\n        node_to_i = {node: i for i, node in enumerate(nodes)}\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        # we'd have no way to serialize/deserialize them.\n        node_to_i: Dict[Optional[Model], Optional[int]]\n        node_to_i = {node: i for i, node in enumerate(nodes)}\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  }
]