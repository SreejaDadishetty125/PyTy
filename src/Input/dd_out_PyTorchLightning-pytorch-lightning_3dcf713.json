[
  {
    "project": "PyTorchLightning/pytorch-lightning",
    "commit": "3dcf7130c554f4511c756ccbb4e3a417103d595d",
    "filename": "tests/base/datamodules.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-pytorch-lightning/tests/base/datamodules.py",
    "file_hunks_size": 10,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "tests/base/datamodules.py:20:4 Inconsistent override [14]: `tests.base.datamodules.TrialMNISTDataModule.setup` overrides method defined in `LightningDataModule` inconsistently. Parameter of type `str` is not a supertype of the overridden parameter `typing.Optional[str]`.",
    "message": " `tests.base.datamodules.TrialMNISTDataModule.setup` overrides method defined in `LightningDataModule` inconsistently. Parameter of type `str` is not a supertype of the overridden parameter `typing.Optional[str]`.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 20,
    "warning_line": "    def setup(self, stage: str = None):"
  },
  {
    "project": "PyTorchLightning/pytorch-lightning",
    "commit": "3dcf7130c554f4511c756ccbb4e3a417103d595d",
    "filename": "tests/base/datamodules.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-pytorch-lightning/tests/base/datamodules.py",
    "file_hunks_size": 10,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "tests/base/datamodules.py:20:20 Incompatible variable type [9]: stage is declared to have type `str` but is used as type `None`.",
    "message": " stage is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 20,
    "warning_line": "    def setup(self, stage: str = None):"
  },
  {
    "project": "PyTorchLightning/pytorch-lightning",
    "commit": "3dcf7130c554f4511c756ccbb4e3a417103d595d",
    "filename": "tests/base/datamodules.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-pytorch-lightning/tests/base/datamodules.py",
    "file_hunks_size": 10,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "tests/base/datamodules.py:63:4 Inconsistent override [14]: `tests.base.datamodules.MNISTDataModule.setup` overrides method defined in `LightningDataModule` inconsistently. Parameter of type `str` is not a supertype of the overridden parameter `typing.Optional[str]`.",
    "message": " `tests.base.datamodules.MNISTDataModule.setup` overrides method defined in `LightningDataModule` inconsistently. Parameter of type `str` is not a supertype of the overridden parameter `typing.Optional[str]`.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 63,
    "warning_line": "    def setup(self, stage: str = None):",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "import os\nfrom torch.utils.data import random_split, DataLoader\n\nfrom pytorch_lightning.core.datamodule import LightningDataModule\nfrom tests.base.datasets import TrialMNIST, MNIST\nfrom torch.utils.data.distributed import DistributedSampler\n",
        "source_code_len": 241,
        "target_code": "import os\nfrom typing import Any, Dict, Optional\n\nfrom pytorch_lightning.core.datamodule import LightningDataModule\nfrom tests.base.datasets import MNIST, TrialMNIST\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.utils.data.distributed import DistributedSampler\n",
        "target_code_len": 280,
        "diff_format": "@@ -1,6 +1,7 @@\n import os\n-from torch.utils.data import random_split, DataLoader\n+from typing import Any, Dict, Optional\n \n from pytorch_lightning.core.datamodule import LightningDataModule\n-from tests.base.datasets import TrialMNIST, MNIST\n+from tests.base.datasets import MNIST, TrialMNIST\n+from torch.utils.data import DataLoader, random_split\n from torch.utils.data.distributed import DistributedSampler\n",
        "source_code_with_indent": "import os\nfrom torch.utils.data import random_split, DataLoader\n\nfrom pytorch_lightning.core.datamodule import LightningDataModule\nfrom tests.base.datasets import TrialMNIST, MNIST\nfrom torch.utils.data.distributed import DistributedSampler\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "import os\nfrom typing import Any, Dict, Optional\n\nfrom pytorch_lightning.core.datamodule import LightningDataModule\nfrom tests.base.datasets import MNIST, TrialMNIST\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.utils.data.distributed import DistributedSampler\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    def setup(self, stage: str = None):\n\n        if stage == 'fit' or stage is None:\n            mnist_full = TrialMNIST(root=self.data_dir, train=True, num_samples=64, download=True)\n            self.mnist_train, self.mnist_val = random_split(mnist_full, [128, 64])\n",
        "source_code_len": 268,
        "target_code": "\n    def setup(self, stage: Optional[str] = None):\n\n        if stage == \"fit\" or stage is None:\n            mnist_full = TrialMNIST(\n                root=self.data_dir, train=True, num_samples=64, download=True\n            )\n            self.mnist_train, self.mnist_val = random_split(mnist_full, [128, 64])\n",
        "target_code_len": 308,
        "diff_format": "@@ -19,6 +20,8 @@\n \n-    def setup(self, stage: str = None):\n+    def setup(self, stage: Optional[str] = None):\n \n-        if stage == 'fit' or stage is None:\n-            mnist_full = TrialMNIST(root=self.data_dir, train=True, num_samples=64, download=True)\n+        if stage == \"fit\" or stage is None:\n+            mnist_full = TrialMNIST(\n+                root=self.data_dir, train=True, num_samples=64, download=True\n+            )\n             self.mnist_train, self.mnist_val = random_split(mnist_full, [128, 64])\n",
        "source_code_with_indent": "\n    <DED>def setup(self, stage: str = None):\n\n        <IND>if stage == 'fit' or stage is None:\n            <IND>mnist_full = TrialMNIST(root=self.data_dir, train=True, num_samples=64, download=True)\n            self.mnist_train, self.mnist_val = random_split(mnist_full, [128, 64])\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def setup(self, stage: Optional[str] = None):\n\n        <IND>if stage == \"fit\" or stage is None:\n            <IND>mnist_full = TrialMNIST(\n                root=self.data_dir, train=True, num_samples=64, download=True\n            )\n            self.mnist_train, self.mnist_val = random_split(mnist_full, [128, 64])\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    def setup(self, stage: str = None):\n\n",
        "source_code_len": 42,
        "target_code": "\n    def setup(self, stage: Optional[str] = None):\n\n",
        "target_code_len": 52,
        "diff_format": "@@ -62,3 +73,3 @@\n \n-    def setup(self, stage: str = None):\n+    def setup(self, stage: Optional[str] = None):\n \n",
        "source_code_with_indent": "\n    <DED>def setup(self, stage: str = None):\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def setup(self, stage: Optional[str] = None):\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "PyTorchLightning/pytorch-lightning",
    "commit": "3dcf7130c554f4511c756ccbb4e3a417103d595d",
    "filename": "tests/base/datamodules.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/PyTorchLightning-pytorch-lightning/tests/base/datamodules.py",
    "file_hunks_size": 10,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "tests/base/datamodules.py:63:20 Incompatible variable type [9]: stage is declared to have type `str` but is used as type `None`.",
    "message": " stage is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 63,
    "warning_line": "    def setup(self, stage: str = None):",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "import os\nfrom torch.utils.data import random_split, DataLoader\n\nfrom pytorch_lightning.core.datamodule import LightningDataModule\nfrom tests.base.datasets import TrialMNIST, MNIST\nfrom torch.utils.data.distributed import DistributedSampler\n",
        "source_code_len": 241,
        "target_code": "import os\nfrom typing import Any, Dict, Optional\n\nfrom pytorch_lightning.core.datamodule import LightningDataModule\nfrom tests.base.datasets import MNIST, TrialMNIST\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.utils.data.distributed import DistributedSampler\n",
        "target_code_len": 280,
        "diff_format": "@@ -1,6 +1,7 @@\n import os\n-from torch.utils.data import random_split, DataLoader\n+from typing import Any, Dict, Optional\n \n from pytorch_lightning.core.datamodule import LightningDataModule\n-from tests.base.datasets import TrialMNIST, MNIST\n+from tests.base.datasets import MNIST, TrialMNIST\n+from torch.utils.data import DataLoader, random_split\n from torch.utils.data.distributed import DistributedSampler\n",
        "source_code_with_indent": "import os\nfrom torch.utils.data import random_split, DataLoader\n\nfrom pytorch_lightning.core.datamodule import LightningDataModule\nfrom tests.base.datasets import TrialMNIST, MNIST\nfrom torch.utils.data.distributed import DistributedSampler\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "import os\nfrom typing import Any, Dict, Optional\n\nfrom pytorch_lightning.core.datamodule import LightningDataModule\nfrom tests.base.datasets import MNIST, TrialMNIST\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.utils.data.distributed import DistributedSampler\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    def setup(self, stage: str = None):\n\n        if stage == 'fit' or stage is None:\n            mnist_full = TrialMNIST(root=self.data_dir, train=True, num_samples=64, download=True)\n            self.mnist_train, self.mnist_val = random_split(mnist_full, [128, 64])\n",
        "source_code_len": 268,
        "target_code": "\n    def setup(self, stage: Optional[str] = None):\n\n        if stage == \"fit\" or stage is None:\n            mnist_full = TrialMNIST(\n                root=self.data_dir, train=True, num_samples=64, download=True\n            )\n            self.mnist_train, self.mnist_val = random_split(mnist_full, [128, 64])\n",
        "target_code_len": 308,
        "diff_format": "@@ -19,6 +20,8 @@\n \n-    def setup(self, stage: str = None):\n+    def setup(self, stage: Optional[str] = None):\n \n-        if stage == 'fit' or stage is None:\n-            mnist_full = TrialMNIST(root=self.data_dir, train=True, num_samples=64, download=True)\n+        if stage == \"fit\" or stage is None:\n+            mnist_full = TrialMNIST(\n+                root=self.data_dir, train=True, num_samples=64, download=True\n+            )\n             self.mnist_train, self.mnist_val = random_split(mnist_full, [128, 64])\n",
        "source_code_with_indent": "\n    <DED>def setup(self, stage: str = None):\n\n        <IND>if stage == 'fit' or stage is None:\n            <IND>mnist_full = TrialMNIST(root=self.data_dir, train=True, num_samples=64, download=True)\n            self.mnist_train, self.mnist_val = random_split(mnist_full, [128, 64])\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def setup(self, stage: Optional[str] = None):\n\n        <IND>if stage == \"fit\" or stage is None:\n            <IND>mnist_full = TrialMNIST(\n                root=self.data_dir, train=True, num_samples=64, download=True\n            )\n            self.mnist_train, self.mnist_val = random_split(mnist_full, [128, 64])\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n    def setup(self, stage: str = None):\n\n",
        "source_code_len": 42,
        "target_code": "\n    def setup(self, stage: Optional[str] = None):\n\n",
        "target_code_len": 52,
        "diff_format": "@@ -62,3 +73,3 @@\n \n-    def setup(self, stage: str = None):\n+    def setup(self, stage: Optional[str] = None):\n \n",
        "source_code_with_indent": "\n    <DED>def setup(self, stage: str = None):\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n    <DED>def setup(self, stage: Optional[str] = None):\n\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  }
]