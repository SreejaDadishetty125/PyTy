[
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/data/utils.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/data/utils.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "elegy/data/utils.py:187:8 Incompatible parameter type [6]: Expected `Multimap` for 1st positional only parameter to call `map_structure` but got `functools.partial[typing.Any]`.",
    "message": " Expected `Multimap` for 1st positional only parameter to call `map_structure` but got `functools.partial[typing.Any]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 187,
    "warning_line": "        functools.partial(_split, indices=train_indices), arrays",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "import six\nfrom elegy import types, utils\n",
        "source_code_len": 42,
        "target_code": "import six\nimport typing_extensions as tpe\n\nfrom elegy import types, utils\n",
        "target_code_len": 75,
        "diff_format": "@@ -13,2 +13,4 @@\n import six\n+import typing_extensions as tpe\n+\n from elegy import types, utils\n",
        "source_code_with_indent": "import six\nfrom elegy import types, utils\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "import six\nimport typing_extensions as tpe\n\nfrom elegy import types, utils\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nclass Multimap(types.Protocol):\n    def __call__(self, *args: types.np.ndarray) -> types.T:\n",
        "source_code_len": 93,
        "target_code": "\nclass Multimap(tpe.Protocol):\n    def __call__(self, *args: types.np.ndarray) -> types.T:\n",
        "target_code_len": 91,
        "diff_format": "@@ -16,3 +18,3 @@\n \n-class Multimap(types.Protocol):\n+class Multimap(tpe.Protocol):\n     def __call__(self, *args: types.np.ndarray) -> types.T:\n",
        "source_code_with_indent": "\nclass Multimap(types.Protocol):\n    <IND>def __call__(self, *args: types.np.ndarray) -> types.T:\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\nclass Multimap(tpe.Protocol):\n    <IND>def __call__(self, *args: types.np.ndarray) -> types.T:\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/data/utils.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/data/utils.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "elegy/data/utils.py:189:31 Incompatible parameter type [6]: Expected `Multimap` for 1st positional only parameter to call `map_structure` but got `functools.partial[typing.Any]`.",
    "message": " Expected `Multimap` for 1st positional only parameter to call `map_structure` but got `functools.partial[typing.Any]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 189,
    "warning_line": "    val_arrays = map_structure(functools.partial(_split, indices=val_indices), arrays)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "import six\nfrom elegy import types, utils\n",
        "source_code_len": 42,
        "target_code": "import six\nimport typing_extensions as tpe\n\nfrom elegy import types, utils\n",
        "target_code_len": 75,
        "diff_format": "@@ -13,2 +13,4 @@\n import six\n+import typing_extensions as tpe\n+\n from elegy import types, utils\n",
        "source_code_with_indent": "import six\nfrom elegy import types, utils\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "import six\nimport typing_extensions as tpe\n\nfrom elegy import types, utils\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\nclass Multimap(types.Protocol):\n    def __call__(self, *args: types.np.ndarray) -> types.T:\n",
        "source_code_len": 93,
        "target_code": "\nclass Multimap(tpe.Protocol):\n    def __call__(self, *args: types.np.ndarray) -> types.T:\n",
        "target_code_len": 91,
        "diff_format": "@@ -16,3 +18,3 @@\n \n-class Multimap(types.Protocol):\n+class Multimap(tpe.Protocol):\n     def __call__(self, *args: types.np.ndarray) -> types.T:\n",
        "source_code_with_indent": "\nclass Multimap(types.Protocol):\n    <IND>def __call__(self, *args: types.np.ndarray) -> types.T:\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\nclass Multimap(tpe.Protocol):\n    <IND>def __call__(self, *args: types.np.ndarray) -> types.T:\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/generalized_module/elegy_module.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/generalized_module/elegy_module.py:33:15 Unbound name [10]: Name `params` is used but not defined in the current scope.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/generalized_module/elegy_module.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/generalized_module/elegy_module.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/generalized_module/elegy_module.py:33:23 Unbound name [10]: Name `states` is used but not defined in the current scope.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/generalized_module/elegy_module.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/hooks.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/hooks.py:42:0 Incompatible variable type [9]: LOCAL is declared to have type `HooksContext` but is used as type `_HooksContext`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/hooks.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/losses/binary_crossentropy.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/losses/binary_crossentropy.py:119:4 Inconsistent override [14]: `elegy.losses.binary_crossentropy.BinaryCrossentropy.call` overrides method defined in `Loss` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/losses/binary_crossentropy.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/losses/binary_crossentropy.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/losses/binary_crossentropy.py:119:4 Inconsistent override [14]: `elegy.losses.binary_crossentropy.BinaryCrossentropy.call` overrides method defined in `Loss` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/losses/binary_crossentropy.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/losses/categorical_crossentropy.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/losses/categorical_crossentropy.py:124:4 Inconsistent override [14]: `elegy.losses.categorical_crossentropy.CategoricalCrossentropy.call` overrides method defined in `Loss` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/losses/categorical_crossentropy.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/losses/categorical_crossentropy.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/losses/categorical_crossentropy.py:124:4 Inconsistent override [14]: `elegy.losses.categorical_crossentropy.CategoricalCrossentropy.call` overrides method defined in `Loss` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/losses/categorical_crossentropy.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/losses/cosine_similarity.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/losses/cosine_similarity.py:127:4 Inconsistent override [14]: `elegy.losses.cosine_similarity.CosineSimilarity.call` overrides method defined in `Loss` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/losses/cosine_similarity.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/losses/cosine_similarity.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/losses/cosine_similarity.py:127:4 Inconsistent override [14]: `elegy.losses.cosine_similarity.CosineSimilarity.call` overrides method defined in `Loss` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/losses/cosine_similarity.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/losses/huber.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/losses/huber.py:162:4 Inconsistent override [14]: `elegy.losses.huber.Huber.call` overrides method defined in `Loss` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/losses/huber.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/losses/huber.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/losses/huber.py:162:4 Inconsistent override [14]: `elegy.losses.huber.Huber.call` overrides method defined in `Loss` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/losses/huber.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/losses/mean_absolute_error.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/losses/mean_absolute_error.py:116:4 Inconsistent override [14]: `elegy.losses.mean_absolute_error.MeanAbsoluteError.call` overrides method defined in `Loss` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/losses/mean_absolute_error.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/losses/mean_absolute_error.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/losses/mean_absolute_error.py:116:4 Inconsistent override [14]: `elegy.losses.mean_absolute_error.MeanAbsoluteError.call` overrides method defined in `Loss` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/losses/mean_absolute_error.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/losses/mean_absolute_percentage_error.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/losses/mean_absolute_percentage_error.py:116:4 Inconsistent override [14]: `elegy.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.call` overrides method defined in `Loss` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/losses/mean_absolute_percentage_error.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/losses/mean_absolute_percentage_error.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/losses/mean_absolute_percentage_error.py:116:4 Inconsistent override [14]: `elegy.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.call` overrides method defined in `Loss` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/losses/mean_absolute_percentage_error.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/losses/mean_squared_error.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/losses/mean_squared_error.py:116:4 Inconsistent override [14]: `elegy.losses.mean_squared_error.MeanSquaredError.call` overrides method defined in `Loss` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/losses/mean_squared_error.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/losses/mean_squared_error.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/losses/mean_squared_error.py:116:4 Inconsistent override [14]: `elegy.losses.mean_squared_error.MeanSquaredError.call` overrides method defined in `Loss` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/losses/mean_squared_error.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/losses/mean_squared_logarithmic_error.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/losses/mean_squared_logarithmic_error.py:119:4 Inconsistent override [14]: `elegy.losses.mean_squared_logarithmic_error.MeanSquaredLogarithmicError.call` overrides method defined in `Loss` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/losses/mean_squared_logarithmic_error.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/losses/mean_squared_logarithmic_error.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/losses/mean_squared_logarithmic_error.py:119:4 Inconsistent override [14]: `elegy.losses.mean_squared_logarithmic_error.MeanSquaredLogarithmicError.call` overrides method defined in `Loss` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/losses/mean_squared_logarithmic_error.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/losses/sparse_categorical_crossentropy.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/losses/sparse_categorical_crossentropy.py:134:4 Inconsistent override [14]: `elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy.call` overrides method defined in `Loss` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/losses/sparse_categorical_crossentropy.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/losses/sparse_categorical_crossentropy.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/losses/sparse_categorical_crossentropy.py:134:4 Inconsistent override [14]: `elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy.call` overrides method defined in `Loss` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/losses/sparse_categorical_crossentropy.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/losses/sparse_categorical_crossentropy.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/losses/sparse_categorical_crossentropy.py:161:12 Incompatible parameter type [6]: Expected `bool` for 4th parameter `check_bounds` to call `sparse_categorical_crossentropy` but got `tp.Optional[bool]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/losses/sparse_categorical_crossentropy.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/metrics/accuracy.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/metrics/accuracy.py:74:4 Inconsistent override [14]: `elegy.metrics.accuracy.Accuracy.call` overrides method defined in `Mean` inconsistently. Could not find parameter `values` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/metrics/accuracy.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/metrics/binary_accuracy.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/metrics/binary_accuracy.py:93:4 Inconsistent override [14]: `elegy.metrics.binary_accuracy.BinaryAccuracy.call` overrides method defined in `Mean` inconsistently. Could not find parameter `values` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/metrics/binary_accuracy.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/metrics/binary_crossentropy.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/metrics/binary_crossentropy.py:71:4 Inconsistent override [14]: `elegy.metrics.binary_crossentropy.BinaryCrossentropy.call` overrides method defined in `Mean` inconsistently. Could not find parameter `values` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/metrics/binary_crossentropy.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/metrics/categorical_accuracy.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/metrics/categorical_accuracy.py:80:4 Inconsistent override [14]: `elegy.metrics.categorical_accuracy.CategoricalAccuracy.call` overrides method defined in `Mean` inconsistently. Could not find parameter `values` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/metrics/categorical_accuracy.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/metrics/f1.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/metrics/f1.py:106:8 Incompatible attribute type [8]: Attribute `precision` declared in class `F1` has type `elegy.metrics.precision.Precision` but is used as type `elegy.module.Module`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/metrics/f1.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/metrics/f1.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/metrics/f1.py:107:8 Incompatible attribute type [8]: Attribute `recall` declared in class `F1` has type `elegy.metrics.recall.Recall` but is used as type `elegy.module.Module`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/metrics/f1.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/metrics/f1.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/metrics/f1.py:109:4 Inconsistent override [14]: `elegy.metrics.f1.F1.call` overrides method defined in `Metric` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/metrics/f1.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/metrics/f1.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/metrics/f1.py:109:4 Inconsistent override [14]: `elegy.metrics.f1.F1.call` overrides method defined in `Metric` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/metrics/f1.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/metrics/mean_absolute_error.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/metrics/mean_absolute_error.py:52:4 Inconsistent override [14]: `elegy.metrics.mean_absolute_error.MeanAbsoluteError.call` overrides method defined in `Mean` inconsistently. Could not find parameter `values` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/metrics/mean_absolute_error.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/metrics/mean_absolute_percentage_error.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/metrics/mean_absolute_percentage_error.py:52:4 Inconsistent override [14]: `elegy.metrics.mean_absolute_percentage_error.MeanAbsolutePercentageError.call` overrides method defined in `Mean` inconsistently. Could not find parameter `values` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/metrics/mean_absolute_percentage_error.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/metrics/mean_squared_error.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/metrics/mean_squared_error.py:52:4 Inconsistent override [14]: `elegy.metrics.mean_squared_error.MeanSquaredError.call` overrides method defined in `Mean` inconsistently. Could not find parameter `values` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/metrics/mean_squared_error.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/metrics/precision.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/metrics/precision.py:113:8 Incompatible attribute type [8]: Attribute `true_positives` declared in class `Precision` has type `ReduceConfusionMatrix` but is used as type `elegy.module.Module`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/metrics/precision.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/metrics/precision.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/metrics/precision.py:114:8 Incompatible attribute type [8]: Attribute `false_positives` declared in class `Precision` has type `ReduceConfusionMatrix` but is used as type `elegy.module.Module`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/metrics/precision.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/metrics/precision.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/metrics/precision.py:118:4 Inconsistent override [14]: `elegy.metrics.precision.Precision.call` overrides method defined in `Metric` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/metrics/precision.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/metrics/precision.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/metrics/precision.py:118:4 Inconsistent override [14]: `elegy.metrics.precision.Precision.call` overrides method defined in `Metric` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/metrics/precision.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/metrics/recall.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/metrics/recall.py:112:8 Incompatible attribute type [8]: Attribute `true_positives` declared in class `Recall` has type `ReduceConfusionMatrix` but is used as type `elegy.module.Module`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/metrics/recall.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/metrics/recall.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/metrics/recall.py:113:8 Incompatible attribute type [8]: Attribute `false_negatives` declared in class `Recall` has type `ReduceConfusionMatrix` but is used as type `elegy.module.Module`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/metrics/recall.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/metrics/recall.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/metrics/recall.py:117:4 Inconsistent override [14]: `elegy.metrics.recall.Recall.call` overrides method defined in `Metric` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/metrics/recall.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/metrics/recall.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/metrics/recall.py:117:4 Inconsistent override [14]: `elegy.metrics.recall.Recall.call` overrides method defined in `Metric` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/metrics/recall.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/metrics/reduce.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/metrics/reduce.py:96:4 Inconsistent override [14]: `elegy.metrics.reduce.Reduce.call` overrides method defined in `Metric` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/metrics/reduce.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/metrics/reduce.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/metrics/reduce.py:96:4 Inconsistent override [14]: `elegy.metrics.reduce.Reduce.call` overrides method defined in `Metric` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/metrics/reduce.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/metrics/reduce_confusion_matrix.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/metrics/reduce_confusion_matrix.py:95:4 Inconsistent override [14]: `elegy.metrics.reduce_confusion_matrix.ReduceConfusionMatrix.call` overrides method defined in `Metric` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/metrics/reduce_confusion_matrix.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/metrics/reduce_confusion_matrix.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/metrics/reduce_confusion_matrix.py:95:4 Inconsistent override [14]: `elegy.metrics.reduce_confusion_matrix.ReduceConfusionMatrix.call` overrides method defined in `Metric` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/metrics/reduce_confusion_matrix.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/metrics/sparse_categorical_accuracy.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/metrics/sparse_categorical_accuracy.py:78:4 Inconsistent override [14]: `elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy.call` overrides method defined in `Mean` inconsistently. Could not find parameter `values` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/metrics/sparse_categorical_accuracy.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/model/model_base.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/model/model_base.py",
    "file_hunks_size": 13,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "elegy/model/model_base.py:449:8 Incompatible return type [7]: Expected `elegy.callbacks.history.History` but got `tp.Dict[str, typing.Any]`.",
    "message": " Expected `elegy.callbacks.history.History` but got `tp.Dict[str, typing.Any]`.",
    "rule_id": "Incompatible return type [7]",
    "warning_line_no": 449,
    "warning_line": "        return self.history"
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/model/model_base.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/model/model_base.py",
    "file_hunks_size": 13,
    "min_patch_found": false,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "elegy/model/model_base.py:688:28 Incompatible parameter type [6]: Expected `data_utils.Multimap` for 1st positional only parameter to call `data_utils.map_structure` but got `typing.Callable[[Named(batch_output, typing.Any)], tp.List[typing.Any]]`.",
    "message": " Expected `data_utils.Multimap` for 1st positional only parameter to call `data_utils.map_structure` but got `typing.Callable[[Named(batch_output, typing.Any)], tp.List[typing.Any]]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 688,
    "warning_line": "                            lambda batch_output: [batch_output], batch_outputs"
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/model/model_test.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/model/model_test.py:28:4 Inconsistent override [14]: `elegy.model.model_test.MLP.call` overrides method defined in `elegy.module.Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/model/model_test.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/model/model_test.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/model/model_test.py:28:4 Inconsistent override [14]: `elegy.model.model_test.MLP.call` overrides method defined in `elegy.module.Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/model/model_test.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/module.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/module.py:49:0 Incompatible variable type [9]: LOCAL is declared to have type `LocalContext` but is used as type `_LocalContext`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/module.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/module.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/module.py:99:49 Unsupported operand [58]: `>` is not supported for operand types `int` and `tp.Optional[int]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/module.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/module.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/module.py:381:19 Unsupported operand [58]: `+` is not supported for operand types `tp.Optional[typing.Tuple[tp.Union[int, str], ...]]` and `tp.Tuple[str]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/module.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/module.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/module.py:536:11 Unsupported operand [58]: `not in` is not supported for right operand type `tp.Optional[tp.Dict[str, types.Parameter]]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/module.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/module.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/module.py:651:8 Incompatible return type [7]: Expected `bool` but got `tp.Optional[bool]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/module.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/module.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/module.py:659:8 Incompatible return type [7]: Expected `bool` but got `tp.Optional[bool]`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/module.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/module.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/module.py:993:33 Incompatible parameter type [6]: Expected `tp.Union[tp.Type[typing.Any], typing.Tuple[tp.Type[typing.Any], ...]]` for 2nd positional only parameter to call `isinstance` but got `tp._SpecialForm`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/module.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/module_test.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/module_test.py:333:8 Inconsistent override [14]: `elegy.module_test.ModuleTest.MyModule.call` overrides method defined in `elegy.module.Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/module_test.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/module_test.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/module_test.py:333:8 Inconsistent override [14]: `elegy.module_test.ModuleTest.MyModule.call` overrides method defined in `elegy.module.Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/module_test.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/module_test.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/module_test.py:510:8 Inconsistent override [14]: `elegy.module_test.ModuleDynamicTest.MyModule.call` overrides method defined in `elegy.module.Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/module_test.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/module_test.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/module_test.py:510:8 Inconsistent override [14]: `elegy.module_test.ModuleDynamicTest.MyModule.call` overrides method defined in `elegy.module.Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/module_test.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nets/resnet.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nets/resnet.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "elegy/nets/resnet.py:40:8 Incompatible variable type [9]: strides is declared to have type `tp.Optional[tp.Tuple[int]]` but is used as type `tp.Tuple[int, int]`.",
    "message": " strides is declared to have type `tp.Optional[tp.Tuple[int]]` but is used as type `tp.Tuple[int, int]`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 40,
    "warning_line": "        strides: tp.Optional[tp.Tuple[int]] = (1, 1),",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "# adapted from the flax library https://github.com/google/flax\n\nimport pickle\nimport typing as tp\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom elegy import hooks, module, nn, types, utils\n\n__all__ = [\n    \"ResNet\",\n    \"ResNet18\",\n    \"ResNet34\",\n    \"ResNet50\",\n    \"ResNet101\",\n    \"ResNet152\",\n    \"ResNet200\",\n]\n\n\nPRETRAINED_URLS = {\n    \"ResNet18\": {\n        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n        \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n    },\n    \"ResNet50\": {\n        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n        \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n    },\n}\n\n\nclass ResNetBlock(module.Module):\n    \"\"\"ResNet (identity) block\"\"\"\n\n    def __init__(\n        self,\n        n_filters: int,\n        strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n        *args,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.n_filters = n_filters\n        self.strides = strides\n\n    def call(self, x: jnp.ndarray):\n        x0 = x\n        x = nn.Conv2D(\n            self.n_filters,\n            (3, 3),\n            with_bias=False,\n            stride=self.strides,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n\n        x = nn.Conv2D(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n\n        if x0.shape != x.shape:\n            x0 = nn.Conv2D(\n                self.n_filters,\n                (1, 1),\n                with_bias=False,\n                stride=self.strides,\n                dtype=self.dtype,\n            )(x0)\n            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n        return jax.nn.relu(x0 + x)\n\n\nclass BottleneckResNetBlock(ResNetBlock):\n    \"\"\"ResNet Bottleneck block.\"\"\"\n\n    def call(self, x: jnp.ndarray):\n        x0 = x\n        x = nn.Conv2D(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n        x = nn.Conv2D(\n            self.n_filters,\n            (3, 3),\n            with_bias=False,\n            stride=self.strides,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n        x = nn.Conv2D(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n\n        if x0.shape != x.shape:\n            x0 = nn.Conv2D(\n                self.n_filters * 4,\n                (1, 1),\n                with_bias=False,\n                stride=self.strides,\n                dtype=self.dtype,\n            )(x0)\n            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n        return jax.nn.relu(x0 + x)\n\n\nclass ResNet(module.Module):\n    \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n    Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n    \"\"\"\n\n    __all__ = [\"__init__\", \"call\"]\n\n    def __init__(\n        self,\n        stages: tp.List[int],\n        block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Arguments:\n            stages: A list of integers representing the number of blocks in each stage.\n                    e.g: [3, 4, 6, 3] for a ResNet50\n            block_type: Which ResNet block type to use.\n            lowres: Optional, whether to use the low resolution version\n                    as described in subsection 4.2 of the orignal paper.\n                    This version is better suited for datasets like CIFAR10. (Default: False)\n            weights: One of None (random initialization) or a path to a weights file\n            dtype: Optional dtype of the convolutions and linear operations,\n                    either jnp.float32 (default) or jnp.float16 for mixed precision.\n        \"\"\"\n\n        super().__init__(*args, **kwargs)\n        self.stages = stages\n        self.block_type = block_type\n        self.lowres = lowres\n\n        if weights is not None:\n            if weights.endswith(\".pkl\"):\n                collections = pickle.load(open(weights, \"rb\"))\n            elif weights == \"imagenet\":\n                clsname = self.__class__.__name__\n                urldict = PRETRAINED_URLS.get(clsname, None)\n                if urldict is None:\n                    raise ValueError(f\"No pretrained weights for {clsname} available\")\n                fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n                collections = pickle.load(open(fname, \"rb\"))\n            else:\n                raise ValueError(\"Unknown weights value: \", weights)\n\n            if isinstance(collections, tuple):\n                parameters, collections = collections\n            elif \"parameters\" in collections:\n                parameters = collections.pop(\"parameters\")\n            else:\n                raise ValueError(\n                    \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n                )\n\n            x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n            # quick but dirty module initialization\n            jax.eval_shape(self.init(rng=types.RNGSeq(42)), x)\n\n            self.set_default_parameters(parameters, collections)\n\n    def call(self, x: jnp.ndarray):\n        x = nn.Conv2D(\n            64,\n            (7, 7) if not self.lowres else (3, 3),\n            stride=(2, 2) if not self.lowres else (1, 1),\n            padding=\"SAME\",\n            with_bias=False,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = module.to_module(jax.nn.relu)()(x)\n\n        if not self.lowres:\n            x = nn.MaxPool(\n                window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n            )(x)\n        for i, block_size in enumerate(self.stages):\n            for j in range(block_size):\n                strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n                x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n        GAP = lambda x: jnp.mean(x, axis=(1, 2))\n        x = module.to_module(GAP)(name=\"global_average_pooling\")(x)\n        x = nn.Linear(1000, dtype=self.dtype)(x)\n        to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n        x = module.to_module(to_float32)(name=\"to_float32\")(x)\n        return x\n\n\nclass ResNet18(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[2, 2, 2, 2],\n            block_type=ResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet34(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 4, 6, 3],\n            block_type=ResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet50(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 4, 6, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet101(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 4, 23, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet152(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 8, 36, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet200(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 24, 36, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n_resnet__init___docstring = \"\"\"\nInstantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n\nArguments:\n    lowres: Optional, whether to use the low resolution version\n            as described in subsection 4.2 of the orignal paper.\n            This version is better suited for datasets like CIFAR10. (Default: False)\n    weights: One of None (random initialization), 'imagenet' (automatic download of\n              weights pretrained on ImageNet) or a path to a weights file\n    dtype: Optional dtype of the convolutions and linear operations, \n           either jnp.float32 (default) or jnp.float16 for mixed precision.\n\"\"\"\n\nResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\nResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\nResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\nResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\nResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\nResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "source_code_len": 10761,
        "target_code": "# # adapted from the flax library https://github.com/google/flax\n\n# import pickle\n# import typing as tp\n\n# import jax\n# import jax.numpy as jnp\n# import numpy as np\n# from elegy import hooks, module, nn, types, utils\n\n# __all__ = [\n#     \"ResNet\",\n#     \"ResNet18\",\n#     \"ResNet34\",\n#     \"ResNet50\",\n#     \"ResNet101\",\n#     \"ResNet152\",\n#     \"ResNet200\",\n# ]\n\n\n# PRETRAINED_URLS = {\n#     \"ResNet18\": {\n#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n#         \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n#     },\n#     \"ResNet50\": {\n#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n#         \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n#     },\n# }\n\n\n# class ResNetBlock(module.Module):\n#     \"\"\"ResNet (identity) block\"\"\"\n\n#     def __init__(\n#         self,\n#         n_filters: int,\n#         strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(*args, **kwargs)\n#         self.n_filters = n_filters\n#         self.strides = strides\n\n#     def __call__(self, x: jnp.ndarray):\n#         x0 = x\n#         x = nn.Conv(\n#             self.n_filters,\n#             (3, 3),\n#             with_bias=False,\n#             stride=self.strides,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n\n#         x = nn.Conv(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n\n#         if x0.shape != x.shape:\n#             x0 = nn.Conv(\n#                 self.n_filters,\n#                 (1, 1),\n#                 with_bias=False,\n#                 stride=self.strides,\n#                 dtype=self.dtype,\n#             )(x0)\n#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n#         return jax.nn.relu(x0 + x)\n\n\n# class BottleneckResNetBlock(ResNetBlock):\n#     \"\"\"ResNet Bottleneck block.\"\"\"\n\n#     def __call__(self, x: jnp.ndarray):\n#         x0 = x\n#         x = nn.Conv(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n#         x = nn.Conv(\n#             self.n_filters,\n#             (3, 3),\n#             with_bias=False,\n#             stride=self.strides,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n#         x = nn.Conv(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n\n#         if x0.shape != x.shape:\n#             x0 = nn.Conv(\n#                 self.n_filters * 4,\n#                 (1, 1),\n#                 with_bias=False,\n#                 stride=self.strides,\n#                 dtype=self.dtype,\n#             )(x0)\n#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n#         return jax.nn.relu(x0 + x)\n\n\n# class ResNet(module.Module):\n#     \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n#     Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n#     \"\"\"\n\n#     __all__ = [\"__init__\", \"call\"]\n\n#     def __init__(\n#         self,\n#         stages: tp.List[int],\n#         block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         \"\"\"\n#         Arguments:\n#             stages: A list of integers representing the number of blocks in each stage.\n#                     e.g: [3, 4, 6, 3] for a ResNet50\n#             block_type: Which ResNet block type to use.\n#             lowres: Optional, whether to use the low resolution version\n#                     as described in subsection 4.2 of the orignal paper.\n#                     This version is better suited for datasets like CIFAR10. (Default: False)\n#             weights: One of None (random initialization) or a path to a weights file\n#             dtype: Optional dtype of the convolutions and linear operations,\n#                     either jnp.float32 (default) or jnp.float16 for mixed precision.\n#         \"\"\"\n\n#         super().__init__(*args, **kwargs)\n#         self.stages = stages\n#         self.block_type = block_type\n#         self.lowres = lowres\n\n#         if weights is not None:\n#             if weights.endswith(\".pkl\"):\n#                 collections = pickle.load(open(weights, \"rb\"))\n#             elif weights == \"imagenet\":\n#                 clsname = self.__class__.__name__\n#                 urldict = PRETRAINED_URLS.get(clsname, None)\n#                 if urldict is None:\n#                     raise ValueError(f\"No pretrained weights for {clsname} available\")\n#                 fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n#                 collections = pickle.load(open(fname, \"rb\"))\n#             else:\n#                 raise ValueError(\"Unknown weights value: \", weights)\n\n#             if isinstance(collections, tuple):\n#                 parameters, collections = collections\n#             elif \"parameters\" in collections:\n#                 parameters = collections.pop(\"parameters\")\n#             else:\n#                 raise ValueError(\n#                     \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n#                 )\n\n#             x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n#             # quick but dirty module initialization\n#             jax.eval_shape(self.init(rng=types.KeySeq(42)), x)\n\n#             self.set_default_parameters(parameters, collections)\n\n#     def __call__(self, x: jnp.ndarray):\n#         x = nn.Conv(\n#             64,\n#             (7, 7) if not self.lowres else (3, 3),\n#             stride=(2, 2) if not self.lowres else (1, 1),\n#             padding=\"SAME\",\n#             with_bias=False,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = module.compact_module(jax.nn.relu)()(x)\n\n#         if not self.lowres:\n#             x = nn.MaxPool(\n#                 window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n#             )(x)\n#         for i, block_size in enumerate(self.stages):\n#             for j in range(block_size):\n#                 strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n#                 x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n#         GAP = lambda x: jnp.mean(x, axis=(1, 2))\n#         x = module.compact_module(GAP)(name=\"global_average_pooling\")(x)\n#         x = nn.Linear(1000, dtype=self.dtype)(x)\n#         to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n#         x = module.compact_module(to_float32)(name=\"to_float32\")(x)\n#         return x\n\n\n# class ResNet18(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[2, 2, 2, 2],\n#             block_type=ResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet34(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 6, 3],\n#             block_type=ResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet50(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 6, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet101(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 23, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet152(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 8, 36, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet200(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 24, 36, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# _resnet__init___docstring = \"\"\"\n# Instantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n\n# Arguments:\n#     lowres: Optional, whether to use the low resolution version\n#             as described in subsection 4.2 of the orignal paper.\n#             This version is better suited for datasets like CIFAR10. (Default: False)\n#     weights: One of None (random initialization), 'imagenet' (automatic download of\n#               weights pretrained on ImageNet) or a path to a weights file\n#     dtype: Optional dtype of the convolutions and linear operations,\n#            either jnp.float32 (default) or jnp.float16 for mixed precision.\n# \"\"\"\n\n# ResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\n# ResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\n# ResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\n# ResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\n# ResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\n# ResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "target_code_len": 11287,
        "diff_format": "@@ -1,336 +1,336 @@\n-# adapted from the flax library https://github.com/google/flax\n-\n-import pickle\n-import typing as tp\n-\n-import jax\n-import jax.numpy as jnp\n-import numpy as np\n-from elegy import hooks, module, nn, types, utils\n-\n-__all__ = [\n-    \"ResNet\",\n-    \"ResNet18\",\n-    \"ResNet34\",\n-    \"ResNet50\",\n-    \"ResNet101\",\n-    \"ResNet152\",\n-    \"ResNet200\",\n-]\n-\n-\n-PRETRAINED_URLS = {\n-    \"ResNet18\": {\n-        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n-        \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n-    },\n-    \"ResNet50\": {\n-        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n-        \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n-    },\n-}\n-\n-\n-class ResNetBlock(module.Module):\n-    \"\"\"ResNet (identity) block\"\"\"\n-\n-    def __init__(\n-        self,\n-        n_filters: int,\n-        strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(*args, **kwargs)\n-        self.n_filters = n_filters\n-        self.strides = strides\n-\n-    def call(self, x: jnp.ndarray):\n-        x0 = x\n-        x = nn.Conv2D(\n-            self.n_filters,\n-            (3, 3),\n-            with_bias=False,\n-            stride=self.strides,\n-            dtype=self.dtype,\n-        )(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-        x = jax.nn.relu(x)\n-\n-        x = nn.Conv2D(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-\n-        if x0.shape != x.shape:\n-            x0 = nn.Conv2D(\n-                self.n_filters,\n-                (1, 1),\n-                with_bias=False,\n-                stride=self.strides,\n-                dtype=self.dtype,\n-            )(x0)\n-            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n-        return jax.nn.relu(x0 + x)\n-\n-\n-class BottleneckResNetBlock(ResNetBlock):\n-    \"\"\"ResNet Bottleneck block.\"\"\"\n-\n-    def call(self, x: jnp.ndarray):\n-        x0 = x\n-        x = nn.Conv2D(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-        x = jax.nn.relu(x)\n-        x = nn.Conv2D(\n-            self.n_filters,\n-            (3, 3),\n-            with_bias=False,\n-            stride=self.strides,\n-            dtype=self.dtype,\n-        )(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-        x = jax.nn.relu(x)\n-        x = nn.Conv2D(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n-\n-        if x0.shape != x.shape:\n-            x0 = nn.Conv2D(\n-                self.n_filters * 4,\n-                (1, 1),\n-                with_bias=False,\n-                stride=self.strides,\n-                dtype=self.dtype,\n-            )(x0)\n-            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n-        return jax.nn.relu(x0 + x)\n-\n-\n-class ResNet(module.Module):\n-    \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n-    Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n-    \"\"\"\n-\n-    __all__ = [\"__init__\", \"call\"]\n-\n-    def __init__(\n-        self,\n-        stages: tp.List[int],\n-        block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Arguments:\n-            stages: A list of integers representing the number of blocks in each stage.\n-                    e.g: [3, 4, 6, 3] for a ResNet50\n-            block_type: Which ResNet block type to use.\n-            lowres: Optional, whether to use the low resolution version\n-                    as described in subsection 4.2 of the orignal paper.\n-                    This version is better suited for datasets like CIFAR10. (Default: False)\n-            weights: One of None (random initialization) or a path to a weights file\n-            dtype: Optional dtype of the convolutions and linear operations,\n-                    either jnp.float32 (default) or jnp.float16 for mixed precision.\n-        \"\"\"\n-\n-        super().__init__(*args, **kwargs)\n-        self.stages = stages\n-        self.block_type = block_type\n-        self.lowres = lowres\n-\n-        if weights is not None:\n-            if weights.endswith(\".pkl\"):\n-                collections = pickle.load(open(weights, \"rb\"))\n-            elif weights == \"imagenet\":\n-                clsname = self.__class__.__name__\n-                urldict = PRETRAINED_URLS.get(clsname, None)\n-                if urldict is None:\n-                    raise ValueError(f\"No pretrained weights for {clsname} available\")\n-                fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n-                collections = pickle.load(open(fname, \"rb\"))\n-            else:\n-                raise ValueError(\"Unknown weights value: \", weights)\n-\n-            if isinstance(collections, tuple):\n-                parameters, collections = collections\n-            elif \"parameters\" in collections:\n-                parameters = collections.pop(\"parameters\")\n-            else:\n-                raise ValueError(\n-                    \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n-                )\n-\n-            x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n-            # quick but dirty module initialization\n-            jax.eval_shape(self.init(rng=types.RNGSeq(42)), x)\n-\n-            self.set_default_parameters(parameters, collections)\n-\n-    def call(self, x: jnp.ndarray):\n-        x = nn.Conv2D(\n-            64,\n-            (7, 7) if not self.lowres else (3, 3),\n-            stride=(2, 2) if not self.lowres else (1, 1),\n-            padding=\"SAME\",\n-            with_bias=False,\n-            dtype=self.dtype,\n-        )(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-        x = module.to_module(jax.nn.relu)()(x)\n-\n-        if not self.lowres:\n-            x = nn.MaxPool(\n-                window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n-            )(x)\n-        for i, block_size in enumerate(self.stages):\n-            for j in range(block_size):\n-                strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n-                x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n-        GAP = lambda x: jnp.mean(x, axis=(1, 2))\n-        x = module.to_module(GAP)(name=\"global_average_pooling\")(x)\n-        x = nn.Linear(1000, dtype=self.dtype)(x)\n-        to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n-        x = module.to_module(to_float32)(name=\"to_float32\")(x)\n-        return x\n-\n-\n-class ResNet18(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[2, 2, 2, 2],\n-            block_type=ResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet34(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 4, 6, 3],\n-            block_type=ResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet50(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 4, 6, 3],\n-            block_type=BottleneckResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet101(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 4, 23, 3],\n-            block_type=BottleneckResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet152(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 8, 36, 3],\n-            block_type=BottleneckResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet200(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 24, 36, 3],\n-            block_type=BottleneckResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-_resnet__init___docstring = \"\"\"\n-Instantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n-\n-Arguments:\n-    lowres: Optional, whether to use the low resolution version\n-            as described in subsection 4.2 of the orignal paper.\n-            This version is better suited for datasets like CIFAR10. (Default: False)\n-    weights: One of None (random initialization), 'imagenet' (automatic download of\n-              weights pretrained on ImageNet) or a path to a weights file\n-    dtype: Optional dtype of the convolutions and linear operations, \n-           either jnp.float32 (default) or jnp.float16 for mixed precision.\n-\"\"\"\n-\n-ResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\n-ResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\n-ResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\n-ResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\n-ResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\n-ResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n+# # adapted from the flax library https://github.com/google/flax\n+\n+# import pickle\n+# import typing as tp\n+\n+# import jax\n+# import jax.numpy as jnp\n+# import numpy as np\n+# from elegy import hooks, module, nn, types, utils\n+\n+# __all__ = [\n+#     \"ResNet\",\n+#     \"ResNet18\",\n+#     \"ResNet34\",\n+#     \"ResNet50\",\n+#     \"ResNet101\",\n+#     \"ResNet152\",\n+#     \"ResNet200\",\n+# ]\n+\n+\n+# PRETRAINED_URLS = {\n+#     \"ResNet18\": {\n+#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n+#         \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n+#     },\n+#     \"ResNet50\": {\n+#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n+#         \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n+#     },\n+# }\n+\n+\n+# class ResNetBlock(module.Module):\n+#     \"\"\"ResNet (identity) block\"\"\"\n+\n+#     def __init__(\n+#         self,\n+#         n_filters: int,\n+#         strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(*args, **kwargs)\n+#         self.n_filters = n_filters\n+#         self.strides = strides\n+\n+#     def __call__(self, x: jnp.ndarray):\n+#         x0 = x\n+#         x = nn.Conv(\n+#             self.n_filters,\n+#             (3, 3),\n+#             with_bias=False,\n+#             stride=self.strides,\n+#             dtype=self.dtype,\n+#         )(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+#         x = jax.nn.relu(x)\n+\n+#         x = nn.Conv(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+\n+#         if x0.shape != x.shape:\n+#             x0 = nn.Conv(\n+#                 self.n_filters,\n+#                 (1, 1),\n+#                 with_bias=False,\n+#                 stride=self.strides,\n+#                 dtype=self.dtype,\n+#             )(x0)\n+#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n+#         return jax.nn.relu(x0 + x)\n+\n+\n+# class BottleneckResNetBlock(ResNetBlock):\n+#     \"\"\"ResNet Bottleneck block.\"\"\"\n+\n+#     def __call__(self, x: jnp.ndarray):\n+#         x0 = x\n+#         x = nn.Conv(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+#         x = jax.nn.relu(x)\n+#         x = nn.Conv(\n+#             self.n_filters,\n+#             (3, 3),\n+#             with_bias=False,\n+#             stride=self.strides,\n+#             dtype=self.dtype,\n+#         )(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+#         x = jax.nn.relu(x)\n+#         x = nn.Conv(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n+\n+#         if x0.shape != x.shape:\n+#             x0 = nn.Conv(\n+#                 self.n_filters * 4,\n+#                 (1, 1),\n+#                 with_bias=False,\n+#                 stride=self.strides,\n+#                 dtype=self.dtype,\n+#             )(x0)\n+#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n+#         return jax.nn.relu(x0 + x)\n+\n+\n+# class ResNet(module.Module):\n+#     \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n+#     Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n+#     \"\"\"\n+\n+#     __all__ = [\"__init__\", \"call\"]\n+\n+#     def __init__(\n+#         self,\n+#         stages: tp.List[int],\n+#         block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         \"\"\"\n+#         Arguments:\n+#             stages: A list of integers representing the number of blocks in each stage.\n+#                     e.g: [3, 4, 6, 3] for a ResNet50\n+#             block_type: Which ResNet block type to use.\n+#             lowres: Optional, whether to use the low resolution version\n+#                     as described in subsection 4.2 of the orignal paper.\n+#                     This version is better suited for datasets like CIFAR10. (Default: False)\n+#             weights: One of None (random initialization) or a path to a weights file\n+#             dtype: Optional dtype of the convolutions and linear operations,\n+#                     either jnp.float32 (default) or jnp.float16 for mixed precision.\n+#         \"\"\"\n+\n+#         super().__init__(*args, **kwargs)\n+#         self.stages = stages\n+#         self.block_type = block_type\n+#         self.lowres = lowres\n+\n+#         if weights is not None:\n+#             if weights.endswith(\".pkl\"):\n+#                 collections = pickle.load(open(weights, \"rb\"))\n+#             elif weights == \"imagenet\":\n+#                 clsname = self.__class__.__name__\n+#                 urldict = PRETRAINED_URLS.get(clsname, None)\n+#                 if urldict is None:\n+#                     raise ValueError(f\"No pretrained weights for {clsname} available\")\n+#                 fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n+#                 collections = pickle.load(open(fname, \"rb\"))\n+#             else:\n+#                 raise ValueError(\"Unknown weights value: \", weights)\n+\n+#             if isinstance(collections, tuple):\n+#                 parameters, collections = collections\n+#             elif \"parameters\" in collections:\n+#                 parameters = collections.pop(\"parameters\")\n+#             else:\n+#                 raise ValueError(\n+#                     \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n+#                 )\n+\n+#             x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n+#             # quick but dirty module initialization\n+#             jax.eval_shape(self.init(rng=types.KeySeq(42)), x)\n+\n+#             self.set_default_parameters(parameters, collections)\n+\n+#     def __call__(self, x: jnp.ndarray):\n+#         x = nn.Conv(\n+#             64,\n+#             (7, 7) if not self.lowres else (3, 3),\n+#             stride=(2, 2) if not self.lowres else (1, 1),\n+#             padding=\"SAME\",\n+#             with_bias=False,\n+#             dtype=self.dtype,\n+#         )(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+#         x = module.compact_module(jax.nn.relu)()(x)\n+\n+#         if not self.lowres:\n+#             x = nn.MaxPool(\n+#                 window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n+#             )(x)\n+#         for i, block_size in enumerate(self.stages):\n+#             for j in range(block_size):\n+#                 strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n+#                 x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n+#         GAP = lambda x: jnp.mean(x, axis=(1, 2))\n+#         x = module.compact_module(GAP)(name=\"global_average_pooling\")(x)\n+#         x = nn.Linear(1000, dtype=self.dtype)(x)\n+#         to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n+#         x = module.compact_module(to_float32)(name=\"to_float32\")(x)\n+#         return x\n+\n+\n+# class ResNet18(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[2, 2, 2, 2],\n+#             block_type=ResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet34(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 4, 6, 3],\n+#             block_type=ResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet50(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 4, 6, 3],\n+#             block_type=BottleneckResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet101(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 4, 23, 3],\n+#             block_type=BottleneckResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet152(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 8, 36, 3],\n+#             block_type=BottleneckResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet200(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 24, 36, 3],\n+#             block_type=BottleneckResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# _resnet__init___docstring = \"\"\"\n+# Instantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n+\n+# Arguments:\n+#     lowres: Optional, whether to use the low resolution version\n+#             as described in subsection 4.2 of the orignal paper.\n+#             This version is better suited for datasets like CIFAR10. (Default: False)\n+#     weights: One of None (random initialization), 'imagenet' (automatic download of\n+#               weights pretrained on ImageNet) or a path to a weights file\n+#     dtype: Optional dtype of the convolutions and linear operations,\n+#            either jnp.float32 (default) or jnp.float16 for mixed precision.\n+# \"\"\"\n+\n+# ResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\n+# ResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\n+# ResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\n+# ResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\n+# ResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\n+# ResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "source_code_with_indent": "# adapted from the flax library https://github.com/google/flax\n\nimport pickle\nimport typing as tp\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom elegy import hooks, module, nn, types, utils\n\n__all__ = [\n    \"ResNet\",\n    \"ResNet18\",\n    \"ResNet34\",\n    \"ResNet50\",\n    \"ResNet101\",\n    \"ResNet152\",\n    \"ResNet200\",\n]\n\n\nPRETRAINED_URLS = {\n    \"ResNet18\": {\n        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n        \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n    },\n    \"ResNet50\": {\n        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n        \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n    },\n}\n\n\nclass ResNetBlock(module.Module):\n    <IND>\"\"\"ResNet (identity) block\"\"\"\n\n    def __init__(\n        self,\n        n_filters: int,\n        strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(*args, **kwargs)\n        self.n_filters = n_filters\n        self.strides = strides\n\n    <DED>def call(self, x: jnp.ndarray):\n        <IND>x0 = x\n        x = nn.Conv2D(\n            self.n_filters,\n            (3, 3),\n            with_bias=False,\n            stride=self.strides,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n\n        x = nn.Conv2D(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n\n        if x0.shape != x.shape:\n            <IND>x0 = nn.Conv2D(\n                self.n_filters,\n                (1, 1),\n                with_bias=False,\n                stride=self.strides,\n                dtype=self.dtype,\n            )(x0)\n            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n        <DED>return jax.nn.relu(x0 + x)\n\n\n<DED><DED>class BottleneckResNetBlock(ResNetBlock):\n    <IND>\"\"\"ResNet Bottleneck block.\"\"\"\n\n    def call(self, x: jnp.ndarray):\n        <IND>x0 = x\n        x = nn.Conv2D(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n        x = nn.Conv2D(\n            self.n_filters,\n            (3, 3),\n            with_bias=False,\n            stride=self.strides,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n        x = nn.Conv2D(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n\n        if x0.shape != x.shape:\n            <IND>x0 = nn.Conv2D(\n                self.n_filters * 4,\n                (1, 1),\n                with_bias=False,\n                stride=self.strides,\n                dtype=self.dtype,\n            )(x0)\n            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n        <DED>return jax.nn.relu(x0 + x)\n\n\n<DED><DED>class ResNet(module.Module):\n    <IND>\"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n    Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n    \"\"\"\n\n    __all__ = [\"__init__\", \"call\"]\n\n    def __init__(\n        self,\n        stages: tp.List[int],\n        block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>\"\"\"\n        Arguments:\n            stages: A list of integers representing the number of blocks in each stage.\n                    e.g: [3, 4, 6, 3] for a ResNet50\n            block_type: Which ResNet block type to use.\n            lowres: Optional, whether to use the low resolution version\n                    as described in subsection 4.2 of the orignal paper.\n                    This version is better suited for datasets like CIFAR10. (Default: False)\n            weights: One of None (random initialization) or a path to a weights file\n            dtype: Optional dtype of the convolutions and linear operations,\n                    either jnp.float32 (default) or jnp.float16 for mixed precision.\n        \"\"\"\n\n        super().__init__(*args, **kwargs)\n        self.stages = stages\n        self.block_type = block_type\n        self.lowres = lowres\n\n        if weights is not None:\n            <IND>if weights.endswith(\".pkl\"):\n                <IND>collections = pickle.load(open(weights, \"rb\"))\n            <DED>elif weights == \"imagenet\":\n                <IND>clsname = self.__class__.__name__\n                urldict = PRETRAINED_URLS.get(clsname, None)\n                if urldict is None:\n                    <IND>raise ValueError(f\"No pretrained weights for {clsname} available\")\n                <DED>fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n                collections = pickle.load(open(fname, \"rb\"))\n            <DED>else:\n                <IND>raise ValueError(\"Unknown weights value: \", weights)\n\n            <DED>if isinstance(collections, tuple):\n                <IND>parameters, collections = collections\n            <DED>elif \"parameters\" in collections:\n                <IND>parameters = collections.pop(\"parameters\")\n            <DED>else:\n                <IND>raise ValueError(\n                    \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n                )\n\n            <DED>x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n            # quick but dirty module initialization\n            jax.eval_shape(self.init(rng=types.RNGSeq(42)), x)\n\n            self.set_default_parameters(parameters, collections)\n\n    <DED><DED>def call(self, x: jnp.ndarray):\n        <IND>x = nn.Conv2D(\n            64,\n            (7, 7) if not self.lowres else (3, 3),\n            stride=(2, 2) if not self.lowres else (1, 1),\n            padding=\"SAME\",\n            with_bias=False,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = module.to_module(jax.nn.relu)()(x)\n\n        if not self.lowres:\n            <IND>x = nn.MaxPool(\n                window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n            )(x)\n        <DED>for i, block_size in enumerate(self.stages):\n            <IND>for j in range(block_size):\n                <IND>strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n                x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n        <DED><DED>GAP = lambda x: jnp.mean(x, axis=(1, 2))\n        x = module.to_module(GAP)(name=\"global_average_pooling\")(x)\n        x = nn.Linear(1000, dtype=self.dtype)(x)\n        to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n        x = module.to_module(to_float32)(name=\"to_float32\")(x)\n        return x\n\n\n<DED><DED>class ResNet18(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[2, 2, 2, 2],\n            block_type=ResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet34(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 4, 6, 3],\n            block_type=ResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet50(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 4, 6, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet101(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 4, 23, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet152(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 8, 36, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet200(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 24, 36, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>_resnet__init___docstring = \"\"\"\nInstantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n\nArguments:\n    lowres: Optional, whether to use the low resolution version\n            as described in subsection 4.2 of the orignal paper.\n            This version is better suited for datasets like CIFAR10. (Default: False)\n    weights: One of None (random initialization), 'imagenet' (automatic download of\n              weights pretrained on ImageNet) or a path to a weights file\n    dtype: Optional dtype of the convolutions and linear operations, \n           either jnp.float32 (default) or jnp.float16 for mixed precision.\n\"\"\"\n\nResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\nResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\nResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\nResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\nResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\nResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "# # adapted from the flax library https://github.com/google/flax\n\n# import pickle\n# import typing as tp\n\n# import jax\n# import jax.numpy as jnp\n# import numpy as np\n# from elegy import hooks, module, nn, types, utils\n\n# __all__ = [\n#     \"ResNet\",\n#     \"ResNet18\",\n#     \"ResNet34\",\n#     \"ResNet50\",\n#     \"ResNet101\",\n#     \"ResNet152\",\n#     \"ResNet200\",\n# ]\n\n\n# PRETRAINED_URLS = {\n#     \"ResNet18\": {\n#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n#         \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n#     },\n#     \"ResNet50\": {\n#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n#         \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n#     },\n# }\n\n\n# class ResNetBlock(module.Module):\n#     \"\"\"ResNet (identity) block\"\"\"\n\n#     def __init__(\n#         self,\n#         n_filters: int,\n#         strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(*args, **kwargs)\n#         self.n_filters = n_filters\n#         self.strides = strides\n\n#     def __call__(self, x: jnp.ndarray):\n#         x0 = x\n#         x = nn.Conv(\n#             self.n_filters,\n#             (3, 3),\n#             with_bias=False,\n#             stride=self.strides,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n\n#         x = nn.Conv(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n\n#         if x0.shape != x.shape:\n#             x0 = nn.Conv(\n#                 self.n_filters,\n#                 (1, 1),\n#                 with_bias=False,\n#                 stride=self.strides,\n#                 dtype=self.dtype,\n#             )(x0)\n#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n#         return jax.nn.relu(x0 + x)\n\n\n# class BottleneckResNetBlock(ResNetBlock):\n#     \"\"\"ResNet Bottleneck block.\"\"\"\n\n#     def __call__(self, x: jnp.ndarray):\n#         x0 = x\n#         x = nn.Conv(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n#         x = nn.Conv(\n#             self.n_filters,\n#             (3, 3),\n#             with_bias=False,\n#             stride=self.strides,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n#         x = nn.Conv(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n\n#         if x0.shape != x.shape:\n#             x0 = nn.Conv(\n#                 self.n_filters * 4,\n#                 (1, 1),\n#                 with_bias=False,\n#                 stride=self.strides,\n#                 dtype=self.dtype,\n#             )(x0)\n#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n#         return jax.nn.relu(x0 + x)\n\n\n# class ResNet(module.Module):\n#     \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n#     Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n#     \"\"\"\n\n#     __all__ = [\"__init__\", \"call\"]\n\n#     def __init__(\n#         self,\n#         stages: tp.List[int],\n#         block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         \"\"\"\n#         Arguments:\n#             stages: A list of integers representing the number of blocks in each stage.\n#                     e.g: [3, 4, 6, 3] for a ResNet50\n#             block_type: Which ResNet block type to use.\n#             lowres: Optional, whether to use the low resolution version\n#                     as described in subsection 4.2 of the orignal paper.\n#                     This version is better suited for datasets like CIFAR10. (Default: False)\n#             weights: One of None (random initialization) or a path to a weights file\n#             dtype: Optional dtype of the convolutions and linear operations,\n#                     either jnp.float32 (default) or jnp.float16 for mixed precision.\n#         \"\"\"\n\n#         super().__init__(*args, **kwargs)\n#         self.stages = stages\n#         self.block_type = block_type\n#         self.lowres = lowres\n\n#         if weights is not None:\n#             if weights.endswith(\".pkl\"):\n#                 collections = pickle.load(open(weights, \"rb\"))\n#             elif weights == \"imagenet\":\n#                 clsname = self.__class__.__name__\n#                 urldict = PRETRAINED_URLS.get(clsname, None)\n#                 if urldict is None:\n#                     raise ValueError(f\"No pretrained weights for {clsname} available\")\n#                 fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n#                 collections = pickle.load(open(fname, \"rb\"))\n#             else:\n#                 raise ValueError(\"Unknown weights value: \", weights)\n\n#             if isinstance(collections, tuple):\n#                 parameters, collections = collections\n#             elif \"parameters\" in collections:\n#                 parameters = collections.pop(\"parameters\")\n#             else:\n#                 raise ValueError(\n#                     \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n#                 )\n\n#             x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n#             # quick but dirty module initialization\n#             jax.eval_shape(self.init(rng=types.KeySeq(42)), x)\n\n#             self.set_default_parameters(parameters, collections)\n\n#     def __call__(self, x: jnp.ndarray):\n#         x = nn.Conv(\n#             64,\n#             (7, 7) if not self.lowres else (3, 3),\n#             stride=(2, 2) if not self.lowres else (1, 1),\n#             padding=\"SAME\",\n#             with_bias=False,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = module.compact_module(jax.nn.relu)()(x)\n\n#         if not self.lowres:\n#             x = nn.MaxPool(\n#                 window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n#             )(x)\n#         for i, block_size in enumerate(self.stages):\n#             for j in range(block_size):\n#                 strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n#                 x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n#         GAP = lambda x: jnp.mean(x, axis=(1, 2))\n#         x = module.compact_module(GAP)(name=\"global_average_pooling\")(x)\n#         x = nn.Linear(1000, dtype=self.dtype)(x)\n#         to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n#         x = module.compact_module(to_float32)(name=\"to_float32\")(x)\n#         return x\n\n\n# class ResNet18(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[2, 2, 2, 2],\n#             block_type=ResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet34(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 6, 3],\n#             block_type=ResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet50(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 6, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet101(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 23, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet152(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 8, 36, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet200(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 24, 36, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# _resnet__init___docstring = \"\"\"\n# Instantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n\n# Arguments:\n#     lowres: Optional, whether to use the low resolution version\n#             as described in subsection 4.2 of the orignal paper.\n#             This version is better suited for datasets like CIFAR10. (Default: False)\n#     weights: One of None (random initialization), 'imagenet' (automatic download of\n#               weights pretrained on ImageNet) or a path to a weights file\n#     dtype: Optional dtype of the convolutions and linear operations,\n#            either jnp.float32 (default) or jnp.float16 for mixed precision.\n# \"\"\"\n\n# ResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\n# ResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\n# ResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\n# ResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\n# ResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\n# ResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nets/resnet.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nets/resnet.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "elegy/nets/resnet.py:48:4 Inconsistent override [14]: `elegy.nets.resnet.ResNetBlock.call` overrides method defined in `module.Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `elegy.nets.resnet.ResNetBlock.call` overrides method defined in `module.Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 48,
    "warning_line": "    def call(self, x: jnp.ndarray):",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "# adapted from the flax library https://github.com/google/flax\n\nimport pickle\nimport typing as tp\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom elegy import hooks, module, nn, types, utils\n\n__all__ = [\n    \"ResNet\",\n    \"ResNet18\",\n    \"ResNet34\",\n    \"ResNet50\",\n    \"ResNet101\",\n    \"ResNet152\",\n    \"ResNet200\",\n]\n\n\nPRETRAINED_URLS = {\n    \"ResNet18\": {\n        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n        \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n    },\n    \"ResNet50\": {\n        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n        \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n    },\n}\n\n\nclass ResNetBlock(module.Module):\n    \"\"\"ResNet (identity) block\"\"\"\n\n    def __init__(\n        self,\n        n_filters: int,\n        strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n        *args,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.n_filters = n_filters\n        self.strides = strides\n\n    def call(self, x: jnp.ndarray):\n        x0 = x\n        x = nn.Conv2D(\n            self.n_filters,\n            (3, 3),\n            with_bias=False,\n            stride=self.strides,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n\n        x = nn.Conv2D(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n\n        if x0.shape != x.shape:\n            x0 = nn.Conv2D(\n                self.n_filters,\n                (1, 1),\n                with_bias=False,\n                stride=self.strides,\n                dtype=self.dtype,\n            )(x0)\n            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n        return jax.nn.relu(x0 + x)\n\n\nclass BottleneckResNetBlock(ResNetBlock):\n    \"\"\"ResNet Bottleneck block.\"\"\"\n\n    def call(self, x: jnp.ndarray):\n        x0 = x\n        x = nn.Conv2D(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n        x = nn.Conv2D(\n            self.n_filters,\n            (3, 3),\n            with_bias=False,\n            stride=self.strides,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n        x = nn.Conv2D(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n\n        if x0.shape != x.shape:\n            x0 = nn.Conv2D(\n                self.n_filters * 4,\n                (1, 1),\n                with_bias=False,\n                stride=self.strides,\n                dtype=self.dtype,\n            )(x0)\n            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n        return jax.nn.relu(x0 + x)\n\n\nclass ResNet(module.Module):\n    \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n    Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n    \"\"\"\n\n    __all__ = [\"__init__\", \"call\"]\n\n    def __init__(\n        self,\n        stages: tp.List[int],\n        block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Arguments:\n            stages: A list of integers representing the number of blocks in each stage.\n                    e.g: [3, 4, 6, 3] for a ResNet50\n            block_type: Which ResNet block type to use.\n            lowres: Optional, whether to use the low resolution version\n                    as described in subsection 4.2 of the orignal paper.\n                    This version is better suited for datasets like CIFAR10. (Default: False)\n            weights: One of None (random initialization) or a path to a weights file\n            dtype: Optional dtype of the convolutions and linear operations,\n                    either jnp.float32 (default) or jnp.float16 for mixed precision.\n        \"\"\"\n\n        super().__init__(*args, **kwargs)\n        self.stages = stages\n        self.block_type = block_type\n        self.lowres = lowres\n\n        if weights is not None:\n            if weights.endswith(\".pkl\"):\n                collections = pickle.load(open(weights, \"rb\"))\n            elif weights == \"imagenet\":\n                clsname = self.__class__.__name__\n                urldict = PRETRAINED_URLS.get(clsname, None)\n                if urldict is None:\n                    raise ValueError(f\"No pretrained weights for {clsname} available\")\n                fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n                collections = pickle.load(open(fname, \"rb\"))\n            else:\n                raise ValueError(\"Unknown weights value: \", weights)\n\n            if isinstance(collections, tuple):\n                parameters, collections = collections\n            elif \"parameters\" in collections:\n                parameters = collections.pop(\"parameters\")\n            else:\n                raise ValueError(\n                    \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n                )\n\n            x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n            # quick but dirty module initialization\n            jax.eval_shape(self.init(rng=types.RNGSeq(42)), x)\n\n            self.set_default_parameters(parameters, collections)\n\n    def call(self, x: jnp.ndarray):\n        x = nn.Conv2D(\n            64,\n            (7, 7) if not self.lowres else (3, 3),\n            stride=(2, 2) if not self.lowres else (1, 1),\n            padding=\"SAME\",\n            with_bias=False,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = module.to_module(jax.nn.relu)()(x)\n\n        if not self.lowres:\n            x = nn.MaxPool(\n                window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n            )(x)\n        for i, block_size in enumerate(self.stages):\n            for j in range(block_size):\n                strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n                x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n        GAP = lambda x: jnp.mean(x, axis=(1, 2))\n        x = module.to_module(GAP)(name=\"global_average_pooling\")(x)\n        x = nn.Linear(1000, dtype=self.dtype)(x)\n        to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n        x = module.to_module(to_float32)(name=\"to_float32\")(x)\n        return x\n\n\nclass ResNet18(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[2, 2, 2, 2],\n            block_type=ResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet34(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 4, 6, 3],\n            block_type=ResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet50(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 4, 6, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet101(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 4, 23, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet152(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 8, 36, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet200(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 24, 36, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n_resnet__init___docstring = \"\"\"\nInstantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n\nArguments:\n    lowres: Optional, whether to use the low resolution version\n            as described in subsection 4.2 of the orignal paper.\n            This version is better suited for datasets like CIFAR10. (Default: False)\n    weights: One of None (random initialization), 'imagenet' (automatic download of\n              weights pretrained on ImageNet) or a path to a weights file\n    dtype: Optional dtype of the convolutions and linear operations, \n           either jnp.float32 (default) or jnp.float16 for mixed precision.\n\"\"\"\n\nResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\nResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\nResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\nResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\nResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\nResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "source_code_len": 10761,
        "target_code": "# # adapted from the flax library https://github.com/google/flax\n\n# import pickle\n# import typing as tp\n\n# import jax\n# import jax.numpy as jnp\n# import numpy as np\n# from elegy import hooks, module, nn, types, utils\n\n# __all__ = [\n#     \"ResNet\",\n#     \"ResNet18\",\n#     \"ResNet34\",\n#     \"ResNet50\",\n#     \"ResNet101\",\n#     \"ResNet152\",\n#     \"ResNet200\",\n# ]\n\n\n# PRETRAINED_URLS = {\n#     \"ResNet18\": {\n#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n#         \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n#     },\n#     \"ResNet50\": {\n#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n#         \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n#     },\n# }\n\n\n# class ResNetBlock(module.Module):\n#     \"\"\"ResNet (identity) block\"\"\"\n\n#     def __init__(\n#         self,\n#         n_filters: int,\n#         strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(*args, **kwargs)\n#         self.n_filters = n_filters\n#         self.strides = strides\n\n#     def __call__(self, x: jnp.ndarray):\n#         x0 = x\n#         x = nn.Conv(\n#             self.n_filters,\n#             (3, 3),\n#             with_bias=False,\n#             stride=self.strides,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n\n#         x = nn.Conv(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n\n#         if x0.shape != x.shape:\n#             x0 = nn.Conv(\n#                 self.n_filters,\n#                 (1, 1),\n#                 with_bias=False,\n#                 stride=self.strides,\n#                 dtype=self.dtype,\n#             )(x0)\n#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n#         return jax.nn.relu(x0 + x)\n\n\n# class BottleneckResNetBlock(ResNetBlock):\n#     \"\"\"ResNet Bottleneck block.\"\"\"\n\n#     def __call__(self, x: jnp.ndarray):\n#         x0 = x\n#         x = nn.Conv(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n#         x = nn.Conv(\n#             self.n_filters,\n#             (3, 3),\n#             with_bias=False,\n#             stride=self.strides,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n#         x = nn.Conv(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n\n#         if x0.shape != x.shape:\n#             x0 = nn.Conv(\n#                 self.n_filters * 4,\n#                 (1, 1),\n#                 with_bias=False,\n#                 stride=self.strides,\n#                 dtype=self.dtype,\n#             )(x0)\n#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n#         return jax.nn.relu(x0 + x)\n\n\n# class ResNet(module.Module):\n#     \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n#     Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n#     \"\"\"\n\n#     __all__ = [\"__init__\", \"call\"]\n\n#     def __init__(\n#         self,\n#         stages: tp.List[int],\n#         block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         \"\"\"\n#         Arguments:\n#             stages: A list of integers representing the number of blocks in each stage.\n#                     e.g: [3, 4, 6, 3] for a ResNet50\n#             block_type: Which ResNet block type to use.\n#             lowres: Optional, whether to use the low resolution version\n#                     as described in subsection 4.2 of the orignal paper.\n#                     This version is better suited for datasets like CIFAR10. (Default: False)\n#             weights: One of None (random initialization) or a path to a weights file\n#             dtype: Optional dtype of the convolutions and linear operations,\n#                     either jnp.float32 (default) or jnp.float16 for mixed precision.\n#         \"\"\"\n\n#         super().__init__(*args, **kwargs)\n#         self.stages = stages\n#         self.block_type = block_type\n#         self.lowres = lowres\n\n#         if weights is not None:\n#             if weights.endswith(\".pkl\"):\n#                 collections = pickle.load(open(weights, \"rb\"))\n#             elif weights == \"imagenet\":\n#                 clsname = self.__class__.__name__\n#                 urldict = PRETRAINED_URLS.get(clsname, None)\n#                 if urldict is None:\n#                     raise ValueError(f\"No pretrained weights for {clsname} available\")\n#                 fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n#                 collections = pickle.load(open(fname, \"rb\"))\n#             else:\n#                 raise ValueError(\"Unknown weights value: \", weights)\n\n#             if isinstance(collections, tuple):\n#                 parameters, collections = collections\n#             elif \"parameters\" in collections:\n#                 parameters = collections.pop(\"parameters\")\n#             else:\n#                 raise ValueError(\n#                     \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n#                 )\n\n#             x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n#             # quick but dirty module initialization\n#             jax.eval_shape(self.init(rng=types.KeySeq(42)), x)\n\n#             self.set_default_parameters(parameters, collections)\n\n#     def __call__(self, x: jnp.ndarray):\n#         x = nn.Conv(\n#             64,\n#             (7, 7) if not self.lowres else (3, 3),\n#             stride=(2, 2) if not self.lowres else (1, 1),\n#             padding=\"SAME\",\n#             with_bias=False,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = module.compact_module(jax.nn.relu)()(x)\n\n#         if not self.lowres:\n#             x = nn.MaxPool(\n#                 window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n#             )(x)\n#         for i, block_size in enumerate(self.stages):\n#             for j in range(block_size):\n#                 strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n#                 x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n#         GAP = lambda x: jnp.mean(x, axis=(1, 2))\n#         x = module.compact_module(GAP)(name=\"global_average_pooling\")(x)\n#         x = nn.Linear(1000, dtype=self.dtype)(x)\n#         to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n#         x = module.compact_module(to_float32)(name=\"to_float32\")(x)\n#         return x\n\n\n# class ResNet18(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[2, 2, 2, 2],\n#             block_type=ResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet34(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 6, 3],\n#             block_type=ResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet50(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 6, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet101(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 23, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet152(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 8, 36, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet200(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 24, 36, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# _resnet__init___docstring = \"\"\"\n# Instantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n\n# Arguments:\n#     lowres: Optional, whether to use the low resolution version\n#             as described in subsection 4.2 of the orignal paper.\n#             This version is better suited for datasets like CIFAR10. (Default: False)\n#     weights: One of None (random initialization), 'imagenet' (automatic download of\n#               weights pretrained on ImageNet) or a path to a weights file\n#     dtype: Optional dtype of the convolutions and linear operations,\n#            either jnp.float32 (default) or jnp.float16 for mixed precision.\n# \"\"\"\n\n# ResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\n# ResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\n# ResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\n# ResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\n# ResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\n# ResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "target_code_len": 11287,
        "diff_format": "@@ -1,336 +1,336 @@\n-# adapted from the flax library https://github.com/google/flax\n-\n-import pickle\n-import typing as tp\n-\n-import jax\n-import jax.numpy as jnp\n-import numpy as np\n-from elegy import hooks, module, nn, types, utils\n-\n-__all__ = [\n-    \"ResNet\",\n-    \"ResNet18\",\n-    \"ResNet34\",\n-    \"ResNet50\",\n-    \"ResNet101\",\n-    \"ResNet152\",\n-    \"ResNet200\",\n-]\n-\n-\n-PRETRAINED_URLS = {\n-    \"ResNet18\": {\n-        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n-        \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n-    },\n-    \"ResNet50\": {\n-        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n-        \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n-    },\n-}\n-\n-\n-class ResNetBlock(module.Module):\n-    \"\"\"ResNet (identity) block\"\"\"\n-\n-    def __init__(\n-        self,\n-        n_filters: int,\n-        strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(*args, **kwargs)\n-        self.n_filters = n_filters\n-        self.strides = strides\n-\n-    def call(self, x: jnp.ndarray):\n-        x0 = x\n-        x = nn.Conv2D(\n-            self.n_filters,\n-            (3, 3),\n-            with_bias=False,\n-            stride=self.strides,\n-            dtype=self.dtype,\n-        )(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-        x = jax.nn.relu(x)\n-\n-        x = nn.Conv2D(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-\n-        if x0.shape != x.shape:\n-            x0 = nn.Conv2D(\n-                self.n_filters,\n-                (1, 1),\n-                with_bias=False,\n-                stride=self.strides,\n-                dtype=self.dtype,\n-            )(x0)\n-            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n-        return jax.nn.relu(x0 + x)\n-\n-\n-class BottleneckResNetBlock(ResNetBlock):\n-    \"\"\"ResNet Bottleneck block.\"\"\"\n-\n-    def call(self, x: jnp.ndarray):\n-        x0 = x\n-        x = nn.Conv2D(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-        x = jax.nn.relu(x)\n-        x = nn.Conv2D(\n-            self.n_filters,\n-            (3, 3),\n-            with_bias=False,\n-            stride=self.strides,\n-            dtype=self.dtype,\n-        )(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-        x = jax.nn.relu(x)\n-        x = nn.Conv2D(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n-\n-        if x0.shape != x.shape:\n-            x0 = nn.Conv2D(\n-                self.n_filters * 4,\n-                (1, 1),\n-                with_bias=False,\n-                stride=self.strides,\n-                dtype=self.dtype,\n-            )(x0)\n-            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n-        return jax.nn.relu(x0 + x)\n-\n-\n-class ResNet(module.Module):\n-    \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n-    Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n-    \"\"\"\n-\n-    __all__ = [\"__init__\", \"call\"]\n-\n-    def __init__(\n-        self,\n-        stages: tp.List[int],\n-        block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Arguments:\n-            stages: A list of integers representing the number of blocks in each stage.\n-                    e.g: [3, 4, 6, 3] for a ResNet50\n-            block_type: Which ResNet block type to use.\n-            lowres: Optional, whether to use the low resolution version\n-                    as described in subsection 4.2 of the orignal paper.\n-                    This version is better suited for datasets like CIFAR10. (Default: False)\n-            weights: One of None (random initialization) or a path to a weights file\n-            dtype: Optional dtype of the convolutions and linear operations,\n-                    either jnp.float32 (default) or jnp.float16 for mixed precision.\n-        \"\"\"\n-\n-        super().__init__(*args, **kwargs)\n-        self.stages = stages\n-        self.block_type = block_type\n-        self.lowres = lowres\n-\n-        if weights is not None:\n-            if weights.endswith(\".pkl\"):\n-                collections = pickle.load(open(weights, \"rb\"))\n-            elif weights == \"imagenet\":\n-                clsname = self.__class__.__name__\n-                urldict = PRETRAINED_URLS.get(clsname, None)\n-                if urldict is None:\n-                    raise ValueError(f\"No pretrained weights for {clsname} available\")\n-                fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n-                collections = pickle.load(open(fname, \"rb\"))\n-            else:\n-                raise ValueError(\"Unknown weights value: \", weights)\n-\n-            if isinstance(collections, tuple):\n-                parameters, collections = collections\n-            elif \"parameters\" in collections:\n-                parameters = collections.pop(\"parameters\")\n-            else:\n-                raise ValueError(\n-                    \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n-                )\n-\n-            x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n-            # quick but dirty module initialization\n-            jax.eval_shape(self.init(rng=types.RNGSeq(42)), x)\n-\n-            self.set_default_parameters(parameters, collections)\n-\n-    def call(self, x: jnp.ndarray):\n-        x = nn.Conv2D(\n-            64,\n-            (7, 7) if not self.lowres else (3, 3),\n-            stride=(2, 2) if not self.lowres else (1, 1),\n-            padding=\"SAME\",\n-            with_bias=False,\n-            dtype=self.dtype,\n-        )(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-        x = module.to_module(jax.nn.relu)()(x)\n-\n-        if not self.lowres:\n-            x = nn.MaxPool(\n-                window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n-            )(x)\n-        for i, block_size in enumerate(self.stages):\n-            for j in range(block_size):\n-                strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n-                x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n-        GAP = lambda x: jnp.mean(x, axis=(1, 2))\n-        x = module.to_module(GAP)(name=\"global_average_pooling\")(x)\n-        x = nn.Linear(1000, dtype=self.dtype)(x)\n-        to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n-        x = module.to_module(to_float32)(name=\"to_float32\")(x)\n-        return x\n-\n-\n-class ResNet18(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[2, 2, 2, 2],\n-            block_type=ResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet34(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 4, 6, 3],\n-            block_type=ResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet50(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 4, 6, 3],\n-            block_type=BottleneckResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet101(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 4, 23, 3],\n-            block_type=BottleneckResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet152(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 8, 36, 3],\n-            block_type=BottleneckResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet200(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 24, 36, 3],\n-            block_type=BottleneckResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-_resnet__init___docstring = \"\"\"\n-Instantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n-\n-Arguments:\n-    lowres: Optional, whether to use the low resolution version\n-            as described in subsection 4.2 of the orignal paper.\n-            This version is better suited for datasets like CIFAR10. (Default: False)\n-    weights: One of None (random initialization), 'imagenet' (automatic download of\n-              weights pretrained on ImageNet) or a path to a weights file\n-    dtype: Optional dtype of the convolutions and linear operations, \n-           either jnp.float32 (default) or jnp.float16 for mixed precision.\n-\"\"\"\n-\n-ResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\n-ResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\n-ResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\n-ResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\n-ResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\n-ResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n+# # adapted from the flax library https://github.com/google/flax\n+\n+# import pickle\n+# import typing as tp\n+\n+# import jax\n+# import jax.numpy as jnp\n+# import numpy as np\n+# from elegy import hooks, module, nn, types, utils\n+\n+# __all__ = [\n+#     \"ResNet\",\n+#     \"ResNet18\",\n+#     \"ResNet34\",\n+#     \"ResNet50\",\n+#     \"ResNet101\",\n+#     \"ResNet152\",\n+#     \"ResNet200\",\n+# ]\n+\n+\n+# PRETRAINED_URLS = {\n+#     \"ResNet18\": {\n+#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n+#         \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n+#     },\n+#     \"ResNet50\": {\n+#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n+#         \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n+#     },\n+# }\n+\n+\n+# class ResNetBlock(module.Module):\n+#     \"\"\"ResNet (identity) block\"\"\"\n+\n+#     def __init__(\n+#         self,\n+#         n_filters: int,\n+#         strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(*args, **kwargs)\n+#         self.n_filters = n_filters\n+#         self.strides = strides\n+\n+#     def __call__(self, x: jnp.ndarray):\n+#         x0 = x\n+#         x = nn.Conv(\n+#             self.n_filters,\n+#             (3, 3),\n+#             with_bias=False,\n+#             stride=self.strides,\n+#             dtype=self.dtype,\n+#         )(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+#         x = jax.nn.relu(x)\n+\n+#         x = nn.Conv(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+\n+#         if x0.shape != x.shape:\n+#             x0 = nn.Conv(\n+#                 self.n_filters,\n+#                 (1, 1),\n+#                 with_bias=False,\n+#                 stride=self.strides,\n+#                 dtype=self.dtype,\n+#             )(x0)\n+#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n+#         return jax.nn.relu(x0 + x)\n+\n+\n+# class BottleneckResNetBlock(ResNetBlock):\n+#     \"\"\"ResNet Bottleneck block.\"\"\"\n+\n+#     def __call__(self, x: jnp.ndarray):\n+#         x0 = x\n+#         x = nn.Conv(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+#         x = jax.nn.relu(x)\n+#         x = nn.Conv(\n+#             self.n_filters,\n+#             (3, 3),\n+#             with_bias=False,\n+#             stride=self.strides,\n+#             dtype=self.dtype,\n+#         )(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+#         x = jax.nn.relu(x)\n+#         x = nn.Conv(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n+\n+#         if x0.shape != x.shape:\n+#             x0 = nn.Conv(\n+#                 self.n_filters * 4,\n+#                 (1, 1),\n+#                 with_bias=False,\n+#                 stride=self.strides,\n+#                 dtype=self.dtype,\n+#             )(x0)\n+#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n+#         return jax.nn.relu(x0 + x)\n+\n+\n+# class ResNet(module.Module):\n+#     \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n+#     Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n+#     \"\"\"\n+\n+#     __all__ = [\"__init__\", \"call\"]\n+\n+#     def __init__(\n+#         self,\n+#         stages: tp.List[int],\n+#         block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         \"\"\"\n+#         Arguments:\n+#             stages: A list of integers representing the number of blocks in each stage.\n+#                     e.g: [3, 4, 6, 3] for a ResNet50\n+#             block_type: Which ResNet block type to use.\n+#             lowres: Optional, whether to use the low resolution version\n+#                     as described in subsection 4.2 of the orignal paper.\n+#                     This version is better suited for datasets like CIFAR10. (Default: False)\n+#             weights: One of None (random initialization) or a path to a weights file\n+#             dtype: Optional dtype of the convolutions and linear operations,\n+#                     either jnp.float32 (default) or jnp.float16 for mixed precision.\n+#         \"\"\"\n+\n+#         super().__init__(*args, **kwargs)\n+#         self.stages = stages\n+#         self.block_type = block_type\n+#         self.lowres = lowres\n+\n+#         if weights is not None:\n+#             if weights.endswith(\".pkl\"):\n+#                 collections = pickle.load(open(weights, \"rb\"))\n+#             elif weights == \"imagenet\":\n+#                 clsname = self.__class__.__name__\n+#                 urldict = PRETRAINED_URLS.get(clsname, None)\n+#                 if urldict is None:\n+#                     raise ValueError(f\"No pretrained weights for {clsname} available\")\n+#                 fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n+#                 collections = pickle.load(open(fname, \"rb\"))\n+#             else:\n+#                 raise ValueError(\"Unknown weights value: \", weights)\n+\n+#             if isinstance(collections, tuple):\n+#                 parameters, collections = collections\n+#             elif \"parameters\" in collections:\n+#                 parameters = collections.pop(\"parameters\")\n+#             else:\n+#                 raise ValueError(\n+#                     \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n+#                 )\n+\n+#             x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n+#             # quick but dirty module initialization\n+#             jax.eval_shape(self.init(rng=types.KeySeq(42)), x)\n+\n+#             self.set_default_parameters(parameters, collections)\n+\n+#     def __call__(self, x: jnp.ndarray):\n+#         x = nn.Conv(\n+#             64,\n+#             (7, 7) if not self.lowres else (3, 3),\n+#             stride=(2, 2) if not self.lowres else (1, 1),\n+#             padding=\"SAME\",\n+#             with_bias=False,\n+#             dtype=self.dtype,\n+#         )(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+#         x = module.compact_module(jax.nn.relu)()(x)\n+\n+#         if not self.lowres:\n+#             x = nn.MaxPool(\n+#                 window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n+#             )(x)\n+#         for i, block_size in enumerate(self.stages):\n+#             for j in range(block_size):\n+#                 strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n+#                 x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n+#         GAP = lambda x: jnp.mean(x, axis=(1, 2))\n+#         x = module.compact_module(GAP)(name=\"global_average_pooling\")(x)\n+#         x = nn.Linear(1000, dtype=self.dtype)(x)\n+#         to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n+#         x = module.compact_module(to_float32)(name=\"to_float32\")(x)\n+#         return x\n+\n+\n+# class ResNet18(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[2, 2, 2, 2],\n+#             block_type=ResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet34(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 4, 6, 3],\n+#             block_type=ResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet50(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 4, 6, 3],\n+#             block_type=BottleneckResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet101(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 4, 23, 3],\n+#             block_type=BottleneckResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet152(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 8, 36, 3],\n+#             block_type=BottleneckResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet200(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 24, 36, 3],\n+#             block_type=BottleneckResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# _resnet__init___docstring = \"\"\"\n+# Instantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n+\n+# Arguments:\n+#     lowres: Optional, whether to use the low resolution version\n+#             as described in subsection 4.2 of the orignal paper.\n+#             This version is better suited for datasets like CIFAR10. (Default: False)\n+#     weights: One of None (random initialization), 'imagenet' (automatic download of\n+#               weights pretrained on ImageNet) or a path to a weights file\n+#     dtype: Optional dtype of the convolutions and linear operations,\n+#            either jnp.float32 (default) or jnp.float16 for mixed precision.\n+# \"\"\"\n+\n+# ResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\n+# ResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\n+# ResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\n+# ResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\n+# ResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\n+# ResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "source_code_with_indent": "# adapted from the flax library https://github.com/google/flax\n\nimport pickle\nimport typing as tp\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom elegy import hooks, module, nn, types, utils\n\n__all__ = [\n    \"ResNet\",\n    \"ResNet18\",\n    \"ResNet34\",\n    \"ResNet50\",\n    \"ResNet101\",\n    \"ResNet152\",\n    \"ResNet200\",\n]\n\n\nPRETRAINED_URLS = {\n    \"ResNet18\": {\n        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n        \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n    },\n    \"ResNet50\": {\n        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n        \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n    },\n}\n\n\nclass ResNetBlock(module.Module):\n    <IND>\"\"\"ResNet (identity) block\"\"\"\n\n    def __init__(\n        self,\n        n_filters: int,\n        strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(*args, **kwargs)\n        self.n_filters = n_filters\n        self.strides = strides\n\n    <DED>def call(self, x: jnp.ndarray):\n        <IND>x0 = x\n        x = nn.Conv2D(\n            self.n_filters,\n            (3, 3),\n            with_bias=False,\n            stride=self.strides,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n\n        x = nn.Conv2D(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n\n        if x0.shape != x.shape:\n            <IND>x0 = nn.Conv2D(\n                self.n_filters,\n                (1, 1),\n                with_bias=False,\n                stride=self.strides,\n                dtype=self.dtype,\n            )(x0)\n            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n        <DED>return jax.nn.relu(x0 + x)\n\n\n<DED><DED>class BottleneckResNetBlock(ResNetBlock):\n    <IND>\"\"\"ResNet Bottleneck block.\"\"\"\n\n    def call(self, x: jnp.ndarray):\n        <IND>x0 = x\n        x = nn.Conv2D(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n        x = nn.Conv2D(\n            self.n_filters,\n            (3, 3),\n            with_bias=False,\n            stride=self.strides,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n        x = nn.Conv2D(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n\n        if x0.shape != x.shape:\n            <IND>x0 = nn.Conv2D(\n                self.n_filters * 4,\n                (1, 1),\n                with_bias=False,\n                stride=self.strides,\n                dtype=self.dtype,\n            )(x0)\n            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n        <DED>return jax.nn.relu(x0 + x)\n\n\n<DED><DED>class ResNet(module.Module):\n    <IND>\"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n    Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n    \"\"\"\n\n    __all__ = [\"__init__\", \"call\"]\n\n    def __init__(\n        self,\n        stages: tp.List[int],\n        block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>\"\"\"\n        Arguments:\n            stages: A list of integers representing the number of blocks in each stage.\n                    e.g: [3, 4, 6, 3] for a ResNet50\n            block_type: Which ResNet block type to use.\n            lowres: Optional, whether to use the low resolution version\n                    as described in subsection 4.2 of the orignal paper.\n                    This version is better suited for datasets like CIFAR10. (Default: False)\n            weights: One of None (random initialization) or a path to a weights file\n            dtype: Optional dtype of the convolutions and linear operations,\n                    either jnp.float32 (default) or jnp.float16 for mixed precision.\n        \"\"\"\n\n        super().__init__(*args, **kwargs)\n        self.stages = stages\n        self.block_type = block_type\n        self.lowres = lowres\n\n        if weights is not None:\n            <IND>if weights.endswith(\".pkl\"):\n                <IND>collections = pickle.load(open(weights, \"rb\"))\n            <DED>elif weights == \"imagenet\":\n                <IND>clsname = self.__class__.__name__\n                urldict = PRETRAINED_URLS.get(clsname, None)\n                if urldict is None:\n                    <IND>raise ValueError(f\"No pretrained weights for {clsname} available\")\n                <DED>fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n                collections = pickle.load(open(fname, \"rb\"))\n            <DED>else:\n                <IND>raise ValueError(\"Unknown weights value: \", weights)\n\n            <DED>if isinstance(collections, tuple):\n                <IND>parameters, collections = collections\n            <DED>elif \"parameters\" in collections:\n                <IND>parameters = collections.pop(\"parameters\")\n            <DED>else:\n                <IND>raise ValueError(\n                    \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n                )\n\n            <DED>x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n            # quick but dirty module initialization\n            jax.eval_shape(self.init(rng=types.RNGSeq(42)), x)\n\n            self.set_default_parameters(parameters, collections)\n\n    <DED><DED>def call(self, x: jnp.ndarray):\n        <IND>x = nn.Conv2D(\n            64,\n            (7, 7) if not self.lowres else (3, 3),\n            stride=(2, 2) if not self.lowres else (1, 1),\n            padding=\"SAME\",\n            with_bias=False,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = module.to_module(jax.nn.relu)()(x)\n\n        if not self.lowres:\n            <IND>x = nn.MaxPool(\n                window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n            )(x)\n        <DED>for i, block_size in enumerate(self.stages):\n            <IND>for j in range(block_size):\n                <IND>strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n                x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n        <DED><DED>GAP = lambda x: jnp.mean(x, axis=(1, 2))\n        x = module.to_module(GAP)(name=\"global_average_pooling\")(x)\n        x = nn.Linear(1000, dtype=self.dtype)(x)\n        to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n        x = module.to_module(to_float32)(name=\"to_float32\")(x)\n        return x\n\n\n<DED><DED>class ResNet18(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[2, 2, 2, 2],\n            block_type=ResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet34(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 4, 6, 3],\n            block_type=ResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet50(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 4, 6, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet101(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 4, 23, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet152(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 8, 36, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet200(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 24, 36, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>_resnet__init___docstring = \"\"\"\nInstantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n\nArguments:\n    lowres: Optional, whether to use the low resolution version\n            as described in subsection 4.2 of the orignal paper.\n            This version is better suited for datasets like CIFAR10. (Default: False)\n    weights: One of None (random initialization), 'imagenet' (automatic download of\n              weights pretrained on ImageNet) or a path to a weights file\n    dtype: Optional dtype of the convolutions and linear operations, \n           either jnp.float32 (default) or jnp.float16 for mixed precision.\n\"\"\"\n\nResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\nResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\nResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\nResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\nResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\nResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "# # adapted from the flax library https://github.com/google/flax\n\n# import pickle\n# import typing as tp\n\n# import jax\n# import jax.numpy as jnp\n# import numpy as np\n# from elegy import hooks, module, nn, types, utils\n\n# __all__ = [\n#     \"ResNet\",\n#     \"ResNet18\",\n#     \"ResNet34\",\n#     \"ResNet50\",\n#     \"ResNet101\",\n#     \"ResNet152\",\n#     \"ResNet200\",\n# ]\n\n\n# PRETRAINED_URLS = {\n#     \"ResNet18\": {\n#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n#         \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n#     },\n#     \"ResNet50\": {\n#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n#         \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n#     },\n# }\n\n\n# class ResNetBlock(module.Module):\n#     \"\"\"ResNet (identity) block\"\"\"\n\n#     def __init__(\n#         self,\n#         n_filters: int,\n#         strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(*args, **kwargs)\n#         self.n_filters = n_filters\n#         self.strides = strides\n\n#     def __call__(self, x: jnp.ndarray):\n#         x0 = x\n#         x = nn.Conv(\n#             self.n_filters,\n#             (3, 3),\n#             with_bias=False,\n#             stride=self.strides,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n\n#         x = nn.Conv(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n\n#         if x0.shape != x.shape:\n#             x0 = nn.Conv(\n#                 self.n_filters,\n#                 (1, 1),\n#                 with_bias=False,\n#                 stride=self.strides,\n#                 dtype=self.dtype,\n#             )(x0)\n#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n#         return jax.nn.relu(x0 + x)\n\n\n# class BottleneckResNetBlock(ResNetBlock):\n#     \"\"\"ResNet Bottleneck block.\"\"\"\n\n#     def __call__(self, x: jnp.ndarray):\n#         x0 = x\n#         x = nn.Conv(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n#         x = nn.Conv(\n#             self.n_filters,\n#             (3, 3),\n#             with_bias=False,\n#             stride=self.strides,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n#         x = nn.Conv(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n\n#         if x0.shape != x.shape:\n#             x0 = nn.Conv(\n#                 self.n_filters * 4,\n#                 (1, 1),\n#                 with_bias=False,\n#                 stride=self.strides,\n#                 dtype=self.dtype,\n#             )(x0)\n#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n#         return jax.nn.relu(x0 + x)\n\n\n# class ResNet(module.Module):\n#     \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n#     Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n#     \"\"\"\n\n#     __all__ = [\"__init__\", \"call\"]\n\n#     def __init__(\n#         self,\n#         stages: tp.List[int],\n#         block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         \"\"\"\n#         Arguments:\n#             stages: A list of integers representing the number of blocks in each stage.\n#                     e.g: [3, 4, 6, 3] for a ResNet50\n#             block_type: Which ResNet block type to use.\n#             lowres: Optional, whether to use the low resolution version\n#                     as described in subsection 4.2 of the orignal paper.\n#                     This version is better suited for datasets like CIFAR10. (Default: False)\n#             weights: One of None (random initialization) or a path to a weights file\n#             dtype: Optional dtype of the convolutions and linear operations,\n#                     either jnp.float32 (default) or jnp.float16 for mixed precision.\n#         \"\"\"\n\n#         super().__init__(*args, **kwargs)\n#         self.stages = stages\n#         self.block_type = block_type\n#         self.lowres = lowres\n\n#         if weights is not None:\n#             if weights.endswith(\".pkl\"):\n#                 collections = pickle.load(open(weights, \"rb\"))\n#             elif weights == \"imagenet\":\n#                 clsname = self.__class__.__name__\n#                 urldict = PRETRAINED_URLS.get(clsname, None)\n#                 if urldict is None:\n#                     raise ValueError(f\"No pretrained weights for {clsname} available\")\n#                 fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n#                 collections = pickle.load(open(fname, \"rb\"))\n#             else:\n#                 raise ValueError(\"Unknown weights value: \", weights)\n\n#             if isinstance(collections, tuple):\n#                 parameters, collections = collections\n#             elif \"parameters\" in collections:\n#                 parameters = collections.pop(\"parameters\")\n#             else:\n#                 raise ValueError(\n#                     \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n#                 )\n\n#             x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n#             # quick but dirty module initialization\n#             jax.eval_shape(self.init(rng=types.KeySeq(42)), x)\n\n#             self.set_default_parameters(parameters, collections)\n\n#     def __call__(self, x: jnp.ndarray):\n#         x = nn.Conv(\n#             64,\n#             (7, 7) if not self.lowres else (3, 3),\n#             stride=(2, 2) if not self.lowres else (1, 1),\n#             padding=\"SAME\",\n#             with_bias=False,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = module.compact_module(jax.nn.relu)()(x)\n\n#         if not self.lowres:\n#             x = nn.MaxPool(\n#                 window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n#             )(x)\n#         for i, block_size in enumerate(self.stages):\n#             for j in range(block_size):\n#                 strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n#                 x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n#         GAP = lambda x: jnp.mean(x, axis=(1, 2))\n#         x = module.compact_module(GAP)(name=\"global_average_pooling\")(x)\n#         x = nn.Linear(1000, dtype=self.dtype)(x)\n#         to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n#         x = module.compact_module(to_float32)(name=\"to_float32\")(x)\n#         return x\n\n\n# class ResNet18(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[2, 2, 2, 2],\n#             block_type=ResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet34(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 6, 3],\n#             block_type=ResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet50(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 6, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet101(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 23, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet152(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 8, 36, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet200(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 24, 36, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# _resnet__init___docstring = \"\"\"\n# Instantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n\n# Arguments:\n#     lowres: Optional, whether to use the low resolution version\n#             as described in subsection 4.2 of the orignal paper.\n#             This version is better suited for datasets like CIFAR10. (Default: False)\n#     weights: One of None (random initialization), 'imagenet' (automatic download of\n#               weights pretrained on ImageNet) or a path to a weights file\n#     dtype: Optional dtype of the convolutions and linear operations,\n#            either jnp.float32 (default) or jnp.float16 for mixed precision.\n# \"\"\"\n\n# ResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\n# ResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\n# ResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\n# ResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\n# ResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\n# ResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nets/resnet.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nets/resnet.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "elegy/nets/resnet.py:48:4 Inconsistent override [14]: `elegy.nets.resnet.ResNetBlock.call` overrides method defined in `module.Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "message": " `elegy.nets.resnet.ResNetBlock.call` overrides method defined in `module.Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 48,
    "warning_line": "    def call(self, x: jnp.ndarray):",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "# adapted from the flax library https://github.com/google/flax\n\nimport pickle\nimport typing as tp\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom elegy import hooks, module, nn, types, utils\n\n__all__ = [\n    \"ResNet\",\n    \"ResNet18\",\n    \"ResNet34\",\n    \"ResNet50\",\n    \"ResNet101\",\n    \"ResNet152\",\n    \"ResNet200\",\n]\n\n\nPRETRAINED_URLS = {\n    \"ResNet18\": {\n        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n        \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n    },\n    \"ResNet50\": {\n        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n        \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n    },\n}\n\n\nclass ResNetBlock(module.Module):\n    \"\"\"ResNet (identity) block\"\"\"\n\n    def __init__(\n        self,\n        n_filters: int,\n        strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n        *args,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.n_filters = n_filters\n        self.strides = strides\n\n    def call(self, x: jnp.ndarray):\n        x0 = x\n        x = nn.Conv2D(\n            self.n_filters,\n            (3, 3),\n            with_bias=False,\n            stride=self.strides,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n\n        x = nn.Conv2D(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n\n        if x0.shape != x.shape:\n            x0 = nn.Conv2D(\n                self.n_filters,\n                (1, 1),\n                with_bias=False,\n                stride=self.strides,\n                dtype=self.dtype,\n            )(x0)\n            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n        return jax.nn.relu(x0 + x)\n\n\nclass BottleneckResNetBlock(ResNetBlock):\n    \"\"\"ResNet Bottleneck block.\"\"\"\n\n    def call(self, x: jnp.ndarray):\n        x0 = x\n        x = nn.Conv2D(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n        x = nn.Conv2D(\n            self.n_filters,\n            (3, 3),\n            with_bias=False,\n            stride=self.strides,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n        x = nn.Conv2D(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n\n        if x0.shape != x.shape:\n            x0 = nn.Conv2D(\n                self.n_filters * 4,\n                (1, 1),\n                with_bias=False,\n                stride=self.strides,\n                dtype=self.dtype,\n            )(x0)\n            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n        return jax.nn.relu(x0 + x)\n\n\nclass ResNet(module.Module):\n    \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n    Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n    \"\"\"\n\n    __all__ = [\"__init__\", \"call\"]\n\n    def __init__(\n        self,\n        stages: tp.List[int],\n        block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Arguments:\n            stages: A list of integers representing the number of blocks in each stage.\n                    e.g: [3, 4, 6, 3] for a ResNet50\n            block_type: Which ResNet block type to use.\n            lowres: Optional, whether to use the low resolution version\n                    as described in subsection 4.2 of the orignal paper.\n                    This version is better suited for datasets like CIFAR10. (Default: False)\n            weights: One of None (random initialization) or a path to a weights file\n            dtype: Optional dtype of the convolutions and linear operations,\n                    either jnp.float32 (default) or jnp.float16 for mixed precision.\n        \"\"\"\n\n        super().__init__(*args, **kwargs)\n        self.stages = stages\n        self.block_type = block_type\n        self.lowres = lowres\n\n        if weights is not None:\n            if weights.endswith(\".pkl\"):\n                collections = pickle.load(open(weights, \"rb\"))\n            elif weights == \"imagenet\":\n                clsname = self.__class__.__name__\n                urldict = PRETRAINED_URLS.get(clsname, None)\n                if urldict is None:\n                    raise ValueError(f\"No pretrained weights for {clsname} available\")\n                fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n                collections = pickle.load(open(fname, \"rb\"))\n            else:\n                raise ValueError(\"Unknown weights value: \", weights)\n\n            if isinstance(collections, tuple):\n                parameters, collections = collections\n            elif \"parameters\" in collections:\n                parameters = collections.pop(\"parameters\")\n            else:\n                raise ValueError(\n                    \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n                )\n\n            x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n            # quick but dirty module initialization\n            jax.eval_shape(self.init(rng=types.RNGSeq(42)), x)\n\n            self.set_default_parameters(parameters, collections)\n\n    def call(self, x: jnp.ndarray):\n        x = nn.Conv2D(\n            64,\n            (7, 7) if not self.lowres else (3, 3),\n            stride=(2, 2) if not self.lowres else (1, 1),\n            padding=\"SAME\",\n            with_bias=False,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = module.to_module(jax.nn.relu)()(x)\n\n        if not self.lowres:\n            x = nn.MaxPool(\n                window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n            )(x)\n        for i, block_size in enumerate(self.stages):\n            for j in range(block_size):\n                strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n                x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n        GAP = lambda x: jnp.mean(x, axis=(1, 2))\n        x = module.to_module(GAP)(name=\"global_average_pooling\")(x)\n        x = nn.Linear(1000, dtype=self.dtype)(x)\n        to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n        x = module.to_module(to_float32)(name=\"to_float32\")(x)\n        return x\n\n\nclass ResNet18(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[2, 2, 2, 2],\n            block_type=ResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet34(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 4, 6, 3],\n            block_type=ResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet50(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 4, 6, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet101(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 4, 23, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet152(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 8, 36, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet200(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 24, 36, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n_resnet__init___docstring = \"\"\"\nInstantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n\nArguments:\n    lowres: Optional, whether to use the low resolution version\n            as described in subsection 4.2 of the orignal paper.\n            This version is better suited for datasets like CIFAR10. (Default: False)\n    weights: One of None (random initialization), 'imagenet' (automatic download of\n              weights pretrained on ImageNet) or a path to a weights file\n    dtype: Optional dtype of the convolutions and linear operations, \n           either jnp.float32 (default) or jnp.float16 for mixed precision.\n\"\"\"\n\nResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\nResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\nResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\nResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\nResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\nResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "source_code_len": 10761,
        "target_code": "# # adapted from the flax library https://github.com/google/flax\n\n# import pickle\n# import typing as tp\n\n# import jax\n# import jax.numpy as jnp\n# import numpy as np\n# from elegy import hooks, module, nn, types, utils\n\n# __all__ = [\n#     \"ResNet\",\n#     \"ResNet18\",\n#     \"ResNet34\",\n#     \"ResNet50\",\n#     \"ResNet101\",\n#     \"ResNet152\",\n#     \"ResNet200\",\n# ]\n\n\n# PRETRAINED_URLS = {\n#     \"ResNet18\": {\n#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n#         \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n#     },\n#     \"ResNet50\": {\n#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n#         \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n#     },\n# }\n\n\n# class ResNetBlock(module.Module):\n#     \"\"\"ResNet (identity) block\"\"\"\n\n#     def __init__(\n#         self,\n#         n_filters: int,\n#         strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(*args, **kwargs)\n#         self.n_filters = n_filters\n#         self.strides = strides\n\n#     def __call__(self, x: jnp.ndarray):\n#         x0 = x\n#         x = nn.Conv(\n#             self.n_filters,\n#             (3, 3),\n#             with_bias=False,\n#             stride=self.strides,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n\n#         x = nn.Conv(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n\n#         if x0.shape != x.shape:\n#             x0 = nn.Conv(\n#                 self.n_filters,\n#                 (1, 1),\n#                 with_bias=False,\n#                 stride=self.strides,\n#                 dtype=self.dtype,\n#             )(x0)\n#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n#         return jax.nn.relu(x0 + x)\n\n\n# class BottleneckResNetBlock(ResNetBlock):\n#     \"\"\"ResNet Bottleneck block.\"\"\"\n\n#     def __call__(self, x: jnp.ndarray):\n#         x0 = x\n#         x = nn.Conv(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n#         x = nn.Conv(\n#             self.n_filters,\n#             (3, 3),\n#             with_bias=False,\n#             stride=self.strides,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n#         x = nn.Conv(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n\n#         if x0.shape != x.shape:\n#             x0 = nn.Conv(\n#                 self.n_filters * 4,\n#                 (1, 1),\n#                 with_bias=False,\n#                 stride=self.strides,\n#                 dtype=self.dtype,\n#             )(x0)\n#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n#         return jax.nn.relu(x0 + x)\n\n\n# class ResNet(module.Module):\n#     \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n#     Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n#     \"\"\"\n\n#     __all__ = [\"__init__\", \"call\"]\n\n#     def __init__(\n#         self,\n#         stages: tp.List[int],\n#         block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         \"\"\"\n#         Arguments:\n#             stages: A list of integers representing the number of blocks in each stage.\n#                     e.g: [3, 4, 6, 3] for a ResNet50\n#             block_type: Which ResNet block type to use.\n#             lowres: Optional, whether to use the low resolution version\n#                     as described in subsection 4.2 of the orignal paper.\n#                     This version is better suited for datasets like CIFAR10. (Default: False)\n#             weights: One of None (random initialization) or a path to a weights file\n#             dtype: Optional dtype of the convolutions and linear operations,\n#                     either jnp.float32 (default) or jnp.float16 for mixed precision.\n#         \"\"\"\n\n#         super().__init__(*args, **kwargs)\n#         self.stages = stages\n#         self.block_type = block_type\n#         self.lowres = lowres\n\n#         if weights is not None:\n#             if weights.endswith(\".pkl\"):\n#                 collections = pickle.load(open(weights, \"rb\"))\n#             elif weights == \"imagenet\":\n#                 clsname = self.__class__.__name__\n#                 urldict = PRETRAINED_URLS.get(clsname, None)\n#                 if urldict is None:\n#                     raise ValueError(f\"No pretrained weights for {clsname} available\")\n#                 fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n#                 collections = pickle.load(open(fname, \"rb\"))\n#             else:\n#                 raise ValueError(\"Unknown weights value: \", weights)\n\n#             if isinstance(collections, tuple):\n#                 parameters, collections = collections\n#             elif \"parameters\" in collections:\n#                 parameters = collections.pop(\"parameters\")\n#             else:\n#                 raise ValueError(\n#                     \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n#                 )\n\n#             x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n#             # quick but dirty module initialization\n#             jax.eval_shape(self.init(rng=types.KeySeq(42)), x)\n\n#             self.set_default_parameters(parameters, collections)\n\n#     def __call__(self, x: jnp.ndarray):\n#         x = nn.Conv(\n#             64,\n#             (7, 7) if not self.lowres else (3, 3),\n#             stride=(2, 2) if not self.lowres else (1, 1),\n#             padding=\"SAME\",\n#             with_bias=False,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = module.compact_module(jax.nn.relu)()(x)\n\n#         if not self.lowres:\n#             x = nn.MaxPool(\n#                 window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n#             )(x)\n#         for i, block_size in enumerate(self.stages):\n#             for j in range(block_size):\n#                 strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n#                 x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n#         GAP = lambda x: jnp.mean(x, axis=(1, 2))\n#         x = module.compact_module(GAP)(name=\"global_average_pooling\")(x)\n#         x = nn.Linear(1000, dtype=self.dtype)(x)\n#         to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n#         x = module.compact_module(to_float32)(name=\"to_float32\")(x)\n#         return x\n\n\n# class ResNet18(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[2, 2, 2, 2],\n#             block_type=ResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet34(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 6, 3],\n#             block_type=ResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet50(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 6, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet101(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 23, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet152(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 8, 36, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet200(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 24, 36, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# _resnet__init___docstring = \"\"\"\n# Instantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n\n# Arguments:\n#     lowres: Optional, whether to use the low resolution version\n#             as described in subsection 4.2 of the orignal paper.\n#             This version is better suited for datasets like CIFAR10. (Default: False)\n#     weights: One of None (random initialization), 'imagenet' (automatic download of\n#               weights pretrained on ImageNet) or a path to a weights file\n#     dtype: Optional dtype of the convolutions and linear operations,\n#            either jnp.float32 (default) or jnp.float16 for mixed precision.\n# \"\"\"\n\n# ResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\n# ResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\n# ResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\n# ResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\n# ResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\n# ResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "target_code_len": 11287,
        "diff_format": "@@ -1,336 +1,336 @@\n-# adapted from the flax library https://github.com/google/flax\n-\n-import pickle\n-import typing as tp\n-\n-import jax\n-import jax.numpy as jnp\n-import numpy as np\n-from elegy import hooks, module, nn, types, utils\n-\n-__all__ = [\n-    \"ResNet\",\n-    \"ResNet18\",\n-    \"ResNet34\",\n-    \"ResNet50\",\n-    \"ResNet101\",\n-    \"ResNet152\",\n-    \"ResNet200\",\n-]\n-\n-\n-PRETRAINED_URLS = {\n-    \"ResNet18\": {\n-        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n-        \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n-    },\n-    \"ResNet50\": {\n-        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n-        \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n-    },\n-}\n-\n-\n-class ResNetBlock(module.Module):\n-    \"\"\"ResNet (identity) block\"\"\"\n-\n-    def __init__(\n-        self,\n-        n_filters: int,\n-        strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(*args, **kwargs)\n-        self.n_filters = n_filters\n-        self.strides = strides\n-\n-    def call(self, x: jnp.ndarray):\n-        x0 = x\n-        x = nn.Conv2D(\n-            self.n_filters,\n-            (3, 3),\n-            with_bias=False,\n-            stride=self.strides,\n-            dtype=self.dtype,\n-        )(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-        x = jax.nn.relu(x)\n-\n-        x = nn.Conv2D(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-\n-        if x0.shape != x.shape:\n-            x0 = nn.Conv2D(\n-                self.n_filters,\n-                (1, 1),\n-                with_bias=False,\n-                stride=self.strides,\n-                dtype=self.dtype,\n-            )(x0)\n-            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n-        return jax.nn.relu(x0 + x)\n-\n-\n-class BottleneckResNetBlock(ResNetBlock):\n-    \"\"\"ResNet Bottleneck block.\"\"\"\n-\n-    def call(self, x: jnp.ndarray):\n-        x0 = x\n-        x = nn.Conv2D(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-        x = jax.nn.relu(x)\n-        x = nn.Conv2D(\n-            self.n_filters,\n-            (3, 3),\n-            with_bias=False,\n-            stride=self.strides,\n-            dtype=self.dtype,\n-        )(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-        x = jax.nn.relu(x)\n-        x = nn.Conv2D(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n-\n-        if x0.shape != x.shape:\n-            x0 = nn.Conv2D(\n-                self.n_filters * 4,\n-                (1, 1),\n-                with_bias=False,\n-                stride=self.strides,\n-                dtype=self.dtype,\n-            )(x0)\n-            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n-        return jax.nn.relu(x0 + x)\n-\n-\n-class ResNet(module.Module):\n-    \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n-    Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n-    \"\"\"\n-\n-    __all__ = [\"__init__\", \"call\"]\n-\n-    def __init__(\n-        self,\n-        stages: tp.List[int],\n-        block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Arguments:\n-            stages: A list of integers representing the number of blocks in each stage.\n-                    e.g: [3, 4, 6, 3] for a ResNet50\n-            block_type: Which ResNet block type to use.\n-            lowres: Optional, whether to use the low resolution version\n-                    as described in subsection 4.2 of the orignal paper.\n-                    This version is better suited for datasets like CIFAR10. (Default: False)\n-            weights: One of None (random initialization) or a path to a weights file\n-            dtype: Optional dtype of the convolutions and linear operations,\n-                    either jnp.float32 (default) or jnp.float16 for mixed precision.\n-        \"\"\"\n-\n-        super().__init__(*args, **kwargs)\n-        self.stages = stages\n-        self.block_type = block_type\n-        self.lowres = lowres\n-\n-        if weights is not None:\n-            if weights.endswith(\".pkl\"):\n-                collections = pickle.load(open(weights, \"rb\"))\n-            elif weights == \"imagenet\":\n-                clsname = self.__class__.__name__\n-                urldict = PRETRAINED_URLS.get(clsname, None)\n-                if urldict is None:\n-                    raise ValueError(f\"No pretrained weights for {clsname} available\")\n-                fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n-                collections = pickle.load(open(fname, \"rb\"))\n-            else:\n-                raise ValueError(\"Unknown weights value: \", weights)\n-\n-            if isinstance(collections, tuple):\n-                parameters, collections = collections\n-            elif \"parameters\" in collections:\n-                parameters = collections.pop(\"parameters\")\n-            else:\n-                raise ValueError(\n-                    \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n-                )\n-\n-            x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n-            # quick but dirty module initialization\n-            jax.eval_shape(self.init(rng=types.RNGSeq(42)), x)\n-\n-            self.set_default_parameters(parameters, collections)\n-\n-    def call(self, x: jnp.ndarray):\n-        x = nn.Conv2D(\n-            64,\n-            (7, 7) if not self.lowres else (3, 3),\n-            stride=(2, 2) if not self.lowres else (1, 1),\n-            padding=\"SAME\",\n-            with_bias=False,\n-            dtype=self.dtype,\n-        )(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-        x = module.to_module(jax.nn.relu)()(x)\n-\n-        if not self.lowres:\n-            x = nn.MaxPool(\n-                window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n-            )(x)\n-        for i, block_size in enumerate(self.stages):\n-            for j in range(block_size):\n-                strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n-                x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n-        GAP = lambda x: jnp.mean(x, axis=(1, 2))\n-        x = module.to_module(GAP)(name=\"global_average_pooling\")(x)\n-        x = nn.Linear(1000, dtype=self.dtype)(x)\n-        to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n-        x = module.to_module(to_float32)(name=\"to_float32\")(x)\n-        return x\n-\n-\n-class ResNet18(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[2, 2, 2, 2],\n-            block_type=ResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet34(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 4, 6, 3],\n-            block_type=ResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet50(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 4, 6, 3],\n-            block_type=BottleneckResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet101(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 4, 23, 3],\n-            block_type=BottleneckResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet152(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 8, 36, 3],\n-            block_type=BottleneckResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet200(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 24, 36, 3],\n-            block_type=BottleneckResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-_resnet__init___docstring = \"\"\"\n-Instantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n-\n-Arguments:\n-    lowres: Optional, whether to use the low resolution version\n-            as described in subsection 4.2 of the orignal paper.\n-            This version is better suited for datasets like CIFAR10. (Default: False)\n-    weights: One of None (random initialization), 'imagenet' (automatic download of\n-              weights pretrained on ImageNet) or a path to a weights file\n-    dtype: Optional dtype of the convolutions and linear operations, \n-           either jnp.float32 (default) or jnp.float16 for mixed precision.\n-\"\"\"\n-\n-ResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\n-ResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\n-ResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\n-ResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\n-ResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\n-ResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n+# # adapted from the flax library https://github.com/google/flax\n+\n+# import pickle\n+# import typing as tp\n+\n+# import jax\n+# import jax.numpy as jnp\n+# import numpy as np\n+# from elegy import hooks, module, nn, types, utils\n+\n+# __all__ = [\n+#     \"ResNet\",\n+#     \"ResNet18\",\n+#     \"ResNet34\",\n+#     \"ResNet50\",\n+#     \"ResNet101\",\n+#     \"ResNet152\",\n+#     \"ResNet200\",\n+# ]\n+\n+\n+# PRETRAINED_URLS = {\n+#     \"ResNet18\": {\n+#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n+#         \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n+#     },\n+#     \"ResNet50\": {\n+#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n+#         \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n+#     },\n+# }\n+\n+\n+# class ResNetBlock(module.Module):\n+#     \"\"\"ResNet (identity) block\"\"\"\n+\n+#     def __init__(\n+#         self,\n+#         n_filters: int,\n+#         strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(*args, **kwargs)\n+#         self.n_filters = n_filters\n+#         self.strides = strides\n+\n+#     def __call__(self, x: jnp.ndarray):\n+#         x0 = x\n+#         x = nn.Conv(\n+#             self.n_filters,\n+#             (3, 3),\n+#             with_bias=False,\n+#             stride=self.strides,\n+#             dtype=self.dtype,\n+#         )(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+#         x = jax.nn.relu(x)\n+\n+#         x = nn.Conv(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+\n+#         if x0.shape != x.shape:\n+#             x0 = nn.Conv(\n+#                 self.n_filters,\n+#                 (1, 1),\n+#                 with_bias=False,\n+#                 stride=self.strides,\n+#                 dtype=self.dtype,\n+#             )(x0)\n+#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n+#         return jax.nn.relu(x0 + x)\n+\n+\n+# class BottleneckResNetBlock(ResNetBlock):\n+#     \"\"\"ResNet Bottleneck block.\"\"\"\n+\n+#     def __call__(self, x: jnp.ndarray):\n+#         x0 = x\n+#         x = nn.Conv(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+#         x = jax.nn.relu(x)\n+#         x = nn.Conv(\n+#             self.n_filters,\n+#             (3, 3),\n+#             with_bias=False,\n+#             stride=self.strides,\n+#             dtype=self.dtype,\n+#         )(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+#         x = jax.nn.relu(x)\n+#         x = nn.Conv(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n+\n+#         if x0.shape != x.shape:\n+#             x0 = nn.Conv(\n+#                 self.n_filters * 4,\n+#                 (1, 1),\n+#                 with_bias=False,\n+#                 stride=self.strides,\n+#                 dtype=self.dtype,\n+#             )(x0)\n+#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n+#         return jax.nn.relu(x0 + x)\n+\n+\n+# class ResNet(module.Module):\n+#     \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n+#     Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n+#     \"\"\"\n+\n+#     __all__ = [\"__init__\", \"call\"]\n+\n+#     def __init__(\n+#         self,\n+#         stages: tp.List[int],\n+#         block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         \"\"\"\n+#         Arguments:\n+#             stages: A list of integers representing the number of blocks in each stage.\n+#                     e.g: [3, 4, 6, 3] for a ResNet50\n+#             block_type: Which ResNet block type to use.\n+#             lowres: Optional, whether to use the low resolution version\n+#                     as described in subsection 4.2 of the orignal paper.\n+#                     This version is better suited for datasets like CIFAR10. (Default: False)\n+#             weights: One of None (random initialization) or a path to a weights file\n+#             dtype: Optional dtype of the convolutions and linear operations,\n+#                     either jnp.float32 (default) or jnp.float16 for mixed precision.\n+#         \"\"\"\n+\n+#         super().__init__(*args, **kwargs)\n+#         self.stages = stages\n+#         self.block_type = block_type\n+#         self.lowres = lowres\n+\n+#         if weights is not None:\n+#             if weights.endswith(\".pkl\"):\n+#                 collections = pickle.load(open(weights, \"rb\"))\n+#             elif weights == \"imagenet\":\n+#                 clsname = self.__class__.__name__\n+#                 urldict = PRETRAINED_URLS.get(clsname, None)\n+#                 if urldict is None:\n+#                     raise ValueError(f\"No pretrained weights for {clsname} available\")\n+#                 fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n+#                 collections = pickle.load(open(fname, \"rb\"))\n+#             else:\n+#                 raise ValueError(\"Unknown weights value: \", weights)\n+\n+#             if isinstance(collections, tuple):\n+#                 parameters, collections = collections\n+#             elif \"parameters\" in collections:\n+#                 parameters = collections.pop(\"parameters\")\n+#             else:\n+#                 raise ValueError(\n+#                     \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n+#                 )\n+\n+#             x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n+#             # quick but dirty module initialization\n+#             jax.eval_shape(self.init(rng=types.KeySeq(42)), x)\n+\n+#             self.set_default_parameters(parameters, collections)\n+\n+#     def __call__(self, x: jnp.ndarray):\n+#         x = nn.Conv(\n+#             64,\n+#             (7, 7) if not self.lowres else (3, 3),\n+#             stride=(2, 2) if not self.lowres else (1, 1),\n+#             padding=\"SAME\",\n+#             with_bias=False,\n+#             dtype=self.dtype,\n+#         )(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+#         x = module.compact_module(jax.nn.relu)()(x)\n+\n+#         if not self.lowres:\n+#             x = nn.MaxPool(\n+#                 window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n+#             )(x)\n+#         for i, block_size in enumerate(self.stages):\n+#             for j in range(block_size):\n+#                 strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n+#                 x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n+#         GAP = lambda x: jnp.mean(x, axis=(1, 2))\n+#         x = module.compact_module(GAP)(name=\"global_average_pooling\")(x)\n+#         x = nn.Linear(1000, dtype=self.dtype)(x)\n+#         to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n+#         x = module.compact_module(to_float32)(name=\"to_float32\")(x)\n+#         return x\n+\n+\n+# class ResNet18(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[2, 2, 2, 2],\n+#             block_type=ResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet34(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 4, 6, 3],\n+#             block_type=ResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet50(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 4, 6, 3],\n+#             block_type=BottleneckResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet101(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 4, 23, 3],\n+#             block_type=BottleneckResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet152(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 8, 36, 3],\n+#             block_type=BottleneckResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet200(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 24, 36, 3],\n+#             block_type=BottleneckResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# _resnet__init___docstring = \"\"\"\n+# Instantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n+\n+# Arguments:\n+#     lowres: Optional, whether to use the low resolution version\n+#             as described in subsection 4.2 of the orignal paper.\n+#             This version is better suited for datasets like CIFAR10. (Default: False)\n+#     weights: One of None (random initialization), 'imagenet' (automatic download of\n+#               weights pretrained on ImageNet) or a path to a weights file\n+#     dtype: Optional dtype of the convolutions and linear operations,\n+#            either jnp.float32 (default) or jnp.float16 for mixed precision.\n+# \"\"\"\n+\n+# ResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\n+# ResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\n+# ResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\n+# ResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\n+# ResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\n+# ResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "source_code_with_indent": "# adapted from the flax library https://github.com/google/flax\n\nimport pickle\nimport typing as tp\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom elegy import hooks, module, nn, types, utils\n\n__all__ = [\n    \"ResNet\",\n    \"ResNet18\",\n    \"ResNet34\",\n    \"ResNet50\",\n    \"ResNet101\",\n    \"ResNet152\",\n    \"ResNet200\",\n]\n\n\nPRETRAINED_URLS = {\n    \"ResNet18\": {\n        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n        \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n    },\n    \"ResNet50\": {\n        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n        \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n    },\n}\n\n\nclass ResNetBlock(module.Module):\n    <IND>\"\"\"ResNet (identity) block\"\"\"\n\n    def __init__(\n        self,\n        n_filters: int,\n        strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(*args, **kwargs)\n        self.n_filters = n_filters\n        self.strides = strides\n\n    <DED>def call(self, x: jnp.ndarray):\n        <IND>x0 = x\n        x = nn.Conv2D(\n            self.n_filters,\n            (3, 3),\n            with_bias=False,\n            stride=self.strides,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n\n        x = nn.Conv2D(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n\n        if x0.shape != x.shape:\n            <IND>x0 = nn.Conv2D(\n                self.n_filters,\n                (1, 1),\n                with_bias=False,\n                stride=self.strides,\n                dtype=self.dtype,\n            )(x0)\n            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n        <DED>return jax.nn.relu(x0 + x)\n\n\n<DED><DED>class BottleneckResNetBlock(ResNetBlock):\n    <IND>\"\"\"ResNet Bottleneck block.\"\"\"\n\n    def call(self, x: jnp.ndarray):\n        <IND>x0 = x\n        x = nn.Conv2D(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n        x = nn.Conv2D(\n            self.n_filters,\n            (3, 3),\n            with_bias=False,\n            stride=self.strides,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n        x = nn.Conv2D(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n\n        if x0.shape != x.shape:\n            <IND>x0 = nn.Conv2D(\n                self.n_filters * 4,\n                (1, 1),\n                with_bias=False,\n                stride=self.strides,\n                dtype=self.dtype,\n            )(x0)\n            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n        <DED>return jax.nn.relu(x0 + x)\n\n\n<DED><DED>class ResNet(module.Module):\n    <IND>\"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n    Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n    \"\"\"\n\n    __all__ = [\"__init__\", \"call\"]\n\n    def __init__(\n        self,\n        stages: tp.List[int],\n        block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>\"\"\"\n        Arguments:\n            stages: A list of integers representing the number of blocks in each stage.\n                    e.g: [3, 4, 6, 3] for a ResNet50\n            block_type: Which ResNet block type to use.\n            lowres: Optional, whether to use the low resolution version\n                    as described in subsection 4.2 of the orignal paper.\n                    This version is better suited for datasets like CIFAR10. (Default: False)\n            weights: One of None (random initialization) or a path to a weights file\n            dtype: Optional dtype of the convolutions and linear operations,\n                    either jnp.float32 (default) or jnp.float16 for mixed precision.\n        \"\"\"\n\n        super().__init__(*args, **kwargs)\n        self.stages = stages\n        self.block_type = block_type\n        self.lowres = lowres\n\n        if weights is not None:\n            <IND>if weights.endswith(\".pkl\"):\n                <IND>collections = pickle.load(open(weights, \"rb\"))\n            <DED>elif weights == \"imagenet\":\n                <IND>clsname = self.__class__.__name__\n                urldict = PRETRAINED_URLS.get(clsname, None)\n                if urldict is None:\n                    <IND>raise ValueError(f\"No pretrained weights for {clsname} available\")\n                <DED>fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n                collections = pickle.load(open(fname, \"rb\"))\n            <DED>else:\n                <IND>raise ValueError(\"Unknown weights value: \", weights)\n\n            <DED>if isinstance(collections, tuple):\n                <IND>parameters, collections = collections\n            <DED>elif \"parameters\" in collections:\n                <IND>parameters = collections.pop(\"parameters\")\n            <DED>else:\n                <IND>raise ValueError(\n                    \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n                )\n\n            <DED>x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n            # quick but dirty module initialization\n            jax.eval_shape(self.init(rng=types.RNGSeq(42)), x)\n\n            self.set_default_parameters(parameters, collections)\n\n    <DED><DED>def call(self, x: jnp.ndarray):\n        <IND>x = nn.Conv2D(\n            64,\n            (7, 7) if not self.lowres else (3, 3),\n            stride=(2, 2) if not self.lowres else (1, 1),\n            padding=\"SAME\",\n            with_bias=False,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = module.to_module(jax.nn.relu)()(x)\n\n        if not self.lowres:\n            <IND>x = nn.MaxPool(\n                window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n            )(x)\n        <DED>for i, block_size in enumerate(self.stages):\n            <IND>for j in range(block_size):\n                <IND>strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n                x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n        <DED><DED>GAP = lambda x: jnp.mean(x, axis=(1, 2))\n        x = module.to_module(GAP)(name=\"global_average_pooling\")(x)\n        x = nn.Linear(1000, dtype=self.dtype)(x)\n        to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n        x = module.to_module(to_float32)(name=\"to_float32\")(x)\n        return x\n\n\n<DED><DED>class ResNet18(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[2, 2, 2, 2],\n            block_type=ResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet34(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 4, 6, 3],\n            block_type=ResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet50(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 4, 6, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet101(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 4, 23, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet152(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 8, 36, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet200(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 24, 36, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>_resnet__init___docstring = \"\"\"\nInstantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n\nArguments:\n    lowres: Optional, whether to use the low resolution version\n            as described in subsection 4.2 of the orignal paper.\n            This version is better suited for datasets like CIFAR10. (Default: False)\n    weights: One of None (random initialization), 'imagenet' (automatic download of\n              weights pretrained on ImageNet) or a path to a weights file\n    dtype: Optional dtype of the convolutions and linear operations, \n           either jnp.float32 (default) or jnp.float16 for mixed precision.\n\"\"\"\n\nResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\nResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\nResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\nResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\nResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\nResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "# # adapted from the flax library https://github.com/google/flax\n\n# import pickle\n# import typing as tp\n\n# import jax\n# import jax.numpy as jnp\n# import numpy as np\n# from elegy import hooks, module, nn, types, utils\n\n# __all__ = [\n#     \"ResNet\",\n#     \"ResNet18\",\n#     \"ResNet34\",\n#     \"ResNet50\",\n#     \"ResNet101\",\n#     \"ResNet152\",\n#     \"ResNet200\",\n# ]\n\n\n# PRETRAINED_URLS = {\n#     \"ResNet18\": {\n#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n#         \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n#     },\n#     \"ResNet50\": {\n#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n#         \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n#     },\n# }\n\n\n# class ResNetBlock(module.Module):\n#     \"\"\"ResNet (identity) block\"\"\"\n\n#     def __init__(\n#         self,\n#         n_filters: int,\n#         strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(*args, **kwargs)\n#         self.n_filters = n_filters\n#         self.strides = strides\n\n#     def __call__(self, x: jnp.ndarray):\n#         x0 = x\n#         x = nn.Conv(\n#             self.n_filters,\n#             (3, 3),\n#             with_bias=False,\n#             stride=self.strides,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n\n#         x = nn.Conv(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n\n#         if x0.shape != x.shape:\n#             x0 = nn.Conv(\n#                 self.n_filters,\n#                 (1, 1),\n#                 with_bias=False,\n#                 stride=self.strides,\n#                 dtype=self.dtype,\n#             )(x0)\n#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n#         return jax.nn.relu(x0 + x)\n\n\n# class BottleneckResNetBlock(ResNetBlock):\n#     \"\"\"ResNet Bottleneck block.\"\"\"\n\n#     def __call__(self, x: jnp.ndarray):\n#         x0 = x\n#         x = nn.Conv(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n#         x = nn.Conv(\n#             self.n_filters,\n#             (3, 3),\n#             with_bias=False,\n#             stride=self.strides,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n#         x = nn.Conv(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n\n#         if x0.shape != x.shape:\n#             x0 = nn.Conv(\n#                 self.n_filters * 4,\n#                 (1, 1),\n#                 with_bias=False,\n#                 stride=self.strides,\n#                 dtype=self.dtype,\n#             )(x0)\n#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n#         return jax.nn.relu(x0 + x)\n\n\n# class ResNet(module.Module):\n#     \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n#     Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n#     \"\"\"\n\n#     __all__ = [\"__init__\", \"call\"]\n\n#     def __init__(\n#         self,\n#         stages: tp.List[int],\n#         block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         \"\"\"\n#         Arguments:\n#             stages: A list of integers representing the number of blocks in each stage.\n#                     e.g: [3, 4, 6, 3] for a ResNet50\n#             block_type: Which ResNet block type to use.\n#             lowres: Optional, whether to use the low resolution version\n#                     as described in subsection 4.2 of the orignal paper.\n#                     This version is better suited for datasets like CIFAR10. (Default: False)\n#             weights: One of None (random initialization) or a path to a weights file\n#             dtype: Optional dtype of the convolutions and linear operations,\n#                     either jnp.float32 (default) or jnp.float16 for mixed precision.\n#         \"\"\"\n\n#         super().__init__(*args, **kwargs)\n#         self.stages = stages\n#         self.block_type = block_type\n#         self.lowres = lowres\n\n#         if weights is not None:\n#             if weights.endswith(\".pkl\"):\n#                 collections = pickle.load(open(weights, \"rb\"))\n#             elif weights == \"imagenet\":\n#                 clsname = self.__class__.__name__\n#                 urldict = PRETRAINED_URLS.get(clsname, None)\n#                 if urldict is None:\n#                     raise ValueError(f\"No pretrained weights for {clsname} available\")\n#                 fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n#                 collections = pickle.load(open(fname, \"rb\"))\n#             else:\n#                 raise ValueError(\"Unknown weights value: \", weights)\n\n#             if isinstance(collections, tuple):\n#                 parameters, collections = collections\n#             elif \"parameters\" in collections:\n#                 parameters = collections.pop(\"parameters\")\n#             else:\n#                 raise ValueError(\n#                     \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n#                 )\n\n#             x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n#             # quick but dirty module initialization\n#             jax.eval_shape(self.init(rng=types.KeySeq(42)), x)\n\n#             self.set_default_parameters(parameters, collections)\n\n#     def __call__(self, x: jnp.ndarray):\n#         x = nn.Conv(\n#             64,\n#             (7, 7) if not self.lowres else (3, 3),\n#             stride=(2, 2) if not self.lowres else (1, 1),\n#             padding=\"SAME\",\n#             with_bias=False,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = module.compact_module(jax.nn.relu)()(x)\n\n#         if not self.lowres:\n#             x = nn.MaxPool(\n#                 window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n#             )(x)\n#         for i, block_size in enumerate(self.stages):\n#             for j in range(block_size):\n#                 strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n#                 x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n#         GAP = lambda x: jnp.mean(x, axis=(1, 2))\n#         x = module.compact_module(GAP)(name=\"global_average_pooling\")(x)\n#         x = nn.Linear(1000, dtype=self.dtype)(x)\n#         to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n#         x = module.compact_module(to_float32)(name=\"to_float32\")(x)\n#         return x\n\n\n# class ResNet18(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[2, 2, 2, 2],\n#             block_type=ResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet34(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 6, 3],\n#             block_type=ResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet50(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 6, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet101(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 23, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet152(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 8, 36, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet200(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 24, 36, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# _resnet__init___docstring = \"\"\"\n# Instantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n\n# Arguments:\n#     lowres: Optional, whether to use the low resolution version\n#             as described in subsection 4.2 of the orignal paper.\n#             This version is better suited for datasets like CIFAR10. (Default: False)\n#     weights: One of None (random initialization), 'imagenet' (automatic download of\n#               weights pretrained on ImageNet) or a path to a weights file\n#     dtype: Optional dtype of the convolutions and linear operations,\n#            either jnp.float32 (default) or jnp.float16 for mixed precision.\n# \"\"\"\n\n# ResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\n# ResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\n# ResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\n# ResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\n# ResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\n# ResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nets/resnet.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nets/resnet.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "elegy/nets/resnet.py:170:4 Inconsistent override [14]: `elegy.nets.resnet.ResNet.call` overrides method defined in `module.Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "message": " `elegy.nets.resnet.ResNet.call` overrides method defined in `module.Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 170,
    "warning_line": "    def call(self, x: jnp.ndarray):",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "# adapted from the flax library https://github.com/google/flax\n\nimport pickle\nimport typing as tp\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom elegy import hooks, module, nn, types, utils\n\n__all__ = [\n    \"ResNet\",\n    \"ResNet18\",\n    \"ResNet34\",\n    \"ResNet50\",\n    \"ResNet101\",\n    \"ResNet152\",\n    \"ResNet200\",\n]\n\n\nPRETRAINED_URLS = {\n    \"ResNet18\": {\n        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n        \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n    },\n    \"ResNet50\": {\n        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n        \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n    },\n}\n\n\nclass ResNetBlock(module.Module):\n    \"\"\"ResNet (identity) block\"\"\"\n\n    def __init__(\n        self,\n        n_filters: int,\n        strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n        *args,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.n_filters = n_filters\n        self.strides = strides\n\n    def call(self, x: jnp.ndarray):\n        x0 = x\n        x = nn.Conv2D(\n            self.n_filters,\n            (3, 3),\n            with_bias=False,\n            stride=self.strides,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n\n        x = nn.Conv2D(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n\n        if x0.shape != x.shape:\n            x0 = nn.Conv2D(\n                self.n_filters,\n                (1, 1),\n                with_bias=False,\n                stride=self.strides,\n                dtype=self.dtype,\n            )(x0)\n            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n        return jax.nn.relu(x0 + x)\n\n\nclass BottleneckResNetBlock(ResNetBlock):\n    \"\"\"ResNet Bottleneck block.\"\"\"\n\n    def call(self, x: jnp.ndarray):\n        x0 = x\n        x = nn.Conv2D(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n        x = nn.Conv2D(\n            self.n_filters,\n            (3, 3),\n            with_bias=False,\n            stride=self.strides,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n        x = nn.Conv2D(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n\n        if x0.shape != x.shape:\n            x0 = nn.Conv2D(\n                self.n_filters * 4,\n                (1, 1),\n                with_bias=False,\n                stride=self.strides,\n                dtype=self.dtype,\n            )(x0)\n            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n        return jax.nn.relu(x0 + x)\n\n\nclass ResNet(module.Module):\n    \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n    Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n    \"\"\"\n\n    __all__ = [\"__init__\", \"call\"]\n\n    def __init__(\n        self,\n        stages: tp.List[int],\n        block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Arguments:\n            stages: A list of integers representing the number of blocks in each stage.\n                    e.g: [3, 4, 6, 3] for a ResNet50\n            block_type: Which ResNet block type to use.\n            lowres: Optional, whether to use the low resolution version\n                    as described in subsection 4.2 of the orignal paper.\n                    This version is better suited for datasets like CIFAR10. (Default: False)\n            weights: One of None (random initialization) or a path to a weights file\n            dtype: Optional dtype of the convolutions and linear operations,\n                    either jnp.float32 (default) or jnp.float16 for mixed precision.\n        \"\"\"\n\n        super().__init__(*args, **kwargs)\n        self.stages = stages\n        self.block_type = block_type\n        self.lowres = lowres\n\n        if weights is not None:\n            if weights.endswith(\".pkl\"):\n                collections = pickle.load(open(weights, \"rb\"))\n            elif weights == \"imagenet\":\n                clsname = self.__class__.__name__\n                urldict = PRETRAINED_URLS.get(clsname, None)\n                if urldict is None:\n                    raise ValueError(f\"No pretrained weights for {clsname} available\")\n                fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n                collections = pickle.load(open(fname, \"rb\"))\n            else:\n                raise ValueError(\"Unknown weights value: \", weights)\n\n            if isinstance(collections, tuple):\n                parameters, collections = collections\n            elif \"parameters\" in collections:\n                parameters = collections.pop(\"parameters\")\n            else:\n                raise ValueError(\n                    \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n                )\n\n            x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n            # quick but dirty module initialization\n            jax.eval_shape(self.init(rng=types.RNGSeq(42)), x)\n\n            self.set_default_parameters(parameters, collections)\n\n    def call(self, x: jnp.ndarray):\n        x = nn.Conv2D(\n            64,\n            (7, 7) if not self.lowres else (3, 3),\n            stride=(2, 2) if not self.lowres else (1, 1),\n            padding=\"SAME\",\n            with_bias=False,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = module.to_module(jax.nn.relu)()(x)\n\n        if not self.lowres:\n            x = nn.MaxPool(\n                window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n            )(x)\n        for i, block_size in enumerate(self.stages):\n            for j in range(block_size):\n                strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n                x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n        GAP = lambda x: jnp.mean(x, axis=(1, 2))\n        x = module.to_module(GAP)(name=\"global_average_pooling\")(x)\n        x = nn.Linear(1000, dtype=self.dtype)(x)\n        to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n        x = module.to_module(to_float32)(name=\"to_float32\")(x)\n        return x\n\n\nclass ResNet18(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[2, 2, 2, 2],\n            block_type=ResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet34(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 4, 6, 3],\n            block_type=ResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet50(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 4, 6, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet101(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 4, 23, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet152(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 8, 36, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet200(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 24, 36, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n_resnet__init___docstring = \"\"\"\nInstantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n\nArguments:\n    lowres: Optional, whether to use the low resolution version\n            as described in subsection 4.2 of the orignal paper.\n            This version is better suited for datasets like CIFAR10. (Default: False)\n    weights: One of None (random initialization), 'imagenet' (automatic download of\n              weights pretrained on ImageNet) or a path to a weights file\n    dtype: Optional dtype of the convolutions and linear operations, \n           either jnp.float32 (default) or jnp.float16 for mixed precision.\n\"\"\"\n\nResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\nResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\nResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\nResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\nResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\nResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "source_code_len": 10761,
        "target_code": "# # adapted from the flax library https://github.com/google/flax\n\n# import pickle\n# import typing as tp\n\n# import jax\n# import jax.numpy as jnp\n# import numpy as np\n# from elegy import hooks, module, nn, types, utils\n\n# __all__ = [\n#     \"ResNet\",\n#     \"ResNet18\",\n#     \"ResNet34\",\n#     \"ResNet50\",\n#     \"ResNet101\",\n#     \"ResNet152\",\n#     \"ResNet200\",\n# ]\n\n\n# PRETRAINED_URLS = {\n#     \"ResNet18\": {\n#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n#         \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n#     },\n#     \"ResNet50\": {\n#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n#         \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n#     },\n# }\n\n\n# class ResNetBlock(module.Module):\n#     \"\"\"ResNet (identity) block\"\"\"\n\n#     def __init__(\n#         self,\n#         n_filters: int,\n#         strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(*args, **kwargs)\n#         self.n_filters = n_filters\n#         self.strides = strides\n\n#     def __call__(self, x: jnp.ndarray):\n#         x0 = x\n#         x = nn.Conv(\n#             self.n_filters,\n#             (3, 3),\n#             with_bias=False,\n#             stride=self.strides,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n\n#         x = nn.Conv(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n\n#         if x0.shape != x.shape:\n#             x0 = nn.Conv(\n#                 self.n_filters,\n#                 (1, 1),\n#                 with_bias=False,\n#                 stride=self.strides,\n#                 dtype=self.dtype,\n#             )(x0)\n#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n#         return jax.nn.relu(x0 + x)\n\n\n# class BottleneckResNetBlock(ResNetBlock):\n#     \"\"\"ResNet Bottleneck block.\"\"\"\n\n#     def __call__(self, x: jnp.ndarray):\n#         x0 = x\n#         x = nn.Conv(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n#         x = nn.Conv(\n#             self.n_filters,\n#             (3, 3),\n#             with_bias=False,\n#             stride=self.strides,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n#         x = nn.Conv(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n\n#         if x0.shape != x.shape:\n#             x0 = nn.Conv(\n#                 self.n_filters * 4,\n#                 (1, 1),\n#                 with_bias=False,\n#                 stride=self.strides,\n#                 dtype=self.dtype,\n#             )(x0)\n#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n#         return jax.nn.relu(x0 + x)\n\n\n# class ResNet(module.Module):\n#     \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n#     Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n#     \"\"\"\n\n#     __all__ = [\"__init__\", \"call\"]\n\n#     def __init__(\n#         self,\n#         stages: tp.List[int],\n#         block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         \"\"\"\n#         Arguments:\n#             stages: A list of integers representing the number of blocks in each stage.\n#                     e.g: [3, 4, 6, 3] for a ResNet50\n#             block_type: Which ResNet block type to use.\n#             lowres: Optional, whether to use the low resolution version\n#                     as described in subsection 4.2 of the orignal paper.\n#                     This version is better suited for datasets like CIFAR10. (Default: False)\n#             weights: One of None (random initialization) or a path to a weights file\n#             dtype: Optional dtype of the convolutions and linear operations,\n#                     either jnp.float32 (default) or jnp.float16 for mixed precision.\n#         \"\"\"\n\n#         super().__init__(*args, **kwargs)\n#         self.stages = stages\n#         self.block_type = block_type\n#         self.lowres = lowres\n\n#         if weights is not None:\n#             if weights.endswith(\".pkl\"):\n#                 collections = pickle.load(open(weights, \"rb\"))\n#             elif weights == \"imagenet\":\n#                 clsname = self.__class__.__name__\n#                 urldict = PRETRAINED_URLS.get(clsname, None)\n#                 if urldict is None:\n#                     raise ValueError(f\"No pretrained weights for {clsname} available\")\n#                 fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n#                 collections = pickle.load(open(fname, \"rb\"))\n#             else:\n#                 raise ValueError(\"Unknown weights value: \", weights)\n\n#             if isinstance(collections, tuple):\n#                 parameters, collections = collections\n#             elif \"parameters\" in collections:\n#                 parameters = collections.pop(\"parameters\")\n#             else:\n#                 raise ValueError(\n#                     \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n#                 )\n\n#             x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n#             # quick but dirty module initialization\n#             jax.eval_shape(self.init(rng=types.KeySeq(42)), x)\n\n#             self.set_default_parameters(parameters, collections)\n\n#     def __call__(self, x: jnp.ndarray):\n#         x = nn.Conv(\n#             64,\n#             (7, 7) if not self.lowres else (3, 3),\n#             stride=(2, 2) if not self.lowres else (1, 1),\n#             padding=\"SAME\",\n#             with_bias=False,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = module.compact_module(jax.nn.relu)()(x)\n\n#         if not self.lowres:\n#             x = nn.MaxPool(\n#                 window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n#             )(x)\n#         for i, block_size in enumerate(self.stages):\n#             for j in range(block_size):\n#                 strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n#                 x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n#         GAP = lambda x: jnp.mean(x, axis=(1, 2))\n#         x = module.compact_module(GAP)(name=\"global_average_pooling\")(x)\n#         x = nn.Linear(1000, dtype=self.dtype)(x)\n#         to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n#         x = module.compact_module(to_float32)(name=\"to_float32\")(x)\n#         return x\n\n\n# class ResNet18(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[2, 2, 2, 2],\n#             block_type=ResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet34(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 6, 3],\n#             block_type=ResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet50(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 6, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet101(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 23, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet152(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 8, 36, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet200(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 24, 36, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# _resnet__init___docstring = \"\"\"\n# Instantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n\n# Arguments:\n#     lowres: Optional, whether to use the low resolution version\n#             as described in subsection 4.2 of the orignal paper.\n#             This version is better suited for datasets like CIFAR10. (Default: False)\n#     weights: One of None (random initialization), 'imagenet' (automatic download of\n#               weights pretrained on ImageNet) or a path to a weights file\n#     dtype: Optional dtype of the convolutions and linear operations,\n#            either jnp.float32 (default) or jnp.float16 for mixed precision.\n# \"\"\"\n\n# ResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\n# ResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\n# ResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\n# ResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\n# ResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\n# ResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "target_code_len": 11287,
        "diff_format": "@@ -1,336 +1,336 @@\n-# adapted from the flax library https://github.com/google/flax\n-\n-import pickle\n-import typing as tp\n-\n-import jax\n-import jax.numpy as jnp\n-import numpy as np\n-from elegy import hooks, module, nn, types, utils\n-\n-__all__ = [\n-    \"ResNet\",\n-    \"ResNet18\",\n-    \"ResNet34\",\n-    \"ResNet50\",\n-    \"ResNet101\",\n-    \"ResNet152\",\n-    \"ResNet200\",\n-]\n-\n-\n-PRETRAINED_URLS = {\n-    \"ResNet18\": {\n-        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n-        \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n-    },\n-    \"ResNet50\": {\n-        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n-        \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n-    },\n-}\n-\n-\n-class ResNetBlock(module.Module):\n-    \"\"\"ResNet (identity) block\"\"\"\n-\n-    def __init__(\n-        self,\n-        n_filters: int,\n-        strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(*args, **kwargs)\n-        self.n_filters = n_filters\n-        self.strides = strides\n-\n-    def call(self, x: jnp.ndarray):\n-        x0 = x\n-        x = nn.Conv2D(\n-            self.n_filters,\n-            (3, 3),\n-            with_bias=False,\n-            stride=self.strides,\n-            dtype=self.dtype,\n-        )(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-        x = jax.nn.relu(x)\n-\n-        x = nn.Conv2D(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-\n-        if x0.shape != x.shape:\n-            x0 = nn.Conv2D(\n-                self.n_filters,\n-                (1, 1),\n-                with_bias=False,\n-                stride=self.strides,\n-                dtype=self.dtype,\n-            )(x0)\n-            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n-        return jax.nn.relu(x0 + x)\n-\n-\n-class BottleneckResNetBlock(ResNetBlock):\n-    \"\"\"ResNet Bottleneck block.\"\"\"\n-\n-    def call(self, x: jnp.ndarray):\n-        x0 = x\n-        x = nn.Conv2D(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-        x = jax.nn.relu(x)\n-        x = nn.Conv2D(\n-            self.n_filters,\n-            (3, 3),\n-            with_bias=False,\n-            stride=self.strides,\n-            dtype=self.dtype,\n-        )(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-        x = jax.nn.relu(x)\n-        x = nn.Conv2D(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n-\n-        if x0.shape != x.shape:\n-            x0 = nn.Conv2D(\n-                self.n_filters * 4,\n-                (1, 1),\n-                with_bias=False,\n-                stride=self.strides,\n-                dtype=self.dtype,\n-            )(x0)\n-            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n-        return jax.nn.relu(x0 + x)\n-\n-\n-class ResNet(module.Module):\n-    \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n-    Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n-    \"\"\"\n-\n-    __all__ = [\"__init__\", \"call\"]\n-\n-    def __init__(\n-        self,\n-        stages: tp.List[int],\n-        block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Arguments:\n-            stages: A list of integers representing the number of blocks in each stage.\n-                    e.g: [3, 4, 6, 3] for a ResNet50\n-            block_type: Which ResNet block type to use.\n-            lowres: Optional, whether to use the low resolution version\n-                    as described in subsection 4.2 of the orignal paper.\n-                    This version is better suited for datasets like CIFAR10. (Default: False)\n-            weights: One of None (random initialization) or a path to a weights file\n-            dtype: Optional dtype of the convolutions and linear operations,\n-                    either jnp.float32 (default) or jnp.float16 for mixed precision.\n-        \"\"\"\n-\n-        super().__init__(*args, **kwargs)\n-        self.stages = stages\n-        self.block_type = block_type\n-        self.lowres = lowres\n-\n-        if weights is not None:\n-            if weights.endswith(\".pkl\"):\n-                collections = pickle.load(open(weights, \"rb\"))\n-            elif weights == \"imagenet\":\n-                clsname = self.__class__.__name__\n-                urldict = PRETRAINED_URLS.get(clsname, None)\n-                if urldict is None:\n-                    raise ValueError(f\"No pretrained weights for {clsname} available\")\n-                fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n-                collections = pickle.load(open(fname, \"rb\"))\n-            else:\n-                raise ValueError(\"Unknown weights value: \", weights)\n-\n-            if isinstance(collections, tuple):\n-                parameters, collections = collections\n-            elif \"parameters\" in collections:\n-                parameters = collections.pop(\"parameters\")\n-            else:\n-                raise ValueError(\n-                    \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n-                )\n-\n-            x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n-            # quick but dirty module initialization\n-            jax.eval_shape(self.init(rng=types.RNGSeq(42)), x)\n-\n-            self.set_default_parameters(parameters, collections)\n-\n-    def call(self, x: jnp.ndarray):\n-        x = nn.Conv2D(\n-            64,\n-            (7, 7) if not self.lowres else (3, 3),\n-            stride=(2, 2) if not self.lowres else (1, 1),\n-            padding=\"SAME\",\n-            with_bias=False,\n-            dtype=self.dtype,\n-        )(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-        x = module.to_module(jax.nn.relu)()(x)\n-\n-        if not self.lowres:\n-            x = nn.MaxPool(\n-                window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n-            )(x)\n-        for i, block_size in enumerate(self.stages):\n-            for j in range(block_size):\n-                strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n-                x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n-        GAP = lambda x: jnp.mean(x, axis=(1, 2))\n-        x = module.to_module(GAP)(name=\"global_average_pooling\")(x)\n-        x = nn.Linear(1000, dtype=self.dtype)(x)\n-        to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n-        x = module.to_module(to_float32)(name=\"to_float32\")(x)\n-        return x\n-\n-\n-class ResNet18(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[2, 2, 2, 2],\n-            block_type=ResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet34(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 4, 6, 3],\n-            block_type=ResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet50(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 4, 6, 3],\n-            block_type=BottleneckResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet101(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 4, 23, 3],\n-            block_type=BottleneckResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet152(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 8, 36, 3],\n-            block_type=BottleneckResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet200(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 24, 36, 3],\n-            block_type=BottleneckResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-_resnet__init___docstring = \"\"\"\n-Instantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n-\n-Arguments:\n-    lowres: Optional, whether to use the low resolution version\n-            as described in subsection 4.2 of the orignal paper.\n-            This version is better suited for datasets like CIFAR10. (Default: False)\n-    weights: One of None (random initialization), 'imagenet' (automatic download of\n-              weights pretrained on ImageNet) or a path to a weights file\n-    dtype: Optional dtype of the convolutions and linear operations, \n-           either jnp.float32 (default) or jnp.float16 for mixed precision.\n-\"\"\"\n-\n-ResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\n-ResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\n-ResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\n-ResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\n-ResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\n-ResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n+# # adapted from the flax library https://github.com/google/flax\n+\n+# import pickle\n+# import typing as tp\n+\n+# import jax\n+# import jax.numpy as jnp\n+# import numpy as np\n+# from elegy import hooks, module, nn, types, utils\n+\n+# __all__ = [\n+#     \"ResNet\",\n+#     \"ResNet18\",\n+#     \"ResNet34\",\n+#     \"ResNet50\",\n+#     \"ResNet101\",\n+#     \"ResNet152\",\n+#     \"ResNet200\",\n+# ]\n+\n+\n+# PRETRAINED_URLS = {\n+#     \"ResNet18\": {\n+#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n+#         \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n+#     },\n+#     \"ResNet50\": {\n+#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n+#         \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n+#     },\n+# }\n+\n+\n+# class ResNetBlock(module.Module):\n+#     \"\"\"ResNet (identity) block\"\"\"\n+\n+#     def __init__(\n+#         self,\n+#         n_filters: int,\n+#         strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(*args, **kwargs)\n+#         self.n_filters = n_filters\n+#         self.strides = strides\n+\n+#     def __call__(self, x: jnp.ndarray):\n+#         x0 = x\n+#         x = nn.Conv(\n+#             self.n_filters,\n+#             (3, 3),\n+#             with_bias=False,\n+#             stride=self.strides,\n+#             dtype=self.dtype,\n+#         )(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+#         x = jax.nn.relu(x)\n+\n+#         x = nn.Conv(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+\n+#         if x0.shape != x.shape:\n+#             x0 = nn.Conv(\n+#                 self.n_filters,\n+#                 (1, 1),\n+#                 with_bias=False,\n+#                 stride=self.strides,\n+#                 dtype=self.dtype,\n+#             )(x0)\n+#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n+#         return jax.nn.relu(x0 + x)\n+\n+\n+# class BottleneckResNetBlock(ResNetBlock):\n+#     \"\"\"ResNet Bottleneck block.\"\"\"\n+\n+#     def __call__(self, x: jnp.ndarray):\n+#         x0 = x\n+#         x = nn.Conv(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+#         x = jax.nn.relu(x)\n+#         x = nn.Conv(\n+#             self.n_filters,\n+#             (3, 3),\n+#             with_bias=False,\n+#             stride=self.strides,\n+#             dtype=self.dtype,\n+#         )(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+#         x = jax.nn.relu(x)\n+#         x = nn.Conv(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n+\n+#         if x0.shape != x.shape:\n+#             x0 = nn.Conv(\n+#                 self.n_filters * 4,\n+#                 (1, 1),\n+#                 with_bias=False,\n+#                 stride=self.strides,\n+#                 dtype=self.dtype,\n+#             )(x0)\n+#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n+#         return jax.nn.relu(x0 + x)\n+\n+\n+# class ResNet(module.Module):\n+#     \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n+#     Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n+#     \"\"\"\n+\n+#     __all__ = [\"__init__\", \"call\"]\n+\n+#     def __init__(\n+#         self,\n+#         stages: tp.List[int],\n+#         block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         \"\"\"\n+#         Arguments:\n+#             stages: A list of integers representing the number of blocks in each stage.\n+#                     e.g: [3, 4, 6, 3] for a ResNet50\n+#             block_type: Which ResNet block type to use.\n+#             lowres: Optional, whether to use the low resolution version\n+#                     as described in subsection 4.2 of the orignal paper.\n+#                     This version is better suited for datasets like CIFAR10. (Default: False)\n+#             weights: One of None (random initialization) or a path to a weights file\n+#             dtype: Optional dtype of the convolutions and linear operations,\n+#                     either jnp.float32 (default) or jnp.float16 for mixed precision.\n+#         \"\"\"\n+\n+#         super().__init__(*args, **kwargs)\n+#         self.stages = stages\n+#         self.block_type = block_type\n+#         self.lowres = lowres\n+\n+#         if weights is not None:\n+#             if weights.endswith(\".pkl\"):\n+#                 collections = pickle.load(open(weights, \"rb\"))\n+#             elif weights == \"imagenet\":\n+#                 clsname = self.__class__.__name__\n+#                 urldict = PRETRAINED_URLS.get(clsname, None)\n+#                 if urldict is None:\n+#                     raise ValueError(f\"No pretrained weights for {clsname} available\")\n+#                 fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n+#                 collections = pickle.load(open(fname, \"rb\"))\n+#             else:\n+#                 raise ValueError(\"Unknown weights value: \", weights)\n+\n+#             if isinstance(collections, tuple):\n+#                 parameters, collections = collections\n+#             elif \"parameters\" in collections:\n+#                 parameters = collections.pop(\"parameters\")\n+#             else:\n+#                 raise ValueError(\n+#                     \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n+#                 )\n+\n+#             x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n+#             # quick but dirty module initialization\n+#             jax.eval_shape(self.init(rng=types.KeySeq(42)), x)\n+\n+#             self.set_default_parameters(parameters, collections)\n+\n+#     def __call__(self, x: jnp.ndarray):\n+#         x = nn.Conv(\n+#             64,\n+#             (7, 7) if not self.lowres else (3, 3),\n+#             stride=(2, 2) if not self.lowres else (1, 1),\n+#             padding=\"SAME\",\n+#             with_bias=False,\n+#             dtype=self.dtype,\n+#         )(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+#         x = module.compact_module(jax.nn.relu)()(x)\n+\n+#         if not self.lowres:\n+#             x = nn.MaxPool(\n+#                 window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n+#             )(x)\n+#         for i, block_size in enumerate(self.stages):\n+#             for j in range(block_size):\n+#                 strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n+#                 x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n+#         GAP = lambda x: jnp.mean(x, axis=(1, 2))\n+#         x = module.compact_module(GAP)(name=\"global_average_pooling\")(x)\n+#         x = nn.Linear(1000, dtype=self.dtype)(x)\n+#         to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n+#         x = module.compact_module(to_float32)(name=\"to_float32\")(x)\n+#         return x\n+\n+\n+# class ResNet18(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[2, 2, 2, 2],\n+#             block_type=ResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet34(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 4, 6, 3],\n+#             block_type=ResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet50(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 4, 6, 3],\n+#             block_type=BottleneckResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet101(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 4, 23, 3],\n+#             block_type=BottleneckResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet152(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 8, 36, 3],\n+#             block_type=BottleneckResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet200(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 24, 36, 3],\n+#             block_type=BottleneckResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# _resnet__init___docstring = \"\"\"\n+# Instantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n+\n+# Arguments:\n+#     lowres: Optional, whether to use the low resolution version\n+#             as described in subsection 4.2 of the orignal paper.\n+#             This version is better suited for datasets like CIFAR10. (Default: False)\n+#     weights: One of None (random initialization), 'imagenet' (automatic download of\n+#               weights pretrained on ImageNet) or a path to a weights file\n+#     dtype: Optional dtype of the convolutions and linear operations,\n+#            either jnp.float32 (default) or jnp.float16 for mixed precision.\n+# \"\"\"\n+\n+# ResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\n+# ResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\n+# ResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\n+# ResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\n+# ResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\n+# ResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "source_code_with_indent": "# adapted from the flax library https://github.com/google/flax\n\nimport pickle\nimport typing as tp\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom elegy import hooks, module, nn, types, utils\n\n__all__ = [\n    \"ResNet\",\n    \"ResNet18\",\n    \"ResNet34\",\n    \"ResNet50\",\n    \"ResNet101\",\n    \"ResNet152\",\n    \"ResNet200\",\n]\n\n\nPRETRAINED_URLS = {\n    \"ResNet18\": {\n        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n        \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n    },\n    \"ResNet50\": {\n        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n        \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n    },\n}\n\n\nclass ResNetBlock(module.Module):\n    <IND>\"\"\"ResNet (identity) block\"\"\"\n\n    def __init__(\n        self,\n        n_filters: int,\n        strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(*args, **kwargs)\n        self.n_filters = n_filters\n        self.strides = strides\n\n    <DED>def call(self, x: jnp.ndarray):\n        <IND>x0 = x\n        x = nn.Conv2D(\n            self.n_filters,\n            (3, 3),\n            with_bias=False,\n            stride=self.strides,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n\n        x = nn.Conv2D(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n\n        if x0.shape != x.shape:\n            <IND>x0 = nn.Conv2D(\n                self.n_filters,\n                (1, 1),\n                with_bias=False,\n                stride=self.strides,\n                dtype=self.dtype,\n            )(x0)\n            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n        <DED>return jax.nn.relu(x0 + x)\n\n\n<DED><DED>class BottleneckResNetBlock(ResNetBlock):\n    <IND>\"\"\"ResNet Bottleneck block.\"\"\"\n\n    def call(self, x: jnp.ndarray):\n        <IND>x0 = x\n        x = nn.Conv2D(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n        x = nn.Conv2D(\n            self.n_filters,\n            (3, 3),\n            with_bias=False,\n            stride=self.strides,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n        x = nn.Conv2D(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n\n        if x0.shape != x.shape:\n            <IND>x0 = nn.Conv2D(\n                self.n_filters * 4,\n                (1, 1),\n                with_bias=False,\n                stride=self.strides,\n                dtype=self.dtype,\n            )(x0)\n            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n        <DED>return jax.nn.relu(x0 + x)\n\n\n<DED><DED>class ResNet(module.Module):\n    <IND>\"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n    Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n    \"\"\"\n\n    __all__ = [\"__init__\", \"call\"]\n\n    def __init__(\n        self,\n        stages: tp.List[int],\n        block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>\"\"\"\n        Arguments:\n            stages: A list of integers representing the number of blocks in each stage.\n                    e.g: [3, 4, 6, 3] for a ResNet50\n            block_type: Which ResNet block type to use.\n            lowres: Optional, whether to use the low resolution version\n                    as described in subsection 4.2 of the orignal paper.\n                    This version is better suited for datasets like CIFAR10. (Default: False)\n            weights: One of None (random initialization) or a path to a weights file\n            dtype: Optional dtype of the convolutions and linear operations,\n                    either jnp.float32 (default) or jnp.float16 for mixed precision.\n        \"\"\"\n\n        super().__init__(*args, **kwargs)\n        self.stages = stages\n        self.block_type = block_type\n        self.lowres = lowres\n\n        if weights is not None:\n            <IND>if weights.endswith(\".pkl\"):\n                <IND>collections = pickle.load(open(weights, \"rb\"))\n            <DED>elif weights == \"imagenet\":\n                <IND>clsname = self.__class__.__name__\n                urldict = PRETRAINED_URLS.get(clsname, None)\n                if urldict is None:\n                    <IND>raise ValueError(f\"No pretrained weights for {clsname} available\")\n                <DED>fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n                collections = pickle.load(open(fname, \"rb\"))\n            <DED>else:\n                <IND>raise ValueError(\"Unknown weights value: \", weights)\n\n            <DED>if isinstance(collections, tuple):\n                <IND>parameters, collections = collections\n            <DED>elif \"parameters\" in collections:\n                <IND>parameters = collections.pop(\"parameters\")\n            <DED>else:\n                <IND>raise ValueError(\n                    \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n                )\n\n            <DED>x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n            # quick but dirty module initialization\n            jax.eval_shape(self.init(rng=types.RNGSeq(42)), x)\n\n            self.set_default_parameters(parameters, collections)\n\n    <DED><DED>def call(self, x: jnp.ndarray):\n        <IND>x = nn.Conv2D(\n            64,\n            (7, 7) if not self.lowres else (3, 3),\n            stride=(2, 2) if not self.lowres else (1, 1),\n            padding=\"SAME\",\n            with_bias=False,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = module.to_module(jax.nn.relu)()(x)\n\n        if not self.lowres:\n            <IND>x = nn.MaxPool(\n                window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n            )(x)\n        <DED>for i, block_size in enumerate(self.stages):\n            <IND>for j in range(block_size):\n                <IND>strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n                x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n        <DED><DED>GAP = lambda x: jnp.mean(x, axis=(1, 2))\n        x = module.to_module(GAP)(name=\"global_average_pooling\")(x)\n        x = nn.Linear(1000, dtype=self.dtype)(x)\n        to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n        x = module.to_module(to_float32)(name=\"to_float32\")(x)\n        return x\n\n\n<DED><DED>class ResNet18(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[2, 2, 2, 2],\n            block_type=ResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet34(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 4, 6, 3],\n            block_type=ResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet50(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 4, 6, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet101(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 4, 23, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet152(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 8, 36, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet200(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 24, 36, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>_resnet__init___docstring = \"\"\"\nInstantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n\nArguments:\n    lowres: Optional, whether to use the low resolution version\n            as described in subsection 4.2 of the orignal paper.\n            This version is better suited for datasets like CIFAR10. (Default: False)\n    weights: One of None (random initialization), 'imagenet' (automatic download of\n              weights pretrained on ImageNet) or a path to a weights file\n    dtype: Optional dtype of the convolutions and linear operations, \n           either jnp.float32 (default) or jnp.float16 for mixed precision.\n\"\"\"\n\nResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\nResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\nResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\nResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\nResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\nResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "# # adapted from the flax library https://github.com/google/flax\n\n# import pickle\n# import typing as tp\n\n# import jax\n# import jax.numpy as jnp\n# import numpy as np\n# from elegy import hooks, module, nn, types, utils\n\n# __all__ = [\n#     \"ResNet\",\n#     \"ResNet18\",\n#     \"ResNet34\",\n#     \"ResNet50\",\n#     \"ResNet101\",\n#     \"ResNet152\",\n#     \"ResNet200\",\n# ]\n\n\n# PRETRAINED_URLS = {\n#     \"ResNet18\": {\n#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n#         \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n#     },\n#     \"ResNet50\": {\n#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n#         \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n#     },\n# }\n\n\n# class ResNetBlock(module.Module):\n#     \"\"\"ResNet (identity) block\"\"\"\n\n#     def __init__(\n#         self,\n#         n_filters: int,\n#         strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(*args, **kwargs)\n#         self.n_filters = n_filters\n#         self.strides = strides\n\n#     def __call__(self, x: jnp.ndarray):\n#         x0 = x\n#         x = nn.Conv(\n#             self.n_filters,\n#             (3, 3),\n#             with_bias=False,\n#             stride=self.strides,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n\n#         x = nn.Conv(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n\n#         if x0.shape != x.shape:\n#             x0 = nn.Conv(\n#                 self.n_filters,\n#                 (1, 1),\n#                 with_bias=False,\n#                 stride=self.strides,\n#                 dtype=self.dtype,\n#             )(x0)\n#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n#         return jax.nn.relu(x0 + x)\n\n\n# class BottleneckResNetBlock(ResNetBlock):\n#     \"\"\"ResNet Bottleneck block.\"\"\"\n\n#     def __call__(self, x: jnp.ndarray):\n#         x0 = x\n#         x = nn.Conv(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n#         x = nn.Conv(\n#             self.n_filters,\n#             (3, 3),\n#             with_bias=False,\n#             stride=self.strides,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n#         x = nn.Conv(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n\n#         if x0.shape != x.shape:\n#             x0 = nn.Conv(\n#                 self.n_filters * 4,\n#                 (1, 1),\n#                 with_bias=False,\n#                 stride=self.strides,\n#                 dtype=self.dtype,\n#             )(x0)\n#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n#         return jax.nn.relu(x0 + x)\n\n\n# class ResNet(module.Module):\n#     \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n#     Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n#     \"\"\"\n\n#     __all__ = [\"__init__\", \"call\"]\n\n#     def __init__(\n#         self,\n#         stages: tp.List[int],\n#         block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         \"\"\"\n#         Arguments:\n#             stages: A list of integers representing the number of blocks in each stage.\n#                     e.g: [3, 4, 6, 3] for a ResNet50\n#             block_type: Which ResNet block type to use.\n#             lowres: Optional, whether to use the low resolution version\n#                     as described in subsection 4.2 of the orignal paper.\n#                     This version is better suited for datasets like CIFAR10. (Default: False)\n#             weights: One of None (random initialization) or a path to a weights file\n#             dtype: Optional dtype of the convolutions and linear operations,\n#                     either jnp.float32 (default) or jnp.float16 for mixed precision.\n#         \"\"\"\n\n#         super().__init__(*args, **kwargs)\n#         self.stages = stages\n#         self.block_type = block_type\n#         self.lowres = lowres\n\n#         if weights is not None:\n#             if weights.endswith(\".pkl\"):\n#                 collections = pickle.load(open(weights, \"rb\"))\n#             elif weights == \"imagenet\":\n#                 clsname = self.__class__.__name__\n#                 urldict = PRETRAINED_URLS.get(clsname, None)\n#                 if urldict is None:\n#                     raise ValueError(f\"No pretrained weights for {clsname} available\")\n#                 fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n#                 collections = pickle.load(open(fname, \"rb\"))\n#             else:\n#                 raise ValueError(\"Unknown weights value: \", weights)\n\n#             if isinstance(collections, tuple):\n#                 parameters, collections = collections\n#             elif \"parameters\" in collections:\n#                 parameters = collections.pop(\"parameters\")\n#             else:\n#                 raise ValueError(\n#                     \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n#                 )\n\n#             x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n#             # quick but dirty module initialization\n#             jax.eval_shape(self.init(rng=types.KeySeq(42)), x)\n\n#             self.set_default_parameters(parameters, collections)\n\n#     def __call__(self, x: jnp.ndarray):\n#         x = nn.Conv(\n#             64,\n#             (7, 7) if not self.lowres else (3, 3),\n#             stride=(2, 2) if not self.lowres else (1, 1),\n#             padding=\"SAME\",\n#             with_bias=False,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = module.compact_module(jax.nn.relu)()(x)\n\n#         if not self.lowres:\n#             x = nn.MaxPool(\n#                 window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n#             )(x)\n#         for i, block_size in enumerate(self.stages):\n#             for j in range(block_size):\n#                 strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n#                 x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n#         GAP = lambda x: jnp.mean(x, axis=(1, 2))\n#         x = module.compact_module(GAP)(name=\"global_average_pooling\")(x)\n#         x = nn.Linear(1000, dtype=self.dtype)(x)\n#         to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n#         x = module.compact_module(to_float32)(name=\"to_float32\")(x)\n#         return x\n\n\n# class ResNet18(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[2, 2, 2, 2],\n#             block_type=ResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet34(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 6, 3],\n#             block_type=ResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet50(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 6, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet101(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 23, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet152(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 8, 36, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet200(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 24, 36, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# _resnet__init___docstring = \"\"\"\n# Instantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n\n# Arguments:\n#     lowres: Optional, whether to use the low resolution version\n#             as described in subsection 4.2 of the orignal paper.\n#             This version is better suited for datasets like CIFAR10. (Default: False)\n#     weights: One of None (random initialization), 'imagenet' (automatic download of\n#               weights pretrained on ImageNet) or a path to a weights file\n#     dtype: Optional dtype of the convolutions and linear operations,\n#            either jnp.float32 (default) or jnp.float16 for mixed precision.\n# \"\"\"\n\n# ResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\n# ResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\n# ResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\n# ResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\n# ResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\n# ResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nets/resnet.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nets/resnet.py",
    "file_hunks_size": 1,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "elegy/nets/resnet.py:170:4 Inconsistent override [14]: `elegy.nets.resnet.ResNet.call` overrides method defined in `module.Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "message": " `elegy.nets.resnet.ResNet.call` overrides method defined in `module.Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "rule_id": "Inconsistent override [14]",
    "warning_line_no": 170,
    "warning_line": "    def call(self, x: jnp.ndarray):",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "# adapted from the flax library https://github.com/google/flax\n\nimport pickle\nimport typing as tp\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom elegy import hooks, module, nn, types, utils\n\n__all__ = [\n    \"ResNet\",\n    \"ResNet18\",\n    \"ResNet34\",\n    \"ResNet50\",\n    \"ResNet101\",\n    \"ResNet152\",\n    \"ResNet200\",\n]\n\n\nPRETRAINED_URLS = {\n    \"ResNet18\": {\n        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n        \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n    },\n    \"ResNet50\": {\n        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n        \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n    },\n}\n\n\nclass ResNetBlock(module.Module):\n    \"\"\"ResNet (identity) block\"\"\"\n\n    def __init__(\n        self,\n        n_filters: int,\n        strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n        *args,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.n_filters = n_filters\n        self.strides = strides\n\n    def call(self, x: jnp.ndarray):\n        x0 = x\n        x = nn.Conv2D(\n            self.n_filters,\n            (3, 3),\n            with_bias=False,\n            stride=self.strides,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n\n        x = nn.Conv2D(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n\n        if x0.shape != x.shape:\n            x0 = nn.Conv2D(\n                self.n_filters,\n                (1, 1),\n                with_bias=False,\n                stride=self.strides,\n                dtype=self.dtype,\n            )(x0)\n            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n        return jax.nn.relu(x0 + x)\n\n\nclass BottleneckResNetBlock(ResNetBlock):\n    \"\"\"ResNet Bottleneck block.\"\"\"\n\n    def call(self, x: jnp.ndarray):\n        x0 = x\n        x = nn.Conv2D(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n        x = nn.Conv2D(\n            self.n_filters,\n            (3, 3),\n            with_bias=False,\n            stride=self.strides,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n        x = nn.Conv2D(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n\n        if x0.shape != x.shape:\n            x0 = nn.Conv2D(\n                self.n_filters * 4,\n                (1, 1),\n                with_bias=False,\n                stride=self.strides,\n                dtype=self.dtype,\n            )(x0)\n            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n        return jax.nn.relu(x0 + x)\n\n\nclass ResNet(module.Module):\n    \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n    Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n    \"\"\"\n\n    __all__ = [\"__init__\", \"call\"]\n\n    def __init__(\n        self,\n        stages: tp.List[int],\n        block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Arguments:\n            stages: A list of integers representing the number of blocks in each stage.\n                    e.g: [3, 4, 6, 3] for a ResNet50\n            block_type: Which ResNet block type to use.\n            lowres: Optional, whether to use the low resolution version\n                    as described in subsection 4.2 of the orignal paper.\n                    This version is better suited for datasets like CIFAR10. (Default: False)\n            weights: One of None (random initialization) or a path to a weights file\n            dtype: Optional dtype of the convolutions and linear operations,\n                    either jnp.float32 (default) or jnp.float16 for mixed precision.\n        \"\"\"\n\n        super().__init__(*args, **kwargs)\n        self.stages = stages\n        self.block_type = block_type\n        self.lowres = lowres\n\n        if weights is not None:\n            if weights.endswith(\".pkl\"):\n                collections = pickle.load(open(weights, \"rb\"))\n            elif weights == \"imagenet\":\n                clsname = self.__class__.__name__\n                urldict = PRETRAINED_URLS.get(clsname, None)\n                if urldict is None:\n                    raise ValueError(f\"No pretrained weights for {clsname} available\")\n                fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n                collections = pickle.load(open(fname, \"rb\"))\n            else:\n                raise ValueError(\"Unknown weights value: \", weights)\n\n            if isinstance(collections, tuple):\n                parameters, collections = collections\n            elif \"parameters\" in collections:\n                parameters = collections.pop(\"parameters\")\n            else:\n                raise ValueError(\n                    \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n                )\n\n            x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n            # quick but dirty module initialization\n            jax.eval_shape(self.init(rng=types.RNGSeq(42)), x)\n\n            self.set_default_parameters(parameters, collections)\n\n    def call(self, x: jnp.ndarray):\n        x = nn.Conv2D(\n            64,\n            (7, 7) if not self.lowres else (3, 3),\n            stride=(2, 2) if not self.lowres else (1, 1),\n            padding=\"SAME\",\n            with_bias=False,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = module.to_module(jax.nn.relu)()(x)\n\n        if not self.lowres:\n            x = nn.MaxPool(\n                window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n            )(x)\n        for i, block_size in enumerate(self.stages):\n            for j in range(block_size):\n                strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n                x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n        GAP = lambda x: jnp.mean(x, axis=(1, 2))\n        x = module.to_module(GAP)(name=\"global_average_pooling\")(x)\n        x = nn.Linear(1000, dtype=self.dtype)(x)\n        to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n        x = module.to_module(to_float32)(name=\"to_float32\")(x)\n        return x\n\n\nclass ResNet18(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[2, 2, 2, 2],\n            block_type=ResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet34(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 4, 6, 3],\n            block_type=ResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet50(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 4, 6, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet101(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 4, 23, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet152(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 8, 36, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\nclass ResNet200(ResNet):\n    def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(\n            stages=[3, 24, 36, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n_resnet__init___docstring = \"\"\"\nInstantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n\nArguments:\n    lowres: Optional, whether to use the low resolution version\n            as described in subsection 4.2 of the orignal paper.\n            This version is better suited for datasets like CIFAR10. (Default: False)\n    weights: One of None (random initialization), 'imagenet' (automatic download of\n              weights pretrained on ImageNet) or a path to a weights file\n    dtype: Optional dtype of the convolutions and linear operations, \n           either jnp.float32 (default) or jnp.float16 for mixed precision.\n\"\"\"\n\nResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\nResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\nResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\nResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\nResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\nResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "source_code_len": 10761,
        "target_code": "# # adapted from the flax library https://github.com/google/flax\n\n# import pickle\n# import typing as tp\n\n# import jax\n# import jax.numpy as jnp\n# import numpy as np\n# from elegy import hooks, module, nn, types, utils\n\n# __all__ = [\n#     \"ResNet\",\n#     \"ResNet18\",\n#     \"ResNet34\",\n#     \"ResNet50\",\n#     \"ResNet101\",\n#     \"ResNet152\",\n#     \"ResNet200\",\n# ]\n\n\n# PRETRAINED_URLS = {\n#     \"ResNet18\": {\n#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n#         \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n#     },\n#     \"ResNet50\": {\n#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n#         \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n#     },\n# }\n\n\n# class ResNetBlock(module.Module):\n#     \"\"\"ResNet (identity) block\"\"\"\n\n#     def __init__(\n#         self,\n#         n_filters: int,\n#         strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(*args, **kwargs)\n#         self.n_filters = n_filters\n#         self.strides = strides\n\n#     def __call__(self, x: jnp.ndarray):\n#         x0 = x\n#         x = nn.Conv(\n#             self.n_filters,\n#             (3, 3),\n#             with_bias=False,\n#             stride=self.strides,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n\n#         x = nn.Conv(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n\n#         if x0.shape != x.shape:\n#             x0 = nn.Conv(\n#                 self.n_filters,\n#                 (1, 1),\n#                 with_bias=False,\n#                 stride=self.strides,\n#                 dtype=self.dtype,\n#             )(x0)\n#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n#         return jax.nn.relu(x0 + x)\n\n\n# class BottleneckResNetBlock(ResNetBlock):\n#     \"\"\"ResNet Bottleneck block.\"\"\"\n\n#     def __call__(self, x: jnp.ndarray):\n#         x0 = x\n#         x = nn.Conv(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n#         x = nn.Conv(\n#             self.n_filters,\n#             (3, 3),\n#             with_bias=False,\n#             stride=self.strides,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n#         x = nn.Conv(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n\n#         if x0.shape != x.shape:\n#             x0 = nn.Conv(\n#                 self.n_filters * 4,\n#                 (1, 1),\n#                 with_bias=False,\n#                 stride=self.strides,\n#                 dtype=self.dtype,\n#             )(x0)\n#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n#         return jax.nn.relu(x0 + x)\n\n\n# class ResNet(module.Module):\n#     \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n#     Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n#     \"\"\"\n\n#     __all__ = [\"__init__\", \"call\"]\n\n#     def __init__(\n#         self,\n#         stages: tp.List[int],\n#         block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         \"\"\"\n#         Arguments:\n#             stages: A list of integers representing the number of blocks in each stage.\n#                     e.g: [3, 4, 6, 3] for a ResNet50\n#             block_type: Which ResNet block type to use.\n#             lowres: Optional, whether to use the low resolution version\n#                     as described in subsection 4.2 of the orignal paper.\n#                     This version is better suited for datasets like CIFAR10. (Default: False)\n#             weights: One of None (random initialization) or a path to a weights file\n#             dtype: Optional dtype of the convolutions and linear operations,\n#                     either jnp.float32 (default) or jnp.float16 for mixed precision.\n#         \"\"\"\n\n#         super().__init__(*args, **kwargs)\n#         self.stages = stages\n#         self.block_type = block_type\n#         self.lowres = lowres\n\n#         if weights is not None:\n#             if weights.endswith(\".pkl\"):\n#                 collections = pickle.load(open(weights, \"rb\"))\n#             elif weights == \"imagenet\":\n#                 clsname = self.__class__.__name__\n#                 urldict = PRETRAINED_URLS.get(clsname, None)\n#                 if urldict is None:\n#                     raise ValueError(f\"No pretrained weights for {clsname} available\")\n#                 fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n#                 collections = pickle.load(open(fname, \"rb\"))\n#             else:\n#                 raise ValueError(\"Unknown weights value: \", weights)\n\n#             if isinstance(collections, tuple):\n#                 parameters, collections = collections\n#             elif \"parameters\" in collections:\n#                 parameters = collections.pop(\"parameters\")\n#             else:\n#                 raise ValueError(\n#                     \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n#                 )\n\n#             x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n#             # quick but dirty module initialization\n#             jax.eval_shape(self.init(rng=types.KeySeq(42)), x)\n\n#             self.set_default_parameters(parameters, collections)\n\n#     def __call__(self, x: jnp.ndarray):\n#         x = nn.Conv(\n#             64,\n#             (7, 7) if not self.lowres else (3, 3),\n#             stride=(2, 2) if not self.lowres else (1, 1),\n#             padding=\"SAME\",\n#             with_bias=False,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = module.compact_module(jax.nn.relu)()(x)\n\n#         if not self.lowres:\n#             x = nn.MaxPool(\n#                 window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n#             )(x)\n#         for i, block_size in enumerate(self.stages):\n#             for j in range(block_size):\n#                 strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n#                 x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n#         GAP = lambda x: jnp.mean(x, axis=(1, 2))\n#         x = module.compact_module(GAP)(name=\"global_average_pooling\")(x)\n#         x = nn.Linear(1000, dtype=self.dtype)(x)\n#         to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n#         x = module.compact_module(to_float32)(name=\"to_float32\")(x)\n#         return x\n\n\n# class ResNet18(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[2, 2, 2, 2],\n#             block_type=ResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet34(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 6, 3],\n#             block_type=ResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet50(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 6, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet101(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 23, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet152(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 8, 36, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet200(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 24, 36, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# _resnet__init___docstring = \"\"\"\n# Instantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n\n# Arguments:\n#     lowres: Optional, whether to use the low resolution version\n#             as described in subsection 4.2 of the orignal paper.\n#             This version is better suited for datasets like CIFAR10. (Default: False)\n#     weights: One of None (random initialization), 'imagenet' (automatic download of\n#               weights pretrained on ImageNet) or a path to a weights file\n#     dtype: Optional dtype of the convolutions and linear operations,\n#            either jnp.float32 (default) or jnp.float16 for mixed precision.\n# \"\"\"\n\n# ResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\n# ResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\n# ResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\n# ResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\n# ResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\n# ResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "target_code_len": 11287,
        "diff_format": "@@ -1,336 +1,336 @@\n-# adapted from the flax library https://github.com/google/flax\n-\n-import pickle\n-import typing as tp\n-\n-import jax\n-import jax.numpy as jnp\n-import numpy as np\n-from elegy import hooks, module, nn, types, utils\n-\n-__all__ = [\n-    \"ResNet\",\n-    \"ResNet18\",\n-    \"ResNet34\",\n-    \"ResNet50\",\n-    \"ResNet101\",\n-    \"ResNet152\",\n-    \"ResNet200\",\n-]\n-\n-\n-PRETRAINED_URLS = {\n-    \"ResNet18\": {\n-        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n-        \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n-    },\n-    \"ResNet50\": {\n-        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n-        \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n-    },\n-}\n-\n-\n-class ResNetBlock(module.Module):\n-    \"\"\"ResNet (identity) block\"\"\"\n-\n-    def __init__(\n-        self,\n-        n_filters: int,\n-        strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(*args, **kwargs)\n-        self.n_filters = n_filters\n-        self.strides = strides\n-\n-    def call(self, x: jnp.ndarray):\n-        x0 = x\n-        x = nn.Conv2D(\n-            self.n_filters,\n-            (3, 3),\n-            with_bias=False,\n-            stride=self.strides,\n-            dtype=self.dtype,\n-        )(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-        x = jax.nn.relu(x)\n-\n-        x = nn.Conv2D(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-\n-        if x0.shape != x.shape:\n-            x0 = nn.Conv2D(\n-                self.n_filters,\n-                (1, 1),\n-                with_bias=False,\n-                stride=self.strides,\n-                dtype=self.dtype,\n-            )(x0)\n-            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n-        return jax.nn.relu(x0 + x)\n-\n-\n-class BottleneckResNetBlock(ResNetBlock):\n-    \"\"\"ResNet Bottleneck block.\"\"\"\n-\n-    def call(self, x: jnp.ndarray):\n-        x0 = x\n-        x = nn.Conv2D(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-        x = jax.nn.relu(x)\n-        x = nn.Conv2D(\n-            self.n_filters,\n-            (3, 3),\n-            with_bias=False,\n-            stride=self.strides,\n-            dtype=self.dtype,\n-        )(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-        x = jax.nn.relu(x)\n-        x = nn.Conv2D(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n-\n-        if x0.shape != x.shape:\n-            x0 = nn.Conv2D(\n-                self.n_filters * 4,\n-                (1, 1),\n-                with_bias=False,\n-                stride=self.strides,\n-                dtype=self.dtype,\n-            )(x0)\n-            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n-        return jax.nn.relu(x0 + x)\n-\n-\n-class ResNet(module.Module):\n-    \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n-    Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n-    \"\"\"\n-\n-    __all__ = [\"__init__\", \"call\"]\n-\n-    def __init__(\n-        self,\n-        stages: tp.List[int],\n-        block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Arguments:\n-            stages: A list of integers representing the number of blocks in each stage.\n-                    e.g: [3, 4, 6, 3] for a ResNet50\n-            block_type: Which ResNet block type to use.\n-            lowres: Optional, whether to use the low resolution version\n-                    as described in subsection 4.2 of the orignal paper.\n-                    This version is better suited for datasets like CIFAR10. (Default: False)\n-            weights: One of None (random initialization) or a path to a weights file\n-            dtype: Optional dtype of the convolutions and linear operations,\n-                    either jnp.float32 (default) or jnp.float16 for mixed precision.\n-        \"\"\"\n-\n-        super().__init__(*args, **kwargs)\n-        self.stages = stages\n-        self.block_type = block_type\n-        self.lowres = lowres\n-\n-        if weights is not None:\n-            if weights.endswith(\".pkl\"):\n-                collections = pickle.load(open(weights, \"rb\"))\n-            elif weights == \"imagenet\":\n-                clsname = self.__class__.__name__\n-                urldict = PRETRAINED_URLS.get(clsname, None)\n-                if urldict is None:\n-                    raise ValueError(f\"No pretrained weights for {clsname} available\")\n-                fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n-                collections = pickle.load(open(fname, \"rb\"))\n-            else:\n-                raise ValueError(\"Unknown weights value: \", weights)\n-\n-            if isinstance(collections, tuple):\n-                parameters, collections = collections\n-            elif \"parameters\" in collections:\n-                parameters = collections.pop(\"parameters\")\n-            else:\n-                raise ValueError(\n-                    \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n-                )\n-\n-            x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n-            # quick but dirty module initialization\n-            jax.eval_shape(self.init(rng=types.RNGSeq(42)), x)\n-\n-            self.set_default_parameters(parameters, collections)\n-\n-    def call(self, x: jnp.ndarray):\n-        x = nn.Conv2D(\n-            64,\n-            (7, 7) if not self.lowres else (3, 3),\n-            stride=(2, 2) if not self.lowres else (1, 1),\n-            padding=\"SAME\",\n-            with_bias=False,\n-            dtype=self.dtype,\n-        )(x)\n-        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n-        x = module.to_module(jax.nn.relu)()(x)\n-\n-        if not self.lowres:\n-            x = nn.MaxPool(\n-                window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n-            )(x)\n-        for i, block_size in enumerate(self.stages):\n-            for j in range(block_size):\n-                strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n-                x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n-        GAP = lambda x: jnp.mean(x, axis=(1, 2))\n-        x = module.to_module(GAP)(name=\"global_average_pooling\")(x)\n-        x = nn.Linear(1000, dtype=self.dtype)(x)\n-        to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n-        x = module.to_module(to_float32)(name=\"to_float32\")(x)\n-        return x\n-\n-\n-class ResNet18(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[2, 2, 2, 2],\n-            block_type=ResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet34(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 4, 6, 3],\n-            block_type=ResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet50(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 4, 6, 3],\n-            block_type=BottleneckResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet101(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 4, 23, 3],\n-            block_type=BottleneckResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet152(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 8, 36, 3],\n-            block_type=BottleneckResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-class ResNet200(ResNet):\n-    def __init__(\n-        self,\n-        lowres: bool = False,\n-        weights: tp.Optional[str] = None,\n-        dtype: tp.Optional[tp.Any] = jnp.float32,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            stages=[3, 24, 36, 3],\n-            block_type=BottleneckResNetBlock,\n-            lowres=lowres,\n-            weights=weights,\n-            dtype=dtype,\n-            *args,\n-            **kwargs,\n-        )\n-\n-\n-_resnet__init___docstring = \"\"\"\n-Instantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n-\n-Arguments:\n-    lowres: Optional, whether to use the low resolution version\n-            as described in subsection 4.2 of the orignal paper.\n-            This version is better suited for datasets like CIFAR10. (Default: False)\n-    weights: One of None (random initialization), 'imagenet' (automatic download of\n-              weights pretrained on ImageNet) or a path to a weights file\n-    dtype: Optional dtype of the convolutions and linear operations, \n-           either jnp.float32 (default) or jnp.float16 for mixed precision.\n-\"\"\"\n-\n-ResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\n-ResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\n-ResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\n-ResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\n-ResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\n-ResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n+# # adapted from the flax library https://github.com/google/flax\n+\n+# import pickle\n+# import typing as tp\n+\n+# import jax\n+# import jax.numpy as jnp\n+# import numpy as np\n+# from elegy import hooks, module, nn, types, utils\n+\n+# __all__ = [\n+#     \"ResNet\",\n+#     \"ResNet18\",\n+#     \"ResNet34\",\n+#     \"ResNet50\",\n+#     \"ResNet101\",\n+#     \"ResNet152\",\n+#     \"ResNet200\",\n+# ]\n+\n+\n+# PRETRAINED_URLS = {\n+#     \"ResNet18\": {\n+#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n+#         \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n+#     },\n+#     \"ResNet50\": {\n+#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n+#         \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n+#     },\n+# }\n+\n+\n+# class ResNetBlock(module.Module):\n+#     \"\"\"ResNet (identity) block\"\"\"\n+\n+#     def __init__(\n+#         self,\n+#         n_filters: int,\n+#         strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(*args, **kwargs)\n+#         self.n_filters = n_filters\n+#         self.strides = strides\n+\n+#     def __call__(self, x: jnp.ndarray):\n+#         x0 = x\n+#         x = nn.Conv(\n+#             self.n_filters,\n+#             (3, 3),\n+#             with_bias=False,\n+#             stride=self.strides,\n+#             dtype=self.dtype,\n+#         )(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+#         x = jax.nn.relu(x)\n+\n+#         x = nn.Conv(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+\n+#         if x0.shape != x.shape:\n+#             x0 = nn.Conv(\n+#                 self.n_filters,\n+#                 (1, 1),\n+#                 with_bias=False,\n+#                 stride=self.strides,\n+#                 dtype=self.dtype,\n+#             )(x0)\n+#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n+#         return jax.nn.relu(x0 + x)\n+\n+\n+# class BottleneckResNetBlock(ResNetBlock):\n+#     \"\"\"ResNet Bottleneck block.\"\"\"\n+\n+#     def __call__(self, x: jnp.ndarray):\n+#         x0 = x\n+#         x = nn.Conv(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+#         x = jax.nn.relu(x)\n+#         x = nn.Conv(\n+#             self.n_filters,\n+#             (3, 3),\n+#             with_bias=False,\n+#             stride=self.strides,\n+#             dtype=self.dtype,\n+#         )(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+#         x = jax.nn.relu(x)\n+#         x = nn.Conv(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n+\n+#         if x0.shape != x.shape:\n+#             x0 = nn.Conv(\n+#                 self.n_filters * 4,\n+#                 (1, 1),\n+#                 with_bias=False,\n+#                 stride=self.strides,\n+#                 dtype=self.dtype,\n+#             )(x0)\n+#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n+#         return jax.nn.relu(x0 + x)\n+\n+\n+# class ResNet(module.Module):\n+#     \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n+#     Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n+#     \"\"\"\n+\n+#     __all__ = [\"__init__\", \"call\"]\n+\n+#     def __init__(\n+#         self,\n+#         stages: tp.List[int],\n+#         block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         \"\"\"\n+#         Arguments:\n+#             stages: A list of integers representing the number of blocks in each stage.\n+#                     e.g: [3, 4, 6, 3] for a ResNet50\n+#             block_type: Which ResNet block type to use.\n+#             lowres: Optional, whether to use the low resolution version\n+#                     as described in subsection 4.2 of the orignal paper.\n+#                     This version is better suited for datasets like CIFAR10. (Default: False)\n+#             weights: One of None (random initialization) or a path to a weights file\n+#             dtype: Optional dtype of the convolutions and linear operations,\n+#                     either jnp.float32 (default) or jnp.float16 for mixed precision.\n+#         \"\"\"\n+\n+#         super().__init__(*args, **kwargs)\n+#         self.stages = stages\n+#         self.block_type = block_type\n+#         self.lowres = lowres\n+\n+#         if weights is not None:\n+#             if weights.endswith(\".pkl\"):\n+#                 collections = pickle.load(open(weights, \"rb\"))\n+#             elif weights == \"imagenet\":\n+#                 clsname = self.__class__.__name__\n+#                 urldict = PRETRAINED_URLS.get(clsname, None)\n+#                 if urldict is None:\n+#                     raise ValueError(f\"No pretrained weights for {clsname} available\")\n+#                 fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n+#                 collections = pickle.load(open(fname, \"rb\"))\n+#             else:\n+#                 raise ValueError(\"Unknown weights value: \", weights)\n+\n+#             if isinstance(collections, tuple):\n+#                 parameters, collections = collections\n+#             elif \"parameters\" in collections:\n+#                 parameters = collections.pop(\"parameters\")\n+#             else:\n+#                 raise ValueError(\n+#                     \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n+#                 )\n+\n+#             x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n+#             # quick but dirty module initialization\n+#             jax.eval_shape(self.init(rng=types.KeySeq(42)), x)\n+\n+#             self.set_default_parameters(parameters, collections)\n+\n+#     def __call__(self, x: jnp.ndarray):\n+#         x = nn.Conv(\n+#             64,\n+#             (7, 7) if not self.lowres else (3, 3),\n+#             stride=(2, 2) if not self.lowres else (1, 1),\n+#             padding=\"SAME\",\n+#             with_bias=False,\n+#             dtype=self.dtype,\n+#         )(x)\n+#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n+#         x = module.compact_module(jax.nn.relu)()(x)\n+\n+#         if not self.lowres:\n+#             x = nn.MaxPool(\n+#                 window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n+#             )(x)\n+#         for i, block_size in enumerate(self.stages):\n+#             for j in range(block_size):\n+#                 strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n+#                 x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n+#         GAP = lambda x: jnp.mean(x, axis=(1, 2))\n+#         x = module.compact_module(GAP)(name=\"global_average_pooling\")(x)\n+#         x = nn.Linear(1000, dtype=self.dtype)(x)\n+#         to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n+#         x = module.compact_module(to_float32)(name=\"to_float32\")(x)\n+#         return x\n+\n+\n+# class ResNet18(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[2, 2, 2, 2],\n+#             block_type=ResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet34(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 4, 6, 3],\n+#             block_type=ResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet50(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 4, 6, 3],\n+#             block_type=BottleneckResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet101(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 4, 23, 3],\n+#             block_type=BottleneckResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet152(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 8, 36, 3],\n+#             block_type=BottleneckResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# class ResNet200(ResNet):\n+#     def __init__(\n+#         self,\n+#         lowres: bool = False,\n+#         weights: tp.Optional[str] = None,\n+#         dtype: tp.Optional[tp.Any] = jnp.float32,\n+#         *args,\n+#         **kwargs,\n+#     ):\n+#         super().__init__(\n+#             stages=[3, 24, 36, 3],\n+#             block_type=BottleneckResNetBlock,\n+#             lowres=lowres,\n+#             weights=weights,\n+#             dtype=dtype,\n+#             *args,\n+#             **kwargs,\n+#         )\n+\n+\n+# _resnet__init___docstring = \"\"\"\n+# Instantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n+\n+# Arguments:\n+#     lowres: Optional, whether to use the low resolution version\n+#             as described in subsection 4.2 of the orignal paper.\n+#             This version is better suited for datasets like CIFAR10. (Default: False)\n+#     weights: One of None (random initialization), 'imagenet' (automatic download of\n+#               weights pretrained on ImageNet) or a path to a weights file\n+#     dtype: Optional dtype of the convolutions and linear operations,\n+#            either jnp.float32 (default) or jnp.float16 for mixed precision.\n+# \"\"\"\n+\n+# ResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\n+# ResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\n+# ResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\n+# ResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\n+# ResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\n+# ResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "source_code_with_indent": "# adapted from the flax library https://github.com/google/flax\n\nimport pickle\nimport typing as tp\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom elegy import hooks, module, nn, types, utils\n\n__all__ = [\n    \"ResNet\",\n    \"ResNet18\",\n    \"ResNet34\",\n    \"ResNet50\",\n    \"ResNet101\",\n    \"ResNet152\",\n    \"ResNet200\",\n]\n\n\nPRETRAINED_URLS = {\n    \"ResNet18\": {\n        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n        \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n    },\n    \"ResNet50\": {\n        \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n        \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n    },\n}\n\n\nclass ResNetBlock(module.Module):\n    <IND>\"\"\"ResNet (identity) block\"\"\"\n\n    def __init__(\n        self,\n        n_filters: int,\n        strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(*args, **kwargs)\n        self.n_filters = n_filters\n        self.strides = strides\n\n    <DED>def call(self, x: jnp.ndarray):\n        <IND>x0 = x\n        x = nn.Conv2D(\n            self.n_filters,\n            (3, 3),\n            with_bias=False,\n            stride=self.strides,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n\n        x = nn.Conv2D(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n\n        if x0.shape != x.shape:\n            <IND>x0 = nn.Conv2D(\n                self.n_filters,\n                (1, 1),\n                with_bias=False,\n                stride=self.strides,\n                dtype=self.dtype,\n            )(x0)\n            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n        <DED>return jax.nn.relu(x0 + x)\n\n\n<DED><DED>class BottleneckResNetBlock(ResNetBlock):\n    <IND>\"\"\"ResNet Bottleneck block.\"\"\"\n\n    def call(self, x: jnp.ndarray):\n        <IND>x0 = x\n        x = nn.Conv2D(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n        x = nn.Conv2D(\n            self.n_filters,\n            (3, 3),\n            with_bias=False,\n            stride=self.strides,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = jax.nn.relu(x)\n        x = nn.Conv2D(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n\n        if x0.shape != x.shape:\n            <IND>x0 = nn.Conv2D(\n                self.n_filters * 4,\n                (1, 1),\n                with_bias=False,\n                stride=self.strides,\n                dtype=self.dtype,\n            )(x0)\n            x0 = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x0)\n        <DED>return jax.nn.relu(x0 + x)\n\n\n<DED><DED>class ResNet(module.Module):\n    <IND>\"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n    Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n    \"\"\"\n\n    __all__ = [\"__init__\", \"call\"]\n\n    def __init__(\n        self,\n        stages: tp.List[int],\n        block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>\"\"\"\n        Arguments:\n            stages: A list of integers representing the number of blocks in each stage.\n                    e.g: [3, 4, 6, 3] for a ResNet50\n            block_type: Which ResNet block type to use.\n            lowres: Optional, whether to use the low resolution version\n                    as described in subsection 4.2 of the orignal paper.\n                    This version is better suited for datasets like CIFAR10. (Default: False)\n            weights: One of None (random initialization) or a path to a weights file\n            dtype: Optional dtype of the convolutions and linear operations,\n                    either jnp.float32 (default) or jnp.float16 for mixed precision.\n        \"\"\"\n\n        super().__init__(*args, **kwargs)\n        self.stages = stages\n        self.block_type = block_type\n        self.lowres = lowres\n\n        if weights is not None:\n            <IND>if weights.endswith(\".pkl\"):\n                <IND>collections = pickle.load(open(weights, \"rb\"))\n            <DED>elif weights == \"imagenet\":\n                <IND>clsname = self.__class__.__name__\n                urldict = PRETRAINED_URLS.get(clsname, None)\n                if urldict is None:\n                    <IND>raise ValueError(f\"No pretrained weights for {clsname} available\")\n                <DED>fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n                collections = pickle.load(open(fname, \"rb\"))\n            <DED>else:\n                <IND>raise ValueError(\"Unknown weights value: \", weights)\n\n            <DED>if isinstance(collections, tuple):\n                <IND>parameters, collections = collections\n            <DED>elif \"parameters\" in collections:\n                <IND>parameters = collections.pop(\"parameters\")\n            <DED>else:\n                <IND>raise ValueError(\n                    \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n                )\n\n            <DED>x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n            # quick but dirty module initialization\n            jax.eval_shape(self.init(rng=types.RNGSeq(42)), x)\n\n            self.set_default_parameters(parameters, collections)\n\n    <DED><DED>def call(self, x: jnp.ndarray):\n        <IND>x = nn.Conv2D(\n            64,\n            (7, 7) if not self.lowres else (3, 3),\n            stride=(2, 2) if not self.lowres else (1, 1),\n            padding=\"SAME\",\n            with_bias=False,\n            dtype=self.dtype,\n        )(x)\n        x = nn.BatchNormalization(decay_rate=0.9, eps=1e-5)(x)\n        x = module.to_module(jax.nn.relu)()(x)\n\n        if not self.lowres:\n            <IND>x = nn.MaxPool(\n                window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n            )(x)\n        <DED>for i, block_size in enumerate(self.stages):\n            <IND>for j in range(block_size):\n                <IND>strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n                x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n        <DED><DED>GAP = lambda x: jnp.mean(x, axis=(1, 2))\n        x = module.to_module(GAP)(name=\"global_average_pooling\")(x)\n        x = nn.Linear(1000, dtype=self.dtype)(x)\n        to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n        x = module.to_module(to_float32)(name=\"to_float32\")(x)\n        return x\n\n\n<DED><DED>class ResNet18(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[2, 2, 2, 2],\n            block_type=ResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet34(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 4, 6, 3],\n            block_type=ResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet50(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 4, 6, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet101(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 4, 23, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet152(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 8, 36, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>class ResNet200(ResNet):\n    <IND>def __init__(\n        self,\n        lowres: bool = False,\n        weights: tp.Optional[str] = None,\n        dtype: tp.Optional[tp.Any] = jnp.float32,\n        *args,\n        **kwargs,\n    ):\n        <IND>super().__init__(\n            stages=[3, 24, 36, 3],\n            block_type=BottleneckResNetBlock,\n            lowres=lowres,\n            weights=weights,\n            dtype=dtype,\n            *args,\n            **kwargs,\n        )\n\n\n<DED><DED>_resnet__init___docstring = \"\"\"\nInstantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n\nArguments:\n    lowres: Optional, whether to use the low resolution version\n            as described in subsection 4.2 of the orignal paper.\n            This version is better suited for datasets like CIFAR10. (Default: False)\n    weights: One of None (random initialization), 'imagenet' (automatic download of\n              weights pretrained on ImageNet) or a path to a weights file\n    dtype: Optional dtype of the convolutions and linear operations, \n           either jnp.float32 (default) or jnp.float16 for mixed precision.\n\"\"\"\n\nResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\nResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\nResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\nResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\nResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\nResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "# # adapted from the flax library https://github.com/google/flax\n\n# import pickle\n# import typing as tp\n\n# import jax\n# import jax.numpy as jnp\n# import numpy as np\n# from elegy import hooks, module, nn, types, utils\n\n# __all__ = [\n#     \"ResNet\",\n#     \"ResNet18\",\n#     \"ResNet34\",\n#     \"ResNet50\",\n#     \"ResNet101\",\n#     \"ResNet152\",\n#     \"ResNet200\",\n# ]\n\n\n# PRETRAINED_URLS = {\n#     \"ResNet18\": {\n#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet18_rev1/ResNet18_ImageNet_rev1.pkl\",\n#         \"sha256\": \"02824ae2f29563add46feff14f40c362ae5f9af3f01ea2edc0812e5ca06ca9ae\",\n#     },\n#     \"ResNet50\": {\n#         \"url\": \"https://github.com/poets-ai/elegy-assets/releases/download/resnet50_rev1/ResNet50_ImageNet_rev1.pkl\",\n#         \"sha256\": \"c69086813ccff6b67b2452daabdf64772f8a7f5c04591e1962185129e18989fc\",\n#     },\n# }\n\n\n# class ResNetBlock(module.Module):\n#     \"\"\"ResNet (identity) block\"\"\"\n\n#     def __init__(\n#         self,\n#         n_filters: int,\n#         strides: tp.Optional[tp.Tuple[int]] = (1, 1),\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(*args, **kwargs)\n#         self.n_filters = n_filters\n#         self.strides = strides\n\n#     def __call__(self, x: jnp.ndarray):\n#         x0 = x\n#         x = nn.Conv(\n#             self.n_filters,\n#             (3, 3),\n#             with_bias=False,\n#             stride=self.strides,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n\n#         x = nn.Conv(self.n_filters, (3, 3), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n\n#         if x0.shape != x.shape:\n#             x0 = nn.Conv(\n#                 self.n_filters,\n#                 (1, 1),\n#                 with_bias=False,\n#                 stride=self.strides,\n#                 dtype=self.dtype,\n#             )(x0)\n#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n#         return jax.nn.relu(x0 + x)\n\n\n# class BottleneckResNetBlock(ResNetBlock):\n#     \"\"\"ResNet Bottleneck block.\"\"\"\n\n#     def __call__(self, x: jnp.ndarray):\n#         x0 = x\n#         x = nn.Conv(self.n_filters, (1, 1), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n#         x = nn.Conv(\n#             self.n_filters,\n#             (3, 3),\n#             with_bias=False,\n#             stride=self.strides,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = jax.nn.relu(x)\n#         x = nn.Conv(self.n_filters * 4, (1, 1), with_bias=False, dtype=self.dtype)(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5, scale_init=jnp.zeros)(x)\n\n#         if x0.shape != x.shape:\n#             x0 = nn.Conv(\n#                 self.n_filters * 4,\n#                 (1, 1),\n#                 with_bias=False,\n#                 stride=self.strides,\n#                 dtype=self.dtype,\n#             )(x0)\n#             x0 = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x0)\n#         return jax.nn.relu(x0 + x)\n\n\n# class ResNet(module.Module):\n#     \"\"\"A generic ResNet V1 architecture that can be customized for non-standard configurations\n#     Original Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n#     \"\"\"\n\n#     __all__ = [\"__init__\", \"call\"]\n\n#     def __init__(\n#         self,\n#         stages: tp.List[int],\n#         block_type: tp.Union[tp.Type[ResNetBlock], tp.Type[BottleneckResNetBlock]],\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         \"\"\"\n#         Arguments:\n#             stages: A list of integers representing the number of blocks in each stage.\n#                     e.g: [3, 4, 6, 3] for a ResNet50\n#             block_type: Which ResNet block type to use.\n#             lowres: Optional, whether to use the low resolution version\n#                     as described in subsection 4.2 of the orignal paper.\n#                     This version is better suited for datasets like CIFAR10. (Default: False)\n#             weights: One of None (random initialization) or a path to a weights file\n#             dtype: Optional dtype of the convolutions and linear operations,\n#                     either jnp.float32 (default) or jnp.float16 for mixed precision.\n#         \"\"\"\n\n#         super().__init__(*args, **kwargs)\n#         self.stages = stages\n#         self.block_type = block_type\n#         self.lowres = lowres\n\n#         if weights is not None:\n#             if weights.endswith(\".pkl\"):\n#                 collections = pickle.load(open(weights, \"rb\"))\n#             elif weights == \"imagenet\":\n#                 clsname = self.__class__.__name__\n#                 urldict = PRETRAINED_URLS.get(clsname, None)\n#                 if urldict is None:\n#                     raise ValueError(f\"No pretrained weights for {clsname} available\")\n#                 fname = utils.download_file(urldict[\"url\"], sha256=urldict[\"sha256\"])\n#                 collections = pickle.load(open(fname, \"rb\"))\n#             else:\n#                 raise ValueError(\"Unknown weights value: \", weights)\n\n#             if isinstance(collections, tuple):\n#                 parameters, collections = collections\n#             elif \"parameters\" in collections:\n#                 parameters = collections.pop(\"parameters\")\n#             else:\n#                 raise ValueError(\n#                     \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\"\n#                 )\n\n#             x = np.empty([0, 224, 224, 3], dtype=self.dtype)\n#             # quick but dirty module initialization\n#             jax.eval_shape(self.init(rng=types.KeySeq(42)), x)\n\n#             self.set_default_parameters(parameters, collections)\n\n#     def __call__(self, x: jnp.ndarray):\n#         x = nn.Conv(\n#             64,\n#             (7, 7) if not self.lowres else (3, 3),\n#             stride=(2, 2) if not self.lowres else (1, 1),\n#             padding=\"SAME\",\n#             with_bias=False,\n#             dtype=self.dtype,\n#         )(x)\n#         x = nn.BatchNorm(decay_rate=0.9, eps=1e-5)(x)\n#         x = module.compact_module(jax.nn.relu)()(x)\n\n#         if not self.lowres:\n#             x = nn.MaxPool(\n#                 window_shape=(1, 3, 3, 1), strides=(1, 2, 2, 1), padding=\"SAME\"\n#             )(x)\n#         for i, block_size in enumerate(self.stages):\n#             for j in range(block_size):\n#                 strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n#                 x = self.block_type(64 * 2 ** i, strides=strides, dtype=self.dtype)(x)\n#         GAP = lambda x: jnp.mean(x, axis=(1, 2))\n#         x = module.compact_module(GAP)(name=\"global_average_pooling\")(x)\n#         x = nn.Linear(1000, dtype=self.dtype)(x)\n#         to_float32 = lambda x: jnp.asarray(x, jnp.float32)\n#         x = module.compact_module(to_float32)(name=\"to_float32\")(x)\n#         return x\n\n\n# class ResNet18(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[2, 2, 2, 2],\n#             block_type=ResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet34(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 6, 3],\n#             block_type=ResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet50(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 6, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet101(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 4, 23, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet152(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 8, 36, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# class ResNet200(ResNet):\n#     def __init__(\n#         self,\n#         lowres: bool = False,\n#         weights: tp.Optional[str] = None,\n#         dtype: tp.Optional[tp.Any] = jnp.float32,\n#         *args,\n#         **kwargs,\n#     ):\n#         super().__init__(\n#             stages=[3, 24, 36, 3],\n#             block_type=BottleneckResNetBlock,\n#             lowres=lowres,\n#             weights=weights,\n#             dtype=dtype,\n#             *args,\n#             **kwargs,\n#         )\n\n\n# _resnet__init___docstring = \"\"\"\n# Instantiates the {} architecture from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n\n# Arguments:\n#     lowres: Optional, whether to use the low resolution version\n#             as described in subsection 4.2 of the orignal paper.\n#             This version is better suited for datasets like CIFAR10. (Default: False)\n#     weights: One of None (random initialization), 'imagenet' (automatic download of\n#               weights pretrained on ImageNet) or a path to a weights file\n#     dtype: Optional dtype of the convolutions and linear operations,\n#            either jnp.float32 (default) or jnp.float16 for mixed precision.\n# \"\"\"\n\n# ResNet18.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet18\")\n# ResNet34.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet34\")\n# ResNet50.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet50\")\n# ResNet101.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet101\")\n# ResNet152.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet152\")\n# ResNet200.__init__.__doc__ = _resnet__init___docstring.format(\"ResNet200\")\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/batch_normalization.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/batch_normalization.py:94:8 Incompatible attribute type [8]: Attribute `mean_ema` declared in class `BatchNormalization` has type `ExponentialMovingAverage` but is used as type `module.Module`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/batch_normalization.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/batch_normalization.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/batch_normalization.py:95:8 Incompatible attribute type [8]: Attribute `var_ema` declared in class `BatchNormalization` has type `ExponentialMovingAverage` but is used as type `module.Module`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/batch_normalization.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/batch_normalization.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/batch_normalization.py:97:4 Inconsistent override [14]: `elegy.nn.batch_normalization.BatchNormalization.call` overrides method defined in `module.Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/batch_normalization.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/batch_normalization.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/batch_normalization.py:97:4 Inconsistent override [14]: `elegy.nn.batch_normalization.BatchNormalization.call` overrides method defined in `module.Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/batch_normalization.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/conv.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/conv.py:149:4 Inconsistent override [14]: `elegy.nn.conv.ConvND.call` overrides method defined in `module.Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/conv.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/conv.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/conv.py:149:4 Inconsistent override [14]: `elegy.nn.conv.ConvND.call` overrides method defined in `module.Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/conv.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/conv.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/conv.py:485:4 Inconsistent override [14]: `elegy.nn.conv.ConvNDTranspose.call` overrides method defined in `module.Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/conv.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/conv.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/conv.py:485:4 Inconsistent override [14]: `elegy.nn.conv.ConvNDTranspose.call` overrides method defined in `module.Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/conv.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/dropout.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/dropout.py:57:4 Inconsistent override [14]: `elegy.nn.dropout.Dropout.call` overrides method defined in `Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/dropout.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/dropout.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/dropout.py:57:4 Inconsistent override [14]: `elegy.nn.dropout.Dropout.call` overrides method defined in `Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/dropout.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/embedding.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/embedding.py:105:4 Inconsistent override [14]: `elegy.nn.embedding.Embedding.call` overrides method defined in `Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/embedding.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/embedding.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/embedding.py:105:4 Inconsistent override [14]: `elegy.nn.embedding.Embedding.call` overrides method defined in `Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/embedding.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/flatten.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/flatten.py:92:4 Inconsistent override [14]: `elegy.nn.flatten.Reshape.call` overrides method defined in `Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/flatten.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/flatten.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/flatten.py:92:4 Inconsistent override [14]: `elegy.nn.flatten.Reshape.call` overrides method defined in `Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/flatten.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/layer_normalization.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/layer_normalization.py:84:4 Inconsistent override [14]: `elegy.nn.layer_normalization.LayerNormalization.call` overrides method defined in `module.Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/layer_normalization.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/layer_normalization.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/layer_normalization.py:84:4 Inconsistent override [14]: `elegy.nn.layer_normalization.LayerNormalization.call` overrides method defined in `module.Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/layer_normalization.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/linear.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/linear.py:43:4 Inconsistent override [14]: `elegy.nn.linear.Linear.call` overrides method defined in `module.Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/linear.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/linear.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/linear.py:43:4 Inconsistent override [14]: `elegy.nn.linear.Linear.call` overrides method defined in `module.Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/linear.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/moving_averages.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/moving_averages.py:81:4 Inconsistent override [14]: `elegy.nn.moving_averages.ExponentialMovingAverage.call` overrides method defined in `module.Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/moving_averages.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/moving_averages.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/moving_averages.py:81:4 Inconsistent override [14]: `elegy.nn.moving_averages.ExponentialMovingAverage.call` overrides method defined in `module.Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/moving_averages.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/multi_head_attention.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/multi_head_attention.py:95:4 Inconsistent override [14]: `elegy.nn.multi_head_attention.MultiHeadAttention.call` overrides method defined in `module.Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/multi_head_attention.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/multi_head_attention.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/multi_head_attention.py:95:4 Inconsistent override [14]: `elegy.nn.multi_head_attention.MultiHeadAttention.call` overrides method defined in `module.Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/multi_head_attention.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/pool.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/pool.py:167:4 Inconsistent override [14]: `elegy.nn.pool.MaxPool.call` overrides method defined in `Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/pool.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/pool.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/pool.py:167:4 Inconsistent override [14]: `elegy.nn.pool.MaxPool.call` overrides method defined in `Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/pool.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/pool.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/pool.py:201:4 Inconsistent override [14]: `elegy.nn.pool.AvgPool.call` overrides method defined in `Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/pool.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/pool.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/pool.py:201:4 Inconsistent override [14]: `elegy.nn.pool.AvgPool.call` overrides method defined in `Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/pool.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/transformers.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/transformers.py:50:4 Inconsistent override [14]: `elegy.nn.transformers.TransformerEncoderLayer.call` overrides method defined in `Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/transformers.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/transformers.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/transformers.py:50:4 Inconsistent override [14]: `elegy.nn.transformers.TransformerEncoderLayer.call` overrides method defined in `Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/transformers.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/transformers.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/transformers.py:122:4 Inconsistent override [14]: `elegy.nn.transformers.TransformerEncoder.call` overrides method defined in `Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/transformers.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/transformers.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/transformers.py:122:4 Inconsistent override [14]: `elegy.nn.transformers.TransformerEncoder.call` overrides method defined in `Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/transformers.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/transformers.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/transformers.py:181:4 Inconsistent override [14]: `elegy.nn.transformers.TransformerDecoderLayer.call` overrides method defined in `Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/transformers.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/transformers.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/transformers.py:181:4 Inconsistent override [14]: `elegy.nn.transformers.TransformerDecoderLayer.call` overrides method defined in `Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/transformers.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/transformers.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/transformers.py:249:4 Inconsistent override [14]: `elegy.nn.transformers.TransformerDecoder.call` overrides method defined in `Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/transformers.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/transformers.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/transformers.py:249:4 Inconsistent override [14]: `elegy.nn.transformers.TransformerDecoder.call` overrides method defined in `Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/transformers.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/transformers.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/transformers.py:351:4 Inconsistent override [14]: `elegy.nn.transformers.Transformer.call` overrides method defined in `Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/transformers.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/nn/transformers.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/nn/transformers.py:351:4 Inconsistent override [14]: `elegy.nn.transformers.Transformer.call` overrides method defined in `Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/nn/transformers.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/optimizer.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/optimizer.py:93:19 Call error [29]: `tp.Optional[LRScheduler]` is not a function.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/optimizer.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/optimizer_test.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/optimizer_test.py:21:4 Inconsistent override [14]: `elegy.optimizer_test.MLP.call` overrides method defined in `elegy.module.Module` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/optimizer_test.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/optimizer_test.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/optimizer_test.py:21:4 Inconsistent override [14]: `elegy.optimizer_test.MLP.call` overrides method defined in `elegy.module.Module` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/optimizer_test.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/regularizers/global_l1l2.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/regularizers/global_l1l2.py:58:4 Inconsistent override [14]: `elegy.regularizers.global_l1l2.GlobalL1L2.call` overrides method defined in `Loss` inconsistently. Could not find parameter `Variable(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/regularizers/global_l1l2.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "elegy/regularizers/global_l1l2.py",
    "min_patch_found": false,
    "full_warning_msg": "elegy/regularizers/global_l1l2.py:58:4 Inconsistent override [14]: `elegy.regularizers.global_l1l2.GlobalL1L2.call` overrides method defined in `Loss` inconsistently. Could not find parameter `Keywords(unknown)` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/elegy/regularizers/global_l1l2.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "examples/flax/mnist_vae_test_step.py",
    "min_patch_found": false,
    "full_warning_msg": "examples/flax/mnist_vae_test_step.py:105:8 Incompatible attribute type [8]: Attribute `loss_metrics` declared in class `VariationalAutoEncoder` has type `elegy.model.model.LossMetrics` but is used as type `elegy.module.Module`.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/examples/flax/mnist_vae_test_step.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "examples/flax/mnist_vae_test_step.py",
    "min_patch_found": false,
    "full_warning_msg": "examples/flax/mnist_vae_test_step.py:147:4 Inconsistent override [14]: `examples.flax.mnist_vae_test_step.VariationalAutoEncoder.test_step` overrides method defined in `elegy.model.model.Model` inconsistently. Could not find parameter `class_weight` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/examples/flax/mnist_vae_test_step.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "examples/flax/mnist_vae_test_step.py",
    "min_patch_found": false,
    "full_warning_msg": "examples/flax/mnist_vae_test_step.py:147:4 Inconsistent override [14]: `examples.flax.mnist_vae_test_step.VariationalAutoEncoder.test_step` overrides method defined in `elegy.model.model.Model` inconsistently. Could not find parameter `sample_weight` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/examples/flax/mnist_vae_test_step.py'",
    "dd_fail": true
  },
  {
    "project": "poets-ai/elegy",
    "commit": "904f6148447cffec3638cbd8809590d0dcaaa472",
    "filename": "examples/flax/mnist_vae_test_step.py",
    "min_patch_found": false,
    "full_warning_msg": "examples/flax/mnist_vae_test_step.py:147:4 Inconsistent override [14]: `examples.flax.mnist_vae_test_step.VariationalAutoEncoder.test_step` overrides method defined in `elegy.model.model.Model` inconsistently. Could not find parameter `y_true` in overriding signature.",
    "exception": "[Errno 2] No such file or directory: '/home/chowyi/TypeAnnotation_Study/GitHub/poets-ai-elegy/examples/flax/mnist_vae_test_step.py'",
    "dd_fail": true
  }
]