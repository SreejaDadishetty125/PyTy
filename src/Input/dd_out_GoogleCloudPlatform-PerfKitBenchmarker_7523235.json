[
  {
    "project": "GoogleCloudPlatform/PerfKitBenchmarker",
    "commit": "7523235172cef003820f534a30e1d023f936be5e",
    "filename": "perfkitbenchmarker/dpb_service.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/GoogleCloudPlatform-PerfKitBenchmarker/perfkitbenchmarker/dpb_service.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "perfkitbenchmarker/dpb_service.py:174:16 Incompatible variable type [9]: jarfile is declared to have type `str` but is used as type `None`.",
    "message": " jarfile is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 174,
    "warning_line": "                jarfile: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "  def SubmitJob(self,\n                jarfile: str = None,\n                classname: str = None,\n                pyspark_file: str = None,\n                query_file: str = None,\n                job_poll_interval: float = None,\n                job_stdout_file: str = None,\n                job_arguments: List[str] = None,\n                job_files: List[str] = None,\n                job_jars: List[str] = None,\n                job_type: str = None,\n                properties: Dict[str, str] = None) -> JobResult:\n    \"\"\"Submit a data processing job to the backend.\n",
        "source_code_len": 567,
        "target_code": "  def SubmitJob(self,\n                jarfile: Optional[str] = None,\n                classname: Optional[str] = None,\n                pyspark_file: Optional[str] = None,\n                query_file: Optional[str] = None,\n                job_poll_interval: Optional[float] = None,\n                job_stdout_file: Optional[str] = None,\n                job_arguments: Optional[List[str]] = None,\n                job_files: Optional[List[str]] = None,\n                job_jars: Optional[List[str]] = None,\n                job_type: Optional[str] = None,\n                properties: Optional[Dict[str, str]] = None) -> JobResult:\n    \"\"\"Submit a data processing job to the backend.\n",
        "target_code_len": 677,
        "diff_format": "@@ -173,13 +173,13 @@\n   def SubmitJob(self,\n-                jarfile: str = None,\n-                classname: str = None,\n-                pyspark_file: str = None,\n-                query_file: str = None,\n-                job_poll_interval: float = None,\n-                job_stdout_file: str = None,\n-                job_arguments: List[str] = None,\n-                job_files: List[str] = None,\n-                job_jars: List[str] = None,\n-                job_type: str = None,\n-                properties: Dict[str, str] = None) -> JobResult:\n+                jarfile: Optional[str] = None,\n+                classname: Optional[str] = None,\n+                pyspark_file: Optional[str] = None,\n+                query_file: Optional[str] = None,\n+                job_poll_interval: Optional[float] = None,\n+                job_stdout_file: Optional[str] = None,\n+                job_arguments: Optional[List[str]] = None,\n+                job_files: Optional[List[str]] = None,\n+                job_jars: Optional[List[str]] = None,\n+                job_type: Optional[str] = None,\n+                properties: Optional[Dict[str, str]] = None) -> JobResult:\n     \"\"\"Submit a data processing job to the backend.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "  def SubmitJob(self,\n                jarfile: str = None,\n                classname: str = None,\n                pyspark_file: str = None,\n                query_file: str = None,\n                job_poll_interval: float = None,\n                job_stdout_file: str = None,\n                job_arguments: List[str] = None,\n                job_files: List[str] = None,\n                job_jars: List[str] = None,\n                job_type: str = None,\n                properties: Dict[str, str] = None) -> JobResult:\n    <IND>",
        "target_code_with_indent": "  def SubmitJob(self,\n                jarfile: Optional[str] = None,\n                classname: Optional[str] = None,\n                pyspark_file: Optional[str] = None,\n                query_file: Optional[str] = None,\n                job_poll_interval: Optional[float] = None,\n                job_stdout_file: Optional[str] = None,\n                job_arguments: Optional[List[str]] = None,\n                job_files: Optional[List[str]] = None,\n                job_jars: Optional[List[str]] = None,\n                job_type: Optional[str] = None,\n                properties: Optional[Dict[str, str]] = None) -> JobResult:\n    <IND>"
      }
    ]
  },
  {
    "project": "GoogleCloudPlatform/PerfKitBenchmarker",
    "commit": "7523235172cef003820f534a30e1d023f936be5e",
    "filename": "perfkitbenchmarker/dpb_service.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/GoogleCloudPlatform-PerfKitBenchmarker/perfkitbenchmarker/dpb_service.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "perfkitbenchmarker/dpb_service.py:175:16 Incompatible variable type [9]: classname is declared to have type `str` but is used as type `None`.",
    "message": " classname is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 175,
    "warning_line": "                classname: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "  def SubmitJob(self,\n                jarfile: str = None,\n                classname: str = None,\n                pyspark_file: str = None,\n                query_file: str = None,\n                job_poll_interval: float = None,\n                job_stdout_file: str = None,\n                job_arguments: List[str] = None,\n                job_files: List[str] = None,\n                job_jars: List[str] = None,\n                job_type: str = None,\n                properties: Dict[str, str] = None) -> JobResult:\n    \"\"\"Submit a data processing job to the backend.\n",
        "source_code_len": 567,
        "target_code": "  def SubmitJob(self,\n                jarfile: Optional[str] = None,\n                classname: Optional[str] = None,\n                pyspark_file: Optional[str] = None,\n                query_file: Optional[str] = None,\n                job_poll_interval: Optional[float] = None,\n                job_stdout_file: Optional[str] = None,\n                job_arguments: Optional[List[str]] = None,\n                job_files: Optional[List[str]] = None,\n                job_jars: Optional[List[str]] = None,\n                job_type: Optional[str] = None,\n                properties: Optional[Dict[str, str]] = None) -> JobResult:\n    \"\"\"Submit a data processing job to the backend.\n",
        "target_code_len": 677,
        "diff_format": "@@ -173,13 +173,13 @@\n   def SubmitJob(self,\n-                jarfile: str = None,\n-                classname: str = None,\n-                pyspark_file: str = None,\n-                query_file: str = None,\n-                job_poll_interval: float = None,\n-                job_stdout_file: str = None,\n-                job_arguments: List[str] = None,\n-                job_files: List[str] = None,\n-                job_jars: List[str] = None,\n-                job_type: str = None,\n-                properties: Dict[str, str] = None) -> JobResult:\n+                jarfile: Optional[str] = None,\n+                classname: Optional[str] = None,\n+                pyspark_file: Optional[str] = None,\n+                query_file: Optional[str] = None,\n+                job_poll_interval: Optional[float] = None,\n+                job_stdout_file: Optional[str] = None,\n+                job_arguments: Optional[List[str]] = None,\n+                job_files: Optional[List[str]] = None,\n+                job_jars: Optional[List[str]] = None,\n+                job_type: Optional[str] = None,\n+                properties: Optional[Dict[str, str]] = None) -> JobResult:\n     \"\"\"Submit a data processing job to the backend.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "  def SubmitJob(self,\n                jarfile: str = None,\n                classname: str = None,\n                pyspark_file: str = None,\n                query_file: str = None,\n                job_poll_interval: float = None,\n                job_stdout_file: str = None,\n                job_arguments: List[str] = None,\n                job_files: List[str] = None,\n                job_jars: List[str] = None,\n                job_type: str = None,\n                properties: Dict[str, str] = None) -> JobResult:\n    <IND>",
        "target_code_with_indent": "  def SubmitJob(self,\n                jarfile: Optional[str] = None,\n                classname: Optional[str] = None,\n                pyspark_file: Optional[str] = None,\n                query_file: Optional[str] = None,\n                job_poll_interval: Optional[float] = None,\n                job_stdout_file: Optional[str] = None,\n                job_arguments: Optional[List[str]] = None,\n                job_files: Optional[List[str]] = None,\n                job_jars: Optional[List[str]] = None,\n                job_type: Optional[str] = None,\n                properties: Optional[Dict[str, str]] = None) -> JobResult:\n    <IND>"
      }
    ]
  },
  {
    "project": "GoogleCloudPlatform/PerfKitBenchmarker",
    "commit": "7523235172cef003820f534a30e1d023f936be5e",
    "filename": "perfkitbenchmarker/dpb_service.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/GoogleCloudPlatform-PerfKitBenchmarker/perfkitbenchmarker/dpb_service.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "perfkitbenchmarker/dpb_service.py:176:16 Incompatible variable type [9]: pyspark_file is declared to have type `str` but is used as type `None`.",
    "message": " pyspark_file is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 176,
    "warning_line": "                pyspark_file: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "  def SubmitJob(self,\n                jarfile: str = None,\n                classname: str = None,\n                pyspark_file: str = None,\n                query_file: str = None,\n                job_poll_interval: float = None,\n                job_stdout_file: str = None,\n                job_arguments: List[str] = None,\n                job_files: List[str] = None,\n                job_jars: List[str] = None,\n                job_type: str = None,\n                properties: Dict[str, str] = None) -> JobResult:\n    \"\"\"Submit a data processing job to the backend.\n",
        "source_code_len": 567,
        "target_code": "  def SubmitJob(self,\n                jarfile: Optional[str] = None,\n                classname: Optional[str] = None,\n                pyspark_file: Optional[str] = None,\n                query_file: Optional[str] = None,\n                job_poll_interval: Optional[float] = None,\n                job_stdout_file: Optional[str] = None,\n                job_arguments: Optional[List[str]] = None,\n                job_files: Optional[List[str]] = None,\n                job_jars: Optional[List[str]] = None,\n                job_type: Optional[str] = None,\n                properties: Optional[Dict[str, str]] = None) -> JobResult:\n    \"\"\"Submit a data processing job to the backend.\n",
        "target_code_len": 677,
        "diff_format": "@@ -173,13 +173,13 @@\n   def SubmitJob(self,\n-                jarfile: str = None,\n-                classname: str = None,\n-                pyspark_file: str = None,\n-                query_file: str = None,\n-                job_poll_interval: float = None,\n-                job_stdout_file: str = None,\n-                job_arguments: List[str] = None,\n-                job_files: List[str] = None,\n-                job_jars: List[str] = None,\n-                job_type: str = None,\n-                properties: Dict[str, str] = None) -> JobResult:\n+                jarfile: Optional[str] = None,\n+                classname: Optional[str] = None,\n+                pyspark_file: Optional[str] = None,\n+                query_file: Optional[str] = None,\n+                job_poll_interval: Optional[float] = None,\n+                job_stdout_file: Optional[str] = None,\n+                job_arguments: Optional[List[str]] = None,\n+                job_files: Optional[List[str]] = None,\n+                job_jars: Optional[List[str]] = None,\n+                job_type: Optional[str] = None,\n+                properties: Optional[Dict[str, str]] = None) -> JobResult:\n     \"\"\"Submit a data processing job to the backend.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "  def SubmitJob(self,\n                jarfile: str = None,\n                classname: str = None,\n                pyspark_file: str = None,\n                query_file: str = None,\n                job_poll_interval: float = None,\n                job_stdout_file: str = None,\n                job_arguments: List[str] = None,\n                job_files: List[str] = None,\n                job_jars: List[str] = None,\n                job_type: str = None,\n                properties: Dict[str, str] = None) -> JobResult:\n    <IND>",
        "target_code_with_indent": "  def SubmitJob(self,\n                jarfile: Optional[str] = None,\n                classname: Optional[str] = None,\n                pyspark_file: Optional[str] = None,\n                query_file: Optional[str] = None,\n                job_poll_interval: Optional[float] = None,\n                job_stdout_file: Optional[str] = None,\n                job_arguments: Optional[List[str]] = None,\n                job_files: Optional[List[str]] = None,\n                job_jars: Optional[List[str]] = None,\n                job_type: Optional[str] = None,\n                properties: Optional[Dict[str, str]] = None) -> JobResult:\n    <IND>"
      }
    ]
  },
  {
    "project": "GoogleCloudPlatform/PerfKitBenchmarker",
    "commit": "7523235172cef003820f534a30e1d023f936be5e",
    "filename": "perfkitbenchmarker/dpb_service.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/GoogleCloudPlatform-PerfKitBenchmarker/perfkitbenchmarker/dpb_service.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "perfkitbenchmarker/dpb_service.py:177:16 Incompatible variable type [9]: query_file is declared to have type `str` but is used as type `None`.",
    "message": " query_file is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 177,
    "warning_line": "                query_file: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "  def SubmitJob(self,\n                jarfile: str = None,\n                classname: str = None,\n                pyspark_file: str = None,\n                query_file: str = None,\n                job_poll_interval: float = None,\n                job_stdout_file: str = None,\n                job_arguments: List[str] = None,\n                job_files: List[str] = None,\n                job_jars: List[str] = None,\n                job_type: str = None,\n                properties: Dict[str, str] = None) -> JobResult:\n    \"\"\"Submit a data processing job to the backend.\n",
        "source_code_len": 567,
        "target_code": "  def SubmitJob(self,\n                jarfile: Optional[str] = None,\n                classname: Optional[str] = None,\n                pyspark_file: Optional[str] = None,\n                query_file: Optional[str] = None,\n                job_poll_interval: Optional[float] = None,\n                job_stdout_file: Optional[str] = None,\n                job_arguments: Optional[List[str]] = None,\n                job_files: Optional[List[str]] = None,\n                job_jars: Optional[List[str]] = None,\n                job_type: Optional[str] = None,\n                properties: Optional[Dict[str, str]] = None) -> JobResult:\n    \"\"\"Submit a data processing job to the backend.\n",
        "target_code_len": 677,
        "diff_format": "@@ -173,13 +173,13 @@\n   def SubmitJob(self,\n-                jarfile: str = None,\n-                classname: str = None,\n-                pyspark_file: str = None,\n-                query_file: str = None,\n-                job_poll_interval: float = None,\n-                job_stdout_file: str = None,\n-                job_arguments: List[str] = None,\n-                job_files: List[str] = None,\n-                job_jars: List[str] = None,\n-                job_type: str = None,\n-                properties: Dict[str, str] = None) -> JobResult:\n+                jarfile: Optional[str] = None,\n+                classname: Optional[str] = None,\n+                pyspark_file: Optional[str] = None,\n+                query_file: Optional[str] = None,\n+                job_poll_interval: Optional[float] = None,\n+                job_stdout_file: Optional[str] = None,\n+                job_arguments: Optional[List[str]] = None,\n+                job_files: Optional[List[str]] = None,\n+                job_jars: Optional[List[str]] = None,\n+                job_type: Optional[str] = None,\n+                properties: Optional[Dict[str, str]] = None) -> JobResult:\n     \"\"\"Submit a data processing job to the backend.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "  def SubmitJob(self,\n                jarfile: str = None,\n                classname: str = None,\n                pyspark_file: str = None,\n                query_file: str = None,\n                job_poll_interval: float = None,\n                job_stdout_file: str = None,\n                job_arguments: List[str] = None,\n                job_files: List[str] = None,\n                job_jars: List[str] = None,\n                job_type: str = None,\n                properties: Dict[str, str] = None) -> JobResult:\n    <IND>",
        "target_code_with_indent": "  def SubmitJob(self,\n                jarfile: Optional[str] = None,\n                classname: Optional[str] = None,\n                pyspark_file: Optional[str] = None,\n                query_file: Optional[str] = None,\n                job_poll_interval: Optional[float] = None,\n                job_stdout_file: Optional[str] = None,\n                job_arguments: Optional[List[str]] = None,\n                job_files: Optional[List[str]] = None,\n                job_jars: Optional[List[str]] = None,\n                job_type: Optional[str] = None,\n                properties: Optional[Dict[str, str]] = None) -> JobResult:\n    <IND>"
      }
    ]
  },
  {
    "project": "GoogleCloudPlatform/PerfKitBenchmarker",
    "commit": "7523235172cef003820f534a30e1d023f936be5e",
    "filename": "perfkitbenchmarker/dpb_service.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/GoogleCloudPlatform-PerfKitBenchmarker/perfkitbenchmarker/dpb_service.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "perfkitbenchmarker/dpb_service.py:178:16 Incompatible variable type [9]: job_poll_interval is declared to have type `float` but is used as type `None`.",
    "message": " job_poll_interval is declared to have type `float` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 178,
    "warning_line": "                job_poll_interval: float = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "  def SubmitJob(self,\n                jarfile: str = None,\n                classname: str = None,\n                pyspark_file: str = None,\n                query_file: str = None,\n                job_poll_interval: float = None,\n                job_stdout_file: str = None,\n                job_arguments: List[str] = None,\n                job_files: List[str] = None,\n                job_jars: List[str] = None,\n                job_type: str = None,\n                properties: Dict[str, str] = None) -> JobResult:\n    \"\"\"Submit a data processing job to the backend.\n",
        "source_code_len": 567,
        "target_code": "  def SubmitJob(self,\n                jarfile: Optional[str] = None,\n                classname: Optional[str] = None,\n                pyspark_file: Optional[str] = None,\n                query_file: Optional[str] = None,\n                job_poll_interval: Optional[float] = None,\n                job_stdout_file: Optional[str] = None,\n                job_arguments: Optional[List[str]] = None,\n                job_files: Optional[List[str]] = None,\n                job_jars: Optional[List[str]] = None,\n                job_type: Optional[str] = None,\n                properties: Optional[Dict[str, str]] = None) -> JobResult:\n    \"\"\"Submit a data processing job to the backend.\n",
        "target_code_len": 677,
        "diff_format": "@@ -173,13 +173,13 @@\n   def SubmitJob(self,\n-                jarfile: str = None,\n-                classname: str = None,\n-                pyspark_file: str = None,\n-                query_file: str = None,\n-                job_poll_interval: float = None,\n-                job_stdout_file: str = None,\n-                job_arguments: List[str] = None,\n-                job_files: List[str] = None,\n-                job_jars: List[str] = None,\n-                job_type: str = None,\n-                properties: Dict[str, str] = None) -> JobResult:\n+                jarfile: Optional[str] = None,\n+                classname: Optional[str] = None,\n+                pyspark_file: Optional[str] = None,\n+                query_file: Optional[str] = None,\n+                job_poll_interval: Optional[float] = None,\n+                job_stdout_file: Optional[str] = None,\n+                job_arguments: Optional[List[str]] = None,\n+                job_files: Optional[List[str]] = None,\n+                job_jars: Optional[List[str]] = None,\n+                job_type: Optional[str] = None,\n+                properties: Optional[Dict[str, str]] = None) -> JobResult:\n     \"\"\"Submit a data processing job to the backend.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "  def SubmitJob(self,\n                jarfile: str = None,\n                classname: str = None,\n                pyspark_file: str = None,\n                query_file: str = None,\n                job_poll_interval: float = None,\n                job_stdout_file: str = None,\n                job_arguments: List[str] = None,\n                job_files: List[str] = None,\n                job_jars: List[str] = None,\n                job_type: str = None,\n                properties: Dict[str, str] = None) -> JobResult:\n    <IND>",
        "target_code_with_indent": "  def SubmitJob(self,\n                jarfile: Optional[str] = None,\n                classname: Optional[str] = None,\n                pyspark_file: Optional[str] = None,\n                query_file: Optional[str] = None,\n                job_poll_interval: Optional[float] = None,\n                job_stdout_file: Optional[str] = None,\n                job_arguments: Optional[List[str]] = None,\n                job_files: Optional[List[str]] = None,\n                job_jars: Optional[List[str]] = None,\n                job_type: Optional[str] = None,\n                properties: Optional[Dict[str, str]] = None) -> JobResult:\n    <IND>"
      }
    ]
  },
  {
    "project": "GoogleCloudPlatform/PerfKitBenchmarker",
    "commit": "7523235172cef003820f534a30e1d023f936be5e",
    "filename": "perfkitbenchmarker/dpb_service.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/GoogleCloudPlatform-PerfKitBenchmarker/perfkitbenchmarker/dpb_service.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "perfkitbenchmarker/dpb_service.py:179:16 Incompatible variable type [9]: job_stdout_file is declared to have type `str` but is used as type `None`.",
    "message": " job_stdout_file is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 179,
    "warning_line": "                job_stdout_file: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "  def SubmitJob(self,\n                jarfile: str = None,\n                classname: str = None,\n                pyspark_file: str = None,\n                query_file: str = None,\n                job_poll_interval: float = None,\n                job_stdout_file: str = None,\n                job_arguments: List[str] = None,\n                job_files: List[str] = None,\n                job_jars: List[str] = None,\n                job_type: str = None,\n                properties: Dict[str, str] = None) -> JobResult:\n    \"\"\"Submit a data processing job to the backend.\n",
        "source_code_len": 567,
        "target_code": "  def SubmitJob(self,\n                jarfile: Optional[str] = None,\n                classname: Optional[str] = None,\n                pyspark_file: Optional[str] = None,\n                query_file: Optional[str] = None,\n                job_poll_interval: Optional[float] = None,\n                job_stdout_file: Optional[str] = None,\n                job_arguments: Optional[List[str]] = None,\n                job_files: Optional[List[str]] = None,\n                job_jars: Optional[List[str]] = None,\n                job_type: Optional[str] = None,\n                properties: Optional[Dict[str, str]] = None) -> JobResult:\n    \"\"\"Submit a data processing job to the backend.\n",
        "target_code_len": 677,
        "diff_format": "@@ -173,13 +173,13 @@\n   def SubmitJob(self,\n-                jarfile: str = None,\n-                classname: str = None,\n-                pyspark_file: str = None,\n-                query_file: str = None,\n-                job_poll_interval: float = None,\n-                job_stdout_file: str = None,\n-                job_arguments: List[str] = None,\n-                job_files: List[str] = None,\n-                job_jars: List[str] = None,\n-                job_type: str = None,\n-                properties: Dict[str, str] = None) -> JobResult:\n+                jarfile: Optional[str] = None,\n+                classname: Optional[str] = None,\n+                pyspark_file: Optional[str] = None,\n+                query_file: Optional[str] = None,\n+                job_poll_interval: Optional[float] = None,\n+                job_stdout_file: Optional[str] = None,\n+                job_arguments: Optional[List[str]] = None,\n+                job_files: Optional[List[str]] = None,\n+                job_jars: Optional[List[str]] = None,\n+                job_type: Optional[str] = None,\n+                properties: Optional[Dict[str, str]] = None) -> JobResult:\n     \"\"\"Submit a data processing job to the backend.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "  def SubmitJob(self,\n                jarfile: str = None,\n                classname: str = None,\n                pyspark_file: str = None,\n                query_file: str = None,\n                job_poll_interval: float = None,\n                job_stdout_file: str = None,\n                job_arguments: List[str] = None,\n                job_files: List[str] = None,\n                job_jars: List[str] = None,\n                job_type: str = None,\n                properties: Dict[str, str] = None) -> JobResult:\n    <IND>",
        "target_code_with_indent": "  def SubmitJob(self,\n                jarfile: Optional[str] = None,\n                classname: Optional[str] = None,\n                pyspark_file: Optional[str] = None,\n                query_file: Optional[str] = None,\n                job_poll_interval: Optional[float] = None,\n                job_stdout_file: Optional[str] = None,\n                job_arguments: Optional[List[str]] = None,\n                job_files: Optional[List[str]] = None,\n                job_jars: Optional[List[str]] = None,\n                job_type: Optional[str] = None,\n                properties: Optional[Dict[str, str]] = None) -> JobResult:\n    <IND>"
      }
    ]
  },
  {
    "project": "GoogleCloudPlatform/PerfKitBenchmarker",
    "commit": "7523235172cef003820f534a30e1d023f936be5e",
    "filename": "perfkitbenchmarker/dpb_service.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/GoogleCloudPlatform-PerfKitBenchmarker/perfkitbenchmarker/dpb_service.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "perfkitbenchmarker/dpb_service.py:180:16 Incompatible variable type [9]: job_arguments is declared to have type `List[str]` but is used as type `None`.",
    "message": " job_arguments is declared to have type `List[str]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 180,
    "warning_line": "                job_arguments: List[str] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "  def SubmitJob(self,\n                jarfile: str = None,\n                classname: str = None,\n                pyspark_file: str = None,\n                query_file: str = None,\n                job_poll_interval: float = None,\n                job_stdout_file: str = None,\n                job_arguments: List[str] = None,\n                job_files: List[str] = None,\n                job_jars: List[str] = None,\n                job_type: str = None,\n                properties: Dict[str, str] = None) -> JobResult:\n    \"\"\"Submit a data processing job to the backend.\n",
        "source_code_len": 567,
        "target_code": "  def SubmitJob(self,\n                jarfile: Optional[str] = None,\n                classname: Optional[str] = None,\n                pyspark_file: Optional[str] = None,\n                query_file: Optional[str] = None,\n                job_poll_interval: Optional[float] = None,\n                job_stdout_file: Optional[str] = None,\n                job_arguments: Optional[List[str]] = None,\n                job_files: Optional[List[str]] = None,\n                job_jars: Optional[List[str]] = None,\n                job_type: Optional[str] = None,\n                properties: Optional[Dict[str, str]] = None) -> JobResult:\n    \"\"\"Submit a data processing job to the backend.\n",
        "target_code_len": 677,
        "diff_format": "@@ -173,13 +173,13 @@\n   def SubmitJob(self,\n-                jarfile: str = None,\n-                classname: str = None,\n-                pyspark_file: str = None,\n-                query_file: str = None,\n-                job_poll_interval: float = None,\n-                job_stdout_file: str = None,\n-                job_arguments: List[str] = None,\n-                job_files: List[str] = None,\n-                job_jars: List[str] = None,\n-                job_type: str = None,\n-                properties: Dict[str, str] = None) -> JobResult:\n+                jarfile: Optional[str] = None,\n+                classname: Optional[str] = None,\n+                pyspark_file: Optional[str] = None,\n+                query_file: Optional[str] = None,\n+                job_poll_interval: Optional[float] = None,\n+                job_stdout_file: Optional[str] = None,\n+                job_arguments: Optional[List[str]] = None,\n+                job_files: Optional[List[str]] = None,\n+                job_jars: Optional[List[str]] = None,\n+                job_type: Optional[str] = None,\n+                properties: Optional[Dict[str, str]] = None) -> JobResult:\n     \"\"\"Submit a data processing job to the backend.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "  def SubmitJob(self,\n                jarfile: str = None,\n                classname: str = None,\n                pyspark_file: str = None,\n                query_file: str = None,\n                job_poll_interval: float = None,\n                job_stdout_file: str = None,\n                job_arguments: List[str] = None,\n                job_files: List[str] = None,\n                job_jars: List[str] = None,\n                job_type: str = None,\n                properties: Dict[str, str] = None) -> JobResult:\n    <IND>",
        "target_code_with_indent": "  def SubmitJob(self,\n                jarfile: Optional[str] = None,\n                classname: Optional[str] = None,\n                pyspark_file: Optional[str] = None,\n                query_file: Optional[str] = None,\n                job_poll_interval: Optional[float] = None,\n                job_stdout_file: Optional[str] = None,\n                job_arguments: Optional[List[str]] = None,\n                job_files: Optional[List[str]] = None,\n                job_jars: Optional[List[str]] = None,\n                job_type: Optional[str] = None,\n                properties: Optional[Dict[str, str]] = None) -> JobResult:\n    <IND>"
      }
    ]
  },
  {
    "project": "GoogleCloudPlatform/PerfKitBenchmarker",
    "commit": "7523235172cef003820f534a30e1d023f936be5e",
    "filename": "perfkitbenchmarker/dpb_service.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/GoogleCloudPlatform-PerfKitBenchmarker/perfkitbenchmarker/dpb_service.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "perfkitbenchmarker/dpb_service.py:181:16 Incompatible variable type [9]: job_files is declared to have type `List[str]` but is used as type `None`.",
    "message": " job_files is declared to have type `List[str]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 181,
    "warning_line": "                job_files: List[str] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "  def SubmitJob(self,\n                jarfile: str = None,\n                classname: str = None,\n                pyspark_file: str = None,\n                query_file: str = None,\n                job_poll_interval: float = None,\n                job_stdout_file: str = None,\n                job_arguments: List[str] = None,\n                job_files: List[str] = None,\n                job_jars: List[str] = None,\n                job_type: str = None,\n                properties: Dict[str, str] = None) -> JobResult:\n    \"\"\"Submit a data processing job to the backend.\n",
        "source_code_len": 567,
        "target_code": "  def SubmitJob(self,\n                jarfile: Optional[str] = None,\n                classname: Optional[str] = None,\n                pyspark_file: Optional[str] = None,\n                query_file: Optional[str] = None,\n                job_poll_interval: Optional[float] = None,\n                job_stdout_file: Optional[str] = None,\n                job_arguments: Optional[List[str]] = None,\n                job_files: Optional[List[str]] = None,\n                job_jars: Optional[List[str]] = None,\n                job_type: Optional[str] = None,\n                properties: Optional[Dict[str, str]] = None) -> JobResult:\n    \"\"\"Submit a data processing job to the backend.\n",
        "target_code_len": 677,
        "diff_format": "@@ -173,13 +173,13 @@\n   def SubmitJob(self,\n-                jarfile: str = None,\n-                classname: str = None,\n-                pyspark_file: str = None,\n-                query_file: str = None,\n-                job_poll_interval: float = None,\n-                job_stdout_file: str = None,\n-                job_arguments: List[str] = None,\n-                job_files: List[str] = None,\n-                job_jars: List[str] = None,\n-                job_type: str = None,\n-                properties: Dict[str, str] = None) -> JobResult:\n+                jarfile: Optional[str] = None,\n+                classname: Optional[str] = None,\n+                pyspark_file: Optional[str] = None,\n+                query_file: Optional[str] = None,\n+                job_poll_interval: Optional[float] = None,\n+                job_stdout_file: Optional[str] = None,\n+                job_arguments: Optional[List[str]] = None,\n+                job_files: Optional[List[str]] = None,\n+                job_jars: Optional[List[str]] = None,\n+                job_type: Optional[str] = None,\n+                properties: Optional[Dict[str, str]] = None) -> JobResult:\n     \"\"\"Submit a data processing job to the backend.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "  def SubmitJob(self,\n                jarfile: str = None,\n                classname: str = None,\n                pyspark_file: str = None,\n                query_file: str = None,\n                job_poll_interval: float = None,\n                job_stdout_file: str = None,\n                job_arguments: List[str] = None,\n                job_files: List[str] = None,\n                job_jars: List[str] = None,\n                job_type: str = None,\n                properties: Dict[str, str] = None) -> JobResult:\n    <IND>",
        "target_code_with_indent": "  def SubmitJob(self,\n                jarfile: Optional[str] = None,\n                classname: Optional[str] = None,\n                pyspark_file: Optional[str] = None,\n                query_file: Optional[str] = None,\n                job_poll_interval: Optional[float] = None,\n                job_stdout_file: Optional[str] = None,\n                job_arguments: Optional[List[str]] = None,\n                job_files: Optional[List[str]] = None,\n                job_jars: Optional[List[str]] = None,\n                job_type: Optional[str] = None,\n                properties: Optional[Dict[str, str]] = None) -> JobResult:\n    <IND>"
      }
    ]
  },
  {
    "project": "GoogleCloudPlatform/PerfKitBenchmarker",
    "commit": "7523235172cef003820f534a30e1d023f936be5e",
    "filename": "perfkitbenchmarker/dpb_service.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/GoogleCloudPlatform-PerfKitBenchmarker/perfkitbenchmarker/dpb_service.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "perfkitbenchmarker/dpb_service.py:182:16 Incompatible variable type [9]: job_jars is declared to have type `List[str]` but is used as type `None`.",
    "message": " job_jars is declared to have type `List[str]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 182,
    "warning_line": "                job_jars: List[str] = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "  def SubmitJob(self,\n                jarfile: str = None,\n                classname: str = None,\n                pyspark_file: str = None,\n                query_file: str = None,\n                job_poll_interval: float = None,\n                job_stdout_file: str = None,\n                job_arguments: List[str] = None,\n                job_files: List[str] = None,\n                job_jars: List[str] = None,\n                job_type: str = None,\n                properties: Dict[str, str] = None) -> JobResult:\n    \"\"\"Submit a data processing job to the backend.\n",
        "source_code_len": 567,
        "target_code": "  def SubmitJob(self,\n                jarfile: Optional[str] = None,\n                classname: Optional[str] = None,\n                pyspark_file: Optional[str] = None,\n                query_file: Optional[str] = None,\n                job_poll_interval: Optional[float] = None,\n                job_stdout_file: Optional[str] = None,\n                job_arguments: Optional[List[str]] = None,\n                job_files: Optional[List[str]] = None,\n                job_jars: Optional[List[str]] = None,\n                job_type: Optional[str] = None,\n                properties: Optional[Dict[str, str]] = None) -> JobResult:\n    \"\"\"Submit a data processing job to the backend.\n",
        "target_code_len": 677,
        "diff_format": "@@ -173,13 +173,13 @@\n   def SubmitJob(self,\n-                jarfile: str = None,\n-                classname: str = None,\n-                pyspark_file: str = None,\n-                query_file: str = None,\n-                job_poll_interval: float = None,\n-                job_stdout_file: str = None,\n-                job_arguments: List[str] = None,\n-                job_files: List[str] = None,\n-                job_jars: List[str] = None,\n-                job_type: str = None,\n-                properties: Dict[str, str] = None) -> JobResult:\n+                jarfile: Optional[str] = None,\n+                classname: Optional[str] = None,\n+                pyspark_file: Optional[str] = None,\n+                query_file: Optional[str] = None,\n+                job_poll_interval: Optional[float] = None,\n+                job_stdout_file: Optional[str] = None,\n+                job_arguments: Optional[List[str]] = None,\n+                job_files: Optional[List[str]] = None,\n+                job_jars: Optional[List[str]] = None,\n+                job_type: Optional[str] = None,\n+                properties: Optional[Dict[str, str]] = None) -> JobResult:\n     \"\"\"Submit a data processing job to the backend.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "  def SubmitJob(self,\n                jarfile: str = None,\n                classname: str = None,\n                pyspark_file: str = None,\n                query_file: str = None,\n                job_poll_interval: float = None,\n                job_stdout_file: str = None,\n                job_arguments: List[str] = None,\n                job_files: List[str] = None,\n                job_jars: List[str] = None,\n                job_type: str = None,\n                properties: Dict[str, str] = None) -> JobResult:\n    <IND>",
        "target_code_with_indent": "  def SubmitJob(self,\n                jarfile: Optional[str] = None,\n                classname: Optional[str] = None,\n                pyspark_file: Optional[str] = None,\n                query_file: Optional[str] = None,\n                job_poll_interval: Optional[float] = None,\n                job_stdout_file: Optional[str] = None,\n                job_arguments: Optional[List[str]] = None,\n                job_files: Optional[List[str]] = None,\n                job_jars: Optional[List[str]] = None,\n                job_type: Optional[str] = None,\n                properties: Optional[Dict[str, str]] = None) -> JobResult:\n    <IND>"
      }
    ]
  },
  {
    "project": "GoogleCloudPlatform/PerfKitBenchmarker",
    "commit": "7523235172cef003820f534a30e1d023f936be5e",
    "filename": "perfkitbenchmarker/dpb_service.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/GoogleCloudPlatform-PerfKitBenchmarker/perfkitbenchmarker/dpb_service.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "perfkitbenchmarker/dpb_service.py:183:16 Incompatible variable type [9]: job_type is declared to have type `str` but is used as type `None`.",
    "message": " job_type is declared to have type `str` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 183,
    "warning_line": "                job_type: str = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "  def SubmitJob(self,\n                jarfile: str = None,\n                classname: str = None,\n                pyspark_file: str = None,\n                query_file: str = None,\n                job_poll_interval: float = None,\n                job_stdout_file: str = None,\n                job_arguments: List[str] = None,\n                job_files: List[str] = None,\n                job_jars: List[str] = None,\n                job_type: str = None,\n                properties: Dict[str, str] = None) -> JobResult:\n    \"\"\"Submit a data processing job to the backend.\n",
        "source_code_len": 567,
        "target_code": "  def SubmitJob(self,\n                jarfile: Optional[str] = None,\n                classname: Optional[str] = None,\n                pyspark_file: Optional[str] = None,\n                query_file: Optional[str] = None,\n                job_poll_interval: Optional[float] = None,\n                job_stdout_file: Optional[str] = None,\n                job_arguments: Optional[List[str]] = None,\n                job_files: Optional[List[str]] = None,\n                job_jars: Optional[List[str]] = None,\n                job_type: Optional[str] = None,\n                properties: Optional[Dict[str, str]] = None) -> JobResult:\n    \"\"\"Submit a data processing job to the backend.\n",
        "target_code_len": 677,
        "diff_format": "@@ -173,13 +173,13 @@\n   def SubmitJob(self,\n-                jarfile: str = None,\n-                classname: str = None,\n-                pyspark_file: str = None,\n-                query_file: str = None,\n-                job_poll_interval: float = None,\n-                job_stdout_file: str = None,\n-                job_arguments: List[str] = None,\n-                job_files: List[str] = None,\n-                job_jars: List[str] = None,\n-                job_type: str = None,\n-                properties: Dict[str, str] = None) -> JobResult:\n+                jarfile: Optional[str] = None,\n+                classname: Optional[str] = None,\n+                pyspark_file: Optional[str] = None,\n+                query_file: Optional[str] = None,\n+                job_poll_interval: Optional[float] = None,\n+                job_stdout_file: Optional[str] = None,\n+                job_arguments: Optional[List[str]] = None,\n+                job_files: Optional[List[str]] = None,\n+                job_jars: Optional[List[str]] = None,\n+                job_type: Optional[str] = None,\n+                properties: Optional[Dict[str, str]] = None) -> JobResult:\n     \"\"\"Submit a data processing job to the backend.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "  def SubmitJob(self,\n                jarfile: str = None,\n                classname: str = None,\n                pyspark_file: str = None,\n                query_file: str = None,\n                job_poll_interval: float = None,\n                job_stdout_file: str = None,\n                job_arguments: List[str] = None,\n                job_files: List[str] = None,\n                job_jars: List[str] = None,\n                job_type: str = None,\n                properties: Dict[str, str] = None) -> JobResult:\n    <IND>",
        "target_code_with_indent": "  def SubmitJob(self,\n                jarfile: Optional[str] = None,\n                classname: Optional[str] = None,\n                pyspark_file: Optional[str] = None,\n                query_file: Optional[str] = None,\n                job_poll_interval: Optional[float] = None,\n                job_stdout_file: Optional[str] = None,\n                job_arguments: Optional[List[str]] = None,\n                job_files: Optional[List[str]] = None,\n                job_jars: Optional[List[str]] = None,\n                job_type: Optional[str] = None,\n                properties: Optional[Dict[str, str]] = None) -> JobResult:\n    <IND>"
      }
    ]
  },
  {
    "project": "GoogleCloudPlatform/PerfKitBenchmarker",
    "commit": "7523235172cef003820f534a30e1d023f936be5e",
    "filename": "perfkitbenchmarker/dpb_service.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/GoogleCloudPlatform-PerfKitBenchmarker/perfkitbenchmarker/dpb_service.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "perfkitbenchmarker/dpb_service.py:184:16 Incompatible variable type [9]: properties is declared to have type `Dict[str, str]` but is used as type `None`.",
    "message": " properties is declared to have type `Dict[str, str]` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 184,
    "warning_line": "                properties: Dict[str, str] = None) -> JobResult:",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "  def SubmitJob(self,\n                jarfile: str = None,\n                classname: str = None,\n                pyspark_file: str = None,\n                query_file: str = None,\n                job_poll_interval: float = None,\n                job_stdout_file: str = None,\n                job_arguments: List[str] = None,\n                job_files: List[str] = None,\n                job_jars: List[str] = None,\n                job_type: str = None,\n                properties: Dict[str, str] = None) -> JobResult:\n    \"\"\"Submit a data processing job to the backend.\n",
        "source_code_len": 567,
        "target_code": "  def SubmitJob(self,\n                jarfile: Optional[str] = None,\n                classname: Optional[str] = None,\n                pyspark_file: Optional[str] = None,\n                query_file: Optional[str] = None,\n                job_poll_interval: Optional[float] = None,\n                job_stdout_file: Optional[str] = None,\n                job_arguments: Optional[List[str]] = None,\n                job_files: Optional[List[str]] = None,\n                job_jars: Optional[List[str]] = None,\n                job_type: Optional[str] = None,\n                properties: Optional[Dict[str, str]] = None) -> JobResult:\n    \"\"\"Submit a data processing job to the backend.\n",
        "target_code_len": 677,
        "diff_format": "@@ -173,13 +173,13 @@\n   def SubmitJob(self,\n-                jarfile: str = None,\n-                classname: str = None,\n-                pyspark_file: str = None,\n-                query_file: str = None,\n-                job_poll_interval: float = None,\n-                job_stdout_file: str = None,\n-                job_arguments: List[str] = None,\n-                job_files: List[str] = None,\n-                job_jars: List[str] = None,\n-                job_type: str = None,\n-                properties: Dict[str, str] = None) -> JobResult:\n+                jarfile: Optional[str] = None,\n+                classname: Optional[str] = None,\n+                pyspark_file: Optional[str] = None,\n+                query_file: Optional[str] = None,\n+                job_poll_interval: Optional[float] = None,\n+                job_stdout_file: Optional[str] = None,\n+                job_arguments: Optional[List[str]] = None,\n+                job_files: Optional[List[str]] = None,\n+                job_jars: Optional[List[str]] = None,\n+                job_type: Optional[str] = None,\n+                properties: Optional[Dict[str, str]] = None) -> JobResult:\n     \"\"\"Submit a data processing job to the backend.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "  def SubmitJob(self,\n                jarfile: str = None,\n                classname: str = None,\n                pyspark_file: str = None,\n                query_file: str = None,\n                job_poll_interval: float = None,\n                job_stdout_file: str = None,\n                job_arguments: List[str] = None,\n                job_files: List[str] = None,\n                job_jars: List[str] = None,\n                job_type: str = None,\n                properties: Dict[str, str] = None) -> JobResult:\n    <IND>",
        "target_code_with_indent": "  def SubmitJob(self,\n                jarfile: Optional[str] = None,\n                classname: Optional[str] = None,\n                pyspark_file: Optional[str] = None,\n                query_file: Optional[str] = None,\n                job_poll_interval: Optional[float] = None,\n                job_stdout_file: Optional[str] = None,\n                job_arguments: Optional[List[str]] = None,\n                job_files: Optional[List[str]] = None,\n                job_jars: Optional[List[str]] = None,\n                job_type: Optional[str] = None,\n                properties: Optional[Dict[str, str]] = None) -> JobResult:\n    <IND>"
      }
    ]
  },
  {
    "project": "GoogleCloudPlatform/PerfKitBenchmarker",
    "commit": "7523235172cef003820f534a30e1d023f936be5e",
    "filename": "perfkitbenchmarker/dpb_service.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/GoogleCloudPlatform-PerfKitBenchmarker/perfkitbenchmarker/dpb_service.py",
    "file_hunks_size": 6,
    "min_patch_found": true,
    "single_hunk": true,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "perfkitbenchmarker/dpb_service.py:266:8 Incompatible parameter type [6]: Expected `Dict[str, str]` for 4th parameter `properties` to call `BaseDpbService.SubmitJob` but got `Optional[Dict[str, str]]`.",
    "message": " Expected `Dict[str, str]` for 4th parameter `properties` to call `BaseDpbService.SubmitJob` but got `Optional[Dict[str, str]]`.",
    "rule_id": "Incompatible parameter type [6]",
    "warning_line_no": 266,
    "warning_line": "        properties=properties)",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "  def SubmitJob(self,\n                jarfile: str = None,\n                classname: str = None,\n                pyspark_file: str = None,\n                query_file: str = None,\n                job_poll_interval: float = None,\n                job_stdout_file: str = None,\n                job_arguments: List[str] = None,\n                job_files: List[str] = None,\n                job_jars: List[str] = None,\n                job_type: str = None,\n                properties: Dict[str, str] = None) -> JobResult:\n    \"\"\"Submit a data processing job to the backend.\n",
        "source_code_len": 567,
        "target_code": "  def SubmitJob(self,\n                jarfile: Optional[str] = None,\n                classname: Optional[str] = None,\n                pyspark_file: Optional[str] = None,\n                query_file: Optional[str] = None,\n                job_poll_interval: Optional[float] = None,\n                job_stdout_file: Optional[str] = None,\n                job_arguments: Optional[List[str]] = None,\n                job_files: Optional[List[str]] = None,\n                job_jars: Optional[List[str]] = None,\n                job_type: Optional[str] = None,\n                properties: Optional[Dict[str, str]] = None) -> JobResult:\n    \"\"\"Submit a data processing job to the backend.\n",
        "target_code_len": 677,
        "diff_format": "@@ -173,13 +173,13 @@\n   def SubmitJob(self,\n-                jarfile: str = None,\n-                classname: str = None,\n-                pyspark_file: str = None,\n-                query_file: str = None,\n-                job_poll_interval: float = None,\n-                job_stdout_file: str = None,\n-                job_arguments: List[str] = None,\n-                job_files: List[str] = None,\n-                job_jars: List[str] = None,\n-                job_type: str = None,\n-                properties: Dict[str, str] = None) -> JobResult:\n+                jarfile: Optional[str] = None,\n+                classname: Optional[str] = None,\n+                pyspark_file: Optional[str] = None,\n+                query_file: Optional[str] = None,\n+                job_poll_interval: Optional[float] = None,\n+                job_stdout_file: Optional[str] = None,\n+                job_arguments: Optional[List[str]] = None,\n+                job_files: Optional[List[str]] = None,\n+                job_jars: Optional[List[str]] = None,\n+                job_type: Optional[str] = None,\n+                properties: Optional[Dict[str, str]] = None) -> JobResult:\n     \"\"\"Submit a data processing job to the backend.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "  def SubmitJob(self,\n                jarfile: str = None,\n                classname: str = None,\n                pyspark_file: str = None,\n                query_file: str = None,\n                job_poll_interval: float = None,\n                job_stdout_file: str = None,\n                job_arguments: List[str] = None,\n                job_files: List[str] = None,\n                job_jars: List[str] = None,\n                job_type: str = None,\n                properties: Dict[str, str] = None) -> JobResult:\n    <IND>",
        "target_code_with_indent": "  def SubmitJob(self,\n                jarfile: Optional[str] = None,\n                classname: Optional[str] = None,\n                pyspark_file: Optional[str] = None,\n                query_file: Optional[str] = None,\n                job_poll_interval: Optional[float] = None,\n                job_stdout_file: Optional[str] = None,\n                job_arguments: Optional[List[str]] = None,\n                job_files: Optional[List[str]] = None,\n                job_jars: Optional[List[str]] = None,\n                job_type: Optional[str] = None,\n                properties: Optional[Dict[str, str]] = None) -> JobResult:\n    <IND>"
      }
    ]
  },
  {
    "project": "GoogleCloudPlatform/PerfKitBenchmarker",
    "commit": "7523235172cef003820f534a30e1d023f936be5e",
    "filename": "perfkitbenchmarker/edw_service.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/GoogleCloudPlatform-PerfKitBenchmarker/perfkitbenchmarker/edw_service.py",
    "file_hunks_size": 2,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "perfkitbenchmarker/edw_service.py:142:47 Invalid type [31]: Expression `(float, typing.Dict[(str, str)])` is not a valid type.",
    "message": " Expression `(float, typing.Dict[(str, str)])` is not a valid type.",
    "rule_id": "Invalid type [31]",
    "warning_line_no": 142,
    "warning_line": "  def ExecuteQuery(self, query_name: Text) -> (float, Dict[str, str]):",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "import os\nfrom typing import Dict, List, Text\n\n",
        "source_code_len": 47,
        "target_code": "import os\nfrom typing import Dict, List, Text, Tuple\n\n",
        "target_code_len": 54,
        "diff_format": "@@ -20,3 +20,3 @@\n import os\n-from typing import Dict, List, Text\n+from typing import Dict, List, Text, Tuple\n \n",
        "source_code_with_indent": "import os\nfrom typing import Dict, List, Text\n\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "import os\nfrom typing import Dict, List, Text, Tuple\n\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": true,
        "inside_window": true,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "\n  def ExecuteQuery(self, query_name: Text) -> (float, Dict[str, str]):\n    \"\"\"Executes a query and returns performance details.\n",
        "source_code_len": 129,
        "target_code": "\n  def ExecuteQuery(self, query_name: Text) -> Tuple[float, Dict[str, str]]:\n    \"\"\"Executes a query and returns performance details.\n",
        "target_code_len": 134,
        "diff_format": "@@ -141,3 +141,3 @@\n \n-  def ExecuteQuery(self, query_name: Text) -> (float, Dict[str, str]):\n+  def ExecuteQuery(self, query_name: Text) -> Tuple[float, Dict[str, str]]:\n     \"\"\"Executes a query and returns performance details.\n",
        "source_code_with_indent_exact_match": false,
        "target_code_with_indent_exact_match": false,
        "source_code_with_indent": "\n  <DED>def ExecuteQuery(self, query_name: Text) -> (float, Dict[str, str]):\n    <IND>",
        "target_code_with_indent": "\n  <DED>def ExecuteQuery(self, query_name: Text) -> Tuple[float, Dict[str, str]]:\n    <IND>"
      }
    ]
  }
]