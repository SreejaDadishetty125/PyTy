[
  {
    "project": "joeynmt/joeynmt",
    "commit": "fd7d032db5e88eb0efeab465460ebc0b673eb58d",
    "filename": "joeynmt/data.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/joeynmt-joeynmt/joeynmt/data.py",
    "file_hunks_size": 4,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "joeynmt/data.py:118:17 Incompatible variable type [9]: shuffle is declared to have type `bool` but is used as type `None`.",
    "message": " shuffle is declared to have type `bool` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 118,
    "warning_line": "                 shuffle: bool = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\nclass MyIterator(Iterator):\n    \"\"\"\n    We subclass the torchtext iterator so that we can make batches based\n    on the number of sentences or the total number of tokens.\n    \"\"\"\n\n    def __init__(self,\n                 dataset: Dataset,\n                 batch_size: int,\n                 sort_key: object = None,\n                 device: object = None,\n                 batch_size_fn: object = None,\n                 train: bool = True,\n                 repeat: bool = False,\n                 shuffle: bool = None,\n                 sort: bool = None,\n                 sort_within_batch: bool = None,\n                 num_sort_batches: int = 100):\n        super(MyIterator, self).__init__(\n            dataset, batch_size,\n            sort_key=sort_key, device=device, batch_size_fn=batch_size_fn,\n            train=train, repeat=repeat, shuffle=shuffle, sort=sort,\n            sort_within_batch=sort_within_batch)\n\n        self.num_sort_batches = num_sort_batches\n        self.batches = []\n\n    def create_batches(self):\n        if self.train:\n            def pool(d, random_shuffler):\n                for p in data.batch(d, self.batch_size * self.num_sort_batches):\n                    p_batch = data.batch(\n                        sorted(p, key=self.sort_key),\n                        self.batch_size, self.batch_size_fn)\n                    for b in random_shuffler(list(p_batch)):\n                        yield b\n\n            self.batches = pool(self.data(), self.random_shuffler)\n\n        else:\n            self.batches = []\n            for b in data.batch(self.data(), self.batch_size,\n                                self.batch_size_fn):\n                self.batches.append(sorted(b, key=self.sort_key))\n\n# pylint: disable=global-at-module-level\n",
        "source_code_len": 1755,
        "target_code": "\n# pylint: disable=global-at-module-level\n",
        "target_code_len": 42,
        "diff_format": "@@ -103,47 +103,2 @@\n \n-class MyIterator(Iterator):\n-    \"\"\"\n-    We subclass the torchtext iterator so that we can make batches based\n-    on the number of sentences or the total number of tokens.\n-    \"\"\"\n-\n-    def __init__(self,\n-                 dataset: Dataset,\n-                 batch_size: int,\n-                 sort_key: object = None,\n-                 device: object = None,\n-                 batch_size_fn: object = None,\n-                 train: bool = True,\n-                 repeat: bool = False,\n-                 shuffle: bool = None,\n-                 sort: bool = None,\n-                 sort_within_batch: bool = None,\n-                 num_sort_batches: int = 100):\n-        super(MyIterator, self).__init__(\n-            dataset, batch_size,\n-            sort_key=sort_key, device=device, batch_size_fn=batch_size_fn,\n-            train=train, repeat=repeat, shuffle=shuffle, sort=sort,\n-            sort_within_batch=sort_within_batch)\n-\n-        self.num_sort_batches = num_sort_batches\n-        self.batches = []\n-\n-    def create_batches(self):\n-        if self.train:\n-            def pool(d, random_shuffler):\n-                for p in data.batch(d, self.batch_size * self.num_sort_batches):\n-                    p_batch = data.batch(\n-                        sorted(p, key=self.sort_key),\n-                        self.batch_size, self.batch_size_fn)\n-                    for b in random_shuffler(list(p_batch)):\n-                        yield b\n-\n-            self.batches = pool(self.data(), self.random_shuffler)\n-\n-        else:\n-            self.batches = []\n-            for b in data.batch(self.data(), self.batch_size,\n-                                self.batch_size_fn):\n-                self.batches.append(sorted(b, key=self.sort_key))\n-\n # pylint: disable=global-at-module-level\n",
        "source_code_with_indent": "\n<DED>class MyIterator(Iterator):\n    <IND>\"\"\"\n    We subclass the torchtext iterator so that we can make batches based\n    on the number of sentences or the total number of tokens.\n    \"\"\"\n\n    def __init__(self,\n                 dataset: Dataset,\n                 batch_size: int,\n                 sort_key: object = None,\n                 device: object = None,\n                 batch_size_fn: object = None,\n                 train: bool = True,\n                 repeat: bool = False,\n                 shuffle: bool = None,\n                 sort: bool = None,\n                 sort_within_batch: bool = None,\n                 num_sort_batches: int = 100):\n        <IND>super(MyIterator, self).__init__(\n            dataset, batch_size,\n            sort_key=sort_key, device=device, batch_size_fn=batch_size_fn,\n            train=train, repeat=repeat, shuffle=shuffle, sort=sort,\n            sort_within_batch=sort_within_batch)\n\n        self.num_sort_batches = num_sort_batches\n        self.batches = []\n\n    <DED>def create_batches(self):\n        <IND>if self.train:\n            <IND>def pool(d, random_shuffler):\n                <IND>for p in data.batch(d, self.batch_size * self.num_sort_batches):\n                    <IND>p_batch = data.batch(\n                        sorted(p, key=self.sort_key),\n                        self.batch_size, self.batch_size_fn)\n                    for b in random_shuffler(list(p_batch)):\n                        <IND>yield b\n\n            <DED><DED><DED>self.batches = pool(self.data(), self.random_shuffler)\n\n        <DED>else:\n            <IND>self.batches = []\n            for b in data.batch(self.data(), self.batch_size,\n                                self.batch_size_fn):\n                <IND>self.batches.append(sorted(b, key=self.sort_key))\n\n# pylint: disable=global-at-module-level\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n# pylint: disable=global-at-module-level\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        # optionally shuffle and sort during training\n        # TODO transformer does not require sort_within_batch\n        data_iter = MyIterator(\n            repeat=False, sort=False, dataset=dataset,\n",
        "source_code_len": 203,
        "target_code": "        # optionally shuffle and sort during training\n        data_iter = data.BucketIterator(\n            repeat=False, sort=False, dataset=dataset,\n",
        "target_code_len": 150,
        "diff_format": "@@ -187,4 +143,3 @@\n         # optionally shuffle and sort during training\n-        # TODO transformer does not require sort_within_batch\n-        data_iter = MyIterator(\n+        data_iter = data.BucketIterator(\n             repeat=False, sort=False, dataset=dataset,\n",
        "source_code_with_indent": "        # optionally shuffle and sort during training\n        # TODO transformer does not require sort_within_batch\n        <IND>data_iter = MyIterator(\n            repeat=False, sort=False, dataset=dataset,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        # optionally shuffle and sort during training\n        <IND>data_iter = data.BucketIterator(\n            repeat=False, sort=False, dataset=dataset,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        # don't sort/shuffle for validation/inference\n        data_iter = MyIterator(\n            repeat=False, dataset=dataset,\n",
        "source_code_len": 129,
        "target_code": "        # don't sort/shuffle for validation/inference\n        data_iter = data.BucketIterator(\n            repeat=False, dataset=dataset,\n",
        "target_code_len": 138,
        "diff_format": "@@ -195,3 +150,3 @@\n         # don't sort/shuffle for validation/inference\n-        data_iter = MyIterator(\n+        data_iter = data.BucketIterator(\n             repeat=False, dataset=dataset,\n",
        "source_code_with_indent": "        # don't sort/shuffle for validation/inference\n        <IND>data_iter = MyIterator(\n            repeat=False, dataset=dataset,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        # don't sort/shuffle for validation/inference\n        <IND>data_iter = data.BucketIterator(\n            repeat=False, dataset=dataset,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "joeynmt/joeynmt",
    "commit": "fd7d032db5e88eb0efeab465460ebc0b673eb58d",
    "filename": "joeynmt/data.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/joeynmt-joeynmt/joeynmt/data.py",
    "file_hunks_size": 4,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "joeynmt/data.py:119:17 Incompatible variable type [9]: sort is declared to have type `bool` but is used as type `None`.",
    "message": " sort is declared to have type `bool` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 119,
    "warning_line": "                 sort: bool = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\nclass MyIterator(Iterator):\n    \"\"\"\n    We subclass the torchtext iterator so that we can make batches based\n    on the number of sentences or the total number of tokens.\n    \"\"\"\n\n    def __init__(self,\n                 dataset: Dataset,\n                 batch_size: int,\n                 sort_key: object = None,\n                 device: object = None,\n                 batch_size_fn: object = None,\n                 train: bool = True,\n                 repeat: bool = False,\n                 shuffle: bool = None,\n                 sort: bool = None,\n                 sort_within_batch: bool = None,\n                 num_sort_batches: int = 100):\n        super(MyIterator, self).__init__(\n            dataset, batch_size,\n            sort_key=sort_key, device=device, batch_size_fn=batch_size_fn,\n            train=train, repeat=repeat, shuffle=shuffle, sort=sort,\n            sort_within_batch=sort_within_batch)\n\n        self.num_sort_batches = num_sort_batches\n        self.batches = []\n\n    def create_batches(self):\n        if self.train:\n            def pool(d, random_shuffler):\n                for p in data.batch(d, self.batch_size * self.num_sort_batches):\n                    p_batch = data.batch(\n                        sorted(p, key=self.sort_key),\n                        self.batch_size, self.batch_size_fn)\n                    for b in random_shuffler(list(p_batch)):\n                        yield b\n\n            self.batches = pool(self.data(), self.random_shuffler)\n\n        else:\n            self.batches = []\n            for b in data.batch(self.data(), self.batch_size,\n                                self.batch_size_fn):\n                self.batches.append(sorted(b, key=self.sort_key))\n\n# pylint: disable=global-at-module-level\n",
        "source_code_len": 1755,
        "target_code": "\n# pylint: disable=global-at-module-level\n",
        "target_code_len": 42,
        "diff_format": "@@ -103,47 +103,2 @@\n \n-class MyIterator(Iterator):\n-    \"\"\"\n-    We subclass the torchtext iterator so that we can make batches based\n-    on the number of sentences or the total number of tokens.\n-    \"\"\"\n-\n-    def __init__(self,\n-                 dataset: Dataset,\n-                 batch_size: int,\n-                 sort_key: object = None,\n-                 device: object = None,\n-                 batch_size_fn: object = None,\n-                 train: bool = True,\n-                 repeat: bool = False,\n-                 shuffle: bool = None,\n-                 sort: bool = None,\n-                 sort_within_batch: bool = None,\n-                 num_sort_batches: int = 100):\n-        super(MyIterator, self).__init__(\n-            dataset, batch_size,\n-            sort_key=sort_key, device=device, batch_size_fn=batch_size_fn,\n-            train=train, repeat=repeat, shuffle=shuffle, sort=sort,\n-            sort_within_batch=sort_within_batch)\n-\n-        self.num_sort_batches = num_sort_batches\n-        self.batches = []\n-\n-    def create_batches(self):\n-        if self.train:\n-            def pool(d, random_shuffler):\n-                for p in data.batch(d, self.batch_size * self.num_sort_batches):\n-                    p_batch = data.batch(\n-                        sorted(p, key=self.sort_key),\n-                        self.batch_size, self.batch_size_fn)\n-                    for b in random_shuffler(list(p_batch)):\n-                        yield b\n-\n-            self.batches = pool(self.data(), self.random_shuffler)\n-\n-        else:\n-            self.batches = []\n-            for b in data.batch(self.data(), self.batch_size,\n-                                self.batch_size_fn):\n-                self.batches.append(sorted(b, key=self.sort_key))\n-\n # pylint: disable=global-at-module-level\n",
        "source_code_with_indent": "\n<DED>class MyIterator(Iterator):\n    <IND>\"\"\"\n    We subclass the torchtext iterator so that we can make batches based\n    on the number of sentences or the total number of tokens.\n    \"\"\"\n\n    def __init__(self,\n                 dataset: Dataset,\n                 batch_size: int,\n                 sort_key: object = None,\n                 device: object = None,\n                 batch_size_fn: object = None,\n                 train: bool = True,\n                 repeat: bool = False,\n                 shuffle: bool = None,\n                 sort: bool = None,\n                 sort_within_batch: bool = None,\n                 num_sort_batches: int = 100):\n        <IND>super(MyIterator, self).__init__(\n            dataset, batch_size,\n            sort_key=sort_key, device=device, batch_size_fn=batch_size_fn,\n            train=train, repeat=repeat, shuffle=shuffle, sort=sort,\n            sort_within_batch=sort_within_batch)\n\n        self.num_sort_batches = num_sort_batches\n        self.batches = []\n\n    <DED>def create_batches(self):\n        <IND>if self.train:\n            <IND>def pool(d, random_shuffler):\n                <IND>for p in data.batch(d, self.batch_size * self.num_sort_batches):\n                    <IND>p_batch = data.batch(\n                        sorted(p, key=self.sort_key),\n                        self.batch_size, self.batch_size_fn)\n                    for b in random_shuffler(list(p_batch)):\n                        <IND>yield b\n\n            <DED><DED><DED>self.batches = pool(self.data(), self.random_shuffler)\n\n        <DED>else:\n            <IND>self.batches = []\n            for b in data.batch(self.data(), self.batch_size,\n                                self.batch_size_fn):\n                <IND>self.batches.append(sorted(b, key=self.sort_key))\n\n# pylint: disable=global-at-module-level\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n# pylint: disable=global-at-module-level\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        # optionally shuffle and sort during training\n        # TODO transformer does not require sort_within_batch\n        data_iter = MyIterator(\n            repeat=False, sort=False, dataset=dataset,\n",
        "source_code_len": 203,
        "target_code": "        # optionally shuffle and sort during training\n        data_iter = data.BucketIterator(\n            repeat=False, sort=False, dataset=dataset,\n",
        "target_code_len": 150,
        "diff_format": "@@ -187,4 +143,3 @@\n         # optionally shuffle and sort during training\n-        # TODO transformer does not require sort_within_batch\n-        data_iter = MyIterator(\n+        data_iter = data.BucketIterator(\n             repeat=False, sort=False, dataset=dataset,\n",
        "source_code_with_indent": "        # optionally shuffle and sort during training\n        # TODO transformer does not require sort_within_batch\n        <IND>data_iter = MyIterator(\n            repeat=False, sort=False, dataset=dataset,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        # optionally shuffle and sort during training\n        <IND>data_iter = data.BucketIterator(\n            repeat=False, sort=False, dataset=dataset,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        # don't sort/shuffle for validation/inference\n        data_iter = MyIterator(\n            repeat=False, dataset=dataset,\n",
        "source_code_len": 129,
        "target_code": "        # don't sort/shuffle for validation/inference\n        data_iter = data.BucketIterator(\n            repeat=False, dataset=dataset,\n",
        "target_code_len": 138,
        "diff_format": "@@ -195,3 +150,3 @@\n         # don't sort/shuffle for validation/inference\n-        data_iter = MyIterator(\n+        data_iter = data.BucketIterator(\n             repeat=False, dataset=dataset,\n",
        "source_code_with_indent": "        # don't sort/shuffle for validation/inference\n        <IND>data_iter = MyIterator(\n            repeat=False, dataset=dataset,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        # don't sort/shuffle for validation/inference\n        <IND>data_iter = data.BucketIterator(\n            repeat=False, dataset=dataset,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  },
  {
    "project": "joeynmt/joeynmt",
    "commit": "fd7d032db5e88eb0efeab465460ebc0b673eb58d",
    "filename": "joeynmt/data.py",
    "filename_after_commit": "/home/chowyi/TypeAnnotation_Study/GitHub/joeynmt-joeynmt/joeynmt/data.py",
    "file_hunks_size": 4,
    "min_patch_found": true,
    "single_hunk": false,
    "fit_TFix": false,
    "delete_only_patch": false,
    "has_suppression_all_hunks": false,
    "full_warning_msg": "joeynmt/data.py:120:17 Incompatible variable type [9]: sort_within_batch is declared to have type `bool` but is used as type `None`.",
    "message": " sort_within_batch is declared to have type `bool` but is used as type `None`.",
    "rule_id": "Incompatible variable type [9]",
    "warning_line_no": 120,
    "warning_line": "                 sort_within_batch: bool = None,",
    "min_patch": [
      {
        "hunk_fit_TFix": false,
        "inside_window": true,
        "delete_only": true,
        "has_suppression": false,
        "source_code": "\nclass MyIterator(Iterator):\n    \"\"\"\n    We subclass the torchtext iterator so that we can make batches based\n    on the number of sentences or the total number of tokens.\n    \"\"\"\n\n    def __init__(self,\n                 dataset: Dataset,\n                 batch_size: int,\n                 sort_key: object = None,\n                 device: object = None,\n                 batch_size_fn: object = None,\n                 train: bool = True,\n                 repeat: bool = False,\n                 shuffle: bool = None,\n                 sort: bool = None,\n                 sort_within_batch: bool = None,\n                 num_sort_batches: int = 100):\n        super(MyIterator, self).__init__(\n            dataset, batch_size,\n            sort_key=sort_key, device=device, batch_size_fn=batch_size_fn,\n            train=train, repeat=repeat, shuffle=shuffle, sort=sort,\n            sort_within_batch=sort_within_batch)\n\n        self.num_sort_batches = num_sort_batches\n        self.batches = []\n\n    def create_batches(self):\n        if self.train:\n            def pool(d, random_shuffler):\n                for p in data.batch(d, self.batch_size * self.num_sort_batches):\n                    p_batch = data.batch(\n                        sorted(p, key=self.sort_key),\n                        self.batch_size, self.batch_size_fn)\n                    for b in random_shuffler(list(p_batch)):\n                        yield b\n\n            self.batches = pool(self.data(), self.random_shuffler)\n\n        else:\n            self.batches = []\n            for b in data.batch(self.data(), self.batch_size,\n                                self.batch_size_fn):\n                self.batches.append(sorted(b, key=self.sort_key))\n\n# pylint: disable=global-at-module-level\n",
        "source_code_len": 1755,
        "target_code": "\n# pylint: disable=global-at-module-level\n",
        "target_code_len": 42,
        "diff_format": "@@ -103,47 +103,2 @@\n \n-class MyIterator(Iterator):\n-    \"\"\"\n-    We subclass the torchtext iterator so that we can make batches based\n-    on the number of sentences or the total number of tokens.\n-    \"\"\"\n-\n-    def __init__(self,\n-                 dataset: Dataset,\n-                 batch_size: int,\n-                 sort_key: object = None,\n-                 device: object = None,\n-                 batch_size_fn: object = None,\n-                 train: bool = True,\n-                 repeat: bool = False,\n-                 shuffle: bool = None,\n-                 sort: bool = None,\n-                 sort_within_batch: bool = None,\n-                 num_sort_batches: int = 100):\n-        super(MyIterator, self).__init__(\n-            dataset, batch_size,\n-            sort_key=sort_key, device=device, batch_size_fn=batch_size_fn,\n-            train=train, repeat=repeat, shuffle=shuffle, sort=sort,\n-            sort_within_batch=sort_within_batch)\n-\n-        self.num_sort_batches = num_sort_batches\n-        self.batches = []\n-\n-    def create_batches(self):\n-        if self.train:\n-            def pool(d, random_shuffler):\n-                for p in data.batch(d, self.batch_size * self.num_sort_batches):\n-                    p_batch = data.batch(\n-                        sorted(p, key=self.sort_key),\n-                        self.batch_size, self.batch_size_fn)\n-                    for b in random_shuffler(list(p_batch)):\n-                        yield b\n-\n-            self.batches = pool(self.data(), self.random_shuffler)\n-\n-        else:\n-            self.batches = []\n-            for b in data.batch(self.data(), self.batch_size,\n-                                self.batch_size_fn):\n-                self.batches.append(sorted(b, key=self.sort_key))\n-\n # pylint: disable=global-at-module-level\n",
        "source_code_with_indent": "\n<DED>class MyIterator(Iterator):\n    <IND>\"\"\"\n    We subclass the torchtext iterator so that we can make batches based\n    on the number of sentences or the total number of tokens.\n    \"\"\"\n\n    def __init__(self,\n                 dataset: Dataset,\n                 batch_size: int,\n                 sort_key: object = None,\n                 device: object = None,\n                 batch_size_fn: object = None,\n                 train: bool = True,\n                 repeat: bool = False,\n                 shuffle: bool = None,\n                 sort: bool = None,\n                 sort_within_batch: bool = None,\n                 num_sort_batches: int = 100):\n        <IND>super(MyIterator, self).__init__(\n            dataset, batch_size,\n            sort_key=sort_key, device=device, batch_size_fn=batch_size_fn,\n            train=train, repeat=repeat, shuffle=shuffle, sort=sort,\n            sort_within_batch=sort_within_batch)\n\n        self.num_sort_batches = num_sort_batches\n        self.batches = []\n\n    <DED>def create_batches(self):\n        <IND>if self.train:\n            <IND>def pool(d, random_shuffler):\n                <IND>for p in data.batch(d, self.batch_size * self.num_sort_batches):\n                    <IND>p_batch = data.batch(\n                        sorted(p, key=self.sort_key),\n                        self.batch_size, self.batch_size_fn)\n                    for b in random_shuffler(list(p_batch)):\n                        <IND>yield b\n\n            <DED><DED><DED>self.batches = pool(self.data(), self.random_shuffler)\n\n        <DED>else:\n            <IND>self.batches = []\n            for b in data.batch(self.data(), self.batch_size,\n                                self.batch_size_fn):\n                <IND>self.batches.append(sorted(b, key=self.sort_key))\n\n# pylint: disable=global-at-module-level\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "\n# pylint: disable=global-at-module-level\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        # optionally shuffle and sort during training\n        # TODO transformer does not require sort_within_batch\n        data_iter = MyIterator(\n            repeat=False, sort=False, dataset=dataset,\n",
        "source_code_len": 203,
        "target_code": "        # optionally shuffle and sort during training\n        data_iter = data.BucketIterator(\n            repeat=False, sort=False, dataset=dataset,\n",
        "target_code_len": 150,
        "diff_format": "@@ -187,4 +143,3 @@\n         # optionally shuffle and sort during training\n-        # TODO transformer does not require sort_within_batch\n-        data_iter = MyIterator(\n+        data_iter = data.BucketIterator(\n             repeat=False, sort=False, dataset=dataset,\n",
        "source_code_with_indent": "        # optionally shuffle and sort during training\n        # TODO transformer does not require sort_within_batch\n        <IND>data_iter = MyIterator(\n            repeat=False, sort=False, dataset=dataset,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        # optionally shuffle and sort during training\n        <IND>data_iter = data.BucketIterator(\n            repeat=False, sort=False, dataset=dataset,\n",
        "target_code_with_indent_exact_match": true
      },
      {
        "hunk_fit_TFix": false,
        "inside_window": false,
        "delete_only": false,
        "has_suppression": false,
        "source_code": "        # don't sort/shuffle for validation/inference\n        data_iter = MyIterator(\n            repeat=False, dataset=dataset,\n",
        "source_code_len": 129,
        "target_code": "        # don't sort/shuffle for validation/inference\n        data_iter = data.BucketIterator(\n            repeat=False, dataset=dataset,\n",
        "target_code_len": 138,
        "diff_format": "@@ -195,3 +150,3 @@\n         # don't sort/shuffle for validation/inference\n-        data_iter = MyIterator(\n+        data_iter = data.BucketIterator(\n             repeat=False, dataset=dataset,\n",
        "source_code_with_indent": "        # don't sort/shuffle for validation/inference\n        <IND>data_iter = MyIterator(\n            repeat=False, dataset=dataset,\n",
        "source_code_with_indent_exact_match": true,
        "target_code_with_indent": "        # don't sort/shuffle for validation/inference\n        <IND>data_iter = data.BucketIterator(\n            repeat=False, dataset=dataset,\n",
        "target_code_with_indent_exact_match": true
      }
    ]
  }
]